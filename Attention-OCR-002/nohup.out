I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:02:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x36496e0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:04:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x364d520
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:83:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3651390
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40c, pci bus id: 0000:83:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40c, pci bus id: 0000:84:00.0)
2017-06-01 17:33:01,633 root  INFO     loading data
2017-06-01 17:33:01,673 root  INFO     phase: train
2017-06-01 17:33:01,673 root  INFO     model_dir: model_01_16
2017-06-01 17:33:01,673 root  INFO     load_model: False
2017-06-01 17:33:01,673 root  INFO     output_dir: results
2017-06-01 17:33:01,673 root  INFO     steps_per_checkpoint: 2000
2017-06-01 17:33:01,673 root  INFO     batch_size: 64
2017-06-01 17:33:01,673 root  INFO     num_epoch: 3
2017-06-01 17:33:01,673 root  INFO     learning_rate: 1
2017-06-01 17:33:01,673 root  INFO     reg_val: 0
2017-06-01 17:33:01,674 root  INFO     max_gradient_norm: 5.000000
2017-06-01 17:33:01,674 root  INFO     clip_gradients: True
2017-06-01 17:33:01,674 root  INFO     valid_target_length inf
2017-06-01 17:33:01,674 root  INFO     target_vocab_size: 39
2017-06-01 17:33:01,674 root  INFO     target_embedding_size: 10.000000
2017-06-01 17:33:01,674 root  INFO     attn_num_hidden: 256
2017-06-01 17:33:01,674 root  INFO     attn_num_layers: 2
2017-06-01 17:33:01,674 root  INFO     visualize: True
2017-06-01 17:33:01,674 root  INFO     buckets
2017-06-01 17:33:01,674 root  INFO     [(16, 11), (27, 17), (35, 19), (64, 22), (80, 32)]
2017-06-01 17:33:01,674 root  INFO     ues GRU in the decoder.
2017-06-01 17:34:10,101 root  INFO     Created model with fresh parameters.
Train: :   0%|          | 0/156 [00:00<?, ?it/s]2017-06-01 17:35:46,338 root  INFO     Generating first batch)
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.40GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.34GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=1325 evicted_count=1000 eviction_rate=0.754717 and unsatisfied allocation rate=0.913892
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.40GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.34GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2017-06-01 17:35:55,981 root  INFO     step 0.000000 - time: 6.792066, loss: 3.691753, perplexity: 40.115123, precision: 0.000000, batch_len: 113.000000
Train, loss=3.69175339:   1%|          | 1/156 [00:09<24:55,  9.65s/it]2017-06-01 17:36:00,253 root  INFO     step 1.000000 - time: 4.217168, loss: 3.377113, perplexity: 29.286089, precision: 0.000000, batch_len: 108.000000
Train, loss=3.37711263:   1%|1         | 2/156 [00:13<20:37,  8.03s/it]2017-06-01 17:36:04,357 root  INFO     step 2.000000 - time: 3.955750, loss: 3.260083, perplexity: 26.051692, precision: 0.000000, batch_len: 96.000000
Train, loss=3.26008272:   2%|1         | 3/156 [00:18<17:28,  6.86s/it]2017-06-01 17:36:08,392 root  INFO     step 3.000000 - time: 4.005356, loss: 3.139882, perplexity: 23.101132, precision: 0.000000, batch_len: 93.000000
Train, loss=3.13988161:   3%|2         | 4/156 [00:22<15:13,  6.01s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1014 evicted_count=1000 eviction_rate=0.986193 and unsatisfied allocation rate=0
2017-06-01 17:36:12,653 root  INFO     step 4.000000 - time: 4.129705, loss: 3.066021, perplexity: 21.456357, precision: 0.000000, batch_len: 105.000000
Train, loss=3.06602097:   3%|3         | 5/156 [00:26<13:48,  5.48s/it]2017-06-01 17:36:14,029 root  INFO     step 5.000000 - time: 1.353556, loss: 2.981195, perplexity: 19.711362, precision: 0.000000, batch_len: 104.000000
Train, loss=2.98119521:   4%|3         | 6/156 [00:27<10:37,  4.25s/it]2017-06-01 17:36:15,990 root  INFO     step 6.000000 - time: 1.738375, loss: 2.957513, perplexity: 19.250035, precision: 0.000000, batch_len: 114.000000
Train, loss=2.95751286:   4%|4         | 7/156 [00:29<08:51,  3.56s/it]2017-06-01 17:36:20,025 root  INFO     step 7.000000 - time: 4.013068, loss: 2.992911, perplexity: 19.943656, precision: 0.000000, batch_len: 101.000000
Train, loss=2.99291110:   5%|5         | 8/156 [00:33<09:08,  3.71s/it]2017-06-01 17:36:21,617 root  INFO     step 8.000000 - time: 1.570161, loss: 2.916950, perplexity: 18.484831, precision: 0.000000, batch_len: 103.000000
Train, loss=2.91695046:   6%|5         | 9/156 [00:35<07:31,  3.07s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=3479 evicted_count=3000 eviction_rate=0.862317 and unsatisfied allocation rate=0.824723
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 233 to 256
2017-06-01 17:36:23,256 root  INFO     step 9.000000 - time: 1.541744, loss: 2.887466, perplexity: 17.947780, precision: 0.000000, batch_len: 110.000000
Train, loss=2.88746643:   6%|6         | 10/156 [00:36<06:25,  2.64s/it]2017-06-01 17:36:24,782 root  INFO     step 10.000000 - time: 1.505117, loss: 2.966765, perplexity: 19.428959, precision: 0.000000, batch_len: 92.000000
Train, loss=2.96676469:   7%|7         | 11/156 [00:38<05:34,  2.31s/it]2017-06-01 17:36:30,663 root  INFO     step 11.000000 - time: 5.791338, loss: 2.943310, perplexity: 18.978562, precision: 0.000000, batch_len: 124.000000
Train, loss=2.94331002:   8%|7         | 12/156 [00:44<08:06,  3.38s/it]2017-06-01 17:36:34,897 root  INFO     step 12.000000 - time: 4.097950, loss: 2.920366, perplexity: 18.548071, precision: 0.000000, batch_len: 128.000000
Train, loss=2.92036581:   8%|8         | 13/156 [00:48<08:39,  3.64s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1033 evicted_count=1000 eviction_rate=0.968054 and unsatisfied allocation rate=0
2017-06-01 17:36:41,949 root  INFO     step 13.000000 - time: 6.982774, loss: 2.929096, perplexity: 18.710704, precision: 0.000000, batch_len: 90.000000
Train, loss=2.92909575:   9%|8         | 14/156 [00:55<11:01,  4.66s/it]2017-06-01 17:36:45,854 root  INFO     step 14.000000 - time: 3.878581, loss: 2.892727, perplexity: 18.042438, precision: 0.000000, batch_len: 79.000000
Train, loss=2.89272666:  10%|9         | 15/156 [00:59<10:25,  4.43s/it]2017-06-01 17:36:47,619 root  INFO     step 15.000000 - time: 1.635762, loss: 2.931895, perplexity: 18.763158, precision: 0.000000, batch_len: 107.000000
Train, loss=2.93189526:  10%|#         | 16/156 [01:01<08:28,  3.63s/it]2017-06-01 17:36:49,074 root  INFO     step 16.000000 - time: 1.368585, loss: 3.114968, perplexity: 22.532711, precision: 0.000000, batch_len: 111.000000
Train, loss=3.11496806:  11%|#         | 17/156 [01:02<06:54,  2.98s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1049 evicted_count=1000 eviction_rate=0.953289 and unsatisfied allocation rate=0
2017-06-01 17:36:50,337 root  INFO     step 17.000000 - time: 1.169138, loss: 2.893752, perplexity: 18.060940, precision: 0.000000, batch_len: 97.000000
Train, loss=2.89375162:  12%|#1        | 18/156 [01:03<05:40,  2.46s/it]2017-06-01 17:36:51,808 root  INFO     step 18.000000 - time: 1.463093, loss: 2.864706, perplexity: 17.543891, precision: 0.000000, batch_len: 102.000000
Train, loss=2.86470580:  12%|#2        | 19/156 [01:05<04:56,  2.17s/it]2017-06-01 17:36:53,370 root  INFO     step 19.000000 - time: 1.443802, loss: 2.878270, perplexity: 17.783475, precision: 0.000000, batch_len: 100.000000
Train, loss=2.87826967:  13%|#2        | 20/156 [01:07<04:29,  1.99s/it]2017-06-01 17:36:57,092 root  INFO     step 20.000000 - time: 3.701975, loss: 2.829882, perplexity: 16.943460, precision: 0.000000, batch_len: 81.000000
Train, loss=2.82988191:  13%|#3        | 21/156 [01:10<05:38,  2.51s/it]2017-06-01 17:36:58,806 root  INFO     step 21.000000 - time: 1.606166, loss: 2.808141, perplexity: 16.579061, precision: 0.000000, batch_len: 112.000000
Train, loss=2.80814052:  14%|#4        | 22/156 [01:12<05:03,  2.27s/it]2017-06-01 17:37:00,213 root  INFO     step 22.000000 - time: 1.387433, loss: 2.805710, perplexity: 16.538820, precision: 0.000000, batch_len: 80.000000
Train, loss=2.80571032:  15%|#4        | 23/156 [01:13<04:27,  2.01s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1087 evicted_count=1000 eviction_rate=0.919963 and unsatisfied allocation rate=0
2017-06-01 17:37:04,187 root  INFO     step 23.000000 - time: 3.965950, loss: 2.876696, perplexity: 17.755514, precision: 0.000000, batch_len: 116.000000
Train, loss=2.87669611:  15%|#5        | 24/156 [01:17<05:43,  2.60s/it]2017-06-01 17:37:05,828 root  INFO     step 24.000000 - time: 1.607144, loss: 3.028976, perplexity: 20.676054, precision: 0.000000, batch_len: 88.000000
Train, loss=3.02897620:  16%|#6        | 25/156 [01:19<05:02,  2.31s/it]2017-06-01 17:37:10,101 root  INFO     step 25.000000 - time: 4.200620, loss: 2.867384, perplexity: 17.590943, precision: 0.000000, batch_len: 120.000000
Train, loss=2.86738420:  17%|#6        | 26/156 [01:23<06:17,  2.90s/it]2017-06-01 17:37:11,333 root  INFO     step 26.000000 - time: 1.185028, loss: 2.845594, perplexity: 17.211786, precision: 0.000000, batch_len: 109.000000
Train, loss=2.84559441:  17%|#7        | 27/156 [01:24<05:09,  2.40s/it]2017-06-01 17:37:12,869 root  INFO     step 27.000000 - time: 1.477050, loss: 2.846608, perplexity: 17.229248, precision: 0.000000, batch_len: 106.000000
Train, loss=2.84660840:  18%|#7        | 28/156 [01:26<04:34,  2.14s/it]2017-06-01 17:37:16,971 root  INFO     step 28.000000 - time: 3.988513, loss: 2.854543, perplexity: 17.366507, precision: 0.000000, batch_len: 86.000000
Train, loss=2.85454345:  19%|#8        | 29/156 [01:30<05:46,  2.73s/it]2017-06-01 17:37:18,475 root  INFO     step 29.000000 - time: 1.370705, loss: 2.901554, perplexity: 18.202403, precision: 0.000000, batch_len: 85.000000
Train, loss=2.90155363:  19%|#9        | 30/156 [01:32<04:57,  2.36s/it]2017-06-01 17:37:20,205 root  INFO     step 30.000000 - time: 1.710175, loss: 2.867447, perplexity: 17.592042, precision: 0.000000, batch_len: 125.000000
Train, loss=2.86744666:  20%|#9        | 31/156 [01:33<04:31,  2.17s/it]2017-06-01 17:37:21,824 root  INFO     step 31.000000 - time: 1.583558, loss: 2.875601, perplexity: 17.736085, precision: 0.000000, batch_len: 83.000000
Train, loss=2.87560129:  21%|##        | 32/156 [01:35<04:08,  2.01s/it]2017-06-01 17:37:23,195 root  INFO     step 32.000000 - time: 1.320429, loss: 2.816993, perplexity: 16.726482, precision: 0.000000, batch_len: 115.000000
Train, loss=2.81699324:  21%|##1       | 33/156 [01:36<03:43,  1.82s/it]2017-06-01 17:37:24,743 root  INFO     step 33.000000 - time: 1.431156, loss: 2.839323, perplexity: 17.104175, precision: 0.000000, batch_len: 91.000000
Train, loss=2.83932257:  22%|##1       | 34/156 [01:38<03:31,  1.74s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 6266 get requests, put_count=5671 evicted_count=1000 eviction_rate=0.176336 and unsatisfied allocation rate=0.284232
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 2049 to 2253
2017-06-01 17:37:26,750 root  INFO     step 34.000000 - time: 1.961259, loss: 2.870194, perplexity: 17.640448, precision: 0.000000, batch_len: 123.000000
Train, loss=2.87019444:  22%|##2       | 35/156 [01:40<03:39,  1.82s/it]2017-06-01 17:37:27,969 root  INFO     step 35.000000 - time: 1.214337, loss: 2.910000, perplexity: 18.356805, precision: 0.000000, batch_len: 89.000000
Train, loss=2.91000032:  23%|##3       | 36/156 [01:41<03:16,  1.64s/it]2017-06-01 17:37:29,934 root  INFO     step 36.000000 - time: 1.915962, loss: 2.823100, perplexity: 16.828945, precision: 0.000000, batch_len: 94.000000
Train, loss=2.82310033:  24%|##3       | 37/156 [01:43<03:26,  1.74s/it]2017-06-01 17:37:31,877 root  INFO     step 37.000000 - time: 1.912200, loss: 2.857436, perplexity: 17.416808, precision: 0.000000, batch_len: 118.000000
Train, loss=2.85743570:  24%|##4       | 38/156 [01:45<03:32,  1.80s/it]2017-06-01 17:37:33,603 root  INFO     step 38.000000 - time: 1.665432, loss: 2.858447, perplexity: 17.434432, precision: 0.000000, batch_len: 121.000000
Train, loss=2.85844707:  25%|##5       | 39/156 [01:47<03:27,  1.78s/it]2017-06-01 17:37:35,480 root  INFO     step 39.000000 - time: 1.763214, loss: 2.839554, perplexity: 17.108131, precision: 0.000000, batch_len: 117.000000
Train, loss=2.83955383:  26%|##5       | 40/156 [01:49<03:29,  1.81s/it]2017-06-01 17:37:37,053 root  INFO     step 40.000000 - time: 1.337562, loss: 2.867027, perplexity: 17.584658, precision: 0.000000, batch_len: 87.000000
Train, loss=2.86702681:  26%|##6       | 41/156 [01:50<03:19,  1.74s/it]2017-06-01 17:37:41,255 root  INFO     step 41.000000 - time: 4.195442, loss: 2.812326, perplexity: 16.648597, precision: 0.000000, batch_len: 136.000000
Train, loss=2.81232595:  27%|##6       | 42/156 [01:54<04:42,  2.48s/it]2017-06-01 17:37:43,166 root  INFO     step 42.000000 - time: 1.842235, loss: 2.831671, perplexity: 16.973796, precision: 0.000000, batch_len: 129.000000
Train, loss=2.83167076:  28%|##7       | 43/156 [01:56<04:20,  2.31s/it]2017-06-01 17:37:44,708 root  INFO     step 43.000000 - time: 1.524591, loss: 2.860004, perplexity: 17.461604, precision: 0.000000, batch_len: 137.000000
Train, loss=2.86000443:  28%|##8       | 44/156 [01:58<03:52,  2.08s/it]2017-06-01 17:37:48,933 root  INFO     step 44.000000 - time: 4.146367, loss: 2.830736, perplexity: 16.957932, precision: 0.000000, batch_len: 135.000000
Train, loss=2.83073568:  29%|##8       | 45/156 [02:02<05:02,  2.72s/it]2017-06-01 17:37:50,770 root  INFO     step 45.000000 - time: 1.707154, loss: 2.841734, perplexity: 17.145469, precision: 0.000000, batch_len: 99.000000
Train, loss=2.84173393:  29%|##9       | 46/156 [02:04<04:30,  2.46s/it]2017-06-01 17:37:52,108 root  INFO     step 46.000000 - time: 1.323906, loss: 2.796211, perplexity: 16.382452, precision: 0.000000, batch_len: 98.000000
Train, loss=2.79621077:  30%|###       | 47/156 [02:05<03:51,  2.12s/it]2017-06-01 17:37:53,939 root  INFO     step 47.000000 - time: 1.818827, loss: 2.876635, perplexity: 17.754434, precision: 0.000000, batch_len: 126.000000
Train, loss=2.87663531:  31%|###       | 48/156 [02:07<03:39,  2.03s/it]2017-06-01 17:37:55,561 root  INFO     step 48.000000 - time: 1.598238, loss: 2.837058, perplexity: 17.065482, precision: 0.000000, batch_len: 119.000000
Train, loss=2.83705783:  31%|###1      | 49/156 [02:09<03:24,  1.91s/it]2017-06-01 17:37:57,065 root  INFO     step 49.000000 - time: 1.475602, loss: 2.786010, perplexity: 16.216192, precision: 0.000000, batch_len: 78.000000
Train, loss=2.78601027:  32%|###2      | 50/156 [02:10<03:09,  1.79s/it]2017-06-01 17:37:58,858 root  INFO     step 50.000000 - time: 1.747429, loss: 2.821868, perplexity: 16.808222, precision: 0.000000, batch_len: 133.000000
Train, loss=2.82186818:  33%|###2      | 51/156 [02:12<03:07,  1.79s/it]2017-06-01 17:38:03,258 root  INFO     step 51.000000 - time: 4.260416, loss: 2.853369, perplexity: 17.346127, precision: 0.000000, batch_len: 144.000000
Train, loss=2.85336924:  33%|###3      | 52/156 [02:16<04:27,  2.57s/it]2017-06-01 17:38:05,332 root  INFO     step 52.000000 - time: 1.926847, loss: 2.920313, perplexity: 18.547090, precision: 0.000000, batch_len: 138.000000
Train, loss=2.92031288:  34%|###3      | 53/156 [02:18<04:09,  2.42s/it]2017-06-01 17:38:06,619 root  INFO     step 53.000000 - time: 1.266870, loss: 2.882529, perplexity: 17.859391, precision: 0.000000, batch_len: 84.000000
Train, loss=2.88252950:  35%|###4      | 54/156 [02:20<03:32,  2.08s/it]2017-06-01 17:38:07,918 root  INFO     step 54.000000 - time: 1.015106, loss: 2.845722, perplexity: 17.213986, precision: 0.000000, batch_len: 96.000000
Train, loss=2.84572220:  35%|###5      | 55/156 [02:21<03:06,  1.85s/it]2017-06-01 17:38:09,098 root  INFO     step 55.000000 - time: 1.152848, loss: 2.796446, perplexity: 16.386304, precision: 0.000000, batch_len: 82.000000
Train, loss=2.79644585:  36%|###5      | 56/156 [02:22<02:44,  1.65s/it]2017-06-01 17:38:11,216 root  INFO     step 56.000000 - time: 1.737191, loss: 2.940160, perplexity: 18.918869, precision: 0.000000, batch_len: 122.000000
Train, loss=2.94015980:  37%|###6      | 57/156 [02:24<02:57,  1.79s/it]2017-06-01 17:38:12,728 root  INFO     step 57.000000 - time: 1.492972, loss: 2.842732, perplexity: 17.162585, precision: 0.000000, batch_len: 132.000000
Train, loss=2.84273171:  37%|###7      | 58/156 [02:26<02:47,  1.71s/it]2017-06-01 17:38:14,558 root  INFO     step 58.000000 - time: 1.800637, loss: 2.803869, perplexity: 16.508398, precision: 0.000000, batch_len: 130.000000
Train, loss=2.80386925:  38%|###7      | 59/156 [02:28<02:49,  1.74s/it]2017-06-01 17:38:19,752 root  INFO     step 59.000000 - time: 5.033986, loss: 2.898709, perplexity: 18.150699, precision: 0.000000, batch_len: 152.000000
Train, loss=2.89870906:  38%|###8      | 60/156 [02:33<04:26,  2.78s/it]2017-06-01 17:38:23,608 root  INFO     step 60.000000 - time: 3.783348, loss: 3.044034, perplexity: 20.989735, precision: 0.000000, batch_len: 72.000000
Train, loss=3.04403353:  39%|###9      | 61/156 [02:37<04:54,  3.10s/it]2017-06-01 17:38:28,192 root  INFO     step 61.000000 - time: 4.445617, loss: 2.828847, perplexity: 16.925933, precision: 0.000000, batch_len: 141.000000
Train, loss=2.82884693:  40%|###9      | 62/156 [02:41<05:33,  3.55s/it]2017-06-01 17:38:29,811 root  INFO     step 62.000000 - time: 1.547700, loss: 2.785683, perplexity: 16.210881, precision: 0.000000, batch_len: 134.000000
Train, loss=2.78568268:  40%|####      | 63/156 [02:43<04:36,  2.97s/it]2017-06-01 17:38:34,935 root  INFO     step 63.000000 - time: 5.024261, loss: 2.878609, perplexity: 17.789514, precision: 0.000000, batch_len: 150.000000
Train, loss=2.87860918:  41%|####1     | 64/156 [02:48<05:32,  3.61s/it]2017-06-01 17:38:36,380 root  INFO     step 64.000000 - time: 1.427946, loss: 2.913680, perplexity: 18.424469, precision: 0.000000, batch_len: 76.000000
Train, loss=2.91367960:  42%|####1     | 65/156 [02:50<04:29,  2.96s/it]2017-06-01 17:38:37,850 root  INFO     step 65.000000 - time: 1.451095, loss: 2.799235, perplexity: 16.432077, precision: 0.000000, batch_len: 74.000000
Train, loss=2.79923534:  42%|####2     | 66/156 [02:51<03:46,  2.52s/it]2017-06-01 17:38:41,580 root  INFO     step 66.000000 - time: 3.724985, loss: 2.759382, perplexity: 15.790078, precision: 0.000000, batch_len: 71.000000
Train, loss=2.75938177:  43%|####2     | 67/156 [02:55<04:16,  2.88s/it]2017-06-01 17:38:42,901 root  INFO     step 67.000000 - time: 1.143090, loss: 2.782980, perplexity: 16.167123, precision: 0.000000, batch_len: 77.000000
Train, loss=2.78297973:  44%|####3     | 68/156 [02:56<03:32,  2.41s/it]2017-06-01 17:38:44,353 root  INFO     step 68.000000 - time: 1.436183, loss: 2.823592, perplexity: 16.837225, precision: 0.000000, batch_len: 131.000000
Train, loss=2.82359219:  44%|####4     | 69/156 [02:58<03:04,  2.12s/it]2017-06-01 17:38:45,915 root  INFO     step 69.000000 - time: 1.498139, loss: 2.808065, perplexity: 16.577812, precision: 0.000000, batch_len: 139.000000
Train, loss=2.80806518:  45%|####4     | 70/156 [02:59<02:48,  1.96s/it]2017-06-01 17:38:48,209 root  INFO     step 70.000000 - time: 2.144656, loss: 2.837219, perplexity: 17.068233, precision: 0.000000, batch_len: 142.000000
Train, loss=2.83721900:  46%|####5     | 71/156 [03:01<02:54,  2.06s/it]2017-06-01 17:38:48,358 root  INFO     Generating first batch)
2017-06-01 17:38:52,552 root  INFO     step 71.000000 - time: 1.224646, loss: 2.792272, perplexity: 16.318046, precision: 0.000000, batch_len: 96.000000
Train, loss=2.79227161:  46%|####6     | 72/156 [03:06<03:50,  2.74s/it]2017-06-01 17:38:53,714 root  INFO     step 72.000000 - time: 1.062861, loss: 2.735934, perplexity: 15.424151, precision: 0.000000, batch_len: 93.000000
Train, loss=2.73593450:  47%|####6     | 73/156 [03:07<03:08,  2.27s/it]2017-06-01 17:38:54,901 root  INFO     step 73.000000 - time: 1.021689, loss: 2.788254, perplexity: 16.252614, precision: 0.000000, batch_len: 101.000000
Train, loss=2.78825378:  47%|####7     | 74/156 [03:08<02:39,  1.94s/it]2017-06-01 17:38:56,227 root  INFO     step 74.000000 - time: 1.072898, loss: 2.752662, perplexity: 15.684335, precision: 0.000000, batch_len: 113.000000
Train, loss=2.75266242:  48%|####8     | 75/156 [03:09<02:22,  1.76s/it]2017-06-01 17:38:57,539 root  INFO     step 75.000000 - time: 1.262562, loss: 2.852911, perplexity: 17.338172, precision: 0.000000, batch_len: 111.000000
Train, loss=2.85291052:  49%|####8     | 76/156 [03:11<02:09,  1.62s/it]2017-06-01 17:38:58,881 root  INFO     step 76.000000 - time: 1.338115, loss: 2.833154, perplexity: 16.998987, precision: 0.000000, batch_len: 92.000000
Train, loss=2.83315372:  49%|####9     | 77/156 [03:12<02:01,  1.54s/it]2017-06-01 17:39:00,077 root  INFO     step 77.000000 - time: 1.122796, loss: 2.749228, perplexity: 15.630553, precision: 0.000000, batch_len: 112.000000
Train, loss=2.74922752:  50%|#####     | 78/156 [03:13<01:52,  1.44s/it]2017-06-01 17:39:01,183 root  INFO     step 78.000000 - time: 1.096657, loss: 2.793599, perplexity: 16.339715, precision: 0.000000, batch_len: 105.000000
Train, loss=2.79359865:  51%|#####     | 79/156 [03:14<01:42,  1.34s/it]2017-06-01 17:39:02,551 root  INFO     step 79.000000 - time: 1.358926, loss: 2.774185, perplexity: 16.025560, precision: 0.000000, batch_len: 120.000000
Train, loss=2.77418494:  51%|#####1    | 80/156 [03:16<01:42,  1.35s/it]2017-06-01 17:39:03,674 root  INFO     step 80.000000 - time: 1.015778, loss: 2.845986, perplexity: 17.218534, precision: 0.000000, batch_len: 104.000000
Train, loss=2.84598637:  52%|#####1    | 81/156 [03:17<01:35,  1.28s/it]2017-06-01 17:39:05,744 root  INFO     step 81.000000 - time: 1.932812, loss: 2.875725, perplexity: 17.738276, precision: 0.000000, batch_len: 90.000000
Train, loss=2.87572479:  53%|#####2    | 82/156 [03:19<01:52,  1.52s/it]2017-06-01 17:39:06,794 root  INFO     step 82.000000 - time: 1.011408, loss: 2.813859, perplexity: 16.674147, precision: 0.000000, batch_len: 100.000000
Train, loss=2.81385946:  53%|#####3    | 83/156 [03:20<01:40,  1.38s/it]2017-06-01 17:39:08,220 root  INFO     step 83.000000 - time: 1.376511, loss: 2.906365, perplexity: 18.290200, precision: 0.000000, batch_len: 128.000000
Train, loss=2.90636539:  54%|#####3    | 84/156 [03:21<01:40,  1.39s/it]2017-06-01 17:39:09,594 root  INFO     step 84.000000 - time: 1.238238, loss: 2.817802, perplexity: 16.740019, precision: 0.000000, batch_len: 117.000000
Train, loss=2.81780219:  54%|#####4    | 85/156 [03:23<01:38,  1.39s/it]2017-06-01 17:39:11,012 root  INFO     step 85.000000 - time: 1.408965, loss: 2.784439, perplexity: 16.190738, precision: 0.000000, batch_len: 124.000000
Train, loss=2.78443933:  55%|#####5    | 86/156 [03:24<01:37,  1.40s/it]2017-06-01 17:39:12,375 root  INFO     step 86.000000 - time: 1.251856, loss: 2.819327, perplexity: 16.765558, precision: 0.000000, batch_len: 110.000000
Train, loss=2.81932664:  56%|#####5    | 87/156 [03:26<01:35,  1.39s/it]2017-06-01 17:39:13,597 root  INFO     step 87.000000 - time: 1.093198, loss: 2.776147, perplexity: 16.057040, precision: 0.000000, batch_len: 108.000000
Train, loss=2.77614737:  56%|#####6    | 88/156 [03:27<01:30,  1.34s/it]2017-06-01 17:39:14,836 root  INFO     step 88.000000 - time: 1.052287, loss: 2.769936, perplexity: 15.957614, precision: 0.000000, batch_len: 97.000000
Train, loss=2.76993608:  57%|#####7    | 89/156 [03:28<01:27,  1.31s/it]2017-06-01 17:39:15,866 root  INFO     step 89.000000 - time: 0.894437, loss: 2.823203, perplexity: 16.830667, precision: 0.000000, batch_len: 79.000000
Train, loss=2.82320261:  58%|#####7    | 90/156 [03:29<01:20,  1.22s/it]2017-06-01 17:39:16,790 root  INFO     step 90.000000 - time: 0.910853, loss: 2.849216, perplexity: 17.274233, precision: 0.000000, batch_len: 94.000000
Train, loss=2.84921598:  58%|#####8    | 91/156 [03:30<01:13,  1.13s/it]2017-06-01 17:39:18,347 root  INFO     step 91.000000 - time: 1.530682, loss: 2.815709, perplexity: 16.705017, precision: 0.000000, batch_len: 121.000000
Train, loss=2.81570911:  59%|#####8    | 92/156 [03:32<01:20,  1.26s/it]2017-06-01 17:39:19,583 root  INFO     step 92.000000 - time: 1.158727, loss: 2.793011, perplexity: 16.330119, precision: 0.000000, batch_len: 107.000000
Train, loss=2.79301119:  60%|#####9    | 93/156 [03:33<01:18,  1.25s/it]2017-06-01 17:39:20,652 root  INFO     step 93.000000 - time: 0.973301, loss: 2.722088, perplexity: 15.212057, precision: 0.000000, batch_len: 89.000000
Train, loss=2.72208834:  60%|######    | 94/156 [03:34<01:14,  1.20s/it]2017-06-01 17:39:21,752 root  INFO     step 94.000000 - time: 1.076150, loss: 2.777215, perplexity: 16.074200, precision: 0.000000, batch_len: 99.000000
Train, loss=2.77721548:  61%|######    | 95/156 [03:35<01:11,  1.17s/it]2017-06-01 17:39:22,769 root  INFO     step 95.000000 - time: 0.980381, loss: 2.817412, perplexity: 16.733495, precision: 0.000000, batch_len: 109.000000
Train, loss=2.81741238:  62%|######1   | 96/156 [03:36<01:07,  1.12s/it]2017-06-01 17:39:23,781 root  INFO     step 96.000000 - time: 0.952864, loss: 2.736278, perplexity: 15.429451, precision: 0.000000, batch_len: 86.000000
Train, loss=2.73627806:  62%|######2   | 97/156 [03:37<01:04,  1.09s/it]2017-06-01 17:39:25,034 root  INFO     step 97.000000 - time: 1.221553, loss: 2.882966, perplexity: 17.867185, precision: 0.000000, batch_len: 114.000000
Train, loss=2.88296580:  63%|######2   | 98/156 [03:38<01:06,  1.14s/it]2017-06-01 17:39:26,379 root  INFO     step 98.000000 - time: 1.280612, loss: 2.836824, perplexity: 17.061499, precision: 0.000000, batch_len: 102.000000
Train, loss=2.83682442:  63%|######3   | 99/156 [03:40<01:08,  1.20s/it]2017-06-01 17:39:27,943 root  INFO     step 99.000000 - time: 1.509665, loss: 2.850897, perplexity: 17.303297, precision: 0.000000, batch_len: 129.000000
Train, loss=2.85089707:  64%|######4   | 100/156 [03:41<01:13,  1.31s/it]2017-06-01 17:39:29,115 root  INFO     step 100.000000 - time: 1.059443, loss: 2.808280, perplexity: 16.581374, precision: 0.000000, batch_len: 81.000000
Train, loss=2.80827999:  65%|######4   | 101/156 [03:42<01:09,  1.27s/it]2017-06-01 17:39:30,102 root  INFO     step 101.000000 - time: 0.964975, loss: 2.818348, perplexity: 16.749165, precision: 0.000000, batch_len: 88.000000
Train, loss=2.81834841:  65%|######5   | 102/156 [03:43<01:03,  1.18s/it]2017-06-01 17:39:31,336 root  INFO     step 102.000000 - time: 1.207731, loss: 2.788959, perplexity: 16.264077, precision: 0.000000, batch_len: 80.000000
Train, loss=2.78895879:  66%|######6   | 103/156 [03:44<01:03,  1.20s/it]2017-06-01 17:39:32,463 root  INFO     step 103.000000 - time: 1.093156, loss: 2.808478, perplexity: 16.584655, precision: 0.000000, batch_len: 103.000000
Train, loss=2.80847788:  67%|######6   | 104/156 [03:46<01:01,  1.18s/it]2017-06-01 17:39:33,653 root  INFO     step 104.000000 - time: 1.156078, loss: 2.760639, perplexity: 15.809938, precision: 0.000000, batch_len: 83.000000
Train, loss=2.76063871:  67%|######7   | 105/156 [03:47<01:00,  1.18s/it]2017-06-01 17:39:34,875 root  INFO     step 105.000000 - time: 1.190376, loss: 2.745070, perplexity: 15.565696, precision: 0.000000, batch_len: 91.000000
Train, loss=2.74506950:  68%|######7   | 106/156 [03:48<00:59,  1.19s/it]2017-06-01 17:39:36,010 root  INFO     step 106.000000 - time: 1.026105, loss: 2.810486, perplexity: 16.617990, precision: 0.000000, batch_len: 85.000000
Train, loss=2.81048584:  69%|######8   | 107/156 [03:49<00:57,  1.18s/it]2017-06-01 17:39:37,178 root  INFO     step 107.000000 - time: 1.077301, loss: 2.698907, perplexity: 14.863483, precision: 0.000000, batch_len: 106.000000
Train, loss=2.69890738:  69%|######9   | 108/156 [03:50<00:56,  1.17s/it]2017-06-01 17:39:38,416 root  INFO     step 108.000000 - time: 1.199985, loss: 2.856339, perplexity: 17.397708, precision: 0.000000, batch_len: 119.000000
Train, loss=2.85633850:  70%|######9   | 109/156 [03:52<00:56,  1.19s/it]2017-06-01 17:39:39,691 root  INFO     step 109.000000 - time: 1.247331, loss: 2.902534, perplexity: 18.220249, precision: 0.000000, batch_len: 115.000000
Train, loss=2.90253353:  71%|#######   | 110/156 [03:53<00:56,  1.22s/it]2017-06-01 17:39:41,009 root  INFO     step 110.000000 - time: 1.241258, loss: 2.735813, perplexity: 15.422271, precision: 0.000000, batch_len: 98.000000
Train, loss=2.73581266:  71%|#######1  | 111/156 [03:54<00:56,  1.25s/it]2017-06-01 17:39:42,390 root  INFO     step 111.000000 - time: 1.328539, loss: 2.841051, perplexity: 17.133757, precision: 0.000000, batch_len: 137.000000
Train, loss=2.84105062:  72%|#######1  | 112/156 [03:56<00:56,  1.29s/it]2017-06-01 17:39:43,828 root  INFO     step 112.000000 - time: 1.317411, loss: 2.797983, perplexity: 16.411514, precision: 0.000000, batch_len: 125.000000
Train, loss=2.79798317:  72%|#######2  | 113/156 [03:57<00:57,  1.33s/it]2017-06-01 17:39:44,822 root  INFO     step 113.000000 - time: 0.900087, loss: 2.791466, perplexity: 16.304897, precision: 0.000000, batch_len: 84.000000
Train, loss=2.79146552:  73%|#######3  | 114/156 [03:58<00:51,  1.23s/it]2017-06-01 17:39:45,769 root  INFO     step 114.000000 - time: 0.938402, loss: 2.738737, perplexity: 15.467439, precision: 0.000000, batch_len: 82.000000
Train, loss=2.73873711:  74%|#######3  | 115/156 [03:59<00:46,  1.15s/it]2017-06-01 17:39:47,012 root  INFO     step 115.000000 - time: 1.152295, loss: 2.786327, perplexity: 16.221335, precision: 0.000000, batch_len: 87.000000
Train, loss=2.78632736:  74%|#######4  | 116/156 [04:00<00:46,  1.17s/it]2017-06-01 17:39:48,521 root  INFO     step 116.000000 - time: 1.434224, loss: 2.794577, perplexity: 16.355715, precision: 0.000000, batch_len: 130.000000
Train, loss=2.79457736:  75%|#######5  | 117/156 [04:02<00:49,  1.28s/it]2017-06-01 17:39:49,911 root  INFO     step 117.000000 - time: 1.341154, loss: 2.721438, perplexity: 15.202173, precision: 0.000000, batch_len: 116.000000
Train, loss=2.72143841:  76%|#######5  | 118/156 [04:03<00:49,  1.31s/it]2017-06-01 17:39:51,371 root  INFO     step 118.000000 - time: 1.373025, loss: 2.770528, perplexity: 15.967056, precision: 0.000000, batch_len: 135.000000
Train, loss=2.77052760:  76%|#######6  | 119/156 [04:05<00:50,  1.35s/it]2017-06-01 17:39:52,751 root  INFO     step 119.000000 - time: 1.368084, loss: 2.726063, perplexity: 15.272637, precision: 0.000000, batch_len: 136.000000
Train, loss=2.72606277:  77%|#######6  | 120/156 [04:06<00:49,  1.36s/it]2017-06-01 17:39:54,413 root  INFO     step 120.000000 - time: 1.654963, loss: 2.772271, perplexity: 15.994912, precision: 0.000000, batch_len: 126.000000
Train, loss=2.77227068:  78%|#######7  | 121/156 [04:08<00:50,  1.45s/it]2017-06-01 17:39:55,919 root  INFO     step 121.000000 - time: 1.402928, loss: 2.755815, perplexity: 15.733855, precision: 0.000000, batch_len: 144.000000
Train, loss=2.75581479:  78%|#######8  | 122/156 [04:09<00:49,  1.47s/it]2017-06-01 17:39:57,259 root  INFO     step 122.000000 - time: 1.331884, loss: 2.762972, perplexity: 15.846868, precision: 0.000000, batch_len: 133.000000
Train, loss=2.76297188:  79%|#######8  | 123/156 [04:10<00:47,  1.43s/it]2017-06-01 17:39:58,521 root  INFO     step 123.000000 - time: 1.242244, loss: 2.809989, perplexity: 16.609735, precision: 0.000000, batch_len: 118.000000
Train, loss=2.80998898:  79%|#######9  | 124/156 [04:12<00:44,  1.38s/it]2017-06-01 17:40:00,049 root  INFO     step 124.000000 - time: 1.466199, loss: 2.753373, perplexity: 15.695478, precision: 0.000000, batch_len: 123.000000
Train, loss=2.75337267:  80%|########  | 125/156 [04:13<00:44,  1.42s/it]2017-06-01 17:40:01,537 root  INFO     step 125.000000 - time: 1.077121, loss: 2.807697, perplexity: 16.571707, precision: 0.000000, batch_len: 78.000000
Train, loss=2.80769682:  81%|########  | 126/156 [04:15<00:43,  1.44s/it]2017-06-01 17:40:02,669 root  INFO     step 126.000000 - time: 1.076488, loss: 2.768502, perplexity: 15.934753, precision: 0.000000, batch_len: 96.000000
Train, loss=2.76850247:  81%|########1 | 127/156 [04:16<00:39,  1.35s/it]2017-06-01 17:40:04,159 root  INFO     step 127.000000 - time: 1.333928, loss: 2.784496, perplexity: 16.191649, precision: 0.000000, batch_len: 132.000000
Train, loss=2.78449559:  82%|########2 | 128/156 [04:17<00:38,  1.39s/it]2017-06-01 17:40:05,428 root  INFO     step 128.000000 - time: 1.219928, loss: 2.807509, perplexity: 16.568597, precision: 0.000000, batch_len: 122.000000
Train, loss=2.80750918:  83%|########2 | 129/156 [04:19<00:36,  1.36s/it]2017-06-01 17:40:07,338 root  INFO     step 129.000000 - time: 1.776136, loss: 2.791150, perplexity: 16.299751, precision: 0.000000, batch_len: 141.000000
Train, loss=2.79114985:  83%|########3 | 130/156 [04:21<00:39,  1.52s/it]2017-06-01 17:40:08,808 root  INFO     step 130.000000 - time: 1.437792, loss: 2.755001, perplexity: 15.721058, precision: 0.000000, batch_len: 139.000000
Train, loss=2.75500107:  84%|########3 | 131/156 [04:22<00:37,  1.51s/it]2017-06-01 17:40:10,490 root  INFO     step 131.000000 - time: 1.465132, loss: 2.739867, perplexity: 15.484921, precision: 0.000000, batch_len: 138.000000
Train, loss=2.73986673:  85%|########4 | 132/156 [04:24<00:37,  1.56s/it]2017-06-01 17:40:11,459 root  INFO     step 132.000000 - time: 0.827546, loss: 2.822749, perplexity: 16.823028, precision: 0.000000, batch_len: 74.000000
Train, loss=2.82274866:  85%|########5 | 133/156 [04:25<00:31,  1.38s/it]2017-06-01 17:40:13,519 root  INFO     step 133.000000 - time: 2.046688, loss: 3.210413, perplexity: 24.789310, precision: 0.000000, batch_len: 150.000000
Train, loss=3.21041250:  86%|########5 | 134/156 [04:27<00:34,  1.59s/it]2017-06-01 17:40:15,607 root  INFO     step 134.000000 - time: 2.048723, loss: 2.857318, perplexity: 17.414757, precision: 0.000000, batch_len: 152.000000
Train, loss=2.85731792:  87%|########6 | 135/156 [04:29<00:36,  1.74s/it]2017-06-01 17:40:17,007 root  INFO     step 135.000000 - time: 1.125559, loss: 2.980096, perplexity: 19.689713, precision: 0.000000, batch_len: 72.000000
Train, loss=2.98009634:  87%|########7 | 136/156 [04:30<00:32,  1.64s/it]2017-06-01 17:40:18,331 root  INFO     step 136.000000 - time: 1.304073, loss: 2.818720, perplexity: 16.755392, precision: 0.000000, batch_len: 142.000000
Train, loss=2.81872010:  88%|########7 | 137/156 [04:31<00:29,  1.54s/it]2017-06-01 17:40:19,501 root  INFO     step 137.000000 - time: 1.057759, loss: 2.792055, perplexity: 16.314514, precision: 0.000000, batch_len: 77.000000
Train, loss=2.79205513:  88%|########8 | 138/156 [04:33<00:25,  1.43s/it]2017-06-01 17:40:21,207 root  INFO     step 138.000000 - time: 1.642664, loss: 2.791537, perplexity: 16.306060, precision: 0.000000, batch_len: 134.000000
Train, loss=2.79153681:  89%|########9 | 139/156 [04:34<00:25,  1.51s/it]2017-06-01 17:40:22,235 root  INFO     step 139.000000 - time: 0.975932, loss: 2.794121, perplexity: 16.348257, precision: 0.000000, batch_len: 76.000000
Train, loss=2.79412127:  90%|########9 | 140/156 [04:35<00:21,  1.37s/it]2017-06-01 17:40:23,570 root  INFO     step 140.000000 - time: 1.310547, loss: 2.754395, perplexity: 15.711540, precision: 0.000000, batch_len: 131.000000
Train, loss=2.75439548:  90%|######### | 141/156 [04:37<00:20,  1.36s/it]2017-06-01 17:40:24,632 root  INFO     step 141.000000 - time: 0.873018, loss: 2.751314, perplexity: 15.663202, precision: 0.000000, batch_len: 71.000000
Train, loss=2.75131416:  91%|#########1| 142/156 [04:38<00:17,  1.27s/it]2017-06-01 17:40:24,738 root  INFO     Generating first batch)
2017-06-01 17:40:28,845 root  INFO     step 142.000000 - time: 1.208634, loss: 2.754664, perplexity: 15.715762, precision: 0.000000, batch_len: 113.000000
Train, loss=2.75466418:  92%|#########1| 143/156 [04:42<00:27,  2.15s/it]2017-06-01 17:40:30,050 root  INFO     step 143.000000 - time: 1.114287, loss: 2.779560, perplexity: 16.111924, precision: 0.000000, batch_len: 96.000000
Train, loss=2.77955961:  92%|#########2| 144/156 [04:43<00:22,  1.87s/it]2017-06-01 17:40:31,408 root  INFO     step 144.000000 - time: 1.127405, loss: 2.766859, perplexity: 15.908587, precision: 0.000000, batch_len: 105.000000
Train, loss=2.76685905:  93%|#########2| 145/156 [04:45<00:18,  1.72s/it]2017-06-01 17:40:32,789 root  INFO     step 145.000000 - time: 1.152988, loss: 2.767551, perplexity: 15.919602, precision: 0.000000, batch_len: 93.000000
Train, loss=2.76755118:  94%|#########3| 146/156 [04:46<00:16,  1.61s/it]2017-06-01 17:40:34,111 root  INFO     step 146.000000 - time: 1.289761, loss: 2.799263, perplexity: 16.432524, precision: 0.000000, batch_len: 104.000000
Train, loss=2.79926252:  94%|#########4| 147/156 [04:47<00:13,  1.53s/it]2017-06-01 17:40:35,235 root  INFO     step 147.000000 - time: 1.016305, loss: 2.720863, perplexity: 15.193434, precision: 0.000000, batch_len: 101.000000
Train, loss=2.72086334:  95%|#########4| 148/156 [04:48<00:11,  1.41s/it]2017-06-01 17:40:36,343 root  INFO     step 148.000000 - time: 1.008082, loss: 2.768608, perplexity: 15.936437, precision: 0.000000, batch_len: 90.000000
Train, loss=2.76860809:  96%|#########5| 149/156 [04:50<00:09,  1.32s/it]train_demo.sh: 行 19: 11772 已终止               $py src/launcher.py --phase=train --data-path=sample/sample.txt --data-base-dir=sample --log-path=log_01_16.log --attn-num-hidden 256 --batch-size 64 --model-dir=model_01_16 --initial-learning-rate=1.0 --no-load-model --num-epoch=3 --gpu-id=0 --use-gru --steps-per-checkpoint=2000 --target-embedding-size=10
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:02:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x39941d0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:04:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3998010
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:83:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x39bc820
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40c, pci bus id: 0000:83:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40c, pci bus id: 0000:84:00.0)
2017-06-01 17:41:04,430 root  INFO     loading data
2017-06-01 17:41:04,470 root  INFO     phase: train
2017-06-01 17:41:04,470 root  INFO     model_dir: model_01_16
2017-06-01 17:41:04,470 root  INFO     load_model: False
2017-06-01 17:41:04,470 root  INFO     output_dir: results
2017-06-01 17:41:04,470 root  INFO     steps_per_checkpoint: 2000
2017-06-01 17:41:04,470 root  INFO     batch_size: 64
2017-06-01 17:41:04,470 root  INFO     num_epoch: 3
2017-06-01 17:41:04,470 root  INFO     learning_rate: 1
2017-06-01 17:41:04,470 root  INFO     reg_val: 0
2017-06-01 17:41:04,470 root  INFO     max_gradient_norm: 5.000000
2017-06-01 17:41:04,470 root  INFO     clip_gradients: True
2017-06-01 17:41:04,470 root  INFO     valid_target_length inf
2017-06-01 17:41:04,470 root  INFO     target_vocab_size: 39
2017-06-01 17:41:04,470 root  INFO     target_embedding_size: 10.000000
2017-06-01 17:41:04,470 root  INFO     attn_num_hidden: 256
2017-06-01 17:41:04,471 root  INFO     attn_num_layers: 2
2017-06-01 17:41:04,471 root  INFO     visualize: True
2017-06-01 17:41:04,471 root  INFO     buckets
2017-06-01 17:41:04,471 root  INFO     [(16, 11), (27, 17), (35, 19), (64, 22), (80, 32)]
2017-06-01 17:41:04,471 root  INFO     ues GRU in the decoder.
2017-06-01 17:42:13,307 root  INFO     Created model with fresh parameters.
Train: :   0%|          | 0/156 [00:00<?, ?it/s]2017-06-01 17:42:58,590 root  INFO     Generating first batch)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=1325 evicted_count=1000 eviction_rate=0.754717 and unsatisfied allocation rate=0.913892
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2017-06-01 17:43:05,618 root  INFO     step 0.000000 - time: 4.368258, loss: 3.659590, perplexity: 38.845422, precision: 0.000000, batch_len: 96.000000
Train, loss=3.65959024:   1%|          | 1/156 [00:07<18:10,  7.03s/it]2017-06-01 17:43:08,062 root  INFO     step 1.000000 - time: 2.227776, loss: 3.410568, perplexity: 30.282433, precision: 0.000000, batch_len: 105.000000
Train, loss=3.41056776:   1%|1         | 2/156 [00:09<14:30,  5.66s/it]2017-06-01 17:43:10,419 root  INFO     step 2.000000 - time: 2.166316, loss: 3.271389, perplexity: 26.347924, precision: 0.000000, batch_len: 92.000000
Train, loss=3.27138948:   2%|1         | 3/156 [00:11<11:53,  4.67s/it]2017-06-01 17:43:12,756 root  INFO     step 3.000000 - time: 2.265555, loss: 3.110528, perplexity: 22.432880, precision: 0.000000, batch_len: 113.000000
Train, loss=3.11052775:   3%|2         | 4/156 [00:14<10:03,  3.97s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1014 evicted_count=1000 eviction_rate=0.986193 and unsatisfied allocation rate=0
2017-06-01 17:43:15,080 root  INFO     step 4.000000 - time: 2.252264, loss: 3.059377, perplexity: 21.314283, precision: 0.000000, batch_len: 108.000000
Train, loss=3.05937743:   3%|3         | 5/156 [00:16<08:44,  3.47s/it]2017-06-01 17:43:17,449 root  INFO     step 5.000000 - time: 2.210304, loss: 3.025229, perplexity: 20.598721, precision: 0.000000, batch_len: 102.000000
Train, loss=3.02522898:   4%|3         | 6/156 [00:18<07:51,  3.14s/it]2017-06-01 17:43:21,728 root  INFO     step 6.000000 - time: 4.181477, loss: 2.963837, perplexity: 19.372154, precision: 0.000000, batch_len: 120.000000
Train, loss=2.96383667:   4%|4         | 7/156 [00:23<08:39,  3.48s/it]2017-06-01 17:43:22,376 root  INFO     step 7.000000 - time: 0.514317, loss: 2.921239, perplexity: 18.564277, precision: 0.000000, batch_len: 104.000000
Train, loss=2.92123914:   5%|5         | 8/156 [00:23<06:29,  2.63s/it]2017-06-01 17:43:23,049 root  INFO     step 8.000000 - time: 0.663795, loss: 2.870354, perplexity: 17.643270, precision: 0.000000, batch_len: 114.000000
Train, loss=2.87035441:   6%|5         | 9/156 [00:24<05:00,  2.04s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=2479 evicted_count=2000 eviction_rate=0.806777 and unsatisfied allocation rate=0.824723
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 233 to 256
2017-06-01 17:43:25,309 root  INFO     step 9.000000 - time: 2.122403, loss: 2.946094, perplexity: 19.031468, precision: 0.000000, batch_len: 88.000000
Train, loss=2.94609380:   6%|6         | 10/156 [00:26<05:07,  2.11s/it]2017-06-01 17:43:25,870 root  INFO     step 10.000000 - time: 0.503372, loss: 2.906988, perplexity: 18.301589, precision: 0.000000, batch_len: 97.000000
Train, loss=2.90698791:   7%|7         | 11/156 [00:27<03:58,  1.65s/it]2017-06-01 17:43:29,820 root  INFO     step 11.000000 - time: 0.648504, loss: 2.927085, perplexity: 18.673118, precision: 0.000000, batch_len: 111.000000
Train, loss=2.92708492:   8%|7         | 12/156 [00:31<05:36,  2.34s/it]2017-06-01 17:43:31,974 root  INFO     step 12.000000 - time: 2.116082, loss: 2.911062, perplexity: 18.376308, precision: 0.000000, batch_len: 85.000000
Train, loss=2.91106224:   8%|8         | 13/156 [00:33<05:26,  2.28s/it]2017-06-01 17:43:32,555 root  INFO     step 13.000000 - time: 0.513675, loss: 2.863941, perplexity: 17.530474, precision: 0.000000, batch_len: 110.000000
Train, loss=2.86394072:   9%|8         | 14/156 [00:33<04:11,  1.77s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3133 get requests, put_count=2654 evicted_count=2000 eviction_rate=0.75358 and unsatisfied allocation rate=0.801787
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 372 to 409
2017-06-01 17:43:33,249 root  INFO     step 14.000000 - time: 0.595344, loss: 2.884573, perplexity: 17.895933, precision: 0.000000, batch_len: 121.000000
Train, loss=2.88457346:  10%|9         | 15/156 [00:34<03:24,  1.45s/it]2017-06-01 17:43:33,902 root  INFO     step 15.000000 - time: 0.642370, loss: 2.892771, perplexity: 18.043243, precision: 0.000000, batch_len: 106.000000
Train, loss=2.89277124:  10%|#         | 16/156 [00:35<02:49,  1.21s/it]2017-06-01 17:43:37,003 root  INFO     step 16.000000 - time: 3.077413, loss: 2.960257, perplexity: 19.302924, precision: 0.000000, batch_len: 128.000000
Train, loss=2.96025658:  11%|#         | 17/156 [00:38<04:07,  1.78s/it]2017-06-01 17:43:37,648 root  INFO     step 17.000000 - time: 0.635998, loss: 3.030380, perplexity: 20.705109, precision: 0.000000, batch_len: 101.000000
Train, loss=3.03038049:  12%|#1        | 18/156 [00:39<03:18,  1.44s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1054 evicted_count=1000 eviction_rate=0.948767 and unsatisfied allocation rate=0
2017-06-01 17:43:38,257 root  INFO     step 18.000000 - time: 0.597155, loss: 2.841557, perplexity: 17.142428, precision: 0.000000, batch_len: 90.000000
Train, loss=2.84155655:  12%|#2        | 19/156 [00:39<02:42,  1.19s/it]2017-06-01 17:43:38,775 root  INFO     step 19.000000 - time: 0.501308, loss: 2.812650, perplexity: 16.653996, precision: 0.000000, batch_len: 93.000000
Train, loss=2.81265020:  13%|#2        | 20/156 [00:40<02:14,  1.01it/s]2017-06-01 17:43:39,340 root  INFO     step 20.000000 - time: 0.507649, loss: 2.807529, perplexity: 16.568933, precision: 0.000000, batch_len: 103.000000
Train, loss=2.80752945:  13%|#3        | 21/156 [00:40<01:56,  1.16it/s]2017-06-01 17:43:41,745 root  INFO     step 21.000000 - time: 2.350714, loss: 2.886388, perplexity: 17.928440, precision: 0.000000, batch_len: 117.000000
Train, loss=2.88638830:  14%|#4        | 22/156 [00:43<02:57,  1.32s/it]2017-06-01 17:43:44,513 root  INFO     step 22.000000 - time: 2.650365, loss: 2.903447, perplexity: 18.236898, precision: 0.000000, batch_len: 124.000000
Train, loss=2.90344691:  15%|#4        | 23/156 [00:45<03:53,  1.76s/it]2017-06-01 17:43:45,060 root  INFO     step 23.000000 - time: 0.520618, loss: 2.862412, perplexity: 17.503690, precision: 0.000000, batch_len: 109.000000
Train, loss=2.86241174:  15%|#5        | 24/156 [00:46<03:04,  1.39s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=3270 evicted_count=2000 eviction_rate=0.611621 and unsatisfied allocation rate=0.547264
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 958 to 1053
2017-06-01 17:43:45,734 root  INFO     step 24.000000 - time: 0.587420, loss: 2.818811, perplexity: 16.756914, precision: 0.000000, batch_len: 86.000000
Train, loss=2.81881094:  16%|#6        | 25/156 [00:47<02:34,  1.18s/it]2017-06-01 17:43:46,346 root  INFO     step 25.000000 - time: 0.526334, loss: 2.840488, perplexity: 17.124115, precision: 0.000000, batch_len: 115.000000
Train, loss=2.84048772:  17%|#6        | 26/156 [00:47<02:11,  1.01s/it]2017-06-01 17:43:46,894 root  INFO     step 26.000000 - time: 0.529315, loss: 2.860420, perplexity: 17.468858, precision: 0.000000, batch_len: 112.000000
Train, loss=2.86041975:  17%|#7        | 27/156 [00:48<01:52,  1.15it/s]2017-06-01 17:43:49,149 root  INFO     step 27.000000 - time: 2.092614, loss: 2.827264, perplexity: 16.899167, precision: 0.000000, batch_len: 80.000000
Train, loss=2.82726431:  18%|#7        | 28/156 [00:50<02:44,  1.29s/it]2017-06-01 17:43:49,819 root  INFO     step 28.000000 - time: 0.578462, loss: 2.852211, perplexity: 17.326056, precision: 0.000000, batch_len: 82.000000
Train, loss=2.85221148:  19%|#8        | 29/156 [00:51<02:19,  1.10s/it]2017-06-01 17:43:50,331 root  INFO     step 29.000000 - time: 0.499472, loss: 2.851209, perplexity: 17.308690, precision: 0.000000, batch_len: 100.000000
Train, loss=2.85120869:  19%|#9        | 30/156 [00:51<01:56,  1.08it/s]2017-06-01 17:43:50,991 root  INFO     step 30.000000 - time: 0.602531, loss: 2.850306, perplexity: 17.293077, precision: 0.000000, batch_len: 94.000000
Train, loss=2.85030627:  20%|#9        | 31/156 [00:52<01:45,  1.18it/s]2017-06-01 17:43:51,540 root  INFO     step 31.000000 - time: 0.513662, loss: 2.810681, perplexity: 16.621239, precision: 0.000000, batch_len: 107.000000
Train, loss=2.81068134:  21%|##        | 32/156 [00:52<01:33,  1.32it/s]2017-06-01 17:43:52,053 root  INFO     step 32.000000 - time: 0.463711, loss: 2.826689, perplexity: 16.889443, precision: 0.000000, batch_len: 87.000000
Train, loss=2.82668877:  21%|##1       | 33/156 [00:53<01:24,  1.46it/s]2017-06-01 17:43:52,736 root  INFO     step 33.000000 - time: 0.585552, loss: 2.837876, perplexity: 17.079456, precision: 0.000000, batch_len: 116.000000
Train, loss=2.83787632:  22%|##1       | 34/156 [00:54<01:23,  1.46it/s]2017-06-01 17:43:53,244 root  INFO     step 34.000000 - time: 0.471274, loss: 2.976845, perplexity: 19.625800, precision: 0.000000, batch_len: 89.000000
Train, loss=2.97684503:  22%|##2       | 35/156 [00:54<01:16,  1.59it/s]2017-06-01 17:43:53,937 root  INFO     step 35.000000 - time: 0.610134, loss: 2.846613, perplexity: 17.229326, precision: 0.000000, batch_len: 125.000000
Train, loss=2.84661293:  23%|##3       | 36/156 [00:55<01:17,  1.54it/s]2017-06-01 17:43:54,442 root  INFO     step 36.000000 - time: 0.459103, loss: 2.897070, perplexity: 18.120980, precision: 0.000000, batch_len: 81.000000
Train, loss=2.89707041:  24%|##3       | 37/156 [00:55<01:12,  1.65it/s]2017-06-01 17:43:56,622 root  INFO     step 37.000000 - time: 2.072725, loss: 2.815989, perplexity: 16.709694, precision: 0.000000, batch_len: 79.000000
Train, loss=2.81598902:  24%|##4       | 38/156 [00:58<02:07,  1.08s/it]2017-06-01 17:43:57,211 root  INFO     step 38.000000 - time: 0.482723, loss: 2.817616, perplexity: 16.736894, precision: 0.000000, batch_len: 91.000000
Train, loss=2.81761551:  25%|##5       | 39/156 [00:58<01:48,  1.07it/s]2017-06-01 17:43:58,005 root  INFO     step 39.000000 - time: 0.731693, loss: 2.838450, perplexity: 17.089264, precision: 0.000000, batch_len: 123.000000
Train, loss=2.83845043:  26%|##5       | 40/156 [00:59<01:43,  1.12it/s]2017-06-01 17:43:58,793 root  INFO     step 40.000000 - time: 0.744262, loss: 2.837666, perplexity: 17.075856, precision: 0.000000, batch_len: 129.000000
Train, loss=2.83766556:  26%|##6       | 41/156 [01:00<01:38,  1.16it/s]2017-06-01 17:43:59,436 root  INFO     step 41.000000 - time: 0.623575, loss: 2.822252, perplexity: 16.814679, precision: 0.000000, batch_len: 98.000000
Train, loss=2.82225227:  27%|##6       | 42/156 [01:00<01:30,  1.26it/s]2017-06-01 17:43:59,941 root  INFO     step 42.000000 - time: 0.458341, loss: 2.834148, perplexity: 17.015891, precision: 0.000000, batch_len: 83.000000
Train, loss=2.83414769:  28%|##7       | 43/156 [01:01<01:19,  1.41it/s]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 14625 get requests, put_count=14369 evicted_count=1000 eviction_rate=0.0695943 and unsatisfied allocation rate=0.101265
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 2478 to 2725
2017-06-01 17:44:01,089 root  INFO     step 43.000000 - time: 1.019350, loss: 2.871157, perplexity: 17.657431, precision: 0.000000, batch_len: 126.000000
Train, loss=2.87115669:  28%|##8       | 44/156 [01:02<01:34,  1.19it/s]2017-06-01 17:44:04,387 root  INFO     step 44.000000 - time: 3.276977, loss: 2.831501, perplexity: 16.970911, precision: 0.000000, batch_len: 144.000000
Train, loss=2.83150077:  29%|##8       | 45/156 [01:05<02:55,  1.58s/it]2017-06-01 17:44:05,119 root  INFO     step 45.000000 - time: 0.714606, loss: 2.831410, perplexity: 16.969378, precision: 0.000000, batch_len: 118.000000
Train, loss=2.83141041:  29%|##9       | 46/156 [01:06<02:25,  1.32s/it]2017-06-01 17:44:05,773 root  INFO     step 46.000000 - time: 0.495956, loss: 2.857125, perplexity: 17.411394, precision: 0.000000, batch_len: 99.000000
Train, loss=2.85712481:  30%|###       | 47/156 [01:07<02:02,  1.12s/it]2017-06-01 17:44:06,306 root  INFO     step 47.000000 - time: 0.462079, loss: 2.821584, perplexity: 16.803450, precision: 0.000000, batch_len: 84.000000
Train, loss=2.82158422:  31%|###       | 48/156 [01:07<01:42,  1.06it/s]2017-06-01 17:44:09,580 root  INFO     step 48.000000 - time: 3.204637, loss: 2.828949, perplexity: 16.927668, precision: 0.000000, batch_len: 133.000000
Train, loss=2.82894945:  31%|###1      | 49/156 [01:10<02:55,  1.64s/it]2017-06-01 17:44:13,036 root  INFO     step 49.000000 - time: 3.225350, loss: 2.840495, perplexity: 17.124242, precision: 0.000000, batch_len: 137.000000
Train, loss=2.84049511:  32%|###2      | 50/156 [01:14<03:51,  2.19s/it]2017-06-01 17:44:13,642 root  INFO     step 50.000000 - time: 0.447977, loss: 2.793701, perplexity: 16.341383, precision: 0.000000, batch_len: 78.000000
Train, loss=2.79370070:  33%|###2      | 51/156 [01:15<02:59,  1.71s/it]2017-06-01 17:44:14,284 root  INFO     step 51.000000 - time: 0.584908, loss: 2.894721, perplexity: 18.078453, precision: 0.000000, batch_len: 119.000000
Train, loss=2.89472079:  33%|###3      | 52/156 [01:15<02:24,  1.39s/it]2017-06-01 17:44:20,287 root  INFO     step 52.000000 - time: 5.797736, loss: 2.847139, perplexity: 17.238394, precision: 0.000000, batch_len: 152.000000
Train, loss=2.84713912:  34%|###3      | 53/156 [01:21<04:45,  2.78s/it]2017-06-01 17:44:21,499 root  INFO     step 53.000000 - time: 1.200805, loss: 2.854408, perplexity: 17.364159, precision: 0.000000, batch_len: 135.000000
Train, loss=2.85440826:  35%|###4      | 54/156 [01:22<03:55,  2.31s/it]2017-06-01 17:44:22,755 root  INFO     step 54.000000 - time: 1.175228, loss: 2.831155, perplexity: 16.965041, precision: 0.000000, batch_len: 130.000000
Train, loss=2.83115482:  35%|###5      | 55/156 [01:24<03:21,  1.99s/it]2017-06-01 17:44:23,266 root  INFO     step 55.000000 - time: 0.408505, loss: 2.810380, perplexity: 16.616227, precision: 0.000000, batch_len: 96.000000
Train, loss=2.81037974:  36%|###5      | 56/156 [01:24<02:34,  1.55s/it]2017-06-01 17:44:25,449 root  INFO     step 56.000000 - time: 2.087463, loss: 2.839301, perplexity: 17.103804, precision: 0.000000, batch_len: 72.000000
Train, loss=2.83930087:  37%|###6      | 57/156 [01:26<02:52,  1.74s/it]2017-06-01 17:44:26,281 root  INFO     step 57.000000 - time: 0.775598, loss: 2.916654, perplexity: 18.479345, precision: 0.000000, batch_len: 132.000000
Train, loss=2.91665363:  37%|###7      | 58/156 [01:27<02:23,  1.47s/it]2017-06-01 17:44:27,099 root  INFO     step 58.000000 - time: 0.786991, loss: 2.860859, perplexity: 17.476531, precision: 0.000000, batch_len: 136.000000
Train, loss=2.86085892:  38%|###7      | 59/156 [01:28<02:03,  1.27s/it]2017-06-01 17:44:28,351 root  INFO     step 59.000000 - time: 1.209826, loss: 2.808325, perplexity: 16.582117, precision: 0.000000, batch_len: 138.000000
Train, loss=2.80832481:  38%|###8      | 60/156 [01:29<02:01,  1.27s/it]2017-06-01 17:44:28,927 root  INFO     step 60.000000 - time: 0.568039, loss: 2.920930, perplexity: 18.558546, precision: 0.000000, batch_len: 77.000000
Train, loss=2.92093039:  39%|###9      | 61/156 [01:30<01:40,  1.06s/it]2017-06-01 17:44:29,744 root  INFO     step 61.000000 - time: 0.552869, loss: 2.753022, perplexity: 15.689975, precision: 0.000000, batch_len: 74.000000
Train, loss=2.75302196:  40%|###9      | 62/156 [01:31<01:32,  1.01it/s]2017-06-01 17:44:33,012 root  INFO     step 62.000000 - time: 3.239949, loss: 2.959023, perplexity: 19.279118, precision: 0.000000, batch_len: 141.000000
Train, loss=2.95902252:  40%|####      | 63/156 [01:34<02:35,  1.67s/it]2017-06-01 17:44:33,789 root  INFO     step 63.000000 - time: 0.585499, loss: 2.884165, perplexity: 17.888629, precision: 0.000000, batch_len: 122.000000
Train, loss=2.88416529:  41%|####1     | 64/156 [01:35<02:09,  1.40s/it]2017-06-01 17:44:34,270 root  INFO     step 64.000000 - time: 0.453407, loss: 2.836601, perplexity: 17.057680, precision: 0.000000, batch_len: 76.000000
Train, loss=2.83660054:  42%|####1     | 65/156 [01:35<01:42,  1.13s/it]2017-06-01 17:44:35,091 root  INFO     step 65.000000 - time: 0.802143, loss: 2.771179, perplexity: 15.977464, precision: 0.000000, batch_len: 139.000000
Train, loss=2.77117920:  42%|####2     | 66/156 [01:36<01:33,  1.03s/it]2017-06-01 17:44:35,880 root  INFO     step 66.000000 - time: 0.775286, loss: 2.752580, perplexity: 15.683037, precision: 0.000000, batch_len: 134.000000
Train, loss=2.75257969:  43%|####2     | 67/156 [01:37<01:25,  1.04it/s]2017-06-01 17:44:37,364 root  INFO     step 67.000000 - time: 1.213459, loss: 2.830105, perplexity: 16.947245, precision: 0.000000, batch_len: 142.000000
Train, loss=2.83010530:  44%|####3     | 68/156 [01:38<01:38,  1.12s/it]2017-06-01 17:44:40,860 root  INFO     step 68.000000 - time: 3.440576, loss: 2.961333, perplexity: 19.323718, precision: 0.000000, batch_len: 150.000000
Train, loss=2.96133327:  44%|####4     | 69/156 [01:42<02:39,  1.83s/it]2017-06-01 17:44:42,902 root  INFO     step 69.000000 - time: 2.034606, loss: 3.051738, perplexity: 21.152075, precision: 0.000000, batch_len: 71.000000
Train, loss=3.05173802:  45%|####4     | 70/156 [01:44<02:42,  1.89s/it]2017-06-01 17:44:43,772 root  INFO     step 70.000000 - time: 0.772219, loss: 2.792912, perplexity: 16.328507, precision: 0.000000, batch_len: 131.000000
Train, loss=2.79291248:  46%|####5     | 71/156 [01:45<02:14,  1.59s/it]2017-06-01 17:44:43,843 root  INFO     Generating first batch)
2017-06-01 17:44:46,292 root  INFO     step 71.000000 - time: 0.394494, loss: 2.842707, perplexity: 17.162159, precision: 0.000000, batch_len: 96.000000
Train, loss=2.84270692:  46%|####6     | 72/156 [01:47<02:36,  1.87s/it]2017-06-01 17:44:47,609 root  INFO     step 72.000000 - time: 0.392380, loss: 2.800518, perplexity: 16.453168, precision: 0.000000, batch_len: 93.000000
Train, loss=2.80051804:  47%|####6     | 73/156 [01:49<02:21,  1.70s/it]2017-06-01 17:44:48,133 root  INFO     step 73.000000 - time: 0.406729, loss: 2.810479, perplexity: 16.617879, precision: 0.000000, batch_len: 100.000000
Train, loss=2.81047916:  47%|####7     | 74/156 [01:49<01:50,  1.35s/it]2017-06-01 17:44:48,799 root  INFO     step 74.000000 - time: 0.498395, loss: 2.823213, perplexity: 16.830843, precision: 0.000000, batch_len: 120.000000
Train, loss=2.82321310:  48%|####8     | 75/156 [01:50<01:32,  1.14s/it]2017-06-01 17:44:49,258 root  INFO     step 75.000000 - time: 0.402308, loss: 2.810433, perplexity: 16.617118, precision: 0.000000, batch_len: 97.000000
Train, loss=2.81043339:  49%|####8     | 76/156 [01:50<01:15,  1.07it/s]2017-06-01 17:44:49,771 root  INFO     step 76.000000 - time: 0.392060, loss: 2.800084, perplexity: 16.446030, precision: 0.000000, batch_len: 92.000000
Train, loss=2.80008411:  49%|####9     | 77/156 [01:51<01:04,  1.23it/s]2017-06-01 17:44:50,191 root  INFO     step 77.000000 - time: 0.412722, loss: 2.802566, perplexity: 16.486891, precision: 0.000000, batch_len: 105.000000
Train, loss=2.80256557:  50%|#####     | 78/156 [01:51<00:54,  1.44it/s]2017-06-01 17:44:50,659 root  INFO     step 78.000000 - time: 0.417070, loss: 2.779150, perplexity: 16.105326, precision: 0.000000, batch_len: 110.000000
Train, loss=2.77915001:  51%|#####     | 79/156 [01:52<00:48,  1.60it/s]2017-06-01 17:44:51,183 root  INFO     step 79.000000 - time: 0.485222, loss: 2.796452, perplexity: 16.386405, precision: 0.000000, batch_len: 112.000000
Train, loss=2.79645205:  51%|#####1    | 80/156 [01:52<00:45,  1.68it/s]2017-06-01 17:44:51,825 root  INFO     step 80.000000 - time: 0.599667, loss: 2.789170, perplexity: 16.267509, precision: 0.000000, batch_len: 90.000000
Train, loss=2.78916979:  52%|#####1    | 81/156 [01:53<00:45,  1.64it/s]2017-06-01 17:44:52,236 root  INFO     step 81.000000 - time: 0.402824, loss: 2.831470, perplexity: 16.970389, precision: 0.000000, batch_len: 102.000000
Train, loss=2.83147001:  53%|#####2    | 82/156 [01:53<00:40,  1.82it/s]2017-06-01 17:44:52,662 root  INFO     step 82.000000 - time: 0.422932, loss: 2.741745, perplexity: 15.514033, precision: 0.000000, batch_len: 113.000000
Train, loss=2.74174500:  53%|#####3    | 83/156 [01:54<00:37,  1.95it/s]2017-06-01 17:44:53,273 root  INFO     step 83.000000 - time: 0.409396, loss: 2.803816, perplexity: 16.507525, precision: 0.000000, batch_len: 106.000000
Train, loss=2.80381632:  54%|#####3    | 84/156 [01:54<00:39,  1.84it/s]2017-06-01 17:44:53,704 root  INFO     step 84.000000 - time: 0.418025, loss: 2.808199, perplexity: 16.580033, precision: 0.000000, batch_len: 114.000000
Train, loss=2.80819917:  54%|#####4    | 85/156 [01:55<00:36,  1.96it/s]2017-06-01 17:44:54,165 root  INFO     step 85.000000 - time: 0.416667, loss: 2.791147, perplexity: 16.299701, precision: 0.000000, batch_len: 108.000000
Train, loss=2.79114676:  55%|#####5    | 86/156 [01:55<00:34,  2.02it/s]2017-06-01 17:44:54,596 root  INFO     step 86.000000 - time: 0.401549, loss: 2.757439, perplexity: 15.759430, precision: 0.000000, batch_len: 101.000000
Train, loss=2.75743890:  56%|#####5    | 87/156 [01:56<00:32,  2.10it/s]2017-06-01 17:44:55,145 root  INFO     step 87.000000 - time: 0.408109, loss: 2.770023, perplexity: 15.959007, precision: 0.000000, batch_len: 109.000000
Train, loss=2.77002335:  56%|#####6    | 88/156 [01:56<00:33,  2.01it/s]2017-06-01 17:44:55,653 root  INFO     step 88.000000 - time: 0.484506, loss: 2.818986, perplexity: 16.759855, precision: 0.000000, batch_len: 124.000000
Train, loss=2.81898642:  57%|#####7    | 89/156 [01:57<00:33,  2.00it/s]2017-06-01 17:44:56,164 root  INFO     step 89.000000 - time: 0.472716, loss: 2.806739, perplexity: 16.555839, precision: 0.000000, batch_len: 117.000000
Train, loss=2.80673885:  58%|#####7    | 90/156 [01:57<00:33,  1.98it/s]2017-06-01 17:44:56,692 root  INFO     step 90.000000 - time: 0.405665, loss: 2.790195, perplexity: 16.284187, precision: 0.000000, batch_len: 104.000000
Train, loss=2.79019451:  58%|#####8    | 91/156 [01:58<00:33,  1.96it/s]2017-06-01 17:44:57,212 root  INFO     step 91.000000 - time: 0.495508, loss: 2.812702, perplexity: 16.654866, precision: 0.000000, batch_len: 128.000000
Train, loss=2.81270242:  59%|#####8    | 92/156 [01:58<00:32,  1.95it/s]2017-06-01 17:44:57,652 root  INFO     step 92.000000 - time: 0.413514, loss: 2.790068, perplexity: 16.282122, precision: 0.000000, batch_len: 111.000000
Train, loss=2.79006767:  60%|#####9    | 93/156 [01:59<00:30,  2.03it/s]2017-06-01 17:44:58,040 root  INFO     step 93.000000 - time: 0.373846, loss: 2.787717, perplexity: 16.243887, precision: 0.000000, batch_len: 88.000000
Train, loss=2.78771663:  60%|######    | 94/156 [01:59<00:28,  2.17it/s]2017-06-01 17:44:58,476 root  INFO     step 94.000000 - time: 0.417195, loss: 2.761853, perplexity: 15.829139, precision: 0.000000, batch_len: 107.000000
Train, loss=2.76185250:  61%|######    | 95/156 [01:59<00:27,  2.21it/s]2017-06-01 17:44:58,902 root  INFO     step 95.000000 - time: 0.375563, loss: 2.768692, perplexity: 15.937774, precision: 0.000000, batch_len: 89.000000
Train, loss=2.76869202:  62%|######1   | 96/156 [02:00<00:26,  2.25it/s]2017-06-01 17:44:59,442 root  INFO     step 96.000000 - time: 0.497849, loss: 2.837746, perplexity: 17.077228, precision: 0.000000, batch_len: 129.000000
Train, loss=2.83774590:  62%|######2   | 97/156 [02:00<00:27,  2.11it/s]2017-06-01 17:44:59,838 root  INFO     step 97.000000 - time: 0.361139, loss: 2.843208, perplexity: 17.170758, precision: 0.000000, batch_len: 81.000000
Train, loss=2.84320784:  63%|######2   | 98/156 [02:01<00:26,  2.22it/s]2017-06-01 17:45:00,356 root  INFO     step 98.000000 - time: 0.473031, loss: 2.777014, perplexity: 16.070958, precision: 0.000000, batch_len: 123.000000
Train, loss=2.77701378:  63%|######3   | 99/156 [02:01<00:26,  2.12it/s]2017-06-01 17:45:00,951 root  INFO     step 99.000000 - time: 0.445228, loss: 2.806647, perplexity: 16.554323, precision: 0.000000, batch_len: 91.000000
Train, loss=2.80664730:  64%|######4   | 100/156 [02:02<00:28,  1.97it/s]2017-06-01 17:45:01,391 root  INFO     step 100.000000 - time: 0.403881, loss: 2.755385, perplexity: 15.727101, precision: 0.000000, batch_len: 103.000000
Train, loss=2.75538540:  65%|######4   | 101/156 [02:02<00:26,  2.05it/s]2017-06-01 17:45:01,792 root  INFO     step 101.000000 - time: 0.368788, loss: 2.838496, perplexity: 17.090038, precision: 0.000000, batch_len: 85.000000
Train, loss=2.83849573:  65%|######5   | 102/156 [02:03<00:24,  2.17it/s]2017-06-01 17:45:02,331 root  INFO     step 102.000000 - time: 0.367483, loss: 2.782586, perplexity: 16.160756, precision: 0.000000, batch_len: 86.000000
Train, loss=2.78258586:  66%|######6   | 103/156 [02:03<00:25,  2.06it/s]2017-06-01 17:45:02,733 root  INFO     step 103.000000 - time: 0.360429, loss: 2.789590, perplexity: 16.274344, precision: 0.000000, batch_len: 79.000000
Train, loss=2.78958988:  67%|######6   | 104/156 [02:04<00:23,  2.17it/s]2017-06-01 17:45:03,150 root  INFO     step 104.000000 - time: 0.391091, loss: 2.851672, perplexity: 17.316714, precision: 0.000000, batch_len: 94.000000
Train, loss=2.85167217:  67%|######7   | 105/156 [02:04<00:22,  2.24it/s]2017-06-01 17:45:03,651 root  INFO     step 105.000000 - time: 0.496926, loss: 2.813618, perplexity: 16.670125, precision: 0.000000, batch_len: 121.000000
Train, loss=2.81361818:  68%|######7   | 106/156 [02:05<00:23,  2.16it/s]2017-06-01 17:45:04,116 root  INFO     step 106.000000 - time: 0.424583, loss: 2.765007, perplexity: 15.879155, precision: 0.000000, batch_len: 115.000000
Train, loss=2.76500726:  69%|######8   | 107/156 [02:05<00:22,  2.16it/s]2017-06-01 17:45:04,688 root  INFO     step 107.000000 - time: 0.532474, loss: 2.795124, perplexity: 16.364655, precision: 0.000000, batch_len: 144.000000
Train, loss=2.79512382:  69%|######9   | 108/156 [02:06<00:23,  2.02it/s]2017-06-01 17:45:05,097 root  INFO     step 108.000000 - time: 0.372431, loss: 2.794502, perplexity: 16.354479, precision: 0.000000, batch_len: 87.000000
Train, loss=2.79450178:  70%|######9   | 109/156 [02:06<00:22,  2.13it/s]2017-06-01 17:45:05,504 root  INFO     step 109.000000 - time: 0.396529, loss: 2.805790, perplexity: 16.540133, precision: 0.000000, batch_len: 99.000000
Train, loss=2.80578971:  71%|#######   | 110/156 [02:06<00:20,  2.22it/s]2017-06-01 17:45:05,884 root  INFO     step 110.000000 - time: 0.372890, loss: 2.763078, perplexity: 15.848553, precision: 0.000000, batch_len: 84.000000
Train, loss=2.76307821:  71%|#######1  | 111/156 [02:07<00:19,  2.33it/s]2017-06-01 17:45:06,472 root  INFO     step 111.000000 - time: 0.493696, loss: 2.768573, perplexity: 15.935870, precision: 0.000000, batch_len: 125.000000
Train, loss=2.76857257:  72%|#######1  | 112/156 [02:07<00:20,  2.10it/s]2017-06-01 17:45:06,948 root  INFO     step 112.000000 - time: 0.366191, loss: 2.782025, perplexity: 16.151700, precision: 0.000000, batch_len: 83.000000
Train, loss=2.78202534:  72%|#######2  | 113/156 [02:08<00:20,  2.10it/s]2017-06-01 17:45:07,479 root  INFO     step 113.000000 - time: 0.509720, loss: 2.784924, perplexity: 16.198583, precision: 0.000000, batch_len: 130.000000
Train, loss=2.78492379:  73%|#######3  | 114/156 [02:08<00:20,  2.03it/s]2017-06-01 17:45:07,982 root  INFO     step 114.000000 - time: 0.400246, loss: 2.760405, perplexity: 15.806240, precision: 0.000000, batch_len: 98.000000
Train, loss=2.76040483:  74%|#######3  | 115/156 [02:09<00:20,  2.02it/s]2017-06-01 17:45:08,520 root  INFO     step 115.000000 - time: 0.498429, loss: 2.902108, perplexity: 18.212496, precision: 0.000000, batch_len: 126.000000
Train, loss=2.90210795:  74%|#######4  | 116/156 [02:09<00:20,  1.97it/s]2017-06-01 17:45:09,047 root  INFO     step 116.000000 - time: 0.367887, loss: 2.958338, perplexity: 19.265921, precision: 0.000000, batch_len: 80.000000
Train, loss=2.95833778:  75%|#######5  | 117/156 [02:10<00:20,  1.95it/s]2017-06-01 17:45:09,603 root  INFO     step 117.000000 - time: 0.513704, loss: 2.818075, perplexity: 16.744581, precision: 0.000000, batch_len: 137.000000
Train, loss=2.81807470:  76%|#######5  | 118/156 [02:11<00:20,  1.90it/s]2017-06-01 17:45:10,146 root  INFO     step 118.000000 - time: 0.468021, loss: 2.723839, perplexity: 15.238709, precision: 0.000000, batch_len: 116.000000
Train, loss=2.72383881:  76%|#######6  | 119/156 [02:11<00:19,  1.88it/s]2017-06-01 17:45:10,775 root  INFO     step 119.000000 - time: 0.511529, loss: 2.747883, perplexity: 15.609556, precision: 0.000000, batch_len: 135.000000
Train, loss=2.74788332:  77%|#######6  | 120/156 [02:12<00:20,  1.78it/s]2017-06-01 17:45:11,468 root  INFO     step 120.000000 - time: 0.523429, loss: 2.734612, perplexity: 15.403769, precision: 0.000000, batch_len: 136.000000
Train, loss=2.73461223:  78%|#######7  | 121/156 [02:12<00:21,  1.67it/s]2017-06-01 17:45:12,228 root  INFO     step 121.000000 - time: 0.525703, loss: 2.742960, perplexity: 15.532894, precision: 0.000000, batch_len: 138.000000
Train, loss=2.74295998:  78%|#######8  | 122/156 [02:13<00:22,  1.54it/s]2017-06-01 17:45:12,768 root  INFO     step 122.000000 - time: 0.529658, loss: 2.748572, perplexity: 15.620316, precision: 0.000000, batch_len: 139.000000
Train, loss=2.74857235:  79%|#######8  | 123/156 [02:14<00:20,  1.62it/s]2017-06-01 17:45:13,141 root  INFO     step 123.000000 - time: 0.366576, loss: 2.785456, perplexity: 16.207210, precision: 0.000000, batch_len: 82.000000
Train, loss=2.78545618:  79%|#######9  | 124/156 [02:14<00:17,  1.84it/s]2017-06-01 17:45:13,715 root  INFO     step 124.000000 - time: 0.515281, loss: 2.769309, perplexity: 15.947611, precision: 0.000000, batch_len: 133.000000
Train, loss=2.76930904:  80%|########  | 125/156 [02:15<00:17,  1.81it/s]2017-06-01 17:45:14,240 root  INFO     step 125.000000 - time: 0.492621, loss: 2.796553, perplexity: 16.388062, precision: 0.000000, batch_len: 118.000000
Train, loss=2.79655313:  81%|########  | 126/156 [02:15<00:16,  1.84it/s]2017-06-01 17:45:14,778 root  INFO     step 126.000000 - time: 0.473149, loss: 2.785518, perplexity: 16.208218, precision: 0.000000, batch_len: 119.000000
Train, loss=2.78551841:  81%|########1 | 127/156 [02:16<00:15,  1.84it/s]2017-06-01 17:45:15,305 root  INFO     step 127.000000 - time: 0.397410, loss: 2.780622, perplexity: 16.129058, precision: 0.000000, batch_len: 96.000000
Train, loss=2.78062248:  82%|########2 | 128/156 [02:16<00:15,  1.86it/s]2017-06-01 17:45:16,089 root  INFO     step 128.000000 - time: 0.492771, loss: 2.856352, perplexity: 17.397949, precision: 0.000000, batch_len: 122.000000
Train, loss=2.85635233:  83%|########2 | 129/156 [02:17<00:16,  1.64it/s]2017-06-01 17:45:16,719 root  INFO     step 129.000000 - time: 0.528322, loss: 2.817899, perplexity: 16.741631, precision: 0.000000, batch_len: 141.000000
Train, loss=2.81789851:  83%|########3 | 130/156 [02:18<00:16,  1.62it/s]2017-06-01 17:45:17,473 root  INFO     step 130.000000 - time: 0.697776, loss: 2.954336, perplexity: 19.188985, precision: 0.000000, batch_len: 152.000000
Train, loss=2.95433640:  84%|########3 | 131/156 [02:18<00:16,  1.52it/s]2017-06-01 17:45:17,842 root  INFO     step 131.000000 - time: 0.350288, loss: 3.108648, perplexity: 22.390758, precision: 0.000000, batch_len: 74.000000
Train, loss=3.10864830:  85%|########4 | 132/156 [02:19<00:13,  1.75it/s]2017-06-01 17:45:18,263 root  INFO     step 132.000000 - time: 0.356306, loss: 2.738314, perplexity: 15.460891, precision: 0.000000, batch_len: 78.000000
Train, loss=2.73831367:  85%|########5 | 133/156 [02:19<00:12,  1.90it/s]2017-06-01 17:45:18,788 root  INFO     step 133.000000 - time: 0.519152, loss: 2.773529, perplexity: 16.015052, precision: 0.000000, batch_len: 134.000000
Train, loss=2.77352905:  86%|########5 | 134/156 [02:20<00:11,  1.90it/s]2017-06-01 17:45:19,557 root  INFO     step 134.000000 - time: 0.695835, loss: 2.841487, perplexity: 17.141234, precision: 0.000000, batch_len: 150.000000
Train, loss=2.84148693:  87%|########6 | 135/156 [02:20<00:12,  1.67it/s]2017-06-01 17:45:20,065 root  INFO     step 135.000000 - time: 0.358787, loss: 2.870438, perplexity: 17.644738, precision: 0.000000, batch_len: 76.000000
Train, loss=2.87043762:  87%|########7 | 136/156 [02:21<00:11,  1.75it/s]2017-06-01 17:45:20,574 root  INFO     step 136.000000 - time: 0.506526, loss: 2.757194, perplexity: 15.755568, precision: 0.000000, batch_len: 132.000000
Train, loss=2.75719380:  88%|########7 | 137/156 [02:21<00:10,  1.81it/s]2017-06-01 17:45:22,927 root  INFO     step 137.000000 - time: 2.179861, loss: 2.809130, perplexity: 16.595477, precision: 0.000000, batch_len: 72.000000
Train, loss=2.80913019:  88%|########8 | 138/156 [02:24<00:19,  1.09s/it]2017-06-01 17:45:23,380 root  INFO     step 138.000000 - time: 0.359455, loss: 2.713138, perplexity: 15.076509, precision: 0.000000, batch_len: 77.000000
Train, loss=2.71313787:  89%|########9 | 139/156 [02:24<00:15,  1.11it/s]2017-06-01 17:45:24,062 root  INFO     step 139.000000 - time: 0.519817, loss: 2.898078, perplexity: 18.139256, precision: 0.000000, batch_len: 142.000000
Train, loss=2.89807844:  90%|########9 | 140/156 [02:25<00:13,  1.20it/s]2017-06-01 17:45:24,417 root  INFO     step 140.000000 - time: 0.351433, loss: 2.754433, perplexity: 15.712125, precision: 0.000000, batch_len: 71.000000
Train, loss=2.75443268:  90%|######### | 141/156 [02:25<00:10,  1.45it/s]2017-06-01 17:45:24,937 root  INFO     step 141.000000 - time: 0.508717, loss: 2.784888, perplexity: 16.198008, precision: 0.000000, batch_len: 131.000000
Train, loss=2.78488827:  91%|#########1| 142/156 [02:26<00:08,  1.56it/s]2017-06-01 17:45:25,041 root  INFO     Generating first batch)
2017-06-01 17:45:27,871 root  INFO     step 142.000000 - time: 0.376740, loss: 2.787496, perplexity: 16.240297, precision: 0.000000, batch_len: 89.000000
Train, loss=2.78749561:  92%|#########1| 143/156 [02:29<00:17,  1.33s/it]2017-06-01 17:45:28,533 root  INFO     step 143.000000 - time: 0.394872, loss: 2.732960, perplexity: 15.378347, precision: 0.000000, batch_len: 96.000000
Train, loss=2.73296046:  92%|#########2| 144/156 [02:29<00:13,  1.13s/it]2017-06-01 17:45:29,049 root  INFO     step 144.000000 - time: 0.409017, loss: 2.733877, perplexity: 15.392454, precision: 0.000000, batch_len: 108.000000
Train, loss=2.73387742:  93%|#########2| 145/156 [02:30<00:10,  1.06it/s]2017-06-01 17:45:29,546 root  INFO     step 145.000000 - time: 0.389995, loss: 2.755829, perplexity: 15.734073, precision: 0.000000, batch_len: 93.000000
Train, loss=2.75582862:  94%|#########3| 146/156 [02:30<00:08,  1.23it/s]2017-06-01 17:45:30,016 root  INFO     step 146.000000 - time: 0.416437, loss: 2.765298, perplexity: 15.883775, precision: 0.000000, batch_len: 110.000000
Train, loss=2.76529813:  94%|#########4| 147/156 [02:31<00:06,  1.41it/s]2017-06-01 17:45:30,574 root  INFO     step 147.000000 - time: 0.410951, loss: 2.732540, perplexity: 15.371884, precision: 0.000000, batch_len: 105.000000
Train, loss=2.73254013:  95%|#########4| 148/156 [02:31<00:05,  1.51it/s]2017-06-01 17:45:31,118 root  INFO     step 148.000000 - time: 0.415752, loss: 2.761643, perplexity: 15.825830, precision: 0.000000, batch_len: 113.000000
Train, loss=2.76164341:  96%|#########5| 149/156 [02:32<00:04,  1.59it/s]2017-06-01 17:45:31,534 root  INFO     step 149.000000 - time: 0.374354, loss: 2.798233, perplexity: 16.415611, precision: 0.000000, batch_len: 88.000000
Train, loss=2.79823279:  96%|#########6| 150/156 [02:32<00:03,  1.77it/s]2017-06-01 17:45:31,969 root  INFO     step 150.000000 - time: 0.387638, loss: 2.726460, perplexity: 15.278704, precision: 0.000000, batch_len: 91.000000
Train, loss=2.72645998:  97%|#########6| 151/156 [02:33<00:02,  1.90it/s]2017-06-01 17:45:32,442 root  INFO     step 151.000000 - time: 0.409531, loss: 2.686077, perplexity: 14.673998, precision: 0.000000, batch_len: 107.000000
Train, loss=2.68607712:  97%|#########7| 152/156 [02:33<00:02,  1.96it/s]2017-06-01 17:45:32,920 root  INFO     step 152.000000 - time: 0.412776, loss: 2.684044, perplexity: 14.644193, precision: 0.000000, batch_len: 106.000000
Train, loss=2.68404388:  98%|#########8| 153/156 [02:34<00:01,  2.00it/s]2017-06-01 17:45:33,411 root  INFO     step 153.000000 - time: 0.479593, loss: 2.780079, perplexity: 16.120293, precision: 0.000000, batch_len: 120.000000
Train, loss=2.78007889:  99%|#########8| 154/156 [02:34<00:00,  2.01it/s]2017-06-01 17:45:33,850 root  INFO     step 154.000000 - time: 0.394758, loss: 2.889005, perplexity: 17.975418, precision: 0.000000, batch_len: 92.000000
Train, loss=2.88900518:  99%|#########9| 155/156 [02:35<00:00,  2.08it/s]2017-06-01 17:45:34,325 root  INFO     step 155.000000 - time: 0.415481, loss: 2.814842, perplexity: 16.690538, precision: 0.000000, batch_len: 111.000000
Train, loss=2.81484199: 100%|##########| 156/156 [02:35<00:00,  2.09it/s]2017-06-01 17:45:34,702 root  INFO     step 156.000000 - time: 0.372787, loss: 2.695925, perplexity: 14.819217, precision: 0.000000, batch_len: 86.000000
Train, loss=2.69592476: 157it [02:36,  2.23it/s]                         2017-06-01 17:45:35,203 root  INFO     step 157.000000 - time: 0.484230, loss: 2.803154, perplexity: 16.496587, precision: 0.000000, batch_len: 121.000000
Train, loss=2.80315351: 158it [02:36,  2.16it/s]2017-06-01 17:45:35,632 root  INFO     step 158.000000 - time: 0.415296, loss: 2.827237, perplexity: 16.898707, precision: 0.000000, batch_len: 102.000000
Train, loss=2.82723713: 159it [02:37,  2.21it/s]2017-06-01 17:45:36,015 root  INFO     step 159.000000 - time: 0.365367, loss: 2.732985, perplexity: 15.378724, precision: 0.000000, batch_len: 80.000000
Train, loss=2.73298502: 160it [02:37,  2.31it/s]2017-06-01 17:45:36,552 root  INFO     step 160.000000 - time: 0.482387, loss: 2.784074, perplexity: 16.184821, precision: 0.000000, batch_len: 128.000000
Train, loss=2.78407383: 161it [02:37,  2.16it/s]2017-06-01 17:45:37,022 root  INFO     step 161.000000 - time: 0.396092, loss: 2.744144, perplexity: 15.551303, precision: 0.000000, batch_len: 97.000000
Train, loss=2.74414444: 162it [02:38,  2.15it/s]2017-06-01 17:45:37,530 root  INFO     step 162.000000 - time: 0.489717, loss: 2.782080, perplexity: 16.152586, precision: 0.000000, batch_len: 117.000000
Train, loss=2.78208017: 163it [02:38,  2.09it/s]2017-06-01 17:45:38,002 root  INFO     step 163.000000 - time: 0.413517, loss: 2.819882, perplexity: 16.774878, precision: 0.000000, batch_len: 104.000000
Train, loss=2.81988239: 164it [02:39,  2.10it/s]2017-06-01 17:45:38,440 root  INFO     step 164.000000 - time: 0.406250, loss: 2.702979, perplexity: 14.924122, precision: 0.000000, batch_len: 101.000000
Train, loss=2.70297885: 165it [02:39,  2.15it/s]2017-06-01 17:45:38,823 root  INFO     step 165.000000 - time: 0.362094, loss: 2.709903, perplexity: 15.027814, precision: 0.000000, batch_len: 81.000000
Train, loss=2.70990276: 166it [02:40,  2.27it/s]2017-06-01 17:45:39,423 root  INFO     step 166.000000 - time: 0.466812, loss: 2.739514, perplexity: 15.479466, precision: 0.000000, batch_len: 112.000000
Train, loss=2.73951435: 167it [02:40,  2.05it/s]2017-06-01 17:45:39,905 root  INFO     step 167.000000 - time: 0.419868, loss: 2.774001, perplexity: 16.022607, precision: 0.000000, batch_len: 114.000000
Train, loss=2.77400064: 168it [02:41,  2.06it/s]2017-06-01 17:45:40,337 root  INFO     step 168.000000 - time: 0.424615, loss: 2.764968, perplexity: 15.878534, precision: 0.000000, batch_len: 115.000000
Train, loss=2.76496816: 169it [02:41,  2.13it/s]2017-06-01 17:45:41,001 root  INFO     step 169.000000 - time: 0.479418, loss: 2.796166, perplexity: 16.381726, precision: 0.000000, batch_len: 124.000000
Train, loss=2.79616642: 170it [02:42,  1.89it/s]2017-06-01 17:45:41,381 root  INFO     step 170.000000 - time: 0.361477, loss: 2.775650, perplexity: 16.049056, precision: 0.000000, batch_len: 83.000000
Train, loss=2.77565002: 171it [02:42,  2.07it/s]2017-06-01 17:45:41,801 root  INFO     step 171.000000 - time: 0.404461, loss: 2.738615, perplexity: 15.465544, precision: 0.000000, batch_len: 100.000000
Train, loss=2.73861456: 172it [02:43,  2.15it/s]2017-06-01 17:45:42,251 root  INFO     step 172.000000 - time: 0.416931, loss: 2.757452, perplexity: 15.759633, precision: 0.000000, batch_len: 109.000000
Train, loss=2.75745177: 173it [02:43,  2.17it/s]2017-06-01 17:45:42,847 root  INFO     step 173.000000 - time: 0.476214, loss: 2.829839, perplexity: 16.942729, precision: 0.000000, batch_len: 119.000000
Train, loss=2.82983875: 174it [02:44,  2.00it/s]2017-06-01 17:45:43,449 root  INFO     step 174.000000 - time: 0.592248, loss: 2.810175, perplexity: 16.612832, precision: 0.000000, batch_len: 90.000000
Train, loss=2.81017542: 175it [02:44,  1.88it/s]2017-06-01 17:45:43,941 root  INFO     step 175.000000 - time: 0.372182, loss: 2.921884, perplexity: 18.576253, precision: 0.000000, batch_len: 85.000000
Train, loss=2.92188406: 176it [02:45,  1.93it/s]2017-06-01 17:45:44,557 root  INFO     step 176.000000 - time: 0.482698, loss: 2.792884, perplexity: 16.328048, precision: 0.000000, batch_len: 125.000000
Train, loss=2.79288435: 177it [02:45,  1.82it/s]2017-06-01 17:45:45,095 root  INFO     step 177.000000 - time: 0.390770, loss: 2.776433, perplexity: 16.061619, precision: 0.000000, batch_len: 94.000000
Train, loss=2.77643251: 178it [02:46,  1.83it/s]2017-06-01 17:45:45,627 root  INFO     step 178.000000 - time: 0.473977, loss: 2.699940, perplexity: 14.878842, precision: 0.000000, batch_len: 116.000000
Train, loss=2.69994020: 179it [02:47,  1.85it/s]2017-06-01 17:45:46,038 root  INFO     step 179.000000 - time: 0.404636, loss: 2.746267, perplexity: 15.584344, precision: 0.000000, batch_len: 103.000000
Train, loss=2.74626684: 180it [02:47,  1.99it/s]2017-06-01 17:45:46,464 root  INFO     step 180.000000 - time: 0.367193, loss: 2.768475, perplexity: 15.934320, precision: 0.000000, batch_len: 87.000000
Train, loss=2.76847529: 181it [02:47,  2.09it/s]2017-06-01 17:45:47,002 root  INFO     step 181.000000 - time: 0.483300, loss: 2.800472, perplexity: 16.452407, precision: 0.000000, batch_len: 126.000000
Train, loss=2.80047178: 182it [02:48,  2.01it/s]2017-06-01 17:45:47,383 root  INFO     step 182.000000 - time: 0.357883, loss: 2.790613, perplexity: 16.291006, precision: 0.000000, batch_len: 79.000000
Train, loss=2.79061317: 183it [02:48,  2.16it/s]2017-06-01 17:45:47,974 root  INFO     step 183.000000 - time: 0.523567, loss: 2.778477, perplexity: 16.094493, precision: 0.000000, batch_len: 137.000000
Train, loss=2.77847719: 184it [02:49,  2.00it/s]2017-06-01 17:45:48,480 root  INFO     step 184.000000 - time: 0.477384, loss: 2.725586, perplexity: 15.265363, precision: 0.000000, batch_len: 123.000000
Train, loss=2.72558641: 185it [02:49,  1.99it/s]2017-06-01 17:45:49,030 root  INFO     step 185.000000 - time: 0.524254, loss: 2.727822, perplexity: 15.299529, precision: 0.000000, batch_len: 135.000000
Train, loss=2.72782207: 186it [02:50,  1.94it/s]2017-06-01 17:45:49,453 root  INFO     step 186.000000 - time: 0.356759, loss: 2.688223, perplexity: 14.705526, precision: 0.000000, batch_len: 78.000000
Train, loss=2.68822336: 187it [02:50,  2.05it/s]2017-06-01 17:45:50,080 root  INFO     step 187.000000 - time: 0.496101, loss: 2.817313, perplexity: 16.731827, precision: 0.000000, batch_len: 129.000000
Train, loss=2.81731272: 188it [02:51,  1.89it/s]2017-06-01 17:45:50,506 root  INFO     step 188.000000 - time: 0.370093, loss: 2.808882, perplexity: 16.591351, precision: 0.000000, batch_len: 84.000000
Train, loss=2.80888152: 189it [02:51,  2.00it/s]2017-06-01 17:45:51,041 root  INFO     step 189.000000 - time: 0.395784, loss: 2.669267, perplexity: 14.429391, precision: 0.000000, batch_len: 98.000000
Train, loss=2.66926718: 190it [02:52,  1.96it/s]2017-06-01 17:45:51,750 root  INFO     step 190.000000 - time: 0.471084, loss: 2.820796, perplexity: 16.790211, precision: 0.000000, batch_len: 118.000000
Train, loss=2.82079601: 191it [02:53,  1.76it/s]2017-06-01 17:45:52,311 root  INFO     step 191.000000 - time: 0.515801, loss: 2.678047, perplexity: 14.556639, precision: 0.000000, batch_len: 136.000000
Train, loss=2.67804718: 192it [02:53,  1.76it/s]2017-06-01 17:45:52,759 root  INFO     step 192.000000 - time: 0.394853, loss: 2.786112, perplexity: 16.217839, precision: 0.000000, batch_len: 99.000000
Train, loss=2.78611183: 193it [02:54,  1.88it/s]2017-06-01 17:45:53,297 root  INFO     step 193.000000 - time: 0.534293, loss: 2.721282, perplexity: 15.199792, precision: 0.000000, batch_len: 144.000000
Train, loss=2.72128177: 194it [02:54,  1.87it/s]2017-06-01 17:45:53,890 root  INFO     step 194.000000 - time: 0.522930, loss: 2.731174, perplexity: 15.350898, precision: 0.000000, batch_len: 133.000000
Train, loss=2.73117399: 195it [02:55,  1.81it/s]2017-06-01 17:45:54,382 root  INFO     step 195.000000 - time: 0.364118, loss: 2.713223, perplexity: 15.077786, precision: 0.000000, batch_len: 82.000000
Train, loss=2.71322250: 196it [02:55,  1.87it/s]2017-06-01 17:45:54,896 root  INFO     step 196.000000 - time: 0.490414, loss: 2.750949, perplexity: 15.657490, precision: 0.000000, batch_len: 130.000000
Train, loss=2.75094938: 197it [02:56,  1.90it/s]2017-06-01 17:45:55,832 root  INFO     step 197.000000 - time: 0.718232, loss: 2.960886, perplexity: 19.315087, precision: 0.000000, batch_len: 152.000000
Train, loss=2.96088648: 198it [02:57,  1.54it/s]2017-06-01 17:45:56,428 root  INFO     step 198.000000 - time: 0.476544, loss: 2.857305, perplexity: 17.414536, precision: 0.000000, batch_len: 122.000000
Train, loss=2.85730529: 199it [02:57,  1.58it/s]2017-06-01 17:45:56,947 root  INFO     step 199.000000 - time: 0.515602, loss: 2.731401, perplexity: 15.354379, precision: 0.000000, batch_len: 138.000000
Train, loss=2.73140073: 200it [02:58,  1.67it/s]2017-06-01 17:45:57,839 root  INFO     step 200.000000 - time: 0.704854, loss: 2.801388, perplexity: 16.467484, precision: 0.000000, batch_len: 150.000000
Train, loss=2.80138779: 201it [02:59,  1.46it/s]2017-06-01 17:45:58,254 root  INFO     step 201.000000 - time: 0.400685, loss: 2.900857, perplexity: 18.189731, precision: 0.000000, batch_len: 96.000000
Train, loss=2.90085721: 202it [02:59,  1.65it/s]2017-06-01 17:45:58,926 root  INFO     step 202.000000 - time: 0.516908, loss: 2.744581, perplexity: 15.558086, precision: 0.000000, batch_len: 139.000000
Train, loss=2.74458051: 203it [03:00,  1.60it/s]2017-06-01 17:45:59,574 root  INFO     step 203.000000 - time: 0.350875, loss: 2.782355, perplexity: 16.157031, precision: 0.000000, batch_len: 74.000000
Train, loss=2.78235531: 204it [03:00,  1.58it/s]2017-06-01 17:46:00,144 root  INFO     step 204.000000 - time: 0.526716, loss: 2.824511, perplexity: 16.852703, precision: 0.000000, batch_len: 134.000000
Train, loss=2.82451105: 205it [03:01,  1.63it/s]2017-06-01 17:46:00,574 root  INFO     step 205.000000 - time: 0.411443, loss: 2.824535, perplexity: 16.853105, precision: 0.000000, batch_len: 72.000000
Train, loss=2.82453489: 206it [03:01,  1.79it/s]2017-06-01 17:46:01,105 root  INFO     step 206.000000 - time: 0.520758, loss: 2.737084, perplexity: 15.441897, precision: 0.000000, batch_len: 141.000000
Train, loss=2.73708439: 207it [03:02,  1.82it/s]2017-06-01 17:46:01,627 root  INFO     step 207.000000 - time: 0.360295, loss: 2.708811, perplexity: 15.011421, precision: 0.000000, batch_len: 77.000000
Train, loss=2.70881128: 208it [03:03,  1.85it/s]2017-06-01 17:46:02,292 root  INFO     step 208.000000 - time: 0.505806, loss: 2.773947, perplexity: 16.021751, precision: 0.000000, batch_len: 132.000000
Train, loss=2.77394724: 209it [03:03,  1.73it/s]2017-06-01 17:46:02,691 root  INFO     step 209.000000 - time: 0.360733, loss: 2.723219, perplexity: 15.229272, precision: 0.000000, batch_len: 76.000000
Train, loss=2.72321939: 210it [03:04,  1.91it/s]2017-06-01 17:46:03,305 root  INFO     step 210.000000 - time: 0.491490, loss: 2.737253, perplexity: 15.444504, precision: 0.000000, batch_len: 131.000000
Train, loss=2.73725319: 211it [03:04,  1.81it/s]2017-06-01 17:46:03,664 root  INFO     step 211.000000 - time: 0.342265, loss: 2.708664, perplexity: 15.009205, precision: 0.000000, batch_len: 71.000000
Train, loss=2.70866370: 212it [03:05,  2.03it/s]2017-06-01 17:46:04,245 root  INFO     step 212.000000 - time: 0.521424, loss: 2.777370, perplexity: 16.076691, precision: 0.000000, batch_len: 142.000000
Train, loss=2.77737045: 213it [03:05,  1.92it/s]
input_tensor dim: (?, 1, 32, ?)
CNN outdim before squeeze: (?, 1, ?, 512)
CNN outdim: (?, ?, 512)
using GRU CELL in decoder
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
usage: launcher.py [-h] [--gpu-id GPU_ID] [--use-gru] [--phase {train,test}]
                   [--data-path DATA_PATH] [--data-base-dir DATA_BASE_DIR]
                   [--visualize] [--no-visualize] [--batch-size BATCH_SIZE]
                   [--initial-learning-rate INITIAL_LEARNING_RATE]
                   [--num-epoch NUM_EPOCH]
                   [--steps-per-checkpoint STEPS_PER_CHECKPOINT]
                   [--target-vocab-size TARGET_VOCAB_SIZE]
                   [--model-dir MODEL_DIR]
                   [--target-embedding-size TARGET_EMBEDDING_SIZE]
                   [--attn-num-hidden ATTN_NUM_HIDDEN]
                   [--attn-num-layers ATTN_NUM_LAYERS] [--load-model]
                   [--no-load-model] [--log-path LOG_PATH]
                   [--output-dir OUTPUT_DIR]
                   [--max_gradient_norm MAX_GRADIENT_NORM]
                   [--no-gradient_clipping]
launcher.py: error: argument --no-load-model: ignored explicit argument 'model_01_16/'
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:02:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x2e83430
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:04:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x2e87270
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:83:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x2eabad0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40c, pci bus id: 0000:83:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40c, pci bus id: 0000:84:00.0)
2017-06-01 18:08:00,596 root  INFO     loading data
2017-06-01 18:08:00,643 root  INFO     phase: train
2017-06-01 18:08:00,644 root  INFO     model_dir: model_01_16
2017-06-01 18:08:00,644 root  INFO     load_model: False
2017-06-01 18:08:00,644 root  INFO     output_dir: results
2017-06-01 18:08:00,644 root  INFO     steps_per_checkpoint: 2000
2017-06-01 18:08:00,644 root  INFO     batch_size: 64
2017-06-01 18:08:00,644 root  INFO     num_epoch: 30
2017-06-01 18:08:00,644 root  INFO     learning_rate: 1
2017-06-01 18:08:00,644 root  INFO     reg_val: 0
2017-06-01 18:08:00,644 root  INFO     max_gradient_norm: 5.000000
2017-06-01 18:08:00,644 root  INFO     clip_gradients: True
2017-06-01 18:08:00,644 root  INFO     valid_target_length inf
2017-06-01 18:08:00,644 root  INFO     target_vocab_size: 39
2017-06-01 18:08:00,644 root  INFO     target_embedding_size: 10.000000
2017-06-01 18:08:00,644 root  INFO     attn_num_hidden: 256
2017-06-01 18:08:00,644 root  INFO     attn_num_layers: 2
2017-06-01 18:08:00,645 root  INFO     visualize: True
2017-06-01 18:08:00,645 root  INFO     buckets
2017-06-01 18:08:00,645 root  INFO     [(16, 11), (27, 17), (35, 19), (64, 22), (80, 32)]
2017-06-01 18:08:00,645 root  INFO     ues GRU in the decoder.
2017-06-01 18:09:08,772 root  INFO     Created model with fresh parameters.
Train: :   0%|          | 0/156 [00:00<?, ?it/s]2017-06-01 18:09:30,125 root  INFO     Generating first batch)
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.40GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=1325 evicted_count=1000 eviction_rate=0.754717 and unsatisfied allocation rate=0.913892
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.40GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.34GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.07GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2017-06-01 18:09:39,452 root  INFO     step 0.000000 - time: 6.570628, loss: 3.658736, perplexity: 38.812271, precision: 0.000000, batch_len: 96.000000
Train, loss=3.65873647:   1%|          | 1/156 [00:09<24:06,  9.33s/it]2017-06-01 18:09:43,638 root  INFO     step 1.000000 - time: 3.967057, loss: 3.377748, perplexity: 29.304717, precision: 0.000000, batch_len: 113.000000
Train, loss=3.37774849:   1%|1         | 2/156 [00:13<19:59,  7.79s/it]2017-06-01 18:09:47,793 root  INFO     step 2.000000 - time: 3.993432, loss: 3.247156, perplexity: 25.717100, precision: 0.000000, batch_len: 105.000000
Train, loss=3.24715614:   2%|1         | 3/156 [00:17<17:04,  6.70s/it]2017-06-01 18:09:51,738 root  INFO     step 3.000000 - time: 3.783672, loss: 3.130171, perplexity: 22.877898, precision: 0.000000, batch_len: 102.000000
Train, loss=3.13017130:   3%|2         | 4/156 [00:21<14:52,  5.87s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1014 evicted_count=1000 eviction_rate=0.986193 and unsatisfied allocation rate=0
2017-06-01 18:09:53,280 root  INFO     step 4.000000 - time: 1.516214, loss: 3.081240, perplexity: 21.785403, precision: 0.000000, batch_len: 101.000000
Train, loss=3.08124018:   3%|3         | 5/156 [00:23<11:30,  4.57s/it]2017-06-01 18:09:59,042 root  INFO     step 5.000000 - time: 5.744881, loss: 2.973916, perplexity: 19.568391, precision: 0.000000, batch_len: 120.000000
Train, loss=2.97391558:   4%|3         | 6/156 [00:28<12:19,  4.93s/it]2017-06-01 18:10:03,261 root  INFO     step 6.000000 - time: 4.197265, loss: 2.932580, perplexity: 18.776006, precision: 0.000000, batch_len: 110.000000
Train, loss=2.93257976:   4%|4         | 7/156 [00:33<11:42,  4.72s/it]2017-06-01 18:10:07,027 root  INFO     step 7.000000 - time: 3.683721, loss: 3.018451, perplexity: 20.459580, precision: 0.000000, batch_len: 88.000000
Train, loss=3.01845121:   5%|5         | 8/156 [00:36<10:55,  4.43s/it]2017-06-01 18:10:08,630 root  INFO     step 8.000000 - time: 1.551078, loss: 2.895046, perplexity: 18.084333, precision: 0.000000, batch_len: 97.000000
Train, loss=2.89504600:   6%|5         | 9/156 [00:38<08:46,  3.58s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=2479 evicted_count=2000 eviction_rate=0.806777 and unsatisfied allocation rate=0.824723
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 233 to 256
2017-06-01 18:10:12,523 root  INFO     step 9.000000 - time: 3.884959, loss: 2.936315, perplexity: 18.846275, precision: 0.000000, batch_len: 92.000000
Train, loss=2.93631530:   6%|6         | 10/156 [00:42<08:56,  3.68s/it]2017-06-01 18:10:14,012 root  INFO     step 10.000000 - time: 1.427320, loss: 2.943811, perplexity: 18.988076, precision: 0.000000, batch_len: 93.000000
Train, loss=2.94381118:   7%|7         | 11/156 [00:43<07:17,  3.02s/it]2017-06-01 18:10:15,884 root  INFO     step 11.000000 - time: 1.739889, loss: 2.965184, perplexity: 19.398272, precision: 0.000000, batch_len: 112.000000
Train, loss=2.96518397:   8%|7         | 12/156 [00:45<06:25,  2.68s/it]2017-06-01 18:10:17,606 root  INFO     step 12.000000 - time: 1.635741, loss: 2.862488, perplexity: 17.505030, precision: 0.000000, batch_len: 114.000000
Train, loss=2.86248827:   8%|8         | 13/156 [00:47<05:41,  2.39s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1033 evicted_count=1000 eviction_rate=0.968054 and unsatisfied allocation rate=0
2017-06-01 18:10:18,986 root  INFO     step 13.000000 - time: 1.291667, loss: 2.897916, perplexity: 18.136311, precision: 0.000000, batch_len: 104.000000
Train, loss=2.89791608:   9%|8         | 14/156 [00:48<04:56,  2.09s/it]2017-06-01 18:10:20,637 root  INFO     step 14.000000 - time: 1.608884, loss: 2.815789, perplexity: 16.706352, precision: 0.000000, batch_len: 106.000000
Train, loss=2.81578898:  10%|9         | 15/156 [00:50<04:35,  1.96s/it]2017-06-01 18:10:22,502 root  INFO     step 15.000000 - time: 1.829228, loss: 2.937339, perplexity: 18.865584, precision: 0.000000, batch_len: 121.000000
Train, loss=2.93733931:  10%|#         | 16/156 [00:52<04:30,  1.93s/it]2017-06-01 18:10:27,215 root  INFO     step 16.000000 - time: 4.670224, loss: 2.976213, perplexity: 19.613399, precision: 0.000000, batch_len: 90.000000
Train, loss=2.97621298:  11%|#         | 17/156 [00:57<06:24,  2.76s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1049 evicted_count=1000 eviction_rate=0.953289 and unsatisfied allocation rate=0
2017-06-01 18:10:31,299 root  INFO     step 17.000000 - time: 4.068898, loss: 2.788867, perplexity: 16.262592, precision: 0.000000, batch_len: 81.000000
Train, loss=2.78886747:  12%|#1        | 18/156 [01:01<07:16,  3.16s/it]2017-06-01 18:10:32,877 root  INFO     step 18.000000 - time: 1.547953, loss: 2.874730, perplexity: 17.720641, precision: 0.000000, batch_len: 108.000000
Train, loss=2.87473011:  12%|#2        | 19/156 [01:02<06:07,  2.69s/it]2017-06-01 18:10:36,983 root  INFO     step 19.000000 - time: 4.095549, loss: 2.916143, perplexity: 18.469915, precision: 0.000000, batch_len: 128.000000
Train, loss=2.91614318:  13%|#2        | 20/156 [01:06<07:03,  3.11s/it]2017-06-01 18:10:38,508 root  INFO     step 20.000000 - time: 1.347597, loss: 2.976684, perplexity: 19.622637, precision: 0.000000, batch_len: 115.000000
Train, loss=2.97668386:  13%|#3        | 21/156 [01:08<05:55,  2.64s/it]2017-06-01 18:10:39,954 root  INFO     step 21.000000 - time: 1.379653, loss: 2.937307, perplexity: 18.864972, precision: 0.000000, batch_len: 111.000000
Train, loss=2.93730688:  14%|#4        | 22/156 [01:09<05:05,  2.28s/it]2017-06-01 18:10:41,357 root  INFO     step 22.000000 - time: 1.297707, loss: 2.976528, perplexity: 19.619573, precision: 0.000000, batch_len: 109.000000
Train, loss=2.97652769:  15%|#4        | 23/156 [01:11<04:28,  2.02s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=3175 evicted_count=2000 eviction_rate=0.629921 and unsatisfied allocation rate=0.580559
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 871 to 958
2017-06-01 18:10:43,019 root  INFO     step 23.000000 - time: 1.529401, loss: 2.862264, perplexity: 17.501112, precision: 0.000000, batch_len: 100.000000
Train, loss=2.86226439:  15%|#5        | 24/156 [01:12<04:12,  1.91s/it]2017-06-01 18:10:44,425 root  INFO     step 24.000000 - time: 1.256766, loss: 2.892899, perplexity: 18.045540, precision: 0.000000, batch_len: 89.000000
Train, loss=2.89289856:  16%|#6        | 25/156 [01:14<03:50,  1.76s/it]2017-06-01 18:10:48,216 root  INFO     step 25.000000 - time: 3.716770, loss: 2.893867, perplexity: 18.063033, precision: 0.000000, batch_len: 87.000000
Train, loss=2.89386749:  17%|#6        | 26/156 [01:18<05:07,  2.37s/it]2017-06-01 18:10:49,980 root  INFO     step 26.000000 - time: 1.756565, loss: 2.832815, perplexity: 16.993232, precision: 0.000000, batch_len: 91.000000
Train, loss=2.83281517:  17%|#7        | 27/156 [01:19<04:42,  2.19s/it]2017-06-01 18:10:51,328 root  INFO     step 27.000000 - time: 1.179930, loss: 2.787757, perplexity: 16.244541, precision: 0.000000, batch_len: 80.000000
Train, loss=2.78775692:  18%|#7        | 28/156 [01:21<04:07,  1.94s/it]2017-06-01 18:10:52,856 root  INFO     step 28.000000 - time: 1.490953, loss: 2.863611, perplexity: 17.524703, precision: 0.000000, batch_len: 83.000000
Train, loss=2.86361146:  19%|#8        | 29/156 [01:22<03:50,  1.81s/it]2017-06-01 18:10:54,399 root  INFO     step 29.000000 - time: 1.469677, loss: 2.894196, perplexity: 18.068964, precision: 0.000000, batch_len: 94.000000
Train, loss=2.89419580:  19%|#9        | 30/156 [01:24<03:38,  1.73s/it]2017-06-01 18:10:58,555 root  INFO     step 30.000000 - time: 4.057064, loss: 2.845891, perplexity: 17.216892, precision: 0.000000, batch_len: 79.000000
Train, loss=2.84589100:  20%|#9        | 31/156 [01:28<05:07,  2.46s/it]2017-06-01 18:11:02,534 root  INFO     step 31.000000 - time: 3.936454, loss: 2.864966, perplexity: 17.548455, precision: 0.000000, batch_len: 125.000000
Train, loss=2.86496592:  21%|##        | 32/156 [01:32<06:01,  2.92s/it]2017-06-01 18:11:04,688 root  INFO     step 32.000000 - time: 2.116826, loss: 2.816756, perplexity: 16.722523, precision: 0.000000, batch_len: 123.000000
Train, loss=2.81675649:  21%|##1       | 33/156 [01:34<05:30,  2.69s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 5746 get requests, put_count=5615 evicted_count=1000 eviction_rate=0.178094 and unsatisfied allocation rate=0.229203
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 2049 to 2253
2017-06-01 18:11:05,889 root  INFO     step 33.000000 - time: 1.179134, loss: 2.807420, perplexity: 16.567116, precision: 0.000000, batch_len: 86.000000
Train, loss=2.80741978:  22%|##1       | 34/156 [01:35<04:33,  2.24s/it]2017-06-01 18:11:07,377 root  INFO     step 34.000000 - time: 1.468483, loss: 2.902823, perplexity: 18.225532, precision: 0.000000, batch_len: 129.000000
Train, loss=2.90282345:  22%|##2       | 35/156 [01:37<04:03,  2.02s/it]2017-06-01 18:11:08,960 root  INFO     step 35.000000 - time: 1.542735, loss: 2.960981, perplexity: 19.316906, precision: 0.000000, batch_len: 98.000000
Train, loss=2.96098065:  23%|##3       | 36/156 [01:38<03:46,  1.89s/it]2017-06-01 18:11:10,858 root  INFO     step 36.000000 - time: 1.873181, loss: 2.813098, perplexity: 16.661459, precision: 0.000000, batch_len: 85.000000
Train, loss=2.81309819:  24%|##3       | 37/156 [01:40<03:44,  1.89s/it]2017-06-01 18:11:12,178 root  INFO     step 37.000000 - time: 1.310014, loss: 2.810137, perplexity: 16.612194, precision: 0.000000, batch_len: 107.000000
Train, loss=2.81013703:  24%|##4       | 38/156 [01:42<03:22,  1.72s/it]2017-06-01 18:11:16,146 root  INFO     step 38.000000 - time: 3.941623, loss: 2.783692, perplexity: 16.178640, precision: 0.000000, batch_len: 116.000000
Train, loss=2.78369188:  25%|##5       | 39/156 [01:46<04:40,  2.39s/it]2017-06-01 18:11:18,089 root  INFO     step 39.000000 - time: 1.812699, loss: 2.876042, perplexity: 17.743897, precision: 0.000000, batch_len: 124.000000
Train, loss=2.87604165:  26%|##5       | 40/156 [01:47<04:21,  2.26s/it]2017-06-01 18:11:20,016 root  INFO     step 40.000000 - time: 1.869495, loss: 2.829316, perplexity: 16.933876, precision: 0.000000, batch_len: 119.000000
Train, loss=2.82931614:  26%|##6       | 41/156 [01:49<04:08,  2.16s/it]2017-06-01 18:11:21,361 root  INFO     step 41.000000 - time: 1.285066, loss: 2.823804, perplexity: 16.840790, precision: 0.000000, batch_len: 103.000000
Train, loss=2.82380390:  27%|##6       | 42/156 [01:51<03:38,  1.91s/it]2017-06-01 18:11:22,977 root  INFO     step 42.000000 - time: 1.578981, loss: 2.879298, perplexity: 17.801771, precision: 0.000000, batch_len: 117.000000
Train, loss=2.87929797:  28%|##7       | 43/156 [01:52<03:26,  1.83s/it]2017-06-01 18:11:24,711 root  INFO     step 43.000000 - time: 1.604056, loss: 2.851882, perplexity: 17.320356, precision: 0.000000, batch_len: 99.000000
Train, loss=2.85188246:  28%|##8       | 44/156 [01:54<03:21,  1.80s/it]2017-06-01 18:11:26,027 root  INFO     step 44.000000 - time: 1.234444, loss: 2.825633, perplexity: 16.871626, precision: 0.000000, batch_len: 84.000000
Train, loss=2.82563329:  29%|##8       | 45/156 [01:55<03:03,  1.65s/it]2017-06-01 18:11:27,911 root  INFO     step 45.000000 - time: 1.802429, loss: 2.795165, perplexity: 16.365330, precision: 0.000000, batch_len: 130.000000
Train, loss=2.79516506:  29%|##9       | 46/156 [01:57<03:09,  1.72s/it]2017-06-01 18:11:32,529 root  INFO     step 46.000000 - time: 4.490998, loss: 2.809700, perplexity: 16.604928, precision: 0.000000, batch_len: 135.000000
Train, loss=2.80969954:  30%|###       | 47/156 [02:02<04:42,  2.59s/it]2017-06-01 18:11:34,552 root  INFO     step 47.000000 - time: 1.990420, loss: 2.842194, perplexity: 17.153364, precision: 0.000000, batch_len: 126.000000
Train, loss=2.84219432:  31%|###       | 48/156 [02:04<04:21,  2.42s/it]2017-06-01 18:11:36,292 root  INFO     step 48.000000 - time: 1.620381, loss: 2.866966, perplexity: 17.583585, precision: 0.000000, batch_len: 118.000000
Train, loss=2.86696577:  31%|###1      | 49/156 [02:06<03:57,  2.22s/it]2017-06-01 18:11:41,031 root  INFO     step 49.000000 - time: 4.718122, loss: 2.797898, perplexity: 16.410121, precision: 0.000000, batch_len: 144.000000
Train, loss=2.79789829:  32%|###2      | 50/156 [02:10<05:15,  2.97s/it]2017-06-01 18:11:45,363 root  INFO     step 50.000000 - time: 4.278064, loss: 2.782912, perplexity: 16.166020, precision: 0.000000, batch_len: 136.000000
Train, loss=2.78291154:  33%|###2      | 51/156 [02:15<05:54,  3.38s/it]2017-06-01 18:11:47,562 root  INFO     step 51.000000 - time: 2.102996, loss: 2.802453, perplexity: 16.485036, precision: 0.000000, batch_len: 133.000000
Train, loss=2.80245304:  33%|###3      | 52/156 [02:17<05:14,  3.03s/it]2017-06-01 18:11:49,395 root  INFO     step 52.000000 - time: 1.608300, loss: 2.830874, perplexity: 16.960277, precision: 0.000000, batch_len: 137.000000
Train, loss=2.83087397:  34%|###3      | 53/156 [02:19<04:34,  2.67s/it]2017-06-01 18:11:50,613 root  INFO     step 53.000000 - time: 1.147074, loss: 2.832755, perplexity: 16.992211, precision: 0.000000, batch_len: 82.000000
Train, loss=2.83275509:  35%|###4      | 54/156 [02:20<03:47,  2.23s/it]2017-06-01 18:11:51,851 root  INFO     step 54.000000 - time: 1.099310, loss: 2.800206, perplexity: 16.448034, precision: 0.000000, batch_len: 78.000000
Train, loss=2.80020595:  35%|###5      | 55/156 [02:21<03:15,  1.93s/it]2017-06-01 18:11:53,241 root  INFO     step 55.000000 - time: 1.314034, loss: 2.821737, perplexity: 16.806022, precision: 0.000000, batch_len: 96.000000
Train, loss=2.82173729:  36%|###5      | 56/156 [02:23<02:57,  1.77s/it]2017-06-01 18:11:58,239 root  INFO     step 56.000000 - time: 4.991200, loss: 3.015653, perplexity: 20.402402, precision: 0.000000, batch_len: 152.000000
Train, loss=3.01565266:  37%|###6      | 57/156 [02:28<04:31,  2.74s/it]2017-06-01 18:12:00,244 root  INFO     step 57.000000 - time: 1.834451, loss: 2.977879, perplexity: 19.646095, precision: 0.000000, batch_len: 122.000000
Train, loss=2.97787857:  37%|###7      | 58/156 [02:30<04:06,  2.52s/it]2017-06-01 18:12:02,242 root  INFO     step 58.000000 - time: 1.898985, loss: 2.785411, perplexity: 16.206483, precision: 0.000000, batch_len: 138.000000
Train, loss=2.78541136:  38%|###7      | 59/156 [02:32<03:49,  2.36s/it]2017-06-01 18:12:06,178 root  INFO     step 59.000000 - time: 3.801586, loss: 2.884882, perplexity: 17.901463, precision: 0.000000, batch_len: 72.000000
Train, loss=2.88488245:  38%|###8      | 60/156 [02:36<04:32,  2.83s/it]2017-06-01 18:12:08,159 root  INFO     step 60.000000 - time: 1.825649, loss: 2.883917, perplexity: 17.884190, precision: 0.000000, batch_len: 139.000000
Train, loss=2.88391709:  39%|###9      | 61/156 [02:38<04:04,  2.58s/it]2017-06-01 18:12:09,660 root  INFO     step 61.000000 - time: 1.436605, loss: 2.944209, perplexity: 18.995624, precision: 0.000000, batch_len: 74.000000
Train, loss=2.94420862:  40%|###9      | 62/156 [02:39<03:31,  2.26s/it]2017-06-01 18:12:11,304 root  INFO     step 62.000000 - time: 1.502124, loss: 2.812398, perplexity: 16.649804, precision: 0.000000, batch_len: 134.000000
Train, loss=2.81239843:  40%|####      | 63/156 [02:41<03:12,  2.07s/it]2017-06-01 18:12:12,707 root  INFO     step 63.000000 - time: 1.373826, loss: 2.881858, perplexity: 17.847409, precision: 0.000000, batch_len: 77.000000
Train, loss=2.88185835:  41%|####1     | 64/156 [02:42<02:52,  1.87s/it]2017-06-01 18:12:14,244 root  INFO     step 64.000000 - time: 1.490712, loss: 2.797484, perplexity: 16.403331, precision: 0.000000, batch_len: 76.000000
Train, loss=2.79748440:  42%|####1     | 65/156 [02:44<02:41,  1.77s/it]2017-06-01 18:12:18,641 root  INFO     step 65.000000 - time: 4.355527, loss: 2.814657, perplexity: 16.687447, precision: 0.000000, batch_len: 141.000000
Train, loss=2.81465673:  42%|####2     | 66/156 [02:48<03:50,  2.56s/it]2017-06-01 18:12:20,347 root  INFO     step 66.000000 - time: 1.615227, loss: 2.819206, perplexity: 16.763535, precision: 0.000000, batch_len: 132.000000
Train, loss=2.81920600:  43%|####2     | 67/156 [02:50<03:24,  2.30s/it]2017-06-01 18:12:25,483 root  INFO     step 67.000000 - time: 5.024771, loss: 2.880313, perplexity: 17.819848, precision: 0.000000, batch_len: 150.000000
Train, loss=2.88031292:  44%|####3     | 68/156 [02:55<04:37,  3.15s/it]2017-06-01 18:12:29,185 root  INFO     step 68.000000 - time: 3.604049, loss: 2.952259, perplexity: 19.149155, precision: 0.000000, batch_len: 71.000000
Train, loss=2.95225859:  44%|####4     | 69/156 [02:59<04:48,  3.32s/it]2017-06-01 18:12:31,020 root  INFO     step 69.000000 - time: 1.771774, loss: 2.793271, perplexity: 16.334363, precision: 0.000000, batch_len: 131.000000
Train, loss=2.79327106:  45%|####4     | 70/156 [03:00<04:07,  2.87s/it]2017-06-01 18:12:32,928 root  INFO     step 70.000000 - time: 1.809136, loss: 2.848065, perplexity: 17.254369, precision: 0.000000, batch_len: 142.000000
Train, loss=2.84806538:  46%|####5     | 71/156 [03:02<03:39,  2.58s/it]2017-06-01 18:12:33,083 root  INFO     Generating first batch)
2017-06-01 18:12:37,128 root  INFO     step 71.000000 - time: 1.100150, loss: 2.836850, perplexity: 17.061930, precision: 0.000000, batch_len: 113.000000
Train, loss=2.83684969:  46%|####6     | 72/156 [03:07<04:17,  3.07s/it]2017-06-01 18:12:38,232 root  INFO     step 72.000000 - time: 1.070946, loss: 2.794477, perplexity: 16.354081, precision: 0.000000, batch_len: 96.000000
Train, loss=2.79447746:  47%|####6     | 73/156 [03:08<03:25,  2.48s/it]2017-06-01 18:12:39,560 root  INFO     step 73.000000 - time: 1.210277, loss: 2.785365, perplexity: 16.205726, precision: 0.000000, batch_len: 112.000000
Train, loss=2.78536463:  47%|####7     | 74/156 [03:09<02:54,  2.13s/it]2017-06-01 18:12:40,808 root  INFO     step 74.000000 - time: 1.223871, loss: 2.863329, perplexity: 17.519760, precision: 0.000000, batch_len: 108.000000
Train, loss=2.86332941:  48%|####8     | 75/156 [03:10<02:31,  1.87s/it]2017-06-01 18:12:42,207 root  INFO     step 75.000000 - time: 1.182021, loss: 2.814130, perplexity: 16.678660, precision: 0.000000, batch_len: 101.000000
Train, loss=2.81413007:  49%|####8     | 76/156 [03:12<02:18,  1.73s/it]2017-06-01 18:12:43,345 root  INFO     step 76.000000 - time: 1.074628, loss: 2.822481, perplexity: 16.818532, precision: 0.000000, batch_len: 92.000000
Train, loss=2.82248139:  49%|####9     | 77/156 [03:13<02:02,  1.55s/it]2017-06-01 18:12:44,474 root  INFO     step 77.000000 - time: 1.052107, loss: 2.820401, perplexity: 16.783575, precision: 0.000000, batch_len: 105.000000
Train, loss=2.82040071:  50%|#####     | 78/156 [03:14<01:51,  1.42s/it]2017-06-01 18:12:45,652 root  INFO     step 78.000000 - time: 1.090561, loss: 2.738092, perplexity: 15.457463, precision: 0.000000, batch_len: 106.000000
Train, loss=2.73809195:  51%|#####     | 79/156 [03:15<01:43,  1.35s/it]2017-06-01 18:12:46,616 root  INFO     step 79.000000 - time: 0.922846, loss: 2.788949, perplexity: 16.263925, precision: 0.000000, batch_len: 100.000000
Train, loss=2.78894949:  51%|#####1    | 80/156 [03:16<01:33,  1.23s/it]2017-06-01 18:12:48,130 root  INFO     step 80.000000 - time: 1.458888, loss: 2.801043, perplexity: 16.461812, precision: 0.000000, batch_len: 117.000000
Train, loss=2.80104327:  52%|#####1    | 81/156 [03:18<01:38,  1.32s/it]2017-06-01 18:12:49,472 root  INFO     step 81.000000 - time: 1.253347, loss: 2.785195, perplexity: 16.202975, precision: 0.000000, batch_len: 110.000000
Train, loss=2.78519487:  53%|#####2    | 82/156 [03:19<01:38,  1.33s/it]2017-06-01 18:12:50,548 root  INFO     step 82.000000 - time: 1.019808, loss: 2.772189, perplexity: 15.993604, precision: 0.000000, batch_len: 104.000000
Train, loss=2.77218890:  53%|#####3    | 83/156 [03:20<01:31,  1.25s/it]2017-06-01 18:12:51,903 root  INFO     step 83.000000 - time: 1.339302, loss: 2.782858, perplexity: 16.165161, precision: 0.000000, batch_len: 120.000000
Train, loss=2.78285837:  54%|#####3    | 84/156 [03:21<01:32,  1.28s/it]2017-06-01 18:12:53,072 root  INFO     step 84.000000 - time: 1.033682, loss: 2.802716, perplexity: 16.489367, precision: 0.000000, batch_len: 102.000000
Train, loss=2.80271578:  54%|#####4    | 85/156 [03:22<01:28,  1.25s/it]2017-06-01 18:12:54,255 root  INFO     step 85.000000 - time: 1.175473, loss: 2.797534, perplexity: 16.404152, precision: 0.000000, batch_len: 88.000000
Train, loss=2.79753447:  55%|#####5    | 86/156 [03:24<01:25,  1.23s/it]2017-06-01 18:12:55,608 root  INFO     step 86.000000 - time: 1.188924, loss: 2.791874, perplexity: 16.311566, precision: 0.000000, batch_len: 90.000000
Train, loss=2.79187441:  56%|#####5    | 87/156 [03:25<01:27,  1.27s/it]2017-06-01 18:12:57,014 root  INFO     step 87.000000 - time: 1.354734, loss: 2.824625, perplexity: 16.854616, precision: 0.000000, batch_len: 128.000000
Train, loss=2.82462454:  56%|#####6    | 88/156 [03:26<01:28,  1.31s/it]2017-06-01 18:12:58,492 root  INFO     step 88.000000 - time: 1.315431, loss: 2.806908, perplexity: 16.558642, precision: 0.000000, batch_len: 121.000000
Train, loss=2.80690813:  57%|#####7    | 89/156 [03:28<01:31,  1.36s/it]2017-06-01 18:12:59,449 root  INFO     step 89.000000 - time: 0.893648, loss: 2.735064, perplexity: 15.410723, precision: 0.000000, batch_len: 81.000000
Train, loss=2.73506355:  58%|#####7    | 90/156 [03:29<01:21,  1.24s/it]2017-06-01 18:13:00,728 root  INFO     step 90.000000 - time: 1.266385, loss: 2.747035, perplexity: 15.596324, precision: 0.000000, batch_len: 93.000000
Train, loss=2.74703526:  58%|#####8    | 91/156 [03:30<01:21,  1.25s/it]2017-06-01 18:13:02,044 root  INFO     step 91.000000 - time: 1.278371, loss: 2.804646, perplexity: 16.521231, precision: 0.000000, batch_len: 111.000000
Train, loss=2.80464625:  59%|#####8    | 92/156 [03:31<01:21,  1.27s/it]2017-06-01 18:13:03,216 root  INFO     step 92.000000 - time: 1.122886, loss: 2.837614, perplexity: 17.074981, precision: 0.000000, batch_len: 114.000000
Train, loss=2.83761430:  60%|#####9    | 93/156 [03:33<01:18,  1.24s/it]2017-06-01 18:13:04,193 root  INFO     step 93.000000 - time: 0.939401, loss: 2.865936, perplexity: 17.565492, precision: 0.000000, batch_len: 80.000000
Train, loss=2.86593628:  60%|######    | 94/156 [03:34<01:12,  1.16s/it]2017-06-01 18:13:05,220 root  INFO     step 94.000000 - time: 1.016178, loss: 2.777381, perplexity: 16.076867, precision: 0.000000, batch_len: 89.000000
Train, loss=2.77738142:  61%|######    | 95/156 [03:35<01:08,  1.12s/it]2017-06-01 18:13:06,248 root  INFO     step 95.000000 - time: 0.988372, loss: 2.731751, perplexity: 15.359762, precision: 0.000000, batch_len: 107.000000
Train, loss=2.73175120:  62%|######1   | 96/156 [03:36<01:05,  1.09s/it]2017-06-01 18:13:07,763 root  INFO     step 96.000000 - time: 1.506833, loss: 2.841990, perplexity: 17.149852, precision: 0.000000, batch_len: 123.000000
Train, loss=2.84198952:  62%|######2   | 97/156 [03:37<01:11,  1.22s/it]2017-06-01 18:13:09,037 root  INFO     step 97.000000 - time: 1.264435, loss: 2.901526, perplexity: 18.201908, precision: 0.000000, batch_len: 97.000000
Train, loss=2.90152645:  63%|######2   | 98/156 [03:38<01:11,  1.24s/it]2017-06-01 18:13:10,160 root  INFO     step 98.000000 - time: 1.039845, loss: 2.825119, perplexity: 16.862952, precision: 0.000000, batch_len: 79.000000
Train, loss=2.82511902:  63%|######3   | 99/156 [03:40<01:08,  1.20s/it]2017-06-01 18:13:11,241 root  INFO     step 99.000000 - time: 1.041864, loss: 2.785333, perplexity: 16.205208, precision: 0.000000, batch_len: 85.000000
Train, loss=2.78533268:  64%|######4   | 100/156 [03:41<01:05,  1.17s/it]2017-06-01 18:13:12,561 root  INFO     step 100.000000 - time: 1.274974, loss: 2.749434, perplexity: 15.633784, precision: 0.000000, batch_len: 91.000000
Train, loss=2.74943423:  65%|######4   | 101/156 [03:42<01:06,  1.21s/it]2017-06-01 18:13:13,691 root  INFO     step 101.000000 - time: 1.110339, loss: 2.831369, perplexity: 16.968682, precision: 0.000000, batch_len: 115.000000
Train, loss=2.83136940:  65%|######5   | 102/156 [03:43<01:04,  1.19s/it]2017-06-01 18:13:15,390 root  INFO     step 102.000000 - time: 1.601379, loss: 2.797224, perplexity: 16.399053, precision: 0.000000, batch_len: 124.000000
Train, loss=2.79722357:  66%|######6   | 103/156 [03:45<01:11,  1.34s/it]2017-06-01 18:13:16,566 root  INFO     step 103.000000 - time: 1.106358, loss: 2.819810, perplexity: 16.773670, precision: 0.000000, batch_len: 109.000000
Train, loss=2.81981039:  67%|######6   | 104/156 [03:46<01:07,  1.29s/it]2017-06-01 18:13:17,818 root  INFO     step 104.000000 - time: 1.036928, loss: 2.779209, perplexity: 16.106282, precision: 0.000000, batch_len: 94.000000
Train, loss=2.77920938:  67%|######7   | 105/156 [03:47<01:05,  1.28s/it]2017-06-01 18:13:18,822 root  INFO     step 105.000000 - time: 0.932711, loss: 2.754479, perplexity: 15.712859, precision: 0.000000, batch_len: 86.000000
Train, loss=2.75447941:  68%|######7   | 106/156 [03:48<00:59,  1.20s/it]2017-06-01 18:13:20,071 root  INFO     step 106.000000 - time: 1.219019, loss: 2.838138, perplexity: 17.083923, precision: 0.000000, batch_len: 129.000000
Train, loss=2.83813787:  69%|######8   | 107/156 [03:49<00:59,  1.21s/it]2017-06-01 18:13:21,793 root  INFO     step 107.000000 - time: 1.633147, loss: 2.823261, perplexity: 16.831654, precision: 0.000000, batch_len: 125.000000
Train, loss=2.82326126:  69%|######9   | 108/156 [03:51<01:05,  1.37s/it]2017-06-01 18:13:23,075 root  INFO     step 108.000000 - time: 1.092225, loss: 2.747358, perplexity: 15.601364, precision: 0.000000, batch_len: 103.000000
Train, loss=2.74735832:  70%|######9   | 109/156 [03:52<01:02,  1.34s/it]2017-06-01 18:13:24,199 root  INFO     step 109.000000 - time: 1.109495, loss: 2.802874, perplexity: 16.491982, precision: 0.000000, batch_len: 99.000000
Train, loss=2.80287433:  71%|#######   | 110/156 [03:54<00:58,  1.28s/it]2017-06-01 18:13:25,172 root  INFO     step 110.000000 - time: 0.915008, loss: 2.802752, perplexity: 16.489957, precision: 0.000000, batch_len: 83.000000
Train, loss=2.80275154:  71%|#######1  | 111/156 [03:55<00:53,  1.18s/it]2017-06-01 18:13:26,859 root  INFO     step 111.000000 - time: 1.454384, loss: 2.792808, perplexity: 16.326802, precision: 0.000000, batch_len: 137.000000
Train, loss=2.79280806:  72%|#######1  | 112/156 [03:56<00:58,  1.34s/it]2017-06-01 18:13:28,401 root  INFO     step 112.000000 - time: 1.403973, loss: 2.758553, perplexity: 15.776998, precision: 0.000000, batch_len: 119.000000
Train, loss=2.75855303:  72%|#######2  | 113/156 [03:58<01:00,  1.40s/it]2017-06-01 18:13:30,163 root  INFO     step 113.000000 - time: 1.751559, loss: 2.781668, perplexity: 16.145933, precision: 0.000000, batch_len: 126.000000
Train, loss=2.78166819:  73%|#######3  | 114/156 [04:00<01:03,  1.51s/it]2017-06-01 18:13:31,285 root  INFO     step 114.000000 - time: 0.995152, loss: 2.788954, perplexity: 16.264003, precision: 0.000000, batch_len: 78.000000
Train, loss=2.78895426:  74%|#######3  | 115/156 [04:01<00:57,  1.39s/it]2017-06-01 18:13:32,689 root  INFO     step 115.000000 - time: 1.339757, loss: 2.740412, perplexity: 15.493360, precision: 0.000000, batch_len: 116.000000
Train, loss=2.74041152:  74%|#######4  | 116/156 [04:02<00:55,  1.40s/it]2017-06-01 18:13:34,060 root  INFO     step 116.000000 - time: 1.286114, loss: 2.753966, perplexity: 15.704791, precision: 0.000000, batch_len: 135.000000
Train, loss=2.75396585:  75%|#######5  | 117/156 [04:03<00:54,  1.39s/it]2017-06-01 18:13:35,589 root  INFO     step 117.000000 - time: 1.510274, loss: 2.784696, perplexity: 16.194888, precision: 0.000000, batch_len: 144.000000
Train, loss=2.78469563:  76%|#######5  | 118/156 [04:05<00:54,  1.43s/it]2017-06-01 18:13:36,959 root  INFO     step 118.000000 - time: 1.312722, loss: 2.834373, perplexity: 17.019726, precision: 0.000000, batch_len: 98.000000
Train, loss=2.83437300:  76%|#######6  | 119/156 [04:06<00:52,  1.41s/it]2017-06-01 18:13:37,980 root  INFO     step 119.000000 - time: 0.962151, loss: 2.792942, perplexity: 16.328982, precision: 0.000000, batch_len: 87.000000
Train, loss=2.79294157:  77%|#######6  | 120/156 [04:07<00:46,  1.29s/it]2017-06-01 18:13:39,034 root  INFO     step 120.000000 - time: 1.013767, loss: 2.744959, perplexity: 15.563974, precision: 0.000000, batch_len: 84.000000
Train, loss=2.74495888:  78%|#######7  | 121/156 [04:08<00:42,  1.22s/it]2017-06-01 18:13:40,537 root  INFO     step 121.000000 - time: 1.410659, loss: 2.718235, perplexity: 15.153553, precision: 0.000000, batch_len: 136.000000
Train, loss=2.71823502:  78%|#######8  | 122/156 [04:10<00:44,  1.31s/it]2017-06-01 18:13:41,778 root  INFO     step 122.000000 - time: 1.227216, loss: 2.806587, perplexity: 16.553321, precision: 0.000000, batch_len: 118.000000
Train, loss=2.80658674:  79%|#######8  | 123/156 [04:11<00:42,  1.29s/it]2017-06-01 18:13:43,616 root  INFO     step 123.000000 - time: 1.784355, loss: 2.747098, perplexity: 15.597310, precision: 0.000000, batch_len: 138.000000
Train, loss=2.74709845:  79%|#######9  | 124/156 [04:13<00:46,  1.45s/it]2017-06-01 18:13:45,187 root  INFO     step 124.000000 - time: 1.398194, loss: 2.767617, perplexity: 15.920646, precision: 0.000000, batch_len: 130.000000
Train, loss=2.76761675:  80%|########  | 125/156 [04:15<00:46,  1.49s/it]2017-06-01 18:13:46,636 root  INFO     step 125.000000 - time: 1.336183, loss: 2.751103, perplexity: 15.659894, precision: 0.000000, batch_len: 133.000000
Train, loss=2.75110292:  81%|########  | 126/156 [04:16<00:44,  1.48s/it]2017-06-01 18:13:47,600 root  INFO     step 126.000000 - time: 0.861952, loss: 2.822641, perplexity: 16.821219, precision: 0.000000, batch_len: 82.000000
Train, loss=2.82264113:  81%|########1 | 127/156 [04:17<00:38,  1.32s/it]2017-06-01 18:13:48,926 root  INFO     step 127.000000 - time: 1.242054, loss: 2.754013, perplexity: 15.705529, precision: 0.000000, batch_len: 96.000000
Train, loss=2.75401282:  82%|########2 | 128/156 [04:18<00:37,  1.32s/it]2017-06-01 18:13:50,530 root  INFO     step 128.000000 - time: 1.483428, loss: 2.846825, perplexity: 17.232974, precision: 0.000000, batch_len: 122.000000
Train, loss=2.84682465:  83%|########2 | 129/156 [04:20<00:38,  1.41s/it]2017-06-01 18:13:51,889 root  INFO     step 129.000000 - time: 1.352664, loss: 2.749502, perplexity: 15.634839, precision: 0.000000, batch_len: 134.000000
Train, loss=2.74950171:  83%|########3 | 130/156 [04:21<00:36,  1.39s/it]2017-06-01 18:13:53,791 root  INFO     step 130.000000 - time: 1.459379, loss: 2.737942, perplexity: 15.455153, precision: 0.000000, batch_len: 139.000000
Train, loss=2.73794246:  84%|########3 | 131/156 [04:23<00:38,  1.55s/it]2017-06-01 18:13:55,265 root  INFO     step 131.000000 - time: 1.466085, loss: 2.752305, perplexity: 15.678723, precision: 0.000000, batch_len: 141.000000
Train, loss=2.75230455:  85%|########4 | 132/156 [04:25<00:36,  1.52s/it]2017-06-01 18:13:57,412 root  INFO     step 132.000000 - time: 2.000996, loss: 2.917670, perplexity: 18.498146, precision: 0.000000, batch_len: 150.000000
Train, loss=2.91767049:  85%|########5 | 133/156 [04:27<00:39,  1.71s/it]2017-06-01 18:13:58,381 root  INFO     step 133.000000 - time: 0.961023, loss: 3.079122, perplexity: 21.739298, precision: 0.000000, batch_len: 74.000000
Train, loss=3.07912159:  86%|########5 | 134/156 [04:28<00:32,  1.49s/it]2017-06-01 18:13:59,722 root  INFO     step 134.000000 - time: 1.323817, loss: 2.749107, perplexity: 15.628664, precision: 0.000000, batch_len: 132.000000
Train, loss=2.74910665:  87%|########6 | 135/156 [04:29<00:30,  1.44s/it]2017-06-01 18:14:00,631 root  INFO     step 135.000000 - time: 0.892794, loss: 2.785925, perplexity: 16.214808, precision: 0.000000, batch_len: 76.000000
Train, loss=2.78592491:  87%|########7 | 136/156 [04:30<00:25,  1.28s/it]2017-06-01 18:14:02,042 root  INFO     step 136.000000 - time: 1.399453, loss: 2.854197, perplexity: 17.360483, precision: 0.000000, batch_len: 142.000000
Train, loss=2.85419655:  88%|########7 | 137/156 [04:31<00:25,  1.32s/it]2017-06-01 18:14:03,350 root  INFO     step 137.000000 - time: 1.224453, loss: 2.820158, perplexity: 16.779510, precision: 0.000000, batch_len: 77.000000
Train, loss=2.82015848:  88%|########8 | 138/156 [04:33<00:23,  1.32s/it]2017-06-01 18:14:05,299 root  INFO     step 138.000000 - time: 1.918427, loss: 3.029132, perplexity: 20.679283, precision: 0.000000, batch_len: 152.000000
Train, loss=3.02913237:  89%|########9 | 139/156 [04:35<00:25,  1.51s/it]2017-06-01 18:14:06,624 root  INFO     step 139.000000 - time: 1.173864, loss: 2.993760, perplexity: 19.960596, precision: 0.000000, batch_len: 72.000000
Train, loss=2.99376011:  90%|########9 | 140/156 [04:36<00:23,  1.45s/it]2017-06-01 18:14:07,917 root  INFO     step 140.000000 - time: 1.273926, loss: 2.760031, perplexity: 15.800340, precision: 0.000000, batch_len: 131.000000
Train, loss=2.76003146:  90%|######### | 141/156 [04:37<00:21,  1.40s/it]2017-06-01 18:14:09,080 root  INFO     step 141.000000 - time: 1.039109, loss: 2.743459, perplexity: 15.540651, precision: 0.000000, batch_len: 71.000000
Train, loss=2.74345922:  91%|#########1| 142/156 [04:38<00:18,  1.33s/it]2017-06-01 18:14:09,282 root  INFO     Generating first batch)
2017-06-01 18:14:12,932 root  INFO     step 142.000000 - time: 1.000350, loss: 2.722647, perplexity: 15.220561, precision: 0.000000, batch_len: 96.000000
Train, loss=2.72264719:  92%|#########1| 143/156 [04:42<00:27,  2.09s/it]2017-06-01 18:14:14,397 root  INFO     step 143.000000 - time: 1.185435, loss: 2.769300, perplexity: 15.947463, precision: 0.000000, batch_len: 88.000000
Train, loss=2.76929975:  92%|#########2| 144/156 [04:44<00:22,  1.90s/it]2017-06-01 18:14:15,525 root  INFO     step 144.000000 - time: 1.098262, loss: 2.721106, perplexity: 15.197125, precision: 0.000000, batch_len: 101.000000
Train, loss=2.72110629:  93%|#########2| 145/156 [04:45<00:18,  1.67s/it]2017-06-01 18:14:16,706 root  INFO     step 145.000000 - time: 1.105501, loss: 2.723041, perplexity: 15.226557, precision: 0.000000, batch_len: 108.000000
Train, loss=2.72304106:  94%|#########3| 146/156 [04:46<00:15,  1.52s/it]2017-06-01 18:14:17,910 root  INFO     step 146.000000 - time: 1.071479, loss: 2.736568, perplexity: 15.433932, precision: 0.000000, batch_len: 106.000000
Train, loss=2.73656845:  94%|#########4| 147/156 [04:47<00:12,  1.43s/it]2017-06-01 18:14:18,890 root  INFO     step 147.000000 - time: 0.938760, loss: 2.746820, perplexity: 15.592971, precision: 0.000000, batch_len: 105.000000
Train, loss=2.74682021:  95%|#########4| 148/156 [04:48<00:10,  1.29s/it]2017-06-01 18:14:20,098 root  INFO     step 148.000000 - time: 1.201650, loss: 2.745313, perplexity: 15.569489, precision: 0.000000, batch_len: 110.000000
Train, loss=2.74531317:  96%|#########5| 149/156 [04:49<00:08,  1.27s/it]2017-06-01 18:14:21,742 root  INFO     step 149.000000 - time: 1.515264, loss: 2.852516, perplexity: 17.331336, precision: 0.000000, batch_len: 128.000000
Train, loss=2.85251617:  96%|#########6| 150/156 [04:51<00:08,  1.38s/it]2017-06-01 18:14:22,937 root  INFO     step 150.000000 - time: 1.036045, loss: 2.804779, perplexity: 16.523417, precision: 0.000000, batch_len: 92.000000
Train, loss=2.80477858:  97%|#########6| 151/156 [04:52<00:06,  1.32s/it]2017-06-01 18:14:24,079 root  INFO     step 151.000000 - time: 1.131314, loss: 2.768288, perplexity: 15.931335, precision: 0.000000, batch_len: 113.000000
Train, loss=2.76828790:  97%|#########7| 152/156 [04:53<00:05,  1.27s/it]2017-06-01 18:14:25,275 root  INFO     step 152.000000 - time: 1.142113, loss: 2.737554, perplexity: 15.449144, precision: 0.000000, batch_len: 97.000000
Train, loss=2.73755360:  98%|#########8| 153/156 [04:55<00:03,  1.25s/it]2017-06-01 18:14:26,246 root  INFO     step 153.000000 - time: 0.967536, loss: 2.793458, perplexity: 16.337413, precision: 0.000000, batch_len: 111.000000
Train, loss=2.79345775:  99%|#########8| 154/156 [04:56<00:02,  1.16s/it]2017-06-01 18:14:27,574 root  INFO     step 154.000000 - time: 1.286690, loss: 2.757880, perplexity: 15.766379, precision: 0.000000, batch_len: 104.000000
Train, loss=2.75787973:  99%|#########9| 155/156 [04:57<00:01,  1.21s/it]2017-06-01 18:14:28,906 root  INFO     step 155.000000 - time: 1.270016, loss: 2.757637, perplexity: 15.762545, precision: 0.000000, batch_len: 109.000000
Train, loss=2.75763655: 100%|##########| 156/156 [04:58<00:00,  1.25s/it]2017-06-01 18:14:30,474 root  INFO     step 156.000000 - time: 1.498426, loss: 2.753147, perplexity: 15.691942, precision: 0.000000, batch_len: 120.000000
Train, loss=2.75314736: 157it [05:00,  1.34s/it]                         2017-06-01 18:14:31,793 root  INFO     step 157.000000 - time: 1.292327, loss: 2.747699, perplexity: 15.606676, precision: 0.000000, batch_len: 91.000000
Train, loss=2.74769878: 158it [05:01,  1.34s/it]2017-06-01 18:14:32,932 root  INFO     step 158.000000 - time: 1.099802, loss: 2.811049, perplexity: 16.627351, precision: 0.000000, batch_len: 114.000000
Train, loss=2.81104898: 159it [05:02,  1.28s/it]2017-06-01 18:14:33,964 root  INFO     step 159.000000 - time: 0.995713, loss: 2.774989, perplexity: 16.038456, precision: 0.000000, batch_len: 90.000000
Train, loss=2.77498937: 160it [05:03,  1.20s/it]2017-06-01 18:14:35,503 root  INFO     step 160.000000 - time: 1.485206, loss: 2.760436, perplexity: 15.806738, precision: 0.000000, batch_len: 117.000000
Train, loss=2.76043630: 161it [05:05,  1.30s/it]2017-06-01 18:14:36,834 root  INFO     step 161.000000 - time: 1.299388, loss: 2.727771, perplexity: 15.298752, precision: 0.000000, batch_len: 102.000000
Train, loss=2.72777128: 162it [05:06,  1.31s/it]2017-06-01 18:14:38,353 root  INFO     step 162.000000 - time: 1.347383, loss: 2.718817, perplexity: 15.162371, precision: 0.000000, batch_len: 112.000000
Train, loss=2.71881676: 163it [05:08,  1.37s/it]2017-06-01 18:14:39,508 root  INFO     step 163.000000 - time: 1.004235, loss: 2.768197, perplexity: 15.929887, precision: 0.000000, batch_len: 100.000000
Train, loss=2.76819706: 164it [05:09,  1.31s/it]2017-06-01 18:14:40,554 root  INFO     step 164.000000 - time: 0.970538, loss: 2.755744, perplexity: 15.732749, precision: 0.000000, batch_len: 89.000000
Train, loss=2.75574446: 165it [05:10,  1.23s/it]2017-06-01 18:14:41,783 root  INFO     step 165.000000 - time: 1.121645, loss: 2.731733, perplexity: 15.359487, precision: 0.000000, batch_len: 93.000000
Train, loss=2.73173332: 166it [05:11,  1.23s/it]2017-06-01 18:14:43,184 root  INFO     step 166.000000 - time: 1.203774, loss: 2.733867, perplexity: 15.392300, precision: 0.000000, batch_len: 81.000000
Train, loss=2.73386741: 167it [05:13,  1.28s/it]2017-06-01 18:14:44,248 root  INFO     step 167.000000 - time: 1.055077, loss: 2.709132, perplexity: 15.016232, precision: 0.000000, batch_len: 103.000000
Train, loss=2.70913172: 168it [05:14,  1.22s/it]2017-06-01 18:14:45,714 root  INFO     step 168.000000 - time: 1.329985, loss: 2.827914, perplexity: 16.910145, precision: 0.000000, batch_len: 121.000000
Train, loss=2.82791376: 169it [05:15,  1.29s/it]2017-06-01 18:14:46,885 root  INFO     step 169.000000 - time: 1.103271, loss: 2.764838, perplexity: 15.876464, precision: 0.000000, batch_len: 107.000000
Train, loss=2.76483774: 170it [05:16,  1.26s/it]2017-06-01 18:14:47,956 root  INFO     step 170.000000 - time: 1.035621, loss: 2.781709, perplexity: 16.146587, precision: 0.000000, batch_len: 115.000000
Train, loss=2.78170872: 171it [05:17,  1.20s/it]2017-06-01 18:14:49,614 root  INFO     step 171.000000 - time: 1.643541, loss: 2.769621, perplexity: 15.952593, precision: 0.000000, batch_len: 123.000000
Train, loss=2.76962137: 172it [05:19,  1.34s/it]2017-06-01 18:14:50,995 root  INFO     step 172.000000 - time: 1.374904, loss: 2.695033, perplexity: 14.806012, precision: 0.000000, batch_len: 116.000000
Train, loss=2.69503331: 173it [05:20,  1.35s/it]2017-06-01 18:14:52,331 root  INFO     step 173.000000 - time: 1.308318, loss: 2.788665, perplexity: 16.259296, precision: 0.000000, batch_len: 129.000000
Train, loss=2.78866482: 174it [05:22,  1.35s/it]2017-06-01 18:14:53,660 root  INFO     step 174.000000 - time: 1.307868, loss: 2.715892, perplexity: 15.118094, precision: 0.000000, batch_len: 124.000000
Train, loss=2.71589231: 175it [05:23,  1.34s/it]2017-06-01 18:14:54,739 root  INFO     step 175.000000 - time: 0.939128, loss: 2.786445, perplexity: 16.223246, precision: 0.000000, batch_len: 83.000000
Train, loss=2.78644514: 176it [05:24,  1.26s/it]2017-06-01 18:14:55,963 root  INFO     step 176.000000 - time: 1.193471, loss: 2.656629, perplexity: 14.248172, precision: 0.000000, batch_len: 80.000000
Train, loss=2.65662861: 177it [05:25,  1.25s/it]2017-06-01 18:14:57,141 root  INFO     step 177.000000 - time: 1.153046, loss: 2.725436, perplexity: 15.263074, precision: 0.000000, batch_len: 86.000000
Train, loss=2.72543645: 178it [05:27,  1.23s/it]2017-06-01 18:14:58,751 root  INFO     step 178.000000 - time: 1.438556, loss: 2.789805, perplexity: 16.277848, precision: 0.000000, batch_len: 137.000000
Train, loss=2.78980517: 179it [05:28,  1.34s/it]2017-06-01 18:14:59,989 root  INFO     step 179.000000 - time: 1.104996, loss: 2.757158, perplexity: 15.755008, precision: 0.000000, batch_len: 99.000000
Train, loss=2.75715828: 180it [05:29,  1.31s/it]2017-06-01 18:15:01,244 root  INFO     step 180.000000 - time: 1.215411, loss: 2.743991, perplexity: 15.548923, precision: 0.000000, batch_len: 133.000000
Train, loss=2.74399137: 181it [05:31,  1.29s/it]2017-06-01 18:15:02,411 root  INFO     step 181.000000 - time: 1.152749, loss: 2.790322, perplexity: 16.286260, precision: 0.000000, batch_len: 94.000000
Train, loss=2.79032183: 182it [05:32,  1.26s/it]2017-06-01 18:15:04,013 root  INFO     step 182.000000 - time: 1.562557, loss: 2.782434, perplexity: 16.158306, precision: 0.000000, batch_len: 126.000000
Train, loss=2.78243423: 183it [05:33,  1.36s/it]2017-06-01 18:15:05,405 root  INFO     step 183.000000 - time: 1.344151, loss: 2.754756, perplexity: 15.717213, precision: 0.000000, batch_len: 119.000000
Train, loss=2.75475645: 184it [05:35,  1.37s/it]2017-06-01 18:15:06,437 root  INFO     step 184.000000 - time: 1.003368, loss: 2.809234, perplexity: 16.597202, precision: 0.000000, batch_len: 79.000000
Train, loss=2.80923414: 185it [05:36,  1.27s/it]2017-06-01 18:15:07,817 root  INFO     step 185.000000 - time: 1.316016, loss: 2.799807, perplexity: 16.441478, precision: 0.000000, batch_len: 130.000000
Train, loss=2.79980731: 186it [05:37,  1.30s/it]2017-06-01 18:15:08,975 root  INFO     step 186.000000 - time: 1.153245, loss: 2.748916, perplexity: 15.625687, precision: 0.000000, batch_len: 87.000000
Train, loss=2.74891615: 187it [05:38,  1.26s/it]2017-06-01 18:15:10,647 root  INFO     step 187.000000 - time: 1.641612, loss: 2.740055, perplexity: 15.487831, precision: 0.000000, batch_len: 144.000000
Train, loss=2.74005461: 188it [05:40,  1.38s/it]2017-06-01 18:15:11,999 root  INFO     step 188.000000 - time: 1.323619, loss: 2.728653, perplexity: 15.312250, precision: 0.000000, batch_len: 125.000000
Train, loss=2.72865319: 189it [05:41,  1.37s/it]2017-06-01 18:15:13,219 root  INFO     step 189.000000 - time: 1.046499, loss: 2.739751, perplexity: 15.483131, precision: 0.000000, batch_len: 98.000000
Train, loss=2.73975110: 190it [05:43,  1.33s/it]2017-06-01 18:15:14,556 root  INFO     step 190.000000 - time: 1.264569, loss: 2.927099, perplexity: 18.673376, precision: 0.000000, batch_len: 118.000000
Train, loss=2.92709875: 191it [05:44,  1.33s/it]2017-06-01 18:15:15,907 root  INFO     step 191.000000 - time: 1.244455, loss: 2.851498, perplexity: 17.313701, precision: 0.000000, batch_len: 84.000000
Train, loss=2.85149813: 192it [05:45,  1.34s/it]2017-06-01 18:15:17,128 root  INFO     step 192.000000 - time: 1.080397, loss: 2.748417, perplexity: 15.617884, precision: 0.000000, batch_len: 82.000000
Train, loss=2.74841666: 193it [05:47,  1.30s/it]2017-06-01 18:15:18,313 root  INFO     step 193.000000 - time: 1.052508, loss: 2.809437, perplexity: 16.600562, precision: 0.000000, batch_len: 85.000000
Train, loss=2.80943656: 194it [05:48,  1.27s/it]2017-06-01 18:15:19,836 root  INFO     step 194.000000 - time: 1.305832, loss: 2.716414, perplexity: 15.125986, precision: 0.000000, batch_len: 135.000000
Train, loss=2.71641421: 195it [05:49,  1.34s/it]2017-06-01 18:15:21,215 root  INFO     step 195.000000 - time: 1.364225, loss: 2.683839, perplexity: 14.641187, precision: 0.000000, batch_len: 136.000000
Train, loss=2.68383861: 196it [05:51,  1.35s/it]2017-06-01 18:15:22,519 root  INFO     step 196.000000 - time: 1.261031, loss: 2.779702, perplexity: 16.114221, precision: 0.000000, batch_len: 96.000000
Train, loss=2.77970219: 197it [05:52,  1.34s/it]2017-06-01 18:15:24,096 root  INFO     step 197.000000 - time: 1.384136, loss: 2.774506, perplexity: 16.030707, precision: 0.000000, batch_len: 134.000000
Train, loss=2.77450609: 198it [05:53,  1.41s/it]2017-06-01 18:15:25,566 root  INFO     step 198.000000 - time: 1.370490, loss: 2.752501, perplexity: 15.681811, precision: 0.000000, batch_len: 122.000000
Train, loss=2.75250149: 199it [05:55,  1.43s/it]2017-06-01 18:15:26,622 root  INFO     step 199.000000 - time: 0.994926, loss: 2.773217, perplexity: 16.010063, precision: 0.000000, batch_len: 76.000000
Train, loss=2.77321744: 200it [05:56,  1.32s/it]2017-06-01 18:15:28,499 root  INFO     step 200.000000 - time: 1.858123, loss: 3.033121, perplexity: 20.761922, precision: 0.000000, batch_len: 150.000000
Train, loss=3.03312063: 201it [05:58,  1.48s/it]2017-06-01 18:15:29,921 root  INFO     step 201.000000 - time: 1.391366, loss: 2.856397, perplexity: 17.398721, precision: 0.000000, batch_len: 141.000000
Train, loss=2.85639668: 202it [05:59,  1.47s/it]2017-06-01 18:15:31,783 root  INFO     step 202.000000 - time: 1.829509, loss: 2.745083, perplexity: 15.565911, precision: 0.000000, batch_len: 139.000000
Train, loss=2.74508333: 203it [06:01,  1.58s/it]2017-06-01 18:15:33,276 root  INFO     step 203.000000 - time: 1.457233, loss: 2.717076, perplexity: 15.135994, precision: 0.000000, batch_len: 138.000000
Train, loss=2.71707559: 204it [06:03,  1.56s/it]2017-06-01 18:15:34,307 root  INFO     step 204.000000 - time: 1.017189, loss: 2.776809, perplexity: 16.067671, precision: 0.000000, batch_len: 78.000000
Train, loss=2.77680922: 205it [06:04,  1.40s/it]2017-06-01 18:15:35,374 root  INFO     step 205.000000 - time: 0.891642, loss: 2.708133, perplexity: 15.001245, precision: 0.000000, batch_len: 74.000000
Train, loss=2.70813322: 206it [06:05,  1.30s/it]2017-06-01 18:15:36,440 root  INFO     step 206.000000 - time: 1.042267, loss: 2.718882, perplexity: 15.163354, precision: 0.000000, batch_len: 72.000000
Train, loss=2.71888161: 207it [06:06,  1.23s/it]2017-06-01 18:15:38,233 root  INFO     step 207.000000 - time: 1.707143, loss: 2.800090, perplexity: 16.446124, precision: 0.000000, batch_len: 132.000000
Train, loss=2.80008984: 208it [06:08,  1.40s/it]2017-06-01 18:15:40,231 root  INFO     step 208.000000 - time: 1.931644, loss: 2.884903, perplexity: 17.901839, precision: 0.000000, batch_len: 152.000000
Train, loss=2.88490343: 209it [06:10,  1.58s/it]2017-06-01 18:15:41,400 root  INFO     step 209.000000 - time: 0.933898, loss: 2.907627, perplexity: 18.313291, precision: 0.000000, batch_len: 71.000000
Train, loss=2.90762711: 210it [06:11,  1.46s/it]2017-06-01 18:15:42,765 root  INFO     step 210.000000 - time: 1.336258, loss: 2.786159, perplexity: 16.218597, precision: 0.000000, batch_len: 142.000000
Train, loss=2.78615856: 211it [06:12,  1.43s/it]2017-06-01 18:15:44,442 root  INFO     step 211.000000 - time: 1.619731, loss: 2.710321, perplexity: 15.034107, precision: 0.000000, batch_len: 131.000000
Train, loss=2.71032143: 212it [06:14,  1.50s/it]2017-06-01 18:15:45,578 root  INFO     step 212.000000 - time: 0.997576, loss: 2.728147, perplexity: 15.304502, precision: 0.000000, batch_len: 77.000000
Train, loss=2.72814703: 213it [06:15,  1.39s/it]2017-06-01 18:15:45,769 root  INFO     Generating first batch)
2017-06-01 18:15:49,956 root  INFO     step 213.000000 - time: 1.245644, loss: 2.758168, perplexity: 15.770928, precision: 0.000000, batch_len: 96.000000
Train, loss=2.75816822: 214it [06:19,  2.29s/it]2017-06-01 18:15:51,186 root  INFO     step 214.000000 - time: 1.132379, loss: 2.721312, perplexity: 15.200249, precision: 0.000000, batch_len: 110.000000
Train, loss=2.72131181: 215it [06:21,  1.97s/it]2017-06-01 18:15:52,270 root  INFO     step 215.000000 - time: 1.040710, loss: 2.749308, perplexity: 15.631805, precision: 0.000000, batch_len: 105.000000
Train, loss=2.74930763: 216it [06:22,  1.71s/it]2017-06-01 18:15:53,392 root  INFO     step 216.000000 - time: 0.947676, loss: 2.734344, perplexity: 15.399638, precision: 0.000000, batch_len: 100.000000
Train, loss=2.73434401: 217it [06:23,  1.53s/it]2017-06-01 18:15:54,488 root  INFO     step 217.000000 - time: 0.970575, loss: 2.738271, perplexity: 15.460235, precision: 0.000000, batch_len: 104.000000
Train, loss=2.73827124: 218it [06:24,  1.40s/it]2017-06-01 18:15:56,252 root  INFO     step 218.000000 - time: 1.700195, loss: 2.815013, perplexity: 16.693392, precision: 0.000000, batch_len: 120.000000
Train, loss=2.81501293: 219it [06:26,  1.51s/it]2017-06-01 18:15:57,412 root  INFO     step 219.000000 - time: 1.052592, loss: 2.782796, perplexity: 16.164159, precision: 0.000000, batch_len: 111.000000
Train, loss=2.78279638: 220it [06:27,  1.40s/it]2017-06-01 18:15:58,886 root  INFO     step 220.000000 - time: 1.306794, loss: 2.721962, perplexity: 15.210135, precision: 0.000000, batch_len: 112.000000
Train, loss=2.72196198: 221it [06:28,  1.43s/it]2017-06-01 18:15:59,925 root  INFO     step 221.000000 - time: 0.999641, loss: 2.716808, perplexity: 15.131938, precision: 0.000000, batch_len: 101.000000
Train, loss=2.71680760: 222it [06:29,  1.31s/it]2017-06-01 18:16:00,831 root  INFO     step 222.000000 - time: 0.841708, loss: 2.764518, perplexity: 15.871392, precision: 0.000000, batch_len: 88.000000
Train, loss=2.76451826: 223it [06:30,  1.19s/it]2017-06-01 18:16:02,080 root  INFO     step 223.000000 - time: 1.229735, loss: 2.754491, perplexity: 15.713039, precision: 0.000000, batch_len: 90.000000
Train, loss=2.75449085: 224it [06:31,  1.21s/it]2017-06-01 18:16:03,389 root  INFO     step 224.000000 - time: 1.304098, loss: 2.688742, perplexity: 14.713161, precision: 0.000000, batch_len: 113.000000
Train, loss=2.68874240: 225it [06:33,  1.24s/it]2017-06-01 18:16:04,730 root  INFO     step 225.000000 - time: 1.312268, loss: 2.714641, perplexity: 15.099186, precision: 0.000000, batch_len: 125.000000
Train, loss=2.71464086: 226it [06:34,  1.27s/it]2017-06-01 18:16:05,931 root  INFO     step 226.000000 - time: 1.044910, loss: 2.751002, perplexity: 15.658319, precision: 0.000000, batch_len: 92.000000
Train, loss=2.75100231: 227it [06:35,  1.25s/it]2017-06-01 18:16:07,002 root  INFO     step 227.000000 - time: 1.013792, loss: 2.726304, perplexity: 15.276322, precision: 0.000000, batch_len: 108.000000
Train, loss=2.72630405: 228it [06:36,  1.19s/it]2017-06-01 18:16:08,420 root  INFO     step 228.000000 - time: 1.322513, loss: 2.720306, perplexity: 15.184963, precision: 0.000000, batch_len: 109.000000
Train, loss=2.72030568: 229it [06:38,  1.26s/it]2017-06-01 18:16:09,857 root  INFO     step 229.000000 - time: 1.254521, loss: 2.722367, perplexity: 15.216294, precision: 0.000000, batch_len: 106.000000
Train, loss=2.72236681: 230it [06:39,  1.31s/it]2017-06-01 18:16:10,928 root  INFO     step 230.000000 - time: 1.049327, loss: 2.718090, perplexity: 15.151349, precision: 0.000000, batch_len: 93.000000
Train, loss=2.71808958: 231it [06:40,  1.24s/it]2017-06-01 18:16:12,066 root  INFO     step 231.000000 - time: 1.128866, loss: 2.713641, perplexity: 15.084099, precision: 0.000000, batch_len: 97.000000
Train, loss=2.71364117: 232it [06:41,  1.21s/it]2017-06-01 18:16:13,440 root  INFO     step 232.000000 - time: 1.290063, loss: 2.803368, perplexity: 16.500123, precision: 0.000000, batch_len: 128.000000
Train, loss=2.80336785: 233it [06:43,  1.26s/it]2017-06-01 18:16:14,565 root  INFO     step 233.000000 - time: 1.042867, loss: 2.705865, perplexity: 14.967253, precision: 0.000000, batch_len: 103.000000
Train, loss=2.70586467: 234it [06:44,  1.22s/it]2017-06-01 18:16:15,857 root  INFO     step 234.000000 - time: 1.239622, loss: 2.691021, perplexity: 14.746731, precision: 0.000000, batch_len: 91.000000
Train, loss=2.69102144: 235it [06:45,  1.24s/it]2017-06-01 18:16:16,971 root  INFO     step 235.000000 - time: 1.076469, loss: 2.758694, perplexity: 15.779221, precision: 0.000000, batch_len: 79.000000
Train, loss=2.75869393: 236it [06:46,  1.20s/it]2017-06-01 18:16:18,305 root  INFO     step 236.000000 - time: 1.319883, loss: 2.752182, perplexity: 15.676798, precision: 0.000000, batch_len: 117.000000
Train, loss=2.75218177: 237it [06:48,  1.24s/it]2017-06-01 18:16:19,428 root  INFO     step 237.000000 - time: 0.976093, loss: 2.734947, perplexity: 15.408926, precision: 0.000000, batch_len: 80.000000
Train, loss=2.73494697: 238it [06:49,  1.21s/it]2017-06-01 18:16:20,377 root  INFO     step 238.000000 - time: 0.942454, loss: 2.765789, perplexity: 15.891570, precision: 0.000000, batch_len: 102.000000
Train, loss=2.76578879: 239it [06:50,  1.13s/it]2017-06-01 18:16:21,417 root  INFO     step 239.000000 - time: 1.027713, loss: 2.709180, perplexity: 15.016962, precision: 0.000000, batch_len: 83.000000
Train, loss=2.70918036: 240it [06:51,  1.10s/it]2017-06-01 18:16:22,773 root  INFO     step 240.000000 - time: 1.297072, loss: 2.699073, perplexity: 14.865949, precision: 0.000000, batch_len: 89.000000
Train, loss=2.69907331: 241it [06:52,  1.18s/it]2017-06-01 18:16:24,122 root  INFO     step 241.000000 - time: 1.328601, loss: 2.794081, perplexity: 16.347598, precision: 0.000000, batch_len: 124.000000
Train, loss=2.79408097: 242it [06:53,  1.23s/it]2017-06-01 18:16:25,260 root  INFO     step 242.000000 - time: 1.125368, loss: 2.774813, perplexity: 16.035623, precision: 0.000000, batch_len: 114.000000
Train, loss=2.77481270: 243it [06:55,  1.20s/it]2017-06-01 18:16:26,262 root  INFO     step 243.000000 - time: 0.992675, loss: 2.707218, perplexity: 14.987528, precision: 0.000000, batch_len: 86.000000
Train, loss=2.70721841: 244it [06:56,  1.14s/it]2017-06-01 18:16:27,387 root  INFO     step 244.000000 - time: 1.113497, loss: 2.750658, perplexity: 15.652929, precision: 0.000000, batch_len: 107.000000
Train, loss=2.75065804: 245it [06:57,  1.14s/it]2017-06-01 18:16:28,936 root  INFO     step 245.000000 - time: 1.511853, loss: 2.710678, perplexity: 15.039474, precision: 0.000000, batch_len: 123.000000
Train, loss=2.71067834: 246it [06:58,  1.26s/it]2017-06-01 18:16:30,206 root  INFO     step 246.000000 - time: 1.139766, loss: 2.774934, perplexity: 16.037573, precision: 0.000000, batch_len: 81.000000
Train, loss=2.77493429: 247it [07:00,  1.26s/it]2017-06-01 18:16:31,853 root  INFO     step 247.000000 - time: 1.598242, loss: 2.736509, perplexity: 15.433012, precision: 0.000000, batch_len: 121.000000
Train, loss=2.73650885: 248it [07:01,  1.38s/it]2017-06-01 18:16:32,979 root  INFO     step 248.000000 - time: 1.108702, loss: 2.792809, perplexity: 16.326825, precision: 0.000000, batch_len: 115.000000
Train, loss=2.79280949: 249it [07:02,  1.30s/it]2017-06-01 18:16:34,100 root  INFO     step 249.000000 - time: 1.048626, loss: 2.752911, perplexity: 15.688239, precision: 0.000000, batch_len: 85.000000
Train, loss=2.75291133: 250it [07:03,  1.25s/it]2017-06-01 18:16:35,108 root  INFO     step 250.000000 - time: 0.997051, loss: 2.758787, perplexity: 15.780692, precision: 0.000000, batch_len: 94.000000
Train, loss=2.75878716: 251it [07:04,  1.18s/it]2017-06-01 18:16:36,629 root  INFO     step 251.000000 - time: 1.360722, loss: 2.781929, perplexity: 16.150145, precision: 0.000000, batch_len: 137.000000
Train, loss=2.78192902: 252it [07:06,  1.28s/it]2017-06-01 18:16:38,286 root  INFO     step 252.000000 - time: 1.594218, loss: 2.668266, perplexity: 14.414956, precision: 0.000000, batch_len: 116.000000
Train, loss=2.66826630: 253it [07:08,  1.39s/it]2017-06-01 18:16:39,503 root  INFO     step 253.000000 - time: 1.092739, loss: 2.726187, perplexity: 15.274538, precision: 0.000000, batch_len: 98.000000
Train, loss=2.72618723: 254it [07:09,  1.34s/it]2017-06-01 18:16:40,498 root  INFO     step 254.000000 - time: 0.980690, loss: 2.689851, perplexity: 14.729478, precision: 0.000000, batch_len: 84.000000
Train, loss=2.68985081: 255it [07:10,  1.24s/it]2017-06-01 18:16:41,938 root  INFO     step 255.000000 - time: 1.342604, loss: 2.830513, perplexity: 16.954148, precision: 0.000000, batch_len: 119.000000
Train, loss=2.83051252: 256it [07:11,  1.30s/it]2017-06-01 18:16:43,294 root  INFO     step 256.000000 - time: 1.327115, loss: 2.766479, perplexity: 15.902539, precision: 0.000000, batch_len: 130.000000
Train, loss=2.76647878: 257it [07:13,  1.32s/it]2017-06-01 18:16:44,907 root  INFO     step 257.000000 - time: 1.564812, loss: 2.732069, perplexity: 15.364648, precision: 0.000000, batch_len: 129.000000
Train, loss=2.73206925: 258it [07:14,  1.40s/it]2017-06-01 18:16:46,371 root  INFO     step 258.000000 - time: 1.447203, loss: 2.694331, perplexity: 14.795620, precision: 0.000000, batch_len: 144.000000
Train, loss=2.69433117: 259it [07:16,  1.42s/it]2017-06-01 18:16:47,344 root  INFO     step 259.000000 - time: 0.961905, loss: 2.797040, perplexity: 16.396035, precision: 0.000000, batch_len: 87.000000
Train, loss=2.79703951: 260it [07:17,  1.29s/it]2017-06-01 18:16:48,509 root  INFO     step 260.000000 - time: 1.135554, loss: 2.741853, perplexity: 15.515713, precision: 0.000000, batch_len: 99.000000
Train, loss=2.74185324: 261it [07:18,  1.25s/it]2017-06-01 18:16:49,990 root  INFO     step 261.000000 - time: 1.370900, loss: 2.752422, perplexity: 15.680562, precision: 0.000000, batch_len: 126.000000
Train, loss=2.75242186: 262it [07:19,  1.32s/it]2017-06-01 18:16:51,738 root  INFO     step 262.000000 - time: 1.585349, loss: 2.706032, perplexity: 14.969755, precision: 0.000000, batch_len: 135.000000
Train, loss=2.70603180: 263it [07:21,  1.45s/it]2017-06-01 18:16:52,778 root  INFO     step 263.000000 - time: 0.992459, loss: 2.717554, perplexity: 15.143238, precision: 0.000000, batch_len: 78.000000
Train, loss=2.71755409: 264it [07:22,  1.33s/it]2017-06-01 18:16:54,317 root  INFO     step 264.000000 - time: 1.476057, loss: 2.687966, perplexity: 14.701740, precision: 0.000000, batch_len: 136.000000
Train, loss=2.68796587: 265it [07:24,  1.39s/it]2017-06-01 18:16:55,586 root  INFO     step 265.000000 - time: 1.246653, loss: 2.724087, perplexity: 15.242488, precision: 0.000000, batch_len: 118.000000
Train, loss=2.72408676: 266it [07:25,  1.35s/it]2017-06-01 18:16:56,685 root  INFO     step 266.000000 - time: 1.039644, loss: 2.735381, perplexity: 15.415610, precision: 0.000000, batch_len: 82.000000
Train, loss=2.73538065: 267it [07:26,  1.28s/it]2017-06-01 18:16:58,366 root  INFO     step 267.000000 - time: 1.558444, loss: 2.810549, perplexity: 16.619044, precision: 0.000000, batch_len: 122.000000
Train, loss=2.81054926: 268it [07:28,  1.40s/it]2017-06-01 18:16:59,804 root  INFO     step 268.000000 - time: 1.353165, loss: 2.684387, perplexity: 14.649222, precision: 0.000000, batch_len: 133.000000
Train, loss=2.68438721: 269it [07:29,  1.41s/it]2017-06-01 18:17:01,552 root  INFO     step 269.000000 - time: 1.444508, loss: 2.713288, perplexity: 15.078778, precision: 0.000000, batch_len: 138.000000
Train, loss=2.71328831: 270it [07:31,  1.51s/it]2017-06-01 18:17:02,608 root  INFO     step 270.000000 - time: 0.969732, loss: 2.740439, perplexity: 15.493784, precision: 0.000000, batch_len: 96.000000
Train, loss=2.74043894: 271it [07:32,  1.37s/it]2017-06-01 18:17:04,524 root  INFO     step 271.000000 - time: 1.669574, loss: 2.757089, perplexity: 15.753911, precision: 0.000000, batch_len: 132.000000
Train, loss=2.75708866: 272it [07:34,  1.54s/it]2017-06-01 18:17:05,855 root  INFO     step 272.000000 - time: 1.162800, loss: 2.712666, perplexity: 15.069390, precision: 0.000000, batch_len: 72.000000
Train, loss=2.71266556: 273it [07:35,  1.48s/it]2017-06-01 18:17:07,391 root  INFO     step 273.000000 - time: 1.436330, loss: 2.777446, perplexity: 16.077906, precision: 0.000000, batch_len: 142.000000
Train, loss=2.77744603: 274it [07:37,  1.49s/it]2017-06-01 18:17:08,730 root  INFO     step 274.000000 - time: 1.303941, loss: 2.732568, perplexity: 15.372313, precision: 0.000000, batch_len: 141.000000
Train, loss=2.73256803: 275it [07:38,  1.45s/it]2017-06-01 18:17:09,827 root  INFO     step 275.000000 - time: 1.025067, loss: 2.649679, perplexity: 14.149499, precision: 0.000000, batch_len: 71.000000
Train, loss=2.64967918: 276it [07:39,  1.34s/it]2017-06-01 18:17:11,617 root  INFO     step 276.000000 - time: 1.698268, loss: 2.732477, perplexity: 15.370909, precision: 0.000000, batch_len: 139.000000
Train, loss=2.73247671: 277it [07:41,  1.48s/it]2017-06-01 18:17:12,597 root  INFO     step 277.000000 - time: 0.961533, loss: 2.743768, perplexity: 15.545446, precision: 0.000000, batch_len: 74.000000
Train, loss=2.74376774: 278it [07:42,  1.33s/it]2017-06-01 18:17:14,447 root  INFO     step 278.000000 - time: 1.825424, loss: 3.159554, perplexity: 23.560075, precision: 0.000000, batch_len: 150.000000
Train, loss=3.15955353: 279it [07:44,  1.48s/it]2017-06-01 18:17:15,739 root  INFO     step 279.000000 - time: 1.250202, loss: 2.830523, perplexity: 16.954326, precision: 0.000000, batch_len: 134.000000
Train, loss=2.83052301: 280it [07:45,  1.43s/it]2017-06-01 18:17:16,808 root  INFO     step 280.000000 - time: 1.046223, loss: 2.775437, perplexity: 16.045643, precision: 0.000000, batch_len: 76.000000
Train, loss=2.77543736: 281it [07:46,  1.32s/it]2017-06-01 18:17:18,895 root  INFO     step 281.000000 - time: 2.064563, loss: 2.866244, perplexity: 17.570895, precision: 0.000000, batch_len: 152.000000
Train, loss=2.86624384: 282it [07:48,  1.55s/it]2017-06-01 18:17:20,006 root  INFO     step 282.000000 - time: 0.923810, loss: 2.860835, perplexity: 17.476106, precision: 0.000000, batch_len: 77.000000
Train, loss=2.86083460: 283it [07:49,  1.42s/it]2017-06-01 18:17:21,684 root  INFO     step 283.000000 - time: 1.409152, loss: 2.697363, perplexity: 14.840551, precision: 0.000000, batch_len: 131.000000
Train, loss=2.69736338: 284it [07:51,  1.50s/it]2017-06-01 18:17:21,752 root  INFO     Generating first batch)
2017-06-01 18:17:25,540 root  INFO     step 284.000000 - time: 1.066746, loss: 2.736511, perplexity: 15.433052, precision: 0.000000, batch_len: 96.000000
Train, loss=2.73651147: 285it [07:55,  2.20s/it]2017-06-01 18:17:26,720 root  INFO     step 285.000000 - time: 1.120559, loss: 2.686867, perplexity: 14.685587, precision: 0.000000, batch_len: 93.000000
Train, loss=2.68686652: 286it [07:56,  1.90s/it]2017-06-01 18:17:28,356 root  INFO     step 286.000000 - time: 1.475053, loss: 2.750723, perplexity: 15.653940, precision: 0.000000, batch_len: 128.000000
Train, loss=2.75072265: 287it [07:58,  1.82s/it]2017-06-01 18:17:29,787 root  INFO     step 287.000000 - time: 1.179956, loss: 2.735659, perplexity: 15.419896, precision: 0.000000, batch_len: 105.000000
Train, loss=2.73565865: 288it [07:59,  1.70s/it]2017-06-01 18:17:31,503 root  INFO     step 288.000000 - time: 1.620202, loss: 2.688765, perplexity: 14.713491, precision: 0.000000, batch_len: 120.000000
Train, loss=2.68876481: 289it [08:01,  1.71s/it]2017-06-01 18:17:32,698 root  INFO     step 289.000000 - time: 1.104940, loss: 2.731277, perplexity: 15.352472, precision: 0.000000, batch_len: 111.000000
Train, loss=2.73127651: 290it [08:02,  1.55s/it]2017-06-01 18:17:34,002 root  INFO     step 290.000000 - time: 1.125770, loss: 2.728404, perplexity: 15.308429, precision: 0.000000, batch_len: 113.000000
Train, loss=2.72840357: 291it [08:03,  1.48s/it]2017-06-01 18:17:35,049 root  INFO     step 291.000000 - time: 0.962393, loss: 2.742057, perplexity: 15.518872, precision: 0.000000, batch_len: 106.000000
Train, loss=2.74205685: 292it [08:04,  1.35s/it]2017-06-01 18:17:36,091 root  INFO     step 292.000000 - time: 1.021248, loss: 2.718127, perplexity: 15.151909, precision: 0.000000, batch_len: 90.000000
Train, loss=2.71812654: 293it [08:05,  1.26s/it]2017-06-01 18:17:37,348 root  INFO     step 293.000000 - time: 1.181266, loss: 2.639127, perplexity: 14.000979, precision: 0.000000, batch_len: 101.000000
Train, loss=2.63912725: 294it [08:07,  1.26s/it]2017-06-01 18:17:38,624 root  INFO     step 294.000000 - time: 1.218107, loss: 2.659518, perplexity: 14.289407, precision: 0.000000, batch_len: 110.000000
Train, loss=2.65951848: 295it [08:08,  1.26s/it]2017-06-01 18:17:39,752 root  INFO     step 295.000000 - time: 1.117061, loss: 2.677622, perplexity: 14.550449, precision: 0.000000, batch_len: 108.000000
Train, loss=2.67762184: 296it [08:09,  1.22s/it]2017-06-01 18:17:41,259 root  INFO     step 296.000000 - time: 1.312764, loss: 2.719189, perplexity: 15.168019, precision: 0.000000, batch_len: 117.000000
Train, loss=2.71918917: 297it [08:11,  1.31s/it]2017-06-01 18:17:42,526 root  INFO     step 297.000000 - time: 1.240319, loss: 2.657239, perplexity: 14.256878, precision: 0.000000, batch_len: 112.000000
Train, loss=2.65723944: 298it [08:12,  1.30s/it]2017-06-01 18:17:43,766 root  INFO     step 298.000000 - time: 1.213119, loss: 2.722254, perplexity: 15.214578, precision: 0.000000, batch_len: 97.000000
Train, loss=2.72225404: 299it [08:13,  1.28s/it]2017-06-01 18:17:44,955 root  INFO     step 299.000000 - time: 1.165596, loss: 2.656966, perplexity: 14.252979, precision: 0.000000, batch_len: 100.000000
Train, loss=2.65696597: 300it [08:14,  1.25s/it]2017-06-01 18:17:46,007 root  INFO     step 300.000000 - time: 1.020242, loss: 2.685619, perplexity: 14.667276, precision: 0.000000, batch_len: 104.000000
Train, loss=2.68561888: 301it [08:15,  1.19s/it]2017-06-01 18:17:47,106 root  INFO     step 301.000000 - time: 1.083555, loss: 2.723759, perplexity: 15.237495, precision: 0.000000, batch_len: 92.000000
Train, loss=2.72375917: 302it [08:16,  1.16s/it]2017-06-01 18:17:48,193 root  INFO     step 302.000000 - time: 1.061273, loss: 2.681793, perplexity: 14.611264, precision: 0.000000, batch_len: 89.000000
Train, loss=2.68179274: 303it [08:18,  1.14s/it]2017-06-01 18:17:49,421 root  INFO     step 303.000000 - time: 1.188498, loss: 2.806483, perplexity: 16.551608, precision: 0.000000, batch_len: 121.000000
Train, loss=2.80648327: 304it [08:19,  1.17s/it]2017-06-01 18:17:50,812 root  INFO     step 304.000000 - time: 1.326936, loss: 2.725685, perplexity: 15.266874, precision: 0.000000, batch_len: 115.000000
Train, loss=2.72568536: 305it [08:20,  1.23s/it]2017-06-01 18:17:52,025 root  INFO     step 305.000000 - time: 1.200111, loss: 2.668793, perplexity: 14.422550, precision: 0.000000, batch_len: 88.000000
Train, loss=2.66879296: 306it [08:21,  1.23s/it]2017-06-01 18:17:53,102 root  INFO     step 306.000000 - time: 1.068086, loss: 2.652240, perplexity: 14.185773, precision: 0.000000, batch_len: 103.000000
Train, loss=2.65223956: 307it [08:22,  1.18s/it]2017-06-01 18:17:54,247 root  INFO     step 307.000000 - time: 1.086740, loss: 2.705325, perplexity: 14.959180, precision: 0.000000, batch_len: 102.000000
Train, loss=2.70532513: 308it [08:24,  1.17s/it]2017-06-01 18:17:55,297 root  INFO     step 308.000000 - time: 1.027226, loss: 2.729661, perplexity: 15.327690, precision: 0.000000, batch_len: 114.000000
Train, loss=2.72966099: 309it [08:25,  1.14s/it]2017-06-01 18:17:56,249 root  INFO     step 309.000000 - time: 0.920664, loss: 2.707082, perplexity: 14.985481, precision: 0.000000, batch_len: 81.000000
Train, loss=2.70708179: 310it [08:26,  1.08s/it]2017-06-01 18:17:57,373 root  INFO     step 310.000000 - time: 1.106342, loss: 2.699410, perplexity: 14.870955, precision: 0.000000, batch_len: 83.000000
Train, loss=2.69940996: 311it [08:27,  1.09s/it]2017-06-01 18:17:58,596 root  INFO     step 311.000000 - time: 1.141555, loss: 2.640442, perplexity: 14.019404, precision: 0.000000, batch_len: 86.000000
Train, loss=2.64044237: 312it [08:28,  1.13s/it]2017-06-01 18:17:59,808 root  INFO     step 312.000000 - time: 1.131282, loss: 2.690602, perplexity: 14.740552, precision: 0.000000, batch_len: 109.000000
Train, loss=2.69060230: 313it [08:29,  1.16s/it]2017-06-01 18:18:00,820 root  INFO     step 313.000000 - time: 0.996326, loss: 2.714240, perplexity: 15.093129, precision: 0.000000, batch_len: 79.000000
Train, loss=2.71423960: 314it [08:30,  1.11s/it]2017-06-01 18:18:01,776 root  INFO     step 314.000000 - time: 0.939449, loss: 2.646601, perplexity: 14.106007, precision: 0.000000, batch_len: 87.000000
Train, loss=2.64660072: 315it [08:31,  1.07s/it]2017-06-01 18:18:02,682 root  INFO     step 315.000000 - time: 0.844956, loss: 2.655676, perplexity: 14.234611, precision: 0.000000, batch_len: 82.000000
Train, loss=2.65567636: 316it [08:32,  1.02s/it]2017-06-01 18:18:04,174 root  INFO     step 316.000000 - time: 1.449569, loss: 2.729208, perplexity: 15.320755, precision: 0.000000, batch_len: 124.000000
Train, loss=2.72920847: 317it [08:34,  1.16s/it]2017-06-01 18:18:05,535 root  INFO     step 317.000000 - time: 1.190665, loss: 2.711740, perplexity: 15.055457, precision: 0.000000, batch_len: 98.000000
Train, loss=2.71174049: 318it [08:35,  1.22s/it]2017-06-01 18:18:06,515 root  INFO     step 318.000000 - time: 0.961089, loss: 2.589496, perplexity: 13.323057, precision: 0.000000, batch_len: 80.000000
Train, loss=2.58949614: 319it [08:36,  1.15s/it]2017-06-01 18:18:07,880 root  INFO     step 319.000000 - time: 1.322725, loss: 2.732837, perplexity: 15.376444, precision: 0.000000, batch_len: 125.000000
Train, loss=2.73283672: 320it [08:37,  1.21s/it]2017-06-01 18:18:08,838 root  INFO     step 320.000000 - time: 0.918581, loss: 2.678723, perplexity: 14.566485, precision: 0.000000, batch_len: 107.000000
Train, loss=2.67872334: 321it [08:38,  1.14s/it]2017-06-01 18:18:10,675 root  INFO     step 321.000000 - time: 1.548119, loss: 2.760207, perplexity: 15.803113, precision: 0.000000, batch_len: 118.000000
Train, loss=2.76020694: 322it [08:40,  1.35s/it]2017-06-01 18:18:12,205 root  INFO     step 322.000000 - time: 1.487634, loss: 2.669255, perplexity: 14.429212, precision: 0.000000, batch_len: 116.000000
Train, loss=2.66925478: 323it [08:42,  1.40s/it]2017-06-01 18:18:13,303 root  INFO     step 323.000000 - time: 1.017724, loss: 2.755467, perplexity: 15.728383, precision: 0.000000, batch_len: 85.000000
Train, loss=2.75546694: 324it [08:43,  1.31s/it]2017-06-01 18:18:14,419 root  INFO     step 324.000000 - time: 1.036042, loss: 2.670928, perplexity: 14.453383, precision: 0.000000, batch_len: 91.000000
Train, loss=2.67092848: 325it [08:44,  1.25s/it]2017-06-01 18:18:15,725 root  INFO     step 325.000000 - time: 1.234703, loss: 2.722865, perplexity: 15.223878, precision: 0.000000, batch_len: 130.000000
Train, loss=2.72286510: 326it [08:45,  1.27s/it]2017-06-01 18:18:17,466 root  INFO     step 326.000000 - time: 1.574687, loss: 2.688328, perplexity: 14.707069, precision: 0.000000, batch_len: 144.000000
Train, loss=2.68832827: 327it [08:47,  1.41s/it]2017-06-01 18:18:18,655 root  INFO     step 327.000000 - time: 1.090851, loss: 2.776096, perplexity: 16.056213, precision: 0.000000, batch_len: 94.000000
Train, loss=2.77609587: 328it [08:48,  1.34s/it]2017-06-01 18:18:19,746 root  INFO     step 328.000000 - time: 1.024828, loss: 2.690870, perplexity: 14.744502, precision: 0.000000, batch_len: 84.000000
Train, loss=2.69087029: 329it [08:49,  1.27s/it]2017-06-01 18:18:21,379 root  INFO     step 329.000000 - time: 1.342763, loss: 2.722621, perplexity: 15.220162, precision: 0.000000, batch_len: 123.000000
Train, loss=2.72262096: 330it [08:51,  1.38s/it]2017-06-01 18:18:22,685 root  INFO     step 330.000000 - time: 1.207645, loss: 2.669582, perplexity: 14.433933, precision: 0.000000, batch_len: 133.000000
Train, loss=2.66958189: 331it [08:52,  1.36s/it]2017-06-01 18:18:24,472 root  INFO     step 331.000000 - time: 1.762527, loss: 2.696725, perplexity: 14.831086, precision: 0.000000, batch_len: 126.000000
Train, loss=2.69672537: 332it [08:54,  1.49s/it]2017-06-01 18:18:26,010 root  INFO     step 332.000000 - time: 1.486034, loss: 2.615383, perplexity: 13.672447, precision: 0.000000, batch_len: 136.000000
Train, loss=2.61538267: 333it [08:55,  1.50s/it]2017-06-01 18:18:27,220 root  INFO     step 333.000000 - time: 1.192171, loss: 2.711422, perplexity: 15.050658, precision: 0.000000, batch_len: 99.000000
Train, loss=2.71142173: 334it [08:57,  1.41s/it]2017-06-01 18:18:28,631 root  INFO     step 334.000000 - time: 1.272993, loss: 2.711362, perplexity: 15.049765, precision: 0.000000, batch_len: 129.000000
Train, loss=2.71136236: 335it [08:58,  1.41s/it]2017-06-01 18:18:30,230 root  INFO     step 335.000000 - time: 1.571115, loss: 2.694419, perplexity: 14.796914, precision: 0.000000, batch_len: 119.000000
Train, loss=2.69441867: 336it [09:00,  1.47s/it]2017-06-01 18:18:31,692 root  INFO     step 336.000000 - time: 1.440326, loss: 2.714865, perplexity: 15.102571, precision: 0.000000, batch_len: 137.000000
Train, loss=2.71486497: 337it [09:01,  1.47s/it]2017-06-01 18:18:33,424 root  INFO     step 337.000000 - time: 1.713699, loss: 2.701394, perplexity: 14.900486, precision: 0.000000, batch_len: 122.000000
Train, loss=2.70139384: 338it [09:03,  1.55s/it]2017-06-01 18:18:34,951 root  INFO     step 338.000000 - time: 1.335080, loss: 2.666073, perplexity: 14.383379, precision: 0.000000, batch_len: 135.000000
Train, loss=2.66607332: 339it [09:04,  1.54s/it]2017-06-01 18:18:36,164 root  INFO     step 339.000000 - time: 1.062317, loss: 2.652827, perplexity: 14.194106, precision: 0.000000, batch_len: 96.000000
Train, loss=2.65282679: 340it [09:06,  1.44s/it]2017-06-01 18:18:37,671 root  INFO     step 340.000000 - time: 1.260826, loss: 2.754660, perplexity: 15.715695, precision: 0.000000, batch_len: 134.000000
Train, loss=2.75465989: 341it [09:07,  1.46s/it]2017-06-01 18:18:38,817 root  INFO     step 341.000000 - time: 1.119573, loss: 2.730134, perplexity: 15.334946, precision: 0.000000, batch_len: 78.000000
Train, loss=2.73013425: 342it [09:08,  1.37s/it]2017-06-01 18:18:40,615 root  INFO     step 342.000000 - time: 1.583138, loss: 2.686230, perplexity: 14.676245, precision: 0.000000, batch_len: 139.000000
Train, loss=2.68623018: 343it [09:10,  1.50s/it]2017-06-01 18:18:42,504 root  INFO     step 343.000000 - time: 1.864953, loss: 2.878231, perplexity: 17.782788, precision: 0.000000, batch_len: 152.000000
Train, loss=2.87823105: 344it [09:12,  1.61s/it]2017-06-01 18:18:43,821 root  INFO     step 344.000000 - time: 1.279012, loss: 2.815263, perplexity: 16.697571, precision: 0.000000, batch_len: 131.000000
Train, loss=2.81526327: 345it [09:13,  1.53s/it]2017-06-01 18:18:45,289 root  INFO     step 345.000000 - time: 1.451378, loss: 2.686016, perplexity: 14.673096, precision: 0.000000, batch_len: 132.000000
Train, loss=2.68601561: 346it [09:15,  1.51s/it]2017-06-01 18:18:46,961 root  INFO     step 346.000000 - time: 1.587293, loss: 2.662670, perplexity: 14.334513, precision: 0.000000, batch_len: 141.000000
Train, loss=2.66267014: 347it [09:16,  1.56s/it]2017-06-01 18:18:48,424 root  INFO     step 347.000000 - time: 1.440578, loss: 2.675261, perplexity: 14.516138, precision: 0.000000, batch_len: 138.000000
Train, loss=2.67526102: 348it [09:18,  1.53s/it]2017-06-01 18:18:49,581 root  INFO     step 348.000000 - time: 0.972840, loss: 2.743580, perplexity: 15.542522, precision: 0.000000, batch_len: 74.000000
Train, loss=2.74357963: 349it [09:19,  1.42s/it]2017-06-01 18:18:50,704 root  INFO     step 349.000000 - time: 1.061520, loss: 2.650474, perplexity: 14.160750, precision: 0.000000, batch_len: 72.000000
Train, loss=2.65047407: 350it [09:20,  1.33s/it]2017-06-01 18:18:52,988 root  INFO     step 350.000000 - time: 2.212937, loss: 2.970374, perplexity: 19.499213, precision: 0.000000, batch_len: 150.000000
Train, loss=2.97037411: 351it [09:22,  1.62s/it]2017-06-01 18:18:54,027 root  INFO     step 351.000000 - time: 0.952447, loss: 2.724907, perplexity: 15.254994, precision: 0.000000, batch_len: 76.000000
Train, loss=2.72490692: 352it [09:23,  1.44s/it]2017-06-01 18:18:55,036 root  INFO     step 352.000000 - time: 0.937086, loss: 2.674014, perplexity: 14.498052, precision: 0.000000, batch_len: 71.000000
Train, loss=2.67401433: 353it [09:24,  1.31s/it]2017-06-01 18:18:56,069 root  INFO     step 353.000000 - time: 0.940374, loss: 2.610047, perplexity: 13.599695, precision: 0.000000, batch_len: 77.000000
Train, loss=2.61004734: 354it [09:25,  1.23s/it]2017-06-01 18:18:57,573 root  INFO     step 354.000000 - time: 1.338440, loss: 2.821069, perplexity: 16.794799, precision: 0.000000, batch_len: 142.000000
Train, loss=2.82106924: 355it [09:27,  1.31s/it]2017-06-01 18:18:57,770 root  INFO     Generating first batch)
2017-06-01 18:19:01,067 root  INFO     step 355.000000 - time: 1.076697, loss: 2.720466, perplexity: 15.187404, precision: 0.000000, batch_len: 96.000000
Train, loss=2.72046638: 356it [09:30,  1.97s/it]2017-06-01 18:19:02,686 root  INFO     step 356.000000 - time: 1.047388, loss: 2.669544, perplexity: 14.433389, precision: 0.000000, batch_len: 113.000000
Train, loss=2.66954422: 357it [09:32,  1.86s/it]2017-06-01 18:19:04,007 root  INFO     step 357.000000 - time: 1.244542, loss: 2.587388, perplexity: 13.294994, precision: 0.000000, batch_len: 106.000000
Train, loss=2.58738756: 358it [09:33,  1.70s/it]2017-06-01 18:19:05,564 root  INFO     step 358.000000 - time: 1.479676, loss: 2.649631, perplexity: 14.148820, precision: 0.000000, batch_len: 120.000000
Train, loss=2.64963126: 359it [09:35,  1.66s/it]2017-06-01 18:19:06,898 root  INFO     step 359.000000 - time: 1.128531, loss: 2.679202, perplexity: 14.573460, precision: 0.000000, batch_len: 110.000000
Train, loss=2.67920208: 360it [09:36,  1.56s/it]2017-06-01 18:19:07,940 root  INFO     step 360.000000 - time: 0.982084, loss: 2.661468, perplexity: 14.317292, precision: 0.000000, batch_len: 104.000000
Train, loss=2.66146803: 361it [09:37,  1.40s/it]2017-06-01 18:19:09,014 root  INFO     step 361.000000 - time: 0.957433, loss: 2.719108, perplexity: 15.166782, precision: 0.000000, batch_len: 92.000000
Train, loss=2.71910763: 362it [09:38,  1.31s/it]2017-06-01 18:19:10,343 root  INFO     step 362.000000 - time: 1.282427, loss: 2.628821, perplexity: 13.857428, precision: 0.000000, batch_len: 111.000000
Train, loss=2.62882137: 363it [09:40,  1.31s/it]2017-06-01 18:19:11,531 root  INFO     step 363.000000 - time: 1.163482, loss: 2.654790, perplexity: 14.222005, precision: 0.000000, batch_len: 100.000000
Train, loss=2.65479040: 364it [09:41,  1.28s/it]2017-06-01 18:19:12,672 root  INFO     step 364.000000 - time: 1.082249, loss: 2.644388, perplexity: 14.074825, precision: 0.000000, batch_len: 103.000000
Train, loss=2.64438772: 365it [09:42,  1.23s/it]2017-06-01 18:19:13,789 root  INFO     step 365.000000 - time: 1.038605, loss: 2.652330, perplexity: 14.187055, precision: 0.000000, batch_len: 93.000000
Train, loss=2.65232992: 366it [09:43,  1.20s/it]2017-06-01 18:19:15,122 root  INFO     step 366.000000 - time: 1.225502, loss: 2.681069, perplexity: 14.600692, precision: 0.000000, batch_len: 128.000000
Train, loss=2.68106890: 367it [09:44,  1.24s/it]2017-06-01 18:19:16,419 root  INFO     step 367.000000 - time: 1.235443, loss: 2.695661, perplexity: 14.815309, precision: 0.000000, batch_len: 97.000000
Train, loss=2.69566107: 368it [09:46,  1.26s/it]2017-06-01 18:19:17,672 root  INFO     step 368.000000 - time: 1.158203, loss: 2.614026, perplexity: 13.653912, precision: 0.000000, batch_len: 80.000000
Train, loss=2.61402607: 369it [09:47,  1.26s/it]2017-06-01 18:19:19,049 root  INFO     step 369.000000 - time: 1.286069, loss: 2.736882, perplexity: 15.438775, precision: 0.000000, batch_len: 124.000000
Train, loss=2.73688221: 370it [09:48,  1.29s/it]2017-06-01 18:19:20,157 root  INFO     step 370.000000 - time: 1.076694, loss: 2.735704, perplexity: 15.420595, precision: 0.000000, batch_len: 88.000000
Train, loss=2.73570395: 371it [09:50,  1.24s/it]2017-06-01 18:19:21,252 root  INFO     step 371.000000 - time: 1.091448, loss: 2.643449, perplexity: 14.061616, precision: 0.000000, batch_len: 108.000000
Train, loss=2.64344883: 372it [09:51,  1.19s/it]2017-06-01 18:19:22,305 root  INFO     step 372.000000 - time: 0.995930, loss: 2.673159, perplexity: 14.485656, precision: 0.000000, batch_len: 102.000000
Train, loss=2.67315888: 373it [09:52,  1.15s/it]2017-06-01 18:19:23,703 root  INFO     step 373.000000 - time: 1.280012, loss: 2.654991, perplexity: 14.224860, precision: 0.000000, batch_len: 112.000000
Train, loss=2.65499115: 374it [09:53,  1.23s/it]2017-06-01 18:19:24,927 root  INFO     step 374.000000 - time: 1.180627, loss: 2.590781, perplexity: 13.340183, precision: 0.000000, batch_len: 101.000000
Train, loss=2.59078074: 375it [09:54,  1.23s/it]2017-06-01 18:19:26,035 root  INFO     step 375.000000 - time: 1.085580, loss: 2.635202, perplexity: 13.946128, precision: 0.000000, batch_len: 105.000000
Train, loss=2.63520193: 376it [09:55,  1.19s/it]2017-06-01 18:19:27,226 root  INFO     step 376.000000 - time: 1.161198, loss: 2.642452, perplexity: 14.047610, precision: 0.000000, batch_len: 114.000000
Train, loss=2.64245224: 377it [09:57,  1.19s/it]2017-06-01 18:19:28,921 root  INFO     step 377.000000 - time: 1.632175, loss: 2.773701, perplexity: 16.017802, precision: 0.000000, batch_len: 90.000000
Train, loss=2.77370071: 378it [09:58,  1.34s/it]2017-06-01 18:19:30,377 root  INFO     step 378.000000 - time: 1.417277, loss: 2.669586, perplexity: 14.433998, precision: 0.000000, batch_len: 117.000000
Train, loss=2.66958642: 379it [10:00,  1.38s/it]2017-06-01 18:19:31,652 root  INFO     step 379.000000 - time: 1.120435, loss: 2.732843, perplexity: 15.376547, precision: 0.000000, batch_len: 79.000000
Train, loss=2.73284340: 380it [10:01,  1.35s/it]2017-06-01 18:19:32,889 root  INFO     step 380.000000 - time: 1.193737, loss: 2.607710, perplexity: 13.567950, precision: 0.000000, batch_len: 86.000000
Train, loss=2.60771036: 381it [10:02,  1.31s/it]2017-06-01 18:19:33,925 root  INFO     step 381.000000 - time: 0.975429, loss: 2.589490, perplexity: 13.322977, precision: 0.000000, batch_len: 81.000000
Train, loss=2.58949018: 382it [10:03,  1.23s/it]2017-06-01 18:19:35,123 root  INFO     step 382.000000 - time: 1.044216, loss: 2.589412, perplexity: 13.321942, precision: 0.000000, batch_len: 91.000000
Train, loss=2.58941245: 383it [10:04,  1.22s/it]2017-06-01 18:19:36,147 root  INFO     step 383.000000 - time: 0.990404, loss: 2.575107, perplexity: 13.132717, precision: 0.000000, batch_len: 89.000000
Train, loss=2.57510662: 384it [10:06,  1.16s/it]2017-06-01 18:19:37,167 root  INFO     step 384.000000 - time: 0.936378, loss: 2.639945, perplexity: 14.012427, precision: 0.000000, batch_len: 85.000000
Train, loss=2.63994455: 385it [10:07,  1.12s/it]2017-06-01 18:19:38,633 root  INFO     step 385.000000 - time: 1.447812, loss: 2.724992, perplexity: 15.256289, precision: 0.000000, batch_len: 125.000000
Train, loss=2.72499180: 386it [10:08,  1.22s/it]2017-06-01 18:19:40,243 root  INFO     step 386.000000 - time: 1.567272, loss: 2.635223, perplexity: 13.946418, precision: 0.000000, batch_len: 123.000000
Train, loss=2.63522267: 387it [10:10,  1.34s/it]2017-06-01 18:19:41,637 root  INFO     step 387.000000 - time: 1.323797, loss: 2.662478, perplexity: 14.331755, precision: 0.000000, batch_len: 121.000000
Train, loss=2.66247773: 388it [10:11,  1.36s/it]2017-06-01 18:19:42,816 root  INFO     step 388.000000 - time: 1.031646, loss: 2.715518, perplexity: 15.112433, precision: 0.000000, batch_len: 109.000000
Train, loss=2.71551776: 389it [10:12,  1.30s/it]2017-06-01 18:19:44,080 root  INFO     step 389.000000 - time: 1.238898, loss: 2.796271, perplexity: 16.383436, precision: 0.000000, batch_len: 118.000000
Train, loss=2.79627085: 390it [10:13,  1.29s/it]2017-06-01 18:19:45,513 root  INFO     step 390.000000 - time: 1.267228, loss: 2.708243, perplexity: 15.002894, precision: 0.000000, batch_len: 115.000000
Train, loss=2.70824313: 391it [10:15,  1.33s/it]2017-06-01 18:19:46,696 root  INFO     step 391.000000 - time: 1.166547, loss: 2.610786, perplexity: 13.609750, precision: 0.000000, batch_len: 84.000000
Train, loss=2.61078644: 392it [10:16,  1.29s/it]2017-06-01 18:19:47,837 root  INFO     step 392.000000 - time: 1.097993, loss: 2.648660, perplexity: 14.135081, precision: 0.000000, batch_len: 107.000000
Train, loss=2.64865971: 393it [10:17,  1.24s/it]2017-06-01 18:19:49,221 root  INFO     step 393.000000 - time: 1.326686, loss: 2.664143, perplexity: 14.355636, precision: 0.000000, batch_len: 129.000000
Train, loss=2.66414261: 394it [10:19,  1.29s/it]2017-06-01 18:19:50,148 root  INFO     step 394.000000 - time: 0.886236, loss: 2.732926, perplexity: 15.377822, precision: 0.000000, batch_len: 94.000000
Train, loss=2.73292637: 395it [10:20,  1.18s/it]2017-06-01 18:19:51,206 root  INFO     step 395.000000 - time: 1.027661, loss: 2.595130, perplexity: 13.398332, precision: 0.000000, batch_len: 98.000000
Train, loss=2.59513021: 396it [10:21,  1.14s/it]2017-06-01 18:19:53,061 root  INFO     step 396.000000 - time: 1.696898, loss: 2.732060, perplexity: 15.364512, precision: 0.000000, batch_len: 137.000000
Train, loss=2.73206043: 397it [10:22,  1.36s/it]2017-06-01 18:19:54,581 root  INFO     step 397.000000 - time: 1.377319, loss: 2.670516, perplexity: 14.447415, precision: 0.000000, batch_len: 126.000000
Train, loss=2.67051554: 398it [10:24,  1.41s/it]2017-06-01 18:19:55,943 root  INFO     step 398.000000 - time: 1.334492, loss: 2.608030, perplexity: 13.572282, precision: 0.000000, batch_len: 116.000000
Train, loss=2.60802960: 399it [10:25,  1.39s/it]2017-06-01 18:19:57,134 root  INFO     step 399.000000 - time: 1.166471, loss: 2.635453, perplexity: 13.949627, precision: 0.000000, batch_len: 119.000000
Train, loss=2.63545275: 400it [10:27,  1.33s/it]2017-06-01 18:19:58,191 root  INFO     step 400.000000 - time: 1.043501, loss: 2.652870, perplexity: 14.194725, precision: 0.000000, batch_len: 83.000000
Train, loss=2.65287042: 401it [10:28,  1.25s/it]2017-06-01 18:19:59,608 root  INFO     step 401.000000 - time: 1.370594, loss: 2.637237, perplexity: 13.974543, precision: 0.000000, batch_len: 99.000000
Train, loss=2.63723731: 402it [10:29,  1.30s/it]2017-06-01 18:20:00,680 root  INFO     step 402.000000 - time: 0.944483, loss: 2.596030, perplexity: 13.410390, precision: 0.000000, batch_len: 87.000000
Train, loss=2.59602976: 403it [10:30,  1.23s/it]2017-06-01 18:20:02,210 root  INFO     step 403.000000 - time: 1.447987, loss: 2.648451, perplexity: 14.132129, precision: 0.000000, batch_len: 144.000000
Train, loss=2.64845085: 404it [10:32,  1.32s/it]2017-06-01 18:20:03,618 root  INFO     step 404.000000 - time: 1.265756, loss: 2.668743, perplexity: 14.421831, precision: 0.000000, batch_len: 130.000000
Train, loss=2.66874313: 405it [10:33,  1.35s/it]2017-06-01 18:20:05,319 root  INFO     step 405.000000 - time: 1.495224, loss: 2.607630, perplexity: 13.566856, precision: 0.000000, batch_len: 135.000000
Train, loss=2.60762978: 406it [10:35,  1.45s/it]2017-06-01 18:20:06,660 root  INFO     step 406.000000 - time: 1.285858, loss: 2.624457, perplexity: 13.797079, precision: 0.000000, batch_len: 96.000000
Train, loss=2.62445688: 407it [10:36,  1.42s/it]2017-06-01 18:20:08,009 root  INFO     step 407.000000 - time: 1.336637, loss: 2.649557, perplexity: 14.147771, precision: 0.000000, batch_len: 133.000000
Train, loss=2.64955711: 408it [10:37,  1.40s/it]2017-06-01 18:20:09,098 root  INFO     step 408.000000 - time: 0.967469, loss: 2.671132, perplexity: 14.456326, precision: 0.000000, batch_len: 82.000000
Train, loss=2.67113209: 409it [10:38,  1.31s/it]2017-06-01 18:20:10,139 root  INFO     step 409.000000 - time: 0.913459, loss: 2.595103, perplexity: 13.397971, precision: 0.000000, batch_len: 78.000000
Train, loss=2.59510326: 410it [10:40,  1.23s/it]2017-06-01 18:20:11,850 root  INFO     step 410.000000 - time: 1.550794, loss: 2.695727, perplexity: 14.816284, precision: 0.000000, batch_len: 122.000000
Train, loss=2.69572687: 411it [10:41,  1.37s/it]2017-06-01 18:20:13,519 root  INFO     step 411.000000 - time: 1.626297, loss: 2.639874, perplexity: 14.011438, precision: 0.000000, batch_len: 136.000000
Train, loss=2.63987398: 412it [10:43,  1.46s/it]2017-06-01 18:20:15,415 root  INFO     step 412.000000 - time: 1.753175, loss: 2.921078, perplexity: 18.561281, precision: 0.000000, batch_len: 150.000000
Train, loss=2.92107773: 413it [10:45,  1.59s/it]2017-06-01 18:20:17,286 root  INFO     step 413.000000 - time: 1.744919, loss: 2.709975, perplexity: 15.028903, precision: 0.000000, batch_len: 152.000000
Train, loss=2.70997524: 414it [10:47,  1.68s/it]2017-06-01 18:20:19,014 root  INFO     step 414.000000 - time: 1.653338, loss: 2.680294, perplexity: 14.589386, precision: 0.000000, batch_len: 138.000000
Train, loss=2.68029428: 415it [10:48,  1.69s/it]2017-06-01 18:20:20,573 root  INFO     step 415.000000 - time: 1.492573, loss: 2.688841, perplexity: 14.714613, precision: 0.000000, batch_len: 142.000000
Train, loss=2.68884110: 416it [10:50,  1.65s/it]2017-06-01 18:20:22,040 root  INFO     step 416.000000 - time: 1.327000, loss: 2.610607, perplexity: 13.607303, precision: 0.000000, batch_len: 134.000000
Train, loss=2.61060667: 417it [10:51,  1.60s/it]2017-06-01 18:20:23,371 root  INFO     step 417.000000 - time: 1.257540, loss: 2.653434, perplexity: 14.202724, precision: 0.000000, batch_len: 132.000000
Train, loss=2.65343380: 418it [10:53,  1.52s/it]2017-06-01 18:20:24,415 root  INFO     step 418.000000 - time: 1.012559, loss: 2.697022, perplexity: 14.835485, precision: 0.000000, batch_len: 77.000000
Train, loss=2.69702196: 419it [10:54,  1.37s/it]2017-06-01 18:20:26,010 root  INFO     step 419.000000 - time: 1.514578, loss: 2.546831, perplexity: 12.766578, precision: 0.000000, batch_len: 72.000000
Train, loss=2.54683065: 420it [10:55,  1.44s/it]2017-06-01 18:20:27,621 root  INFO     step 420.000000 - time: 1.536978, loss: 2.754265, perplexity: 15.709488, precision: 0.000000, batch_len: 141.000000
Train, loss=2.75426483: 421it [10:57,  1.49s/it]2017-06-01 18:20:29,198 root  INFO     step 421.000000 - time: 1.569258, loss: 2.614630, perplexity: 13.662163, precision: 0.000000, batch_len: 139.000000
Train, loss=2.61463022: 422it [10:59,  1.52s/it]2017-06-01 18:20:30,204 root  INFO     step 422.000000 - time: 0.965867, loss: 2.674234, perplexity: 14.501240, precision: 0.000000, batch_len: 74.000000
Train, loss=2.67423415: 423it [11:00,  1.36s/it]2017-06-01 18:20:31,301 root  INFO     step 423.000000 - time: 1.020938, loss: 2.581955, perplexity: 13.222970, precision: 0.000000, batch_len: 76.000000
Train, loss=2.58195543: 424it [11:01,  1.28s/it]2017-06-01 18:20:32,317 root  INFO     step 424.000000 - time: 0.937990, loss: 2.512494, perplexity: 12.335652, precision: 0.000000, batch_len: 71.000000
Train, loss=2.51249361: 425it [11:02,  1.20s/it]2017-06-01 18:20:33,985 root  INFO     step 425.000000 - time: 1.528371, loss: 2.712861, perplexity: 15.072337, precision: 0.000000, batch_len: 131.000000
Train, loss=2.71286106: 426it [11:03,  1.34s/it]2017-06-01 18:20:34,159 root  INFO     Generating first batch)
2017-06-01 18:20:38,274 root  INFO     step 426.000000 - time: 1.272371, loss: 2.629478, perplexity: 13.866529, precision: 0.000000, batch_len: 96.000000
Train, loss=2.62947798: 427it [11:08,  2.23s/it]2017-06-01 18:20:39,616 root  INFO     step 427.000000 - time: 1.280769, loss: 2.621744, perplexity: 13.759695, precision: 0.000000, batch_len: 111.000000
Train, loss=2.62174368: 428it [11:09,  1.96s/it]2017-06-01 18:20:41,005 root  INFO     step 428.000000 - time: 1.307288, loss: 2.724811, perplexity: 15.253536, precision: 0.000000, batch_len: 128.000000
Train, loss=2.72481132: 429it [11:10,  1.79s/it]2017-06-01 18:20:42,113 root  INFO     step 429.000000 - time: 1.103283, loss: 2.641091, perplexity: 14.028502, precision: 0.000000, batch_len: 113.000000
Train, loss=2.64109111: 430it [11:11,  1.59s/it]2017-06-01 18:20:43,353 root  INFO     step 430.000000 - time: 0.927843, loss: 2.668700, perplexity: 14.421209, precision: 0.000000, batch_len: 92.000000
Train, loss=2.66869998: 431it [11:13,  1.48s/it]2017-06-01 18:20:44,683 root  INFO     step 431.000000 - time: 1.297467, loss: 2.617991, perplexity: 13.708156, precision: 0.000000, batch_len: 110.000000
Train, loss=2.61799097: 432it [11:14,  1.44s/it]2017-06-01 18:20:46,274 root  INFO     step 432.000000 - time: 1.542516, loss: 2.623217, perplexity: 13.779984, precision: 0.000000, batch_len: 117.000000
Train, loss=2.62321711: 433it [11:16,  1.48s/it]2017-06-01 18:20:47,315 root  INFO     step 433.000000 - time: 1.012646, loss: 2.654039, perplexity: 14.211321, precision: 0.000000, batch_len: 104.000000
Train, loss=2.65403891: 434it [11:17,  1.35s/it]2017-06-01 18:20:48,499 root  INFO     step 434.000000 - time: 1.080621, loss: 2.589537, perplexity: 13.323600, precision: 0.000000, batch_len: 108.000000
Train, loss=2.58953691: 435it [11:18,  1.30s/it]2017-06-01 18:20:49,623 root  INFO     step 435.000000 - time: 1.098225, loss: 2.576657, perplexity: 13.153098, precision: 0.000000, batch_len: 106.000000
Train, loss=2.57665730: 436it [11:19,  1.25s/it]2017-06-01 18:20:50,630 root  INFO     step 436.000000 - time: 0.964628, loss: 2.634957, perplexity: 13.942714, precision: 0.000000, batch_len: 88.000000
Train, loss=2.63495708: 437it [11:20,  1.18s/it]2017-06-01 18:20:51,919 root  INFO     step 437.000000 - time: 1.219522, loss: 2.572914, perplexity: 13.103955, precision: 0.000000, batch_len: 100.000000
Train, loss=2.57291412: 438it [11:21,  1.21s/it]2017-06-01 18:20:53,542 root  INFO     step 438.000000 - time: 1.540811, loss: 2.594439, perplexity: 13.389074, precision: 0.000000, batch_len: 120.000000
Train, loss=2.59443903: 439it [11:23,  1.33s/it]2017-06-01 18:20:54,802 root  INFO     step 439.000000 - time: 1.100395, loss: 2.559486, perplexity: 12.929175, precision: 0.000000, batch_len: 97.000000
Train, loss=2.55948639: 440it [11:24,  1.31s/it]2017-06-01 18:20:55,834 root  INFO     step 440.000000 - time: 1.014859, loss: 2.554691, perplexity: 12.867318, precision: 0.000000, batch_len: 101.000000
Train, loss=2.55469060: 441it [11:25,  1.23s/it]2017-06-01 18:20:57,090 root  INFO     step 441.000000 - time: 1.158574, loss: 2.592606, perplexity: 13.364552, precision: 0.000000, batch_len: 91.000000
Train, loss=2.59260583: 442it [11:26,  1.24s/it]2017-06-01 18:20:58,353 root  INFO     step 442.000000 - time: 1.254782, loss: 2.648429, perplexity: 14.131819, precision: 0.000000, batch_len: 114.000000
Train, loss=2.64842892: 443it [11:28,  1.24s/it]2017-06-01 18:20:59,854 root  INFO     step 443.000000 - time: 1.474786, loss: 2.669783, perplexity: 14.436838, precision: 0.000000, batch_len: 125.000000
Train, loss=2.66978312: 444it [11:29,  1.32s/it]2017-06-01 18:21:00,994 root  INFO     step 444.000000 - time: 1.070488, loss: 2.545123, perplexity: 12.744800, precision: 0.000000, batch_len: 103.000000
Train, loss=2.54512334: 445it [11:30,  1.27s/it]2017-06-01 18:21:01,994 root  INFO     step 445.000000 - time: 0.953509, loss: 2.539453, perplexity: 12.672737, precision: 0.000000, batch_len: 80.000000
Train, loss=2.53945303: 446it [11:31,  1.19s/it]2017-06-01 18:21:02,927 root  INFO     step 446.000000 - time: 0.910533, loss: 2.554847, perplexity: 12.869327, precision: 0.000000, batch_len: 81.000000
Train, loss=2.55484676: 447it [11:32,  1.11s/it]2017-06-01 18:21:03,868 root  INFO     step 447.000000 - time: 0.886099, loss: 2.573486, perplexity: 13.111453, precision: 0.000000, batch_len: 93.000000
Train, loss=2.57348609: 448it [11:33,  1.06s/it]2017-06-01 18:21:05,071 root  INFO     step 448.000000 - time: 1.171025, loss: 2.622744, perplexity: 13.773464, precision: 0.000000, batch_len: 105.000000
Train, loss=2.62274384: 449it [11:34,  1.10s/it]2017-06-01 18:21:06,283 root  INFO     step 449.000000 - time: 1.173459, loss: 2.615267, perplexity: 13.670867, precision: 0.000000, batch_len: 90.000000
Train, loss=2.61526704: 450it [11:36,  1.14s/it]2017-06-01 18:21:07,390 root  INFO     step 450.000000 - time: 1.101206, loss: 2.576575, perplexity: 13.152013, precision: 0.000000, batch_len: 102.000000
Train, loss=2.57657480: 451it [11:37,  1.13s/it]2017-06-01 18:21:08,610 root  INFO     step 451.000000 - time: 1.136532, loss: 2.568881, perplexity: 13.051212, precision: 0.000000, batch_len: 109.000000
Train, loss=2.56888103: 452it [11:38,  1.15s/it]2017-06-01 18:21:09,774 root  INFO     step 452.000000 - time: 1.037835, loss: 2.499269, perplexity: 12.173589, precision: 0.000000, batch_len: 89.000000
Train, loss=2.49926877: 453it [11:39,  1.16s/it]2017-06-01 18:21:10,795 root  INFO     step 453.000000 - time: 1.012886, loss: 2.557799, perplexity: 12.907375, precision: 0.000000, batch_len: 79.000000
Train, loss=2.55779886: 454it [11:40,  1.12s/it]2017-06-01 18:21:12,505 root  INFO     step 454.000000 - time: 1.702588, loss: 2.707223, perplexity: 14.987596, precision: 0.000000, batch_len: 112.000000
Train, loss=2.70722294: 455it [11:42,  1.29s/it]2017-06-01 18:21:13,611 root  INFO     step 455.000000 - time: 0.958659, loss: 2.612237, perplexity: 13.629509, precision: 0.000000, batch_len: 83.000000
Train, loss=2.61223722: 456it [11:43,  1.24s/it]2017-06-01 18:21:14,766 root  INFO     step 456.000000 - time: 1.102629, loss: 2.552749, perplexity: 12.842355, precision: 0.000000, batch_len: 107.000000
Train, loss=2.55274868: 457it [11:44,  1.21s/it]2017-06-01 18:21:16,168 root  INFO     step 457.000000 - time: 1.303178, loss: 2.633233, perplexity: 13.918691, precision: 0.000000, batch_len: 121.000000
Train, loss=2.63323259: 458it [11:46,  1.27s/it]2017-06-01 18:21:17,101 root  INFO     step 458.000000 - time: 0.912045, loss: 2.633627, perplexity: 13.924184, precision: 0.000000, batch_len: 86.000000
Train, loss=2.63362718: 459it [11:46,  1.17s/it]2017-06-01 18:21:18,636 root  INFO     step 459.000000 - time: 1.473999, loss: 2.574652, perplexity: 13.126745, precision: 0.000000, batch_len: 116.000000
Train, loss=2.57465172: 460it [11:48,  1.28s/it]2017-06-01 18:21:20,310 root  INFO     step 460.000000 - time: 1.584415, loss: 2.588194, perplexity: 13.305718, precision: 0.000000, batch_len: 123.000000
Train, loss=2.58819389: 461it [11:50,  1.40s/it]2017-06-01 18:21:21,491 root  INFO     step 461.000000 - time: 1.070856, loss: 2.608452, perplexity: 13.578010, precision: 0.000000, batch_len: 115.000000
Train, loss=2.60845160: 462it [11:51,  1.33s/it]2017-06-01 18:21:22,538 root  INFO     step 462.000000 - time: 1.042360, loss: 2.528209, perplexity: 12.531046, precision: 0.000000, batch_len: 98.000000
Train, loss=2.52820921: 463it [11:52,  1.25s/it]2017-06-01 18:21:23,827 root  INFO     step 463.000000 - time: 1.252910, loss: 2.612044, perplexity: 13.626880, precision: 0.000000, batch_len: 124.000000
Train, loss=2.61204433: 464it [11:53,  1.26s/it]2017-06-01 18:21:25,128 root  INFO     step 464.000000 - time: 1.227467, loss: 2.609782, perplexity: 13.596080, precision: 0.000000, batch_len: 84.000000
Train, loss=2.60978150: 465it [11:55,  1.27s/it]2017-06-01 18:21:26,321 root  INFO     step 465.000000 - time: 1.162632, loss: 2.466077, perplexity: 11.776159, precision: 0.000000, batch_len: 82.000000
Train, loss=2.46607709: 466it [11:56,  1.25s/it]2017-06-01 18:21:27,501 root  INFO     step 466.000000 - time: 1.014465, loss: 2.503399, perplexity: 12.223971, precision: 0.000000, batch_len: 87.000000
Train, loss=2.50339890: 467it [11:57,  1.23s/it]2017-06-01 18:21:28,812 root  INFO     step 467.000000 - time: 1.171624, loss: 2.601676, perplexity: 13.486322, precision: 0.000000, batch_len: 99.000000
Train, loss=2.60167599: 468it [11:58,  1.25s/it]2017-06-01 18:21:30,151 root  INFO     step 468.000000 - time: 1.321877, loss: 2.576268, perplexity: 13.147981, precision: 0.000000, batch_len: 118.000000
Train, loss=2.57626820: 469it [12:00,  1.28s/it]2017-06-01 18:21:31,451 root  INFO     step 469.000000 - time: 1.248454, loss: 2.585665, perplexity: 13.272106, precision: 0.000000, batch_len: 85.000000
Train, loss=2.58566451: 470it [12:01,  1.29s/it]2017-06-01 18:21:32,479 root  INFO     step 470.000000 - time: 0.968998, loss: 2.538688, perplexity: 12.663045, precision: 0.000000, batch_len: 94.000000
Train, loss=2.53868794: 471it [12:02,  1.21s/it]2017-06-01 18:21:34,264 root  INFO     step 471.000000 - time: 1.674450, loss: 2.557482, perplexity: 12.903280, precision: 0.000000, batch_len: 135.000000
Train, loss=2.55748153: 472it [12:04,  1.38s/it]2017-06-01 18:21:35,619 root  INFO     step 472.000000 - time: 1.323555, loss: 2.543947, perplexity: 12.729819, precision: 0.000000, batch_len: 119.000000
Train, loss=2.54394722: 473it [12:05,  1.37s/it]2017-06-01 18:21:37,172 root  INFO     step 473.000000 - time: 1.323032, loss: 2.500983, perplexity: 12.194478, precision: 0.000000, batch_len: 133.000000
Train, loss=2.50098324: 474it [12:07,  1.43s/it]2017-06-01 18:21:38,399 root  INFO     step 474.000000 - time: 1.181278, loss: 2.543355, perplexity: 12.722276, precision: 0.000000, batch_len: 129.000000
Train, loss=2.54335451: 475it [12:08,  1.37s/it]2017-06-01 18:21:39,984 root  INFO     step 475.000000 - time: 1.466529, loss: 2.575257, perplexity: 13.134690, precision: 0.000000, batch_len: 126.000000
Train, loss=2.57525682: 476it [12:09,  1.43s/it]2017-06-01 18:21:41,673 root  INFO     step 476.000000 - time: 1.584252, loss: 2.567553, perplexity: 13.033886, precision: 0.000000, batch_len: 137.000000
Train, loss=2.56755257: 477it [12:11,  1.51s/it]2017-06-01 18:21:43,152 root  INFO     step 477.000000 - time: 1.424095, loss: 2.543685, perplexity: 12.726487, precision: 0.000000, batch_len: 144.000000
Train, loss=2.54368544: 478it [12:13,  1.50s/it]2017-06-01 18:21:44,590 root  INFO     step 478.000000 - time: 1.407005, loss: 2.551197, perplexity: 12.822438, precision: 0.000000, batch_len: 130.000000
Train, loss=2.55119658: 479it [12:14,  1.48s/it]2017-06-01 18:21:45,578 root  INFO     step 479.000000 - time: 0.894965, loss: 2.457505, perplexity: 11.675647, precision: 0.000000, batch_len: 78.000000
Train, loss=2.45750523: 480it [12:15,  1.33s/it]2017-06-01 18:21:47,843 root  INFO     step 480.000000 - time: 2.073969, loss: 2.972298, perplexity: 19.536762, precision: 0.000000, batch_len: 150.000000
Train, loss=2.97229791: 481it [12:17,  1.61s/it]2017-06-01 18:21:49,402 root  INFO     step 481.000000 - time: 1.413175, loss: 2.609678, perplexity: 13.594670, precision: 0.000000, batch_len: 138.000000
Train, loss=2.60967779: 482it [12:19,  1.60s/it]2017-06-01 18:21:50,959 root  INFO     step 482.000000 - time: 1.420361, loss: 2.511919, perplexity: 12.328569, precision: 0.000000, batch_len: 136.000000
Train, loss=2.51191926: 483it [12:20,  1.58s/it]2017-06-01 18:21:52,215 root  INFO     step 483.000000 - time: 1.192540, loss: 2.562670, perplexity: 12.970408, precision: 0.000000, batch_len: 132.000000
Train, loss=2.56267047: 484it [12:22,  1.49s/it]2017-06-01 18:21:53,552 root  INFO     step 484.000000 - time: 1.218300, loss: 2.543055, perplexity: 12.718467, precision: 0.000000, batch_len: 96.000000
Train, loss=2.54305506: 485it [12:23,  1.44s/it]2017-06-01 18:21:54,696 root  INFO     step 485.000000 - time: 1.130157, loss: 2.447891, perplexity: 11.563938, precision: 0.000000, batch_len: 77.000000
Train, loss=2.44789147: 486it [12:24,  1.35s/it]2017-06-01 18:21:56,187 root  INFO     step 486.000000 - time: 1.443495, loss: 2.725775, perplexity: 15.268250, precision: 0.000000, batch_len: 139.000000
Train, loss=2.72577548: 487it [12:26,  1.39s/it]2017-06-01 18:21:57,625 root  INFO     step 487.000000 - time: 1.397200, loss: 2.525261, perplexity: 12.494161, precision: 0.000000, batch_len: 141.000000
Train, loss=2.52526140: 488it [12:27,  1.41s/it]2017-06-01 18:21:58,819 root  INFO     step 488.000000 - time: 1.050916, loss: 2.627223, perplexity: 13.835296, precision: 0.000000, batch_len: 72.000000
Train, loss=2.62722301: 489it [12:28,  1.34s/it]2017-06-01 18:21:59,882 root  INFO     step 489.000000 - time: 1.029432, loss: 2.482381, perplexity: 11.969729, precision: 0.000000, batch_len: 76.000000
Train, loss=2.48238087: 490it [12:29,  1.26s/it]2017-06-01 18:22:00,518 root  INFO     step 490.000000 - time: 0.539774, loss: 2.768169, perplexity: 15.929447, precision: 0.000000, batch_len: 142.000000
Train, loss=2.76816940: 491it [12:30,  1.07s/it]2017-06-01 18:22:01,789 root  INFO     step 491.000000 - time: 1.172494, loss: 2.559222, perplexity: 12.925763, precision: 0.000000, batch_len: 131.000000
Train, loss=2.55922246: 492it [12:31,  1.13s/it]2017-06-01 18:22:03,717 root  INFO     step 492.000000 - time: 1.890411, loss: 2.633006, perplexity: 13.915542, precision: 0.000000, batch_len: 152.000000
Train, loss=2.63300633: 493it [12:33,  1.37s/it]2017-06-01 18:22:05,078 root  INFO     step 493.000000 - time: 1.333324, loss: 2.536545, perplexity: 12.635933, precision: 0.000000, batch_len: 122.000000
Train, loss=2.53654456: 494it [12:34,  1.37s/it]2017-06-01 18:22:06,418 root  INFO     step 494.000000 - time: 1.259583, loss: 2.468461, perplexity: 11.804261, precision: 0.000000, batch_len: 134.000000
Train, loss=2.46846056: 495it [12:36,  1.36s/it]2017-06-01 18:22:07,494 root  INFO     step 495.000000 - time: 1.036989, loss: 2.621088, perplexity: 13.750673, precision: 0.000000, batch_len: 74.000000
Train, loss=2.62108779: 496it [12:37,  1.27s/it]2017-06-01 18:22:08,885 root  INFO     step 496.000000 - time: 1.033370, loss: 2.370759, perplexity: 10.705510, precision: 0.000000, batch_len: 71.000000
Train, loss=2.37075853: 497it [12:38,  1.31s/it]2017-06-01 18:22:08,951 root  INFO     Generating first batch)
2017-06-01 18:22:11,975 root  INFO     step 497.000000 - time: 0.414982, loss: 2.451233, perplexity: 11.602646, precision: 0.000000, batch_len: 96.000000
Train, loss=2.45123315: 498it [12:41,  1.84s/it]2017-06-01 18:22:13,931 root  INFO     step 498.000000 - time: 1.801518, loss: 2.521947, perplexity: 12.452823, precision: 0.000000, batch_len: 90.000000
Train, loss=2.52194738: 499it [12:43,  1.88s/it]2017-06-01 18:22:15,415 root  INFO     step 499.000000 - time: 1.295722, loss: 2.499976, perplexity: 12.182198, precision: 0.000000, batch_len: 113.000000
Train, loss=2.49997568: 500it [12:45,  1.76s/it]2017-06-01 18:22:16,698 root  INFO     step 500.000000 - time: 1.124257, loss: 2.429228, perplexity: 11.350112, precision: 0.000000, batch_len: 110.000000
Train, loss=2.42922759: 501it [12:46,  1.62s/it]2017-06-01 18:22:18,010 root  INFO     step 501.000000 - time: 1.309018, loss: 2.391326, perplexity: 10.927980, precision: 0.000000, batch_len: 117.000000
Train, loss=2.39132643: 502it [12:47,  1.53s/it]2017-06-01 18:22:19,183 root  INFO     step 502.000000 - time: 1.035615, loss: 2.432957, perplexity: 11.392517, precision: 0.000000, batch_len: 102.000000
Train, loss=2.43295670: 503it [12:49,  1.42s/it]2017-06-01 18:22:20,409 root  INFO     step 503.000000 - time: 1.167309, loss: 2.359386, perplexity: 10.584455, precision: 0.000000, batch_len: 101.000000
Train, loss=2.35938644: 504it [12:50,  1.36s/it]2017-06-01 18:22:21,714 root  INFO     step 504.000000 - time: 1.230018, loss: 2.346646, perplexity: 10.450458, precision: 0.000000, batch_len: 92.000000
Train, loss=2.34664583: 505it [12:51,  1.34s/it]2017-06-01 18:22:22,811 root  INFO     step 505.000000 - time: 1.032480, loss: 2.345604, perplexity: 10.439581, precision: 0.000000, batch_len: 105.000000
Train, loss=2.34560442: 506it [12:52,  1.27s/it]2017-06-01 18:22:24,249 root  INFO     step 506.000000 - time: 1.361536, loss: 2.427217, perplexity: 11.327317, precision: 0.000000, batch_len: 120.000000
Train, loss=2.42721725: 507it [12:54,  1.32s/it]2017-06-01 18:22:25,448 root  INFO     step 507.000000 - time: 1.190061, loss: 2.512794, perplexity: 12.339352, precision: 0.000000, batch_len: 128.000000
Train, loss=2.51279354: 508it [12:55,  1.28s/it]2017-06-01 18:22:26,847 root  INFO     step 508.000000 - time: 1.339214, loss: 2.459447, perplexity: 11.698345, precision: 0.000000, batch_len: 111.000000
Train, loss=2.45944738: 509it [12:56,  1.32s/it]2017-06-01 18:22:27,607 root  INFO     step 509.000000 - time: 0.743087, loss: 2.359396, perplexity: 10.584556, precision: 0.000000, batch_len: 108.000000
Train, loss=2.35939598: 510it [12:57,  1.15s/it]2017-06-01 18:22:28,354 root  INFO     step 510.000000 - time: 0.673696, loss: 2.394369, perplexity: 10.961281, precision: 0.000000, batch_len: 88.000000
Train, loss=2.39436913: 511it [12:58,  1.03s/it]2017-06-01 18:22:29,185 root  INFO     step 511.000000 - time: 0.693226, loss: 2.382730, perplexity: 10.834441, precision: 0.015625, batch_len: 97.000000
Train, loss=2.38273001: 512it [12:59,  1.03it/s]2017-06-01 18:22:29,887 root  INFO     step 512.000000 - time: 0.675118, loss: 2.494236, perplexity: 12.112482, precision: 0.000000, batch_len: 93.000000
Train, loss=2.49423647: 513it [12:59,  1.12it/s]2017-06-01 18:22:30,611 root  INFO     step 513.000000 - time: 0.638264, loss: 2.342787, perplexity: 10.410205, precision: 0.000000, batch_len: 86.000000
Train, loss=2.34278655: 514it [13:00,  1.19it/s]2017-06-01 18:22:31,660 root  INFO     step 514.000000 - time: 0.844833, loss: 2.470886, perplexity: 11.832929, precision: 0.000000, batch_len: 121.000000
Train, loss=2.47088623: 515it [13:01,  1.11it/s]2017-06-01 18:22:32,389 root  INFO     step 515.000000 - time: 0.718300, loss: 2.365629, perplexity: 10.650733, precision: 0.000000, batch_len: 109.000000
Train, loss=2.36562872: 516it [13:02,  1.18it/s]2017-06-01 18:22:33,122 root  INFO     step 516.000000 - time: 0.692317, loss: 2.340415, perplexity: 10.385541, precision: 0.000000, batch_len: 106.000000
Train, loss=2.34041452: 517it [13:02,  1.23it/s]2017-06-01 18:22:33,915 root  INFO     step 517.000000 - time: 0.742649, loss: 2.323045, perplexity: 10.206704, precision: 0.000000, batch_len: 112.000000
Train, loss=2.32304478: 518it [13:03,  1.24it/s]2017-06-01 18:22:34,633 root  INFO     step 518.000000 - time: 0.704593, loss: 2.327059, perplexity: 10.247754, precision: 0.000000, batch_len: 104.000000
Train, loss=2.32705855: 519it [13:04,  1.28it/s]2017-06-01 18:22:35,895 root  INFO     step 519.000000 - time: 1.121534, loss: 2.446529, perplexity: 11.548192, precision: 0.000000, batch_len: 103.000000
Train, loss=2.44652891: 520it [13:05,  1.08it/s]2017-06-01 18:22:37,311 root  INFO     step 520.000000 - time: 1.330864, loss: 2.472754, perplexity: 11.855045, precision: 0.000000, batch_len: 89.000000
Train, loss=2.47275352: 521it [13:07,  1.07s/it]2017-06-01 18:22:38,321 root  INFO     step 521.000000 - time: 0.958925, loss: 2.330936, perplexity: 10.287571, precision: 0.000000, batch_len: 81.000000
Train, loss=2.33093643: 522it [13:08,  1.05s/it]2017-06-01 18:22:39,407 root  INFO     step 522.000000 - time: 0.975771, loss: 2.270747, perplexity: 9.686634, precision: 0.000000, batch_len: 80.000000
Train, loss=2.27074695: 523it [13:09,  1.06s/it]2017-06-01 18:22:40,762 root  INFO     step 523.000000 - time: 1.313691, loss: 2.372787, perplexity: 10.727250, precision: 0.000000, batch_len: 91.000000
Train, loss=2.37278724: 524it [13:10,  1.15s/it]2017-06-01 18:22:41,833 root  INFO     step 524.000000 - time: 0.951994, loss: 2.438732, perplexity: 11.458498, precision: 0.000000, batch_len: 79.000000
Train, loss=2.43873167: 525it [13:11,  1.13s/it]2017-06-01 18:22:43,131 root  INFO     step 525.000000 - time: 1.232678, loss: 2.454182, perplexity: 11.636907, precision: 0.000000, batch_len: 107.000000
Train, loss=2.45418167: 526it [13:13,  1.18s/it]2017-06-01 18:22:44,399 root  INFO     step 526.000000 - time: 1.180006, loss: 2.413168, perplexity: 11.169286, precision: 0.000000, batch_len: 100.000000
Train, loss=2.41316772: 527it [13:14,  1.21s/it]2017-06-01 18:22:45,378 root  INFO     step 527.000000 - time: 0.971598, loss: 2.270436, perplexity: 9.683625, precision: 0.000000, batch_len: 83.000000
Train, loss=2.27043629: 528it [13:15,  1.14s/it]2017-06-01 18:22:46,807 root  INFO     step 528.000000 - time: 1.295177, loss: 2.366181, perplexity: 10.656621, precision: 0.000000, batch_len: 116.000000
Train, loss=2.36618137: 529it [13:16,  1.22s/it]2017-06-01 18:22:47,990 root  INFO     step 529.000000 - time: 1.157533, loss: 2.327408, perplexity: 10.251334, precision: 0.000000, batch_len: 124.000000
Train, loss=2.32740784: 530it [13:17,  1.21s/it]2017-06-01 18:22:49,187 root  INFO     step 530.000000 - time: 1.191780, loss: 2.245411, perplexity: 9.444300, precision: 0.000000, batch_len: 115.000000
Train, loss=2.24541140: 531it [13:19,  1.21s/it]2017-06-01 18:22:50,485 root  INFO     step 531.000000 - time: 1.239852, loss: 2.341448, perplexity: 10.396280, precision: 0.000000, batch_len: 87.000000
Train, loss=2.34144807: 532it [13:20,  1.23s/it]2017-06-01 18:22:51,650 root  INFO     step 532.000000 - time: 1.145364, loss: 2.473232, perplexity: 11.860722, precision: 0.000000, batch_len: 114.000000
Train, loss=2.47323227: 533it [13:21,  1.21s/it]2017-06-01 18:22:53,040 root  INFO     step 533.000000 - time: 1.287933, loss: 2.341874, perplexity: 10.400708, precision: 0.000000, batch_len: 125.000000
Train, loss=2.34187388: 534it [13:22,  1.27s/it]2017-06-01 18:22:54,095 root  INFO     step 534.000000 - time: 0.998835, loss: 2.261169, perplexity: 9.594298, precision: 0.000000, batch_len: 84.000000
Train, loss=2.26116896: 535it [13:23,  1.20s/it]2017-06-01 18:22:55,362 root  INFO     step 535.000000 - time: 1.240236, loss: 2.276899, perplexity: 9.746409, precision: 0.000000, batch_len: 123.000000
Train, loss=2.27689886: 536it [13:25,  1.22s/it]2017-06-01 18:22:56,565 root  INFO     step 536.000000 - time: 1.134523, loss: 2.405184, perplexity: 11.080467, precision: 0.000000, batch_len: 85.000000
Train, loss=2.40518379: 537it [13:26,  1.22s/it]2017-06-01 18:22:57,873 root  INFO     step 537.000000 - time: 1.236771, loss: 2.465744, perplexity: 11.772235, precision: 0.000000, batch_len: 98.000000
Train, loss=2.46574378: 538it [13:27,  1.24s/it]2017-06-01 18:22:59,439 root  INFO     step 538.000000 - time: 1.401890, loss: 2.318392, perplexity: 10.159321, precision: 0.000000, batch_len: 126.000000
Train, loss=2.31839156: 539it [13:29,  1.34s/it]2017-06-01 18:23:00,807 root  INFO     step 539.000000 - time: 1.328741, loss: 2.333559, perplexity: 10.314586, precision: 0.000000, batch_len: 135.000000
Train, loss=2.33355904: 540it [13:30,  1.35s/it]2017-06-01 18:23:02,072 root  INFO     step 540.000000 - time: 1.151394, loss: 2.521192, perplexity: 12.443415, precision: 0.000000, batch_len: 129.000000
Train, loss=2.52119160: 541it [13:31,  1.32s/it]2017-06-01 18:23:03,190 root  INFO     step 541.000000 - time: 1.112587, loss: 2.521283, perplexity: 12.444549, precision: 0.000000, batch_len: 94.000000
Train, loss=2.52128267: 542it [13:33,  1.26s/it]2017-06-01 18:23:04,469 root  INFO     step 542.000000 - time: 1.257632, loss: 2.407557, perplexity: 11.106789, precision: 0.000000, batch_len: 99.000000
Train, loss=2.40755653: 543it [13:34,  1.27s/it]2017-06-01 18:23:05,512 root  INFO     step 543.000000 - time: 0.958017, loss: 2.427973, perplexity: 11.335879, precision: 0.000000, batch_len: 82.000000
Train, loss=2.42797279: 544it [13:35,  1.20s/it]2017-06-01 18:23:06,970 root  INFO     step 544.000000 - time: 1.304341, loss: 2.596431, perplexity: 13.415769, precision: 0.000000, batch_len: 133.000000
Train, loss=2.59643078: 545it [13:36,  1.28s/it]2017-06-01 18:23:08,382 root  INFO     step 545.000000 - time: 1.335090, loss: 2.386023, perplexity: 10.870178, precision: 0.000000, batch_len: 137.000000
Train, loss=2.38602304: 546it [13:38,  1.32s/it]2017-06-01 18:23:10,123 root  INFO     step 546.000000 - time: 1.666118, loss: 2.289692, perplexity: 9.871896, precision: 0.000000, batch_len: 144.000000
Train, loss=2.28969193: 547it [13:39,  1.44s/it]2017-06-01 18:23:11,669 root  INFO     step 547.000000 - time: 1.485647, loss: 2.260667, perplexity: 9.589482, precision: 0.000000, batch_len: 118.000000
Train, loss=2.26066685: 548it [13:41,  1.48s/it]2017-06-01 18:23:13,164 root  INFO     step 548.000000 - time: 1.341728, loss: 2.193507, perplexity: 8.966601, precision: 0.000000, batch_len: 119.000000
Train, loss=2.19350672: 549it [13:43,  1.48s/it]2017-06-01 18:23:14,688 root  INFO     step 549.000000 - time: 1.310684, loss: 2.218628, perplexity: 9.194711, precision: 0.000000, batch_len: 136.000000
Train, loss=2.21862841: 550it [13:44,  1.49s/it]2017-06-01 18:23:16,325 root  INFO     step 550.000000 - time: 1.581593, loss: 2.528828, perplexity: 12.538798, precision: 0.000000, batch_len: 130.000000
Train, loss=2.52882767: 551it [13:46,  1.54s/it]2017-06-01 18:23:17,716 root  INFO     step 551.000000 - time: 1.165448, loss: 2.293542, perplexity: 9.909976, precision: 0.000000, batch_len: 78.000000
Train, loss=2.29354191: 552it [13:47,  1.49s/it]2017-06-01 18:23:19,469 root  INFO     step 552.000000 - time: 1.421902, loss: 2.392964, perplexity: 10.945894, precision: 0.000000, batch_len: 138.000000
Train, loss=2.39296436: 553it [13:49,  1.57s/it]2017-06-01 18:23:20,629 root  INFO     step 553.000000 - time: 1.143337, loss: 2.417267, perplexity: 11.215170, precision: 0.000000, batch_len: 96.000000
Train, loss=2.41726732: 554it [13:50,  1.45s/it]2017-06-01 18:23:21,468 root  INFO     step 554.000000 - time: 0.817671, loss: 2.403995, perplexity: 11.067300, precision: 0.000000, batch_len: 76.000000
Train, loss=2.40399480: 555it [13:51,  1.27s/it]2017-06-01 18:23:22,792 root  INFO     step 555.000000 - time: 1.312769, loss: 2.102480, perplexity: 8.186447, precision: 0.000000, batch_len: 72.000000
Train, loss=2.10247993: 556it [13:52,  1.28s/it]2017-06-01 18:23:24,473 root  INFO     step 556.000000 - time: 1.606429, loss: 2.429373, perplexity: 11.351757, precision: 0.000000, batch_len: 139.000000
Train, loss=2.42937255: 557it [13:54,  1.40s/it]2017-06-01 18:23:26,322 root  INFO     step 557.000000 - time: 1.824342, loss: 2.275764, perplexity: 9.735359, precision: 0.000000, batch_len: 150.000000
Train, loss=2.27576447: 558it [13:56,  1.54s/it]2017-06-01 18:23:27,845 root  INFO     step 558.000000 - time: 1.383337, loss: 2.343891, perplexity: 10.421710, precision: 0.000000, batch_len: 152.000000
Train, loss=2.34389114: 559it [13:57,  1.53s/it]2017-06-01 18:23:28,440 root  INFO     step 559.000000 - time: 0.523539, loss: 2.394472, perplexity: 10.962407, precision: 0.000000, batch_len: 141.000000
Train, loss=2.39447188: 560it [13:58,  1.25s/it]2017-06-01 18:23:28,996 root  INFO     step 560.000000 - time: 0.506262, loss: 2.428022, perplexity: 11.336435, precision: 0.000000, batch_len: 132.000000
Train, loss=2.42802191: 561it [13:58,  1.04s/it]2017-06-01 18:23:29,551 root  INFO     step 561.000000 - time: 0.491319, loss: 2.157046, perplexity: 8.645560, precision: 0.000000, batch_len: 122.000000
Train, loss=2.15704584: 562it [13:59,  1.12it/s]2017-06-01 18:23:30,043 root  INFO     step 562.000000 - time: 0.351452, loss: 2.198018, perplexity: 9.007142, precision: 0.000000, batch_len: 71.000000
Train, loss=2.19801784: 563it [13:59,  1.29it/s]2017-06-01 18:23:30,420 root  INFO     step 563.000000 - time: 0.364106, loss: 2.054585, perplexity: 7.803602, precision: 0.046875, batch_len: 74.000000
Train, loss=2.05458546: 564it [14:00,  1.53it/s]2017-06-01 18:23:31,786 root  INFO     step 564.000000 - time: 1.301469, loss: 2.456052, perplexity: 11.658690, precision: 0.000000, batch_len: 134.000000
Train, loss=2.45605183: 565it [14:01,  1.15it/s]2017-06-01 18:23:33,456 root  INFO     step 565.000000 - time: 1.620251, loss: 2.373833, perplexity: 10.738476, precision: 0.000000, batch_len: 142.000000
Train, loss=2.37383318: 566it [14:03,  1.11s/it]2017-06-01 18:23:34,521 root  INFO     step 566.000000 - time: 1.030948, loss: 2.211203, perplexity: 9.126690, precision: 0.000000, batch_len: 77.000000
Train, loss=2.21120310: 567it [14:04,  1.10s/it]2017-06-01 18:23:36,119 root  INFO     step 567.000000 - time: 1.560785, loss: 2.514797, perplexity: 12.364101, precision: 0.000000, batch_len: 131.000000
Train, loss=2.51479721: 568it [14:05,  1.25s/it]2017-06-01 18:23:36,404 root  INFO     Generating first batch)
2017-06-01 18:23:40,551 root  INFO     step 568.000000 - time: 1.245023, loss: 2.166160, perplexity: 8.724713, precision: 0.000000, batch_len: 96.000000
Train, loss=2.16615963: 569it [14:10,  2.20s/it]2017-06-01 18:23:41,729 root  INFO     step 569.000000 - time: 1.090961, loss: 2.221911, perplexity: 9.224942, precision: 0.000000, batch_len: 106.000000
Train, loss=2.22191095: 570it [14:11,  1.89s/it]2017-06-01 18:23:42,783 root  INFO     step 570.000000 - time: 1.009320, loss: 2.258065, perplexity: 9.564566, precision: 0.000000, batch_len: 105.000000
Train, loss=2.25806522: 571it [14:12,  1.64s/it]2017-06-01 18:23:43,851 root  INFO     step 571.000000 - time: 1.006967, loss: 2.176423, perplexity: 8.814716, precision: 0.000000, batch_len: 113.000000
Train, loss=2.17642260: 572it [14:13,  1.47s/it]2017-06-01 18:23:45,462 root  INFO     step 572.000000 - time: 1.540988, loss: 2.225224, perplexity: 9.255552, precision: 0.000000, batch_len: 128.000000
Train, loss=2.22522354: 573it [14:15,  1.51s/it]2017-06-01 18:23:46,847 root  INFO     step 573.000000 - time: 1.280281, loss: 2.225285, perplexity: 9.256121, precision: 0.000000, batch_len: 108.000000
Train, loss=2.22528505: 574it [14:16,  1.47s/it]2017-06-01 18:23:48,014 root  INFO     step 574.000000 - time: 1.091587, loss: 2.167829, perplexity: 8.739291, precision: 0.000000, batch_len: 90.000000
Train, loss=2.16782904: 575it [14:17,  1.38s/it]2017-06-01 18:23:49,232 root  INFO     step 575.000000 - time: 1.100027, loss: 2.079352, perplexity: 7.999283, precision: 0.000000, batch_len: 110.000000
Train, loss=2.07935190: 576it [14:19,  1.33s/it]2017-06-01 18:23:50,290 root  INFO     step 576.000000 - time: 0.971836, loss: 2.188093, perplexity: 8.918192, precision: 0.000000, batch_len: 107.000000
Train, loss=2.18809319: 577it [14:20,  1.25s/it]2017-06-01 18:23:51,840 root  INFO     step 577.000000 - time: 1.510477, loss: 2.039409, perplexity: 7.686067, precision: 0.000000, batch_len: 112.000000
Train, loss=2.03940916: 578it [14:21,  1.34s/it]2017-06-01 18:23:53,111 root  INFO     step 578.000000 - time: 1.254907, loss: 2.310413, perplexity: 10.078583, precision: 0.000000, batch_len: 93.000000
Train, loss=2.31041265: 579it [14:22,  1.32s/it]2017-06-01 18:23:54,303 root  INFO     step 579.000000 - time: 1.129841, loss: 2.427140, perplexity: 11.326447, precision: 0.000000, batch_len: 111.000000
Train, loss=2.42714047: 580it [14:24,  1.28s/it]2017-06-01 18:23:55,407 root  INFO     step 580.000000 - time: 1.041849, loss: 2.330201, perplexity: 10.280009, precision: 0.000000, batch_len: 88.000000
Train, loss=2.33020115: 581it [14:25,  1.23s/it]2017-06-01 18:23:56,509 root  INFO     step 581.000000 - time: 1.063736, loss: 2.089852, perplexity: 8.083721, precision: 0.000000, batch_len: 102.000000
Train, loss=2.08985233: 582it [14:26,  1.19s/it]2017-06-01 18:23:57,838 root  INFO     step 582.000000 - time: 1.287772, loss: 2.027120, perplexity: 7.592187, precision: 0.000000, batch_len: 120.000000
Train, loss=2.02711964: 583it [14:27,  1.23s/it]2017-06-01 18:23:59,064 root  INFO     step 583.000000 - time: 1.196252, loss: 1.971292, perplexity: 7.179949, precision: 0.000000, batch_len: 104.000000
Train, loss=1.97129226: 584it [14:28,  1.23s/it]2017-06-01 18:24:00,409 root  INFO     step 584.000000 - time: 1.152849, loss: 1.892058, perplexity: 6.633005, precision: 0.000000, batch_len: 101.000000
Train, loss=1.89205790: 585it [14:30,  1.26s/it]2017-06-01 18:24:01,559 root  INFO     step 585.000000 - time: 1.094979, loss: 2.099200, perplexity: 8.159642, precision: 0.000000, batch_len: 97.000000
Train, loss=2.09920025: 586it [14:31,  1.23s/it]2017-06-01 18:24:02,778 root  INFO     step 586.000000 - time: 1.112118, loss: 2.138272, perplexity: 8.484766, precision: 0.000000, batch_len: 109.000000
Train, loss=2.13827229: 587it [14:32,  1.23s/it]2017-06-01 18:24:04,095 root  INFO     step 587.000000 - time: 1.241396, loss: 2.234132, perplexity: 9.338371, precision: 0.000000, batch_len: 117.000000
Train, loss=2.23413181: 588it [14:33,  1.25s/it]2017-06-01 18:24:05,658 root  INFO     step 588.000000 - time: 1.531682, loss: 2.426641, perplexity: 11.320797, precision: 0.000000, batch_len: 124.000000
Train, loss=2.42664146: 589it [14:35,  1.35s/it]2017-06-01 18:24:06,825 root  INFO     step 589.000000 - time: 1.118732, loss: 2.381236, perplexity: 10.818267, precision: 0.000000, batch_len: 80.000000
Train, loss=2.38123608: 590it [14:36,  1.29s/it]2017-06-01 18:24:08,223 root  INFO     step 590.000000 - time: 1.378373, loss: 2.402235, perplexity: 11.047841, precision: 0.000000, batch_len: 121.000000
Train, loss=2.40223503: 591it [14:38,  1.32s/it]2017-06-01 18:24:09,322 root  INFO     step 591.000000 - time: 1.072383, loss: 2.205855, perplexity: 9.078011, precision: 0.000000, batch_len: 92.000000
Train, loss=2.20585513: 592it [14:39,  1.26s/it]2017-06-01 18:24:10,218 root  INFO     step 592.000000 - time: 0.883191, loss: 2.035993, perplexity: 7.659852, precision: 0.000000, batch_len: 81.000000
Train, loss=2.03599262: 593it [14:40,  1.15s/it]2017-06-01 18:24:11,447 root  INFO     step 593.000000 - time: 1.023351, loss: 2.038110, perplexity: 7.676090, precision: 0.000000, batch_len: 89.000000
Train, loss=2.03811026: 594it [14:41,  1.17s/it]2017-06-01 18:24:12,823 root  INFO     step 594.000000 - time: 1.257802, loss: 2.014478, perplexity: 7.496813, precision: 0.000000, batch_len: 100.000000
Train, loss=2.01447797: 595it [14:42,  1.23s/it]2017-06-01 18:24:13,864 root  INFO     step 595.000000 - time: 0.991110, loss: 2.199924, perplexity: 9.024328, precision: 0.000000, batch_len: 83.000000
Train, loss=2.19992399: 596it [14:43,  1.18s/it]2017-06-01 18:24:15,262 root  INFO     step 596.000000 - time: 1.360915, loss: 2.204788, perplexity: 9.068333, precision: 0.000000, batch_len: 116.000000
Train, loss=2.20478845: 597it [14:45,  1.24s/it]2017-06-01 18:24:16,387 root  INFO     step 597.000000 - time: 1.104976, loss: 2.146400, perplexity: 8.554008, precision: 0.000000, batch_len: 114.000000
Train, loss=2.14639997: 598it [14:46,  1.21s/it]2017-06-01 18:24:17,389 root  INFO     step 598.000000 - time: 0.986913, loss: 1.962470, perplexity: 7.116881, precision: 0.000000, batch_len: 115.000000
Train, loss=1.96246958: 599it [14:47,  1.15s/it]2017-06-01 18:24:18,870 root  INFO     step 599.000000 - time: 1.469489, loss: 1.971235, perplexity: 7.179535, precision: 0.000000, batch_len: 91.000000
Train, loss=1.97123468: 600it [14:48,  1.25s/it]2017-06-01 18:24:20,098 root  INFO     step 600.000000 - time: 1.161086, loss: 2.090879, perplexity: 8.092028, precision: 0.000000, batch_len: 79.000000
Train, loss=2.09087944: 601it [14:49,  1.24s/it]2017-06-01 18:24:21,100 root  INFO     step 601.000000 - time: 0.995145, loss: 2.007698, perplexity: 7.446159, precision: 0.000000, batch_len: 86.000000
Train, loss=2.00769830: 602it [14:50,  1.17s/it]2017-06-01 18:24:22,197 root  INFO     step 602.000000 - time: 1.088556, loss: 2.168883, perplexity: 8.748505, precision: 0.000000, batch_len: 103.000000
Train, loss=2.16888285: 603it [14:52,  1.15s/it]2017-06-01 18:24:23,532 root  INFO     step 603.000000 - time: 1.298314, loss: 2.002931, perplexity: 7.410746, precision: 0.000000, batch_len: 123.000000
Train, loss=2.00293112: 604it [14:53,  1.20s/it]2017-06-01 18:24:24,613 root  INFO     step 604.000000 - time: 0.942794, loss: 2.027879, perplexity: 7.597956, precision: 0.000000, batch_len: 85.000000
Train, loss=2.02787924: 605it [14:54,  1.17s/it]2017-06-01 18:24:25,763 root  INFO     step 605.000000 - time: 1.140709, loss: 2.029948, perplexity: 7.613692, precision: 0.000000, batch_len: 94.000000
Train, loss=2.02994823: 606it [14:55,  1.16s/it]2017-06-01 18:24:26,861 root  INFO     step 606.000000 - time: 0.956756, loss: 2.161011, perplexity: 8.679904, precision: 0.000000, batch_len: 129.000000
Train, loss=2.16101050: 607it [14:56,  1.14s/it]2017-06-01 18:24:28,204 root  INFO     step 607.000000 - time: 1.213392, loss: 2.102142, perplexity: 8.183683, precision: 0.000000, batch_len: 99.000000
Train, loss=2.10214233: 608it [14:58,  1.20s/it]2017-06-01 18:24:29,403 root  INFO     step 608.000000 - time: 1.193904, loss: 2.073222, perplexity: 7.950396, precision: 0.000000, batch_len: 87.000000
Train, loss=2.07322168: 609it [14:59,  1.20s/it]2017-06-01 18:24:30,861 root  INFO     step 609.000000 - time: 1.430610, loss: 2.227733, perplexity: 9.278811, precision: 0.000000, batch_len: 126.000000
Train, loss=2.22773337: 610it [15:00,  1.28s/it]2017-06-01 18:24:32,296 root  INFO     step 610.000000 - time: 1.378159, loss: 2.108935, perplexity: 8.239465, precision: 0.000000, batch_len: 125.000000
Train, loss=2.10893536: 611it [15:02,  1.33s/it]2017-06-01 18:24:33,386 root  INFO     step 611.000000 - time: 1.019442, loss: 2.120530, perplexity: 8.335555, precision: 0.000000, batch_len: 98.000000
Train, loss=2.12053013: 612it [15:03,  1.25s/it]2017-06-01 18:24:35,300 root  INFO     step 612.000000 - time: 1.751424, loss: 2.401320, perplexity: 11.037731, precision: 0.000000, batch_len: 137.000000
Train, loss=2.40131950: 613it [15:05,  1.45s/it]2017-06-01 18:24:36,309 root  INFO     step 613.000000 - time: 0.951458, loss: 2.129731, perplexity: 8.412601, precision: 0.000000, batch_len: 82.000000
Train, loss=2.12973070: 614it [15:06,  1.32s/it]2017-06-01 18:24:37,729 root  INFO     step 614.000000 - time: 1.330182, loss: 2.097272, perplexity: 8.143922, precision: 0.000000, batch_len: 135.000000
Train, loss=2.09727192: 615it [15:07,  1.35s/it]2017-06-01 18:24:39,191 root  INFO     step 615.000000 - time: 1.364544, loss: 2.038359, perplexity: 7.678001, precision: 0.000000, batch_len: 118.000000
Train, loss=2.03835917: 616it [15:09,  1.38s/it]2017-06-01 18:24:40,704 root  INFO     step 616.000000 - time: 1.407927, loss: 2.010453, perplexity: 7.466695, precision: 0.000000, batch_len: 119.000000
Train, loss=2.01045251: 617it [15:10,  1.42s/it]2017-06-01 18:24:42,394 root  INFO     step 617.000000 - time: 1.661648, loss: 2.075678, perplexity: 7.969945, precision: 0.000000, batch_len: 144.000000
Train, loss=2.07567763: 618it [15:12,  1.50s/it]2017-06-01 18:24:43,843 root  INFO     step 618.000000 - time: 1.400855, loss: 2.382096, perplexity: 10.827569, precision: 0.000000, batch_len: 130.000000
Train, loss=2.38209558: 619it [15:13,  1.49s/it]2017-06-01 18:24:45,385 root  INFO     step 619.000000 - time: 1.401359, loss: 2.029605, perplexity: 7.611079, precision: 0.000000, batch_len: 138.000000
Train, loss=2.02960491: 620it [15:15,  1.50s/it]2017-06-01 18:24:46,451 root  INFO     step 620.000000 - time: 0.901214, loss: 1.989741, perplexity: 7.313638, precision: 0.000000, batch_len: 84.000000
Train, loss=1.98974085: 621it [15:16,  1.37s/it]2017-06-01 18:24:48,189 root  INFO     step 621.000000 - time: 1.645983, loss: 2.217031, perplexity: 9.180039, precision: 0.000000, batch_len: 122.000000
Train, loss=2.21703148: 622it [15:18,  1.48s/it]2017-06-01 18:24:49,725 root  INFO     step 622.000000 - time: 1.396669, loss: 2.040893, perplexity: 7.697481, precision: 0.000000, batch_len: 133.000000
Train, loss=2.04089308: 623it [15:19,  1.50s/it]2017-06-01 18:24:50,769 root  INFO     step 623.000000 - time: 0.982455, loss: 1.888576, perplexity: 6.609951, precision: 0.000000, batch_len: 78.000000
Train, loss=1.88857627: 624it [15:20,  1.36s/it]2017-06-01 18:24:51,894 root  INFO     step 624.000000 - time: 0.934575, loss: 2.048545, perplexity: 7.756610, precision: 0.000000, batch_len: 76.000000
Train, loss=2.04854536: 625it [15:21,  1.29s/it]2017-06-01 18:24:52,936 root  INFO     step 625.000000 - time: 0.986216, loss: 2.062984, perplexity: 7.869417, precision: 0.000000, batch_len: 96.000000
Train, loss=2.06298399: 626it [15:22,  1.22s/it]2017-06-01 18:24:54,577 root  INFO     step 626.000000 - time: 1.564050, loss: 1.972854, perplexity: 7.191168, precision: 0.000000, batch_len: 139.000000
Train, loss=1.97285366: 627it [15:24,  1.34s/it]2017-06-01 18:24:56,192 root  INFO     step 627.000000 - time: 1.585582, loss: 2.255793, perplexity: 9.542854, precision: 0.000000, batch_len: 141.000000
Train, loss=2.25579262: 628it [15:26,  1.43s/it]2017-06-01 18:24:57,834 root  INFO     step 628.000000 - time: 1.538883, loss: 1.932782, perplexity: 6.908704, precision: 0.000000, batch_len: 136.000000
Train, loss=1.93278205: 629it [15:27,  1.49s/it]2017-06-01 18:24:58,833 root  INFO     step 629.000000 - time: 0.951558, loss: 2.071655, perplexity: 7.937952, precision: 0.000000, batch_len: 74.000000
Train, loss=2.07165527: 630it [15:28,  1.34s/it]2017-06-01 18:25:00,995 root  INFO     step 630.000000 - time: 2.032818, loss: 3.324608, perplexity: 27.788113, precision: 0.000000, batch_len: 152.000000
Train, loss=3.32460833: 631it [15:30,  1.59s/it]2017-06-01 18:25:02,519 root  INFO     step 631.000000 - time: 1.517374, loss: 2.349374, perplexity: 10.479008, precision: 0.000000, batch_len: 132.000000
Train, loss=2.34937406: 632it [15:32,  1.57s/it]2017-06-01 18:25:03,657 root  INFO     step 632.000000 - time: 0.914408, loss: 1.943539, perplexity: 6.983419, precision: 0.000000, batch_len: 72.000000
Train, loss=1.94353867: 633it [15:33,  1.44s/it]2017-06-01 18:25:05,230 root  INFO     step 633.000000 - time: 1.402166, loss: 2.265536, perplexity: 9.636291, precision: 0.000000, batch_len: 142.000000
Train, loss=2.26553631: 634it [15:35,  1.48s/it]2017-06-01 18:25:06,118 root  INFO     step 634.000000 - time: 0.864084, loss: 1.802445, perplexity: 6.064457, precision: 0.000000, batch_len: 71.000000
Train, loss=1.80244493: 635it [15:35,  1.30s/it]2017-06-01 18:25:07,662 root  INFO     step 635.000000 - time: 1.478564, loss: 2.119958, perplexity: 8.330791, precision: 0.000000, batch_len: 131.000000
Train, loss=2.11995840: 636it [15:37,  1.37s/it]2017-06-01 18:25:08,292 root  INFO     step 636.000000 - time: 0.590071, loss: 1.876592, perplexity: 6.531211, precision: 0.000000, batch_len: 77.000000
Train, loss=1.87659240: 637it [15:38,  1.15s/it]2017-06-01 18:25:09,557 root  INFO     step 637.000000 - time: 1.143799, loss: 2.767481, perplexity: 15.918482, precision: 0.000000, batch_len: 150.000000
Train, loss=2.76748085: 638it [15:39,  1.19s/it]2017-06-01 18:25:10,506 root  INFO     step 638.000000 - time: 0.839630, loss: 2.072005, perplexity: 7.940730, precision: 0.000000, batch_len: 134.000000
Train, loss=2.07200527: 639it [15:40,  1.11s/it]2017-06-01 18:25:10,700 root  INFO     Generating first batch)
2017-06-01 18:25:14,285 root  INFO     step 639.000000 - time: 1.010846, loss: 1.787369, perplexity: 5.973714, precision: 0.000000, batch_len: 105.000000
Train, loss=1.78736889: 640it [15:44,  1.91s/it]2017-06-01 18:25:15,419 root  INFO     step 640.000000 - time: 1.042890, loss: 1.767565, perplexity: 5.856574, precision: 0.000000, batch_len: 101.000000
Train, loss=1.76756477: 641it [15:45,  1.68s/it]2017-06-01 18:25:16,944 root  INFO     step 641.000000 - time: 1.301800, loss: 1.920110, perplexity: 6.821711, precision: 0.000000, batch_len: 128.000000
Train, loss=1.92011034: 642it [15:46,  1.63s/it]2017-06-01 18:25:18,090 root  INFO     step 642.000000 - time: 1.096745, loss: 1.885761, perplexity: 6.591371, precision: 0.000000, batch_len: 96.000000
Train, loss=1.88576138: 643it [15:47,  1.49s/it]2017-06-01 18:25:20,188 root  INFO     step 643.000000 - time: 1.967847, loss: 2.538954, perplexity: 12.666421, precision: 0.000000, batch_len: 90.000000
Train, loss=2.53895450: 644it [15:50,  1.67s/it]2017-06-01 18:25:21,485 root  INFO     step 644.000000 - time: 1.112204, loss: 2.038144, perplexity: 7.676350, precision: 0.015625, batch_len: 110.000000
Train, loss=2.03814411: 645it [15:51,  1.56s/it]2017-06-01 18:25:22,562 root  INFO     step 645.000000 - time: 1.038257, loss: 1.813315, perplexity: 6.130738, precision: 0.000000, batch_len: 92.000000
Train, loss=1.81331515: 646it [15:52,  1.41s/it]2017-06-01 18:25:23,591 root  INFO     step 646.000000 - time: 0.990720, loss: 1.907375, perplexity: 6.735384, precision: 0.000000, batch_len: 88.000000
Train, loss=1.90737486: 647it [15:53,  1.30s/it]2017-06-01 18:25:24,779 root  INFO     step 647.000000 - time: 1.162690, loss: 1.944937, perplexity: 6.993188, precision: 0.000000, batch_len: 106.000000
Train, loss=1.94493651: 648it [15:54,  1.27s/it]2017-06-01 18:25:26,237 root  INFO     step 648.000000 - time: 1.445195, loss: 1.883133, perplexity: 6.574072, precision: 0.000000, batch_len: 120.000000
Train, loss=1.88313341: 649it [15:56,  1.32s/it]2017-06-01 18:25:27,833 root  INFO     step 649.000000 - time: 1.556725, loss: 1.904694, perplexity: 6.717354, precision: 0.000000, batch_len: 124.000000
Train, loss=1.90469432: 650it [15:57,  1.41s/it]2017-06-01 18:25:29,459 root  INFO     step 650.000000 - time: 1.564245, loss: 1.955164, perplexity: 7.065076, precision: 0.000000, batch_len: 121.000000
Train, loss=1.95516372: 651it [15:59,  1.47s/it]2017-06-01 18:25:30,598 root  INFO     step 651.000000 - time: 1.109138, loss: 1.965236, perplexity: 7.136596, precision: 0.000000, batch_len: 85.000000
Train, loss=1.96523595: 652it [16:00,  1.37s/it]2017-06-01 18:25:31,808 root  INFO     step 652.000000 - time: 1.151759, loss: 2.381858, perplexity: 10.825001, precision: 0.000000, batch_len: 114.000000
Train, loss=2.38185835: 653it [16:01,  1.32s/it]2017-06-01 18:25:32,767 root  INFO     step 653.000000 - time: 0.873493, loss: 1.779039, perplexity: 5.924163, precision: 0.000000, batch_len: 100.000000
Train, loss=1.77903938: 654it [16:02,  1.21s/it]2017-06-01 18:25:34,137 root  INFO     step 654.000000 - time: 1.216885, loss: 1.973913, perplexity: 7.198792, precision: 0.015625, batch_len: 102.000000
Train, loss=1.97391319: 655it [16:04,  1.26s/it]2017-06-01 18:25:35,654 root  INFO     step 655.000000 - time: 1.506828, loss: 1.852902, perplexity: 6.378305, precision: 0.000000, batch_len: 117.000000
Train, loss=1.85290241: 656it [16:05,  1.34s/it]2017-06-01 18:25:36,784 root  INFO     step 656.000000 - time: 1.097120, loss: 1.839071, perplexity: 6.290692, precision: 0.015625, batch_len: 103.000000
Train, loss=1.83907115: 657it [16:06,  1.28s/it]2017-06-01 18:25:37,928 root  INFO     step 657.000000 - time: 1.099650, loss: 1.964440, perplexity: 7.130918, precision: 0.000000, batch_len: 115.000000
Train, loss=1.96443999: 658it [16:07,  1.24s/it]2017-06-01 18:25:38,950 root  INFO     step 658.000000 - time: 1.009106, loss: 2.422671, perplexity: 11.275941, precision: 0.000000, batch_len: 113.000000
Train, loss=2.42267132: 659it [16:08,  1.17s/it]2017-06-01 18:25:40,058 root  INFO     step 659.000000 - time: 1.070210, loss: 1.837616, perplexity: 6.281548, precision: 0.000000, batch_len: 104.000000
Train, loss=1.83761644: 660it [16:09,  1.15s/it]2017-06-01 18:25:41,482 root  INFO     step 660.000000 - time: 1.347272, loss: 1.795534, perplexity: 6.022689, precision: 0.000000, batch_len: 108.000000
Train, loss=1.79553390: 661it [16:11,  1.23s/it]2017-06-01 18:25:42,637 root  INFO     step 661.000000 - time: 1.111899, loss: 1.848966, perplexity: 6.353249, precision: 0.015625, batch_len: 111.000000
Train, loss=1.84896636: 662it [16:12,  1.21s/it]2017-06-01 18:25:43,937 root  INFO     step 662.000000 - time: 1.283205, loss: 1.815179, perplexity: 6.142176, precision: 0.015625, batch_len: 112.000000
Train, loss=1.81517911: 663it [16:13,  1.24s/it]2017-06-01 18:25:45,051 root  INFO     step 663.000000 - time: 1.061479, loss: 1.986824, perplexity: 7.292338, precision: 0.000000, batch_len: 93.000000
Train, loss=1.98682415: 664it [16:14,  1.20s/it]2017-06-01 18:25:46,054 root  INFO     step 664.000000 - time: 0.975095, loss: 2.098890, perplexity: 8.157113, precision: 0.000000, batch_len: 97.000000
Train, loss=2.09889030: 665it [16:15,  1.14s/it]2017-06-01 18:25:47,306 root  INFO     step 665.000000 - time: 1.227840, loss: 1.979048, perplexity: 7.235855, precision: 0.000000, batch_len: 89.000000
Train, loss=1.97904849: 666it [16:17,  1.17s/it]2017-06-01 18:25:48,777 root  INFO     step 666.000000 - time: 1.277184, loss: 2.088787, perplexity: 8.075117, precision: 0.000000, batch_len: 109.000000
Train, loss=2.08878732: 667it [16:18,  1.26s/it]2017-06-01 18:25:49,939 root  INFO     step 667.000000 - time: 1.024847, loss: 1.986959, perplexity: 7.293318, precision: 0.000000, batch_len: 79.000000
Train, loss=1.98695862: 668it [16:19,  1.23s/it]2017-06-01 18:25:50,961 root  INFO     step 668.000000 - time: 0.963845, loss: 1.840307, perplexity: 6.298469, precision: 0.000000, batch_len: 86.000000
Train, loss=1.84030652: 669it [16:20,  1.17s/it]2017-06-01 18:25:52,096 root  INFO     step 669.000000 - time: 1.093566, loss: 1.845104, perplexity: 6.328756, precision: 0.015625, batch_len: 107.000000
Train, loss=1.84510374: 670it [16:21,  1.16s/it]2017-06-01 18:25:53,199 root  INFO     step 670.000000 - time: 0.958272, loss: 1.712011, perplexity: 5.540092, precision: 0.015625, batch_len: 94.000000
Train, loss=1.71201110: 671it [16:23,  1.14s/it]2017-06-01 18:25:54,897 root  INFO     step 671.000000 - time: 1.640939, loss: 1.826386, perplexity: 6.211399, precision: 0.000000, batch_len: 125.000000
Train, loss=1.82638621: 672it [16:24,  1.31s/it]2017-06-01 18:25:56,071 root  INFO     step 672.000000 - time: 1.093076, loss: 1.632792, perplexity: 5.118145, precision: 0.015625, batch_len: 98.000000
Train, loss=1.63279212: 673it [16:25,  1.27s/it]2017-06-01 18:25:57,119 root  INFO     step 673.000000 - time: 0.985321, loss: 1.716831, perplexity: 5.566859, precision: 0.000000, batch_len: 87.000000
Train, loss=1.71683097: 674it [16:26,  1.20s/it]2017-06-01 18:25:58,471 root  INFO     step 674.000000 - time: 1.336689, loss: 1.937748, perplexity: 6.943100, precision: 0.015625, batch_len: 123.000000
Train, loss=1.93774843: 675it [16:28,  1.25s/it]2017-06-01 18:25:59,690 root  INFO     step 675.000000 - time: 1.196760, loss: 2.048332, perplexity: 7.754957, precision: 0.000000, batch_len: 91.000000
Train, loss=2.04833221: 676it [16:29,  1.24s/it]2017-06-01 18:26:01,614 root  INFO     step 676.000000 - time: 1.732080, loss: 2.170996, perplexity: 8.767011, precision: 0.000000, batch_len: 137.000000
Train, loss=2.17099595: 677it [16:31,  1.44s/it]2017-06-01 18:26:02,985 root  INFO     step 677.000000 - time: 1.360744, loss: 1.790610, perplexity: 5.993106, precision: 0.000000, batch_len: 119.000000
Train, loss=1.79060984: 678it [16:32,  1.42s/it]2017-06-01 18:26:04,030 root  INFO     step 678.000000 - time: 0.963984, loss: 1.702649, perplexity: 5.488465, precision: 0.000000, batch_len: 80.000000
Train, loss=1.70264864: 679it [16:33,  1.31s/it]2017-06-01 18:26:05,372 root  INFO     step 679.000000 - time: 1.309512, loss: 1.921669, perplexity: 6.832354, precision: 0.000000, batch_len: 116.000000
Train, loss=1.92166924: 680it [16:35,  1.32s/it]2017-06-01 18:26:06,571 root  INFO     step 680.000000 - time: 1.159414, loss: 1.840282, perplexity: 6.298314, precision: 0.000000, batch_len: 129.000000
Train, loss=1.84028196: 681it [16:36,  1.28s/it]2017-06-01 18:26:08,316 root  INFO     step 681.000000 - time: 1.667959, loss: 1.989354, perplexity: 7.310810, precision: 0.015625, batch_len: 126.000000
Train, loss=1.98935413: 682it [16:38,  1.42s/it]2017-06-01 18:26:09,308 root  INFO     step 682.000000 - time: 0.958415, loss: 2.312290, perplexity: 10.097521, precision: 0.000000, batch_len: 81.000000
Train, loss=2.31228995: 683it [16:39,  1.29s/it]2017-06-01 18:26:10,798 root  INFO     step 683.000000 - time: 1.383208, loss: 2.214138, perplexity: 9.153511, precision: 0.000000, batch_len: 130.000000
Train, loss=2.21413755: 684it [16:40,  1.35s/it]2017-06-01 18:26:11,825 root  INFO     step 684.000000 - time: 0.927136, loss: 1.872441, perplexity: 6.504155, precision: 0.000000, batch_len: 83.000000
Train, loss=1.87244117: 685it [16:41,  1.25s/it]2017-06-01 18:26:12,830 root  INFO     step 685.000000 - time: 0.903166, loss: 1.681352, perplexity: 5.372816, precision: 0.031250, batch_len: 84.000000
Train, loss=1.68135214: 686it [16:42,  1.18s/it]2017-06-01 18:26:14,393 root  INFO     step 686.000000 - time: 1.557335, loss: 1.725379, perplexity: 5.614647, precision: 0.000000, batch_len: 118.000000
Train, loss=1.72537863: 687it [16:44,  1.29s/it]2017-06-01 18:26:15,655 root  INFO     step 687.000000 - time: 1.117871, loss: 1.610782, perplexity: 5.006724, precision: 0.000000, batch_len: 78.000000
Train, loss=1.61078191: 688it [16:45,  1.28s/it]2017-06-01 18:26:17,215 root  INFO     step 688.000000 - time: 1.432224, loss: 1.899850, perplexity: 6.684893, precision: 0.000000, batch_len: 144.000000
Train, loss=1.89985013: 689it [16:47,  1.37s/it]2017-06-01 18:26:18,558 root  INFO     step 689.000000 - time: 1.288995, loss: 1.866986, perplexity: 6.468767, precision: 0.000000, batch_len: 135.000000
Train, loss=1.86698556: 690it [16:48,  1.36s/it]2017-06-01 18:26:19,429 root  INFO     step 690.000000 - time: 0.804177, loss: 2.085151, perplexity: 8.045806, precision: 0.000000, batch_len: 82.000000
Train, loss=2.08515096: 691it [16:49,  1.21s/it]2017-06-01 18:26:20,913 root  INFO     step 691.000000 - time: 1.317757, loss: 1.869767, perplexity: 6.486782, precision: 0.000000, batch_len: 99.000000
Train, loss=1.86976659: 692it [16:50,  1.29s/it]2017-06-01 18:26:22,429 root  INFO     step 692.000000 - time: 1.508771, loss: 1.888989, perplexity: 6.612681, precision: 0.015625, batch_len: 133.000000
Train, loss=1.88898921: 693it [16:52,  1.36s/it]2017-06-01 18:26:23,762 root  INFO     step 693.000000 - time: 1.155259, loss: 1.812008, perplexity: 6.122730, precision: 0.000000, batch_len: 72.000000
Train, loss=1.81200802: 694it [16:53,  1.35s/it]2017-06-01 18:26:24,767 root  INFO     step 694.000000 - time: 0.887468, loss: 1.922792, perplexity: 6.840026, precision: 0.000000, batch_len: 77.000000
Train, loss=1.92279160: 695it [16:54,  1.25s/it]2017-06-01 18:26:26,244 root  INFO     step 695.000000 - time: 1.377742, loss: 1.960977, perplexity: 7.106264, precision: 0.000000, batch_len: 138.000000
Train, loss=1.96097660: 696it [16:56,  1.32s/it]2017-06-01 18:26:27,871 root  INFO     step 696.000000 - time: 1.620102, loss: 2.091131, perplexity: 8.094068, precision: 0.000000, batch_len: 141.000000
Train, loss=2.09113145: 697it [16:57,  1.41s/it]2017-06-01 18:26:29,528 root  INFO     step 697.000000 - time: 1.650277, loss: 1.998895, perplexity: 7.380899, precision: 0.000000, batch_len: 139.000000
Train, loss=1.99889541: 698it [16:59,  1.48s/it]2017-06-01 18:26:31,145 root  INFO     step 698.000000 - time: 1.596240, loss: 1.613807, perplexity: 5.021895, precision: 0.000000, batch_len: 136.000000
Train, loss=1.61380732: 699it [17:01,  1.52s/it]2017-06-01 18:26:32,499 root  INFO     step 699.000000 - time: 1.127515, loss: 1.685960, perplexity: 5.397631, precision: 0.015625, batch_len: 96.000000
Train, loss=1.68596017: 700it [17:02,  1.47s/it]2017-06-01 18:26:33,854 root  INFO     step 700.000000 - time: 1.305656, loss: 1.752281, perplexity: 5.767746, precision: 0.000000, batch_len: 122.000000
Train, loss=1.75228143: 701it [17:03,  1.44s/it]2017-06-01 18:26:34,848 root  INFO     step 701.000000 - time: 0.880700, loss: 1.751709, perplexity: 5.764443, precision: 0.000000, batch_len: 74.000000
Train, loss=1.75170851: 702it [17:04,  1.30s/it]2017-06-01 18:26:36,656 root  INFO     step 702.000000 - time: 1.663752, loss: 2.145890, perplexity: 8.549645, precision: 0.000000, batch_len: 131.000000
Train, loss=2.14588976: 703it [17:06,  1.46s/it]2017-06-01 18:26:38,020 root  INFO     step 703.000000 - time: 1.324317, loss: 1.727074, perplexity: 5.624171, precision: 0.000000, batch_len: 134.000000
Train, loss=1.72707355: 704it [17:07,  1.43s/it]2017-06-01 18:26:39,864 root  INFO     step 704.000000 - time: 1.834822, loss: 1.736941, perplexity: 5.679944, precision: 0.000000, batch_len: 150.000000
Train, loss=1.73694146: 705it [17:09,  1.55s/it]2017-06-01 18:26:40,771 root  INFO     step 705.000000 - time: 0.879377, loss: 1.942149, perplexity: 6.973724, precision: 0.000000, batch_len: 76.000000
Train, loss=1.94214940: 706it [17:10,  1.36s/it]2017-06-01 18:26:42,878 root  INFO     step 706.000000 - time: 1.988835, loss: 2.155105, perplexity: 8.628797, precision: 0.000000, batch_len: 152.000000
Train, loss=2.15510511: 707it [17:12,  1.58s/it]2017-06-01 18:26:44,447 root  INFO     step 707.000000 - time: 1.350141, loss: 1.884355, perplexity: 6.582109, precision: 0.000000, batch_len: 132.000000
Train, loss=1.88435519: 708it [17:14,  1.58s/it]2017-06-01 18:26:45,991 root  INFO     step 708.000000 - time: 1.442378, loss: 1.994197, perplexity: 7.346303, precision: 0.000000, batch_len: 142.000000
Train, loss=1.99419713: 709it [17:15,  1.57s/it]2017-06-01 18:26:47,077 root  INFO     step 709.000000 - time: 0.853722, loss: 1.555314, perplexity: 4.736575, precision: 0.031250, batch_len: 71.000000
Train, loss=1.55531430: 710it [17:16,  1.42s/it]2017-06-01 18:26:47,173 root  INFO     Generating first batch)
2017-06-01 18:26:50,671 root  INFO     step 710.000000 - time: 1.131974, loss: 1.687301, perplexity: 5.404872, precision: 0.000000, batch_len: 96.000000
Train, loss=1.68730068: 711it [17:20,  2.07s/it]2017-06-01 18:26:52,233 root  INFO     step 711.000000 - time: 0.966958, loss: 1.704533, perplexity: 5.498815, precision: 0.000000, batch_len: 92.000000
Train, loss=1.70453262: 712it [17:22,  1.92s/it]2017-06-01 18:26:54,087 root  INFO     step 712.000000 - time: 1.443424, loss: 2.127548, perplexity: 8.394261, precision: 0.000000, batch_len: 128.000000
Train, loss=2.12754822: 713it [17:23,  1.90s/it]2017-06-01 18:26:55,375 root  INFO     step 713.000000 - time: 1.222092, loss: 1.723016, perplexity: 5.601398, precision: 0.000000, batch_len: 97.000000
Train, loss=1.72301626: 714it [17:25,  1.72s/it]2017-06-01 18:26:56,503 root  INFO     step 714.000000 - time: 1.067461, loss: 1.680107, perplexity: 5.366128, precision: 0.015625, batch_len: 89.000000
Train, loss=1.68010664: 715it [17:26,  1.54s/it]2017-06-01 18:26:57,653 root  INFO     step 715.000000 - time: 1.091138, loss: 1.648255, perplexity: 5.197901, precision: 0.000000, batch_len: 108.000000
Train, loss=1.64825487: 716it [17:27,  1.42s/it]2017-06-01 18:26:58,624 root  INFO     step 716.000000 - time: 0.967360, loss: 1.731301, perplexity: 5.647996, precision: 0.000000, batch_len: 104.000000
Train, loss=1.73130083: 717it [17:28,  1.29s/it]2017-06-01 18:26:59,967 root  INFO     step 717.000000 - time: 1.171480, loss: 1.758681, perplexity: 5.804774, precision: 0.000000, batch_len: 102.000000
Train, loss=1.75868070: 718it [17:29,  1.30s/it]2017-06-01 18:27:01,252 root  INFO     step 718.000000 - time: 1.265026, loss: 1.596894, perplexity: 4.937672, precision: 0.031250, batch_len: 105.000000
Train, loss=1.59689391: 719it [17:31,  1.30s/it]2017-06-01 18:27:02,343 root  INFO     step 719.000000 - time: 1.049402, loss: 1.550340, perplexity: 4.713073, precision: 0.031250, batch_len: 88.000000
Train, loss=1.55034006: 720it [17:32,  1.24s/it]2017-06-01 18:27:03,764 root  INFO     step 720.000000 - time: 1.414898, loss: 1.673749, perplexity: 5.332119, precision: 0.000000, batch_len: 120.000000
Train, loss=1.67374873: 721it [17:33,  1.29s/it]2017-06-01 18:27:04,875 root  INFO     step 721.000000 - time: 1.083754, loss: 1.768096, perplexity: 5.859689, precision: 0.000000, batch_len: 113.000000
Train, loss=1.76809645: 722it [17:34,  1.24s/it]2017-06-01 18:27:05,873 root  INFO     step 722.000000 - time: 0.957564, loss: 1.936424, perplexity: 6.933914, precision: 0.015625, batch_len: 101.000000
Train, loss=1.93642437: 723it [17:35,  1.17s/it]2017-06-01 18:27:07,169 root  INFO     step 723.000000 - time: 1.265768, loss: 1.615728, perplexity: 5.031550, precision: 0.015625, batch_len: 103.000000
Train, loss=1.61572802: 724it [17:37,  1.20s/it]2017-06-01 18:27:08,460 root  INFO     step 724.000000 - time: 1.283024, loss: 1.585973, perplexity: 4.884043, precision: 0.015625, batch_len: 111.000000
Train, loss=1.58597326: 725it [17:38,  1.23s/it]2017-06-01 18:27:09,585 root  INFO     step 725.000000 - time: 1.096004, loss: 1.649379, perplexity: 5.203745, precision: 0.000000, batch_len: 109.000000
Train, loss=1.64937854: 726it [17:39,  1.20s/it]2017-06-01 18:27:10,795 root  INFO     step 726.000000 - time: 1.072150, loss: 1.708451, perplexity: 5.520405, precision: 0.000000, batch_len: 93.000000
Train, loss=1.70845127: 727it [17:40,  1.20s/it]2017-06-01 18:27:11,792 root  INFO     step 727.000000 - time: 0.987695, loss: 1.478722, perplexity: 4.387337, precision: 0.031250, batch_len: 100.000000
Train, loss=1.47872233: 728it [17:41,  1.14s/it]2017-06-01 18:27:12,806 root  INFO     step 728.000000 - time: 0.980027, loss: 1.643509, perplexity: 5.173290, precision: 0.000000, batch_len: 110.000000
Train, loss=1.64350891: 729it [17:42,  1.10s/it]2017-06-01 18:27:14,170 root  INFO     step 729.000000 - time: 1.291749, loss: 1.810468, perplexity: 6.113306, precision: 0.000000, batch_len: 112.000000
Train, loss=1.81046772: 730it [17:44,  1.18s/it]2017-06-01 18:27:15,704 root  INFO     step 730.000000 - time: 1.514638, loss: 1.866467, perplexity: 6.465412, precision: 0.015625, batch_len: 117.000000
Train, loss=1.86646676: 731it [17:45,  1.29s/it]2017-06-01 18:27:17,111 root  INFO     step 731.000000 - time: 1.296582, loss: 1.731232, perplexity: 5.647608, precision: 0.000000, batch_len: 124.000000
Train, loss=1.73123205: 732it [17:46,  1.32s/it]2017-06-01 18:27:18,232 root  INFO     step 732.000000 - time: 1.052152, loss: 1.918574, perplexity: 6.811238, precision: 0.000000, batch_len: 106.000000
Train, loss=1.91857386: 733it [17:48,  1.26s/it]2017-06-01 18:27:19,890 root  INFO     step 733.000000 - time: 1.568467, loss: 1.709535, perplexity: 5.526392, precision: 0.015625, batch_len: 90.000000
Train, loss=1.70953524: 734it [17:49,  1.38s/it]2017-06-01 18:27:21,333 root  INFO     step 734.000000 - time: 1.335437, loss: 1.784061, perplexity: 5.953989, precision: 0.000000, batch_len: 114.000000
Train, loss=1.78406143: 735it [17:51,  1.40s/it]2017-06-01 18:27:22,845 root  INFO     step 735.000000 - time: 1.375315, loss: 1.690249, perplexity: 5.420829, precision: 0.015625, batch_len: 121.000000
Train, loss=1.69024885: 736it [17:52,  1.43s/it]2017-06-01 18:27:24,161 root  INFO     step 736.000000 - time: 1.256929, loss: 1.687059, perplexity: 5.403566, precision: 0.062500, batch_len: 91.000000
Train, loss=1.68705916: 737it [17:54,  1.40s/it]2017-06-01 18:27:25,125 root  INFO     step 737.000000 - time: 0.939990, loss: 1.811589, perplexity: 6.120162, precision: 0.015625, batch_len: 86.000000
Train, loss=1.81158853: 738it [17:55,  1.27s/it]2017-06-01 18:27:26,161 root  INFO     step 738.000000 - time: 0.887238, loss: 1.972253, perplexity: 7.186849, precision: 0.015625, batch_len: 85.000000
Train, loss=1.97225285: 739it [17:56,  1.20s/it]2017-06-01 18:27:27,293 root  INFO     step 739.000000 - time: 0.968712, loss: 1.588255, perplexity: 4.895201, precision: 0.031250, batch_len: 80.000000
Train, loss=1.58825541: 740it [17:57,  1.18s/it]2017-06-01 18:27:28,586 root  INFO     step 740.000000 - time: 1.234352, loss: 1.931127, perplexity: 6.897280, precision: 0.031250, batch_len: 87.000000
Train, loss=1.93112719: 741it [17:58,  1.21s/it]2017-06-01 18:27:29,931 root  INFO     step 741.000000 - time: 1.334071, loss: 1.762840, perplexity: 5.828970, precision: 0.000000, batch_len: 94.000000
Train, loss=1.76284027: 742it [17:59,  1.25s/it]2017-06-01 18:27:31,133 root  INFO     step 742.000000 - time: 1.119165, loss: 1.740828, perplexity: 5.702060, precision: 0.000000, batch_len: 107.000000
Train, loss=1.74082756: 743it [18:01,  1.24s/it]2017-06-01 18:27:32,531 root  INFO     step 743.000000 - time: 1.321975, loss: 1.663260, perplexity: 5.276487, precision: 0.015625, batch_len: 125.000000
Train, loss=1.66326046: 744it [18:02,  1.29s/it]2017-06-01 18:27:33,738 root  INFO     step 744.000000 - time: 1.104454, loss: 1.522588, perplexity: 4.584071, precision: 0.031250, batch_len: 115.000000
Train, loss=1.52258754: 745it [18:03,  1.26s/it]2017-06-01 18:27:34,830 root  INFO     step 745.000000 - time: 0.974891, loss: 1.694020, perplexity: 5.441311, precision: 0.015625, batch_len: 79.000000
Train, loss=1.69402003: 746it [18:04,  1.21s/it]2017-06-01 18:27:36,016 root  INFO     step 746.000000 - time: 1.074072, loss: 1.859930, perplexity: 6.423285, precision: 0.015625, batch_len: 83.000000
Train, loss=1.85992968: 747it [18:05,  1.20s/it]2017-06-01 18:27:37,548 root  INFO     step 747.000000 - time: 1.484263, loss: 1.707056, perplexity: 5.512708, precision: 0.000000, batch_len: 116.000000
Train, loss=1.70705593: 748it [18:07,  1.30s/it]2017-06-01 18:27:38,891 root  INFO     step 748.000000 - time: 1.335545, loss: 1.536492, perplexity: 4.648254, precision: 0.000000, batch_len: 119.000000
Train, loss=1.53649163: 749it [18:08,  1.31s/it]2017-06-01 18:27:39,838 root  INFO     step 749.000000 - time: 0.927576, loss: 1.609102, perplexity: 4.998318, precision: 0.015625, batch_len: 81.000000
Train, loss=1.60910153: 750it [18:09,  1.20s/it]2017-06-01 18:27:41,165 root  INFO     step 750.000000 - time: 1.242567, loss: 1.832123, perplexity: 6.247133, precision: 0.000000, batch_len: 129.000000
Train, loss=1.83212256: 751it [18:11,  1.24s/it]2017-06-01 18:27:42,738 root  INFO     step 751.000000 - time: 1.488752, loss: 1.896536, perplexity: 6.662777, precision: 0.000000, batch_len: 135.000000
Train, loss=1.89653635: 752it [18:12,  1.34s/it]2017-06-01 18:27:44,232 root  INFO     step 752.000000 - time: 1.409854, loss: 2.135726, perplexity: 8.463192, precision: 0.000000, batch_len: 133.000000
Train, loss=2.13572645: 753it [18:14,  1.39s/it]2017-06-01 18:27:45,868 root  INFO     step 753.000000 - time: 1.493379, loss: 1.728983, perplexity: 5.634920, precision: 0.000000, batch_len: 137.000000
Train, loss=1.72898293: 754it [18:15,  1.46s/it]2017-06-01 18:27:47,305 root  INFO     step 754.000000 - time: 1.358349, loss: 1.663972, perplexity: 5.280240, precision: 0.000000, batch_len: 144.000000
Train, loss=1.66397154: 755it [18:17,  1.45s/it]2017-06-01 18:27:48,545 root  INFO     step 755.000000 - time: 1.172740, loss: 1.598184, perplexity: 4.944045, precision: 0.000000, batch_len: 99.000000
Train, loss=1.59818387: 756it [18:18,  1.39s/it]2017-06-01 18:27:50,188 root  INFO     step 756.000000 - time: 1.554072, loss: 1.546200, perplexity: 4.693601, precision: 0.000000, batch_len: 118.000000
Train, loss=1.54620004: 757it [18:20,  1.47s/it]2017-06-01 18:27:51,250 root  INFO     step 757.000000 - time: 1.035713, loss: 1.577820, perplexity: 4.844385, precision: 0.000000, batch_len: 84.000000
Train, loss=1.57782030: 758it [18:21,  1.34s/it]2017-06-01 18:27:52,654 root  INFO     step 758.000000 - time: 1.387594, loss: 1.636598, perplexity: 5.137660, precision: 0.000000, batch_len: 126.000000
Train, loss=1.63659763: 759it [18:22,  1.36s/it]2017-06-01 18:27:54,216 root  INFO     step 759.000000 - time: 1.437803, loss: 1.429208, perplexity: 4.175391, precision: 0.000000, batch_len: 123.000000
Train, loss=1.42920792: 760it [18:24,  1.42s/it]2017-06-01 18:27:55,782 root  INFO     step 760.000000 - time: 1.512863, loss: 1.377770, perplexity: 3.966046, precision: 0.015625, batch_len: 136.000000
Train, loss=1.37776971: 761it [18:25,  1.47s/it]2017-06-01 18:27:57,073 root  INFO     step 761.000000 - time: 1.270514, loss: 1.359618, perplexity: 3.894705, precision: 0.046875, batch_len: 98.000000
Train, loss=1.35961795: 762it [18:26,  1.41s/it]2017-06-01 18:27:58,048 root  INFO     step 762.000000 - time: 0.958384, loss: 1.424814, perplexity: 4.157084, precision: 0.046875, batch_len: 82.000000
Train, loss=1.42481375: 763it [18:27,  1.28s/it]2017-06-01 18:27:59,586 root  INFO     step 763.000000 - time: 1.435840, loss: 1.787378, perplexity: 5.973769, precision: 0.000000, batch_len: 130.000000
Train, loss=1.78737807: 764it [18:29,  1.36s/it]2017-06-01 18:28:00,774 root  INFO     step 764.000000 - time: 0.970830, loss: 1.638965, perplexity: 5.149837, precision: 0.000000, batch_len: 96.000000
Train, loss=1.63896513: 765it [18:30,  1.31s/it]2017-06-01 18:28:02,342 root  INFO     step 765.000000 - time: 1.481740, loss: 2.239707, perplexity: 9.390575, precision: 0.015625, batch_len: 122.000000
Train, loss=2.23970652: 766it [18:32,  1.39s/it]2017-06-01 18:28:03,632 root  INFO     step 766.000000 - time: 1.164980, loss: 1.596864, perplexity: 4.937524, precision: 0.000000, batch_len: 78.000000
Train, loss=1.59686399: 767it [18:33,  1.36s/it]2017-06-01 18:28:05,234 root  INFO     step 767.000000 - time: 1.448286, loss: 1.728145, perplexity: 5.630200, precision: 0.015625, batch_len: 141.000000
Train, loss=1.72814500: 768it [18:35,  1.43s/it]2017-06-01 18:28:06,183 root  INFO     step 768.000000 - time: 0.937971, loss: 1.407566, perplexity: 4.085999, precision: 0.000000, batch_len: 74.000000
Train, loss=1.40756631: 769it [18:36,  1.29s/it]2017-06-01 18:28:07,422 root  INFO     step 769.000000 - time: 1.207758, loss: 1.713562, perplexity: 5.548689, precision: 0.000000, batch_len: 134.000000
Train, loss=1.71356177: 770it [18:37,  1.27s/it]2017-06-01 18:28:08,458 root  INFO     step 770.000000 - time: 1.016753, loss: 1.890553, perplexity: 6.623028, precision: 0.000000, batch_len: 77.000000
Train, loss=1.89055264: 771it [18:38,  1.20s/it]2017-06-01 18:28:10,248 root  INFO     step 771.000000 - time: 1.688746, loss: 2.255739, perplexity: 9.542347, precision: 0.000000, batch_len: 138.000000
Train, loss=2.25573945: 772it [18:40,  1.38s/it]2017-06-01 18:28:11,744 root  INFO     step 772.000000 - time: 1.431143, loss: 1.636657, perplexity: 5.137966, precision: 0.000000, batch_len: 139.000000
Train, loss=1.63665724: 773it [18:41,  1.41s/it]2017-06-01 18:28:13,236 root  INFO     step 773.000000 - time: 1.392014, loss: 1.876321, perplexity: 6.529439, precision: 0.000000, batch_len: 142.000000
Train, loss=1.87632108: 774it [18:43,  1.44s/it]2017-06-01 18:28:14,673 root  INFO     step 774.000000 - time: 1.297013, loss: 1.602212, perplexity: 4.963999, precision: 0.000000, batch_len: 132.000000
Train, loss=1.60221171: 775it [18:44,  1.44s/it]2017-06-01 18:28:17,077 root  INFO     step 775.000000 - time: 2.240748, loss: 2.197969, perplexity: 9.006700, precision: 0.000000, batch_len: 152.000000
Train, loss=2.19796872: 776it [18:46,  1.73s/it]2017-06-01 18:28:18,922 root  INFO     step 776.000000 - time: 1.808773, loss: 1.621321, perplexity: 5.059772, precision: 0.000000, batch_len: 150.000000
Train, loss=1.62132144: 777it [18:48,  1.76s/it]2017-06-01 18:28:19,940 root  INFO     step 777.000000 - time: 0.916926, loss: 1.774715, perplexity: 5.898601, precision: 0.046875, batch_len: 76.000000
Train, loss=1.77471519: 778it [18:49,  1.54s/it]2017-06-01 18:28:21,082 root  INFO     step 778.000000 - time: 1.026524, loss: 1.426594, perplexity: 4.164492, precision: 0.015625, batch_len: 72.000000
Train, loss=1.42659438: 779it [18:50,  1.42s/it]2017-06-01 18:28:22,143 root  INFO     step 779.000000 - time: 1.030853, loss: 1.367837, perplexity: 3.926849, precision: 0.078125, batch_len: 71.000000
Train, loss=1.36783743: 780it [18:52,  1.31s/it]2017-06-01 18:28:23,918 root  INFO     step 780.000000 - time: 1.519284, loss: 1.582472, perplexity: 4.866971, precision: 0.000000, batch_len: 131.000000
Train, loss=1.58247185: 781it [18:53,  1.45s/it]2017-06-01 18:28:24,011 root  INFO     Generating first batch)
2017-06-01 18:28:27,854 root  INFO     step 781.000000 - time: 1.046203, loss: 1.297869, perplexity: 3.661487, precision: 0.000000, batch_len: 96.000000
Train, loss=1.29786944: 782it [18:57,  2.20s/it]2017-06-01 18:28:29,477 root  INFO     step 782.000000 - time: 1.383818, loss: 1.490001, perplexity: 4.437102, precision: 0.031250, batch_len: 105.000000
Train, loss=1.49000144: 783it [18:59,  2.02s/it]2017-06-01 18:28:30,632 root  INFO     step 783.000000 - time: 1.069896, loss: 1.799089, perplexity: 6.044137, precision: 0.046875, batch_len: 97.000000
Train, loss=1.79908872: 784it [19:00,  1.76s/it]2017-06-01 18:28:31,785 root  INFO     step 784.000000 - time: 1.042646, loss: 1.533047, perplexity: 4.632269, precision: 0.000000, batch_len: 104.000000
Train, loss=1.53304672: 785it [19:01,  1.58s/it]2017-06-01 18:28:32,867 root  INFO     step 785.000000 - time: 1.016718, loss: 1.850199, perplexity: 6.361087, precision: 0.000000, batch_len: 88.000000
Train, loss=1.85019922: 786it [19:02,  1.43s/it]2017-06-01 18:28:33,870 root  INFO     step 786.000000 - time: 0.965568, loss: 1.507753, perplexity: 4.516572, precision: 0.046875, batch_len: 90.000000
Train, loss=1.50775337: 787it [19:03,  1.30s/it]2017-06-01 18:28:35,226 root  INFO     step 787.000000 - time: 1.271745, loss: 1.298105, perplexity: 3.662350, precision: 0.093750, batch_len: 92.000000
Train, loss=1.29810500: 788it [19:05,  1.32s/it]2017-06-01 18:28:36,712 root  INFO     step 788.000000 - time: 1.458587, loss: 1.411908, perplexity: 4.103778, precision: 0.015625, batch_len: 128.000000
Train, loss=1.41190803: 789it [19:06,  1.37s/it]2017-06-01 18:28:38,144 root  INFO     step 789.000000 - time: 1.335936, loss: 1.422600, perplexity: 4.147892, precision: 0.015625, batch_len: 120.000000
Train, loss=1.42260027: 790it [19:08,  1.39s/it]2017-06-01 18:28:39,289 root  INFO     step 790.000000 - time: 0.970753, loss: 1.429155, perplexity: 4.175170, precision: 0.031250, batch_len: 81.000000
Train, loss=1.42915499: 791it [19:09,  1.31s/it]2017-06-01 18:28:40,293 root  INFO     step 791.000000 - time: 0.994616, loss: 1.542700, perplexity: 4.677203, precision: 0.015625, batch_len: 111.000000
Train, loss=1.54270029: 792it [19:10,  1.22s/it]2017-06-01 18:28:41,564 root  INFO     step 792.000000 - time: 1.184469, loss: 1.558801, perplexity: 4.753117, precision: 0.000000, batch_len: 102.000000
Train, loss=1.55880058: 793it [19:11,  1.24s/it]2017-06-01 18:28:42,938 root  INFO     step 793.000000 - time: 1.350606, loss: 1.873932, perplexity: 6.513856, precision: 0.015625, batch_len: 110.000000
Train, loss=1.87393153: 794it [19:12,  1.28s/it]2017-06-01 18:28:44,096 root  INFO     step 794.000000 - time: 1.014130, loss: 1.747544, perplexity: 5.740486, precision: 0.000000, batch_len: 100.000000
Train, loss=1.74754381: 795it [19:13,  1.24s/it]2017-06-01 18:28:45,171 root  INFO     step 795.000000 - time: 1.028733, loss: 1.333411, perplexity: 3.793961, precision: 0.015625, batch_len: 101.000000
Train, loss=1.33341050: 796it [19:15,  1.19s/it]2017-06-01 18:28:46,412 root  INFO     step 796.000000 - time: 1.060191, loss: 1.736565, perplexity: 5.677807, precision: 0.015625, batch_len: 93.000000
Train, loss=1.73656499: 797it [19:16,  1.21s/it]2017-06-01 18:28:47,415 root  INFO     step 797.000000 - time: 0.995501, loss: 1.488711, perplexity: 4.431379, precision: 0.015625, batch_len: 113.000000
Train, loss=1.48871088: 798it [19:17,  1.15s/it]2017-06-01 18:28:48,908 root  INFO     step 798.000000 - time: 1.471048, loss: 1.391299, perplexity: 4.020070, precision: 0.031250, batch_len: 112.000000
Train, loss=1.39129925: 799it [19:18,  1.25s/it]2017-06-01 18:28:50,133 root  INFO     step 799.000000 - time: 1.205343, loss: 1.493168, perplexity: 4.451172, precision: 0.031250, batch_len: 85.000000
Train, loss=1.49316752: 800it [19:20,  1.24s/it]2017-06-01 18:28:51,522 root  INFO     step 800.000000 - time: 1.328778, loss: 1.234544, perplexity: 3.436812, precision: 0.078125, batch_len: 117.000000
Train, loss=1.23454416: 801it [19:21,  1.29s/it]2017-06-01 18:28:52,478 root  INFO     step 801.000000 - time: 0.907650, loss: 1.494591, perplexity: 4.457511, precision: 0.015625, batch_len: 80.000000
Train, loss=1.49459064: 802it [19:22,  1.19s/it]2017-06-01 18:28:53,444 root  INFO     step 802.000000 - time: 0.933072, loss: 1.979356, perplexity: 7.238081, precision: 0.015625, batch_len: 86.000000
Train, loss=1.97935617: 803it [19:23,  1.12s/it]2017-06-01 18:28:54,531 root  INFO     step 803.000000 - time: 1.066973, loss: 1.514026, perplexity: 4.544991, precision: 0.000000, batch_len: 108.000000
Train, loss=1.51402569: 804it [19:24,  1.11s/it]2017-06-01 18:28:55,745 root  INFO     step 804.000000 - time: 1.108039, loss: 1.797205, perplexity: 6.032764, precision: 0.000000, batch_len: 79.000000
Train, loss=1.79720521: 805it [19:25,  1.14s/it]2017-06-01 18:28:57,075 root  INFO     step 805.000000 - time: 1.194713, loss: 1.854427, perplexity: 6.388039, precision: 0.000000, batch_len: 114.000000
Train, loss=1.85442734: 806it [19:26,  1.20s/it]2017-06-01 18:28:58,194 root  INFO     step 806.000000 - time: 1.111781, loss: 1.546281, perplexity: 4.693980, precision: 0.000000, batch_len: 106.000000
Train, loss=1.54628086: 807it [19:28,  1.17s/it]2017-06-01 18:28:59,294 root  INFO     step 807.000000 - time: 1.079024, loss: 1.341963, perplexity: 3.826547, precision: 0.062500, batch_len: 103.000000
Train, loss=1.34196293: 808it [19:29,  1.15s/it]2017-06-01 18:29:00,363 root  INFO     step 808.000000 - time: 1.031125, loss: 1.408807, perplexity: 4.091070, precision: 0.031250, batch_len: 115.000000
Train, loss=1.40880656: 809it [19:30,  1.13s/it]2017-06-01 18:29:01,476 root  INFO     step 809.000000 - time: 1.080324, loss: 1.332500, perplexity: 3.790507, precision: 0.046875, batch_len: 91.000000
Train, loss=1.33249974: 810it [19:31,  1.12s/it]2017-06-01 18:29:02,806 root  INFO     step 810.000000 - time: 1.288288, loss: 1.588726, perplexity: 4.897507, precision: 0.015625, batch_len: 89.000000
Train, loss=1.58872628: 811it [19:32,  1.19s/it]2017-06-01 18:29:04,072 root  INFO     step 811.000000 - time: 1.089312, loss: 1.400098, perplexity: 4.055596, precision: 0.015625, batch_len: 98.000000
Train, loss=1.40009761: 812it [19:33,  1.21s/it]2017-06-01 18:29:05,426 root  INFO     step 812.000000 - time: 1.279996, loss: 1.459036, perplexity: 4.301813, precision: 0.015625, batch_len: 125.000000
Train, loss=1.45903647: 813it [19:35,  1.25s/it]2017-06-01 18:29:06,811 root  INFO     step 813.000000 - time: 1.315149, loss: 1.519802, perplexity: 4.571319, precision: 0.015625, batch_len: 129.000000
Train, loss=1.51980174: 814it [19:36,  1.29s/it]2017-06-01 18:29:08,004 root  INFO     step 814.000000 - time: 1.175324, loss: 1.485551, perplexity: 4.417398, precision: 0.015625, batch_len: 124.000000
Train, loss=1.48555076: 815it [19:37,  1.26s/it]2017-06-01 18:29:09,406 root  INFO     step 815.000000 - time: 1.343794, loss: 1.596102, perplexity: 4.933762, precision: 0.031250, batch_len: 109.000000
Train, loss=1.59610176: 816it [19:39,  1.30s/it]2017-06-01 18:29:10,481 root  INFO     step 816.000000 - time: 1.054717, loss: 1.391964, perplexity: 4.022745, precision: 0.046875, batch_len: 94.000000
Train, loss=1.39196444: 817it [19:40,  1.24s/it]2017-06-01 18:29:11,680 root  INFO     step 817.000000 - time: 1.103968, loss: 1.361929, perplexity: 3.903718, precision: 0.031250, batch_len: 99.000000
Train, loss=1.36192942: 818it [19:41,  1.22s/it]2017-06-01 18:29:13,191 root  INFO     step 818.000000 - time: 1.339724, loss: 1.320138, perplexity: 3.743940, precision: 0.031250, batch_len: 121.000000
Train, loss=1.32013845: 819it [19:43,  1.31s/it]2017-06-01 18:29:14,197 root  INFO     step 819.000000 - time: 0.995184, loss: 1.345339, perplexity: 3.839489, precision: 0.062500, batch_len: 107.000000
Train, loss=1.34533930: 820it [19:44,  1.22s/it]2017-06-01 18:29:15,904 root  INFO     step 820.000000 - time: 1.620864, loss: 1.612245, perplexity: 5.014057, precision: 0.000000, batch_len: 137.000000
Train, loss=1.61224532: 821it [19:45,  1.37s/it]2017-06-01 18:29:17,482 root  INFO     step 821.000000 - time: 1.539008, loss: 1.524443, perplexity: 4.592587, precision: 0.000000, batch_len: 144.000000
Train, loss=1.52444339: 822it [19:47,  1.43s/it]2017-06-01 18:29:18,506 root  INFO     step 822.000000 - time: 0.959275, loss: 1.517099, perplexity: 4.558979, precision: 0.015625, batch_len: 87.000000
Train, loss=1.51709867: 823it [19:48,  1.31s/it]2017-06-01 18:29:20,009 root  INFO     step 823.000000 - time: 1.363675, loss: 1.466682, perplexity: 4.334828, precision: 0.015625, batch_len: 126.000000
Train, loss=1.46668196: 824it [19:49,  1.37s/it]2017-06-01 18:29:21,246 root  INFO     step 824.000000 - time: 1.211542, loss: 1.469337, perplexity: 4.346352, precision: 0.015625, batch_len: 133.000000
Train, loss=1.46933699: 825it [19:51,  1.33s/it]2017-06-01 18:29:23,037 root  INFO     step 825.000000 - time: 1.755980, loss: 1.353217, perplexity: 3.869856, precision: 0.015625, batch_len: 136.000000
Train, loss=1.35321736: 826it [19:52,  1.47s/it]2017-06-01 18:29:24,545 root  INFO     step 826.000000 - time: 1.405123, loss: 1.440106, perplexity: 4.221145, precision: 0.015625, batch_len: 123.000000
Train, loss=1.44010639: 827it [19:54,  1.48s/it]2017-06-01 18:29:25,623 root  INFO     step 827.000000 - time: 1.035416, loss: 1.606123, perplexity: 4.983454, precision: 0.031250, batch_len: 84.000000
Train, loss=1.60612321: 828it [19:55,  1.36s/it]2017-06-01 18:29:26,674 root  INFO     step 828.000000 - time: 1.000862, loss: 1.768293, perplexity: 5.860843, precision: 0.031250, batch_len: 83.000000
Train, loss=1.76829338: 829it [19:56,  1.27s/it]2017-06-01 18:29:28,193 root  INFO     step 829.000000 - time: 1.423489, loss: 1.923027, perplexity: 6.841635, precision: 0.000000, batch_len: 118.000000
Train, loss=1.92302668: 830it [19:58,  1.34s/it]2017-06-01 18:29:29,599 root  INFO     step 830.000000 - time: 1.355426, loss: 1.549781, perplexity: 4.710437, precision: 0.000000, batch_len: 116.000000
Train, loss=1.54978061: 831it [19:59,  1.36s/it]2017-06-01 18:29:31,321 root  INFO     step 831.000000 - time: 1.696214, loss: 1.522171, perplexity: 4.582163, precision: 0.000000, batch_len: 119.000000
Train, loss=1.52217114: 832it [20:01,  1.47s/it]2017-06-01 18:29:33,048 root  INFO     step 832.000000 - time: 1.435360, loss: 1.796504, perplexity: 6.028536, precision: 0.015625, batch_len: 130.000000
Train, loss=1.79650414: 833it [20:02,  1.55s/it]2017-06-01 18:29:34,281 root  INFO     step 833.000000 - time: 0.964597, loss: 1.455593, perplexity: 4.287023, precision: 0.031250, batch_len: 78.000000
Train, loss=1.45559263: 834it [20:04,  1.45s/it]2017-06-01 18:29:35,231 root  INFO     step 834.000000 - time: 0.885047, loss: 1.342046, perplexity: 3.826867, precision: 0.015625, batch_len: 82.000000
Train, loss=1.34204650: 835it [20:05,  1.30s/it]2017-06-01 18:29:36,634 root  INFO     step 835.000000 - time: 1.243598, loss: 1.044988, perplexity: 2.843365, precision: 0.203125, batch_len: 72.000000
Train, loss=1.04498816: 836it [20:06,  1.33s/it]2017-06-01 18:29:38,279 root  INFO     step 836.000000 - time: 1.595279, loss: 1.677937, perplexity: 5.354496, precision: 0.015625, batch_len: 122.000000
Train, loss=1.67793655: 837it [20:08,  1.43s/it]2017-06-01 18:29:39,651 root  INFO     step 837.000000 - time: 1.359612, loss: 1.416671, perplexity: 4.123370, precision: 0.000000, batch_len: 135.000000
Train, loss=1.41667068: 838it [20:09,  1.41s/it]2017-06-01 18:29:40,657 root  INFO     step 838.000000 - time: 0.953276, loss: 1.224066, perplexity: 3.400989, precision: 0.046875, batch_len: 74.000000
Train, loss=1.22406626: 839it [20:10,  1.29s/it]2017-06-01 18:29:41,835 root  INFO     step 839.000000 - time: 1.111530, loss: 1.502666, perplexity: 4.493654, precision: 0.031250, batch_len: 96.000000
Train, loss=1.50266612: 840it [20:11,  1.26s/it]2017-06-01 18:29:43,374 root  INFO     step 840.000000 - time: 1.485378, loss: 1.412108, perplexity: 4.104597, precision: 0.031250, batch_len: 138.000000
Train, loss=1.41210759: 841it [20:13,  1.34s/it]2017-06-01 18:29:45,067 root  INFO     step 841.000000 - time: 1.631109, loss: 1.434899, perplexity: 4.199220, precision: 0.000000, batch_len: 141.000000
Train, loss=1.43489885: 842it [20:14,  1.45s/it]2017-06-01 18:29:46,574 root  INFO     step 842.000000 - time: 1.430551, loss: 1.251872, perplexity: 3.496882, precision: 0.031250, batch_len: 139.000000
Train, loss=1.25187159: 843it [20:16,  1.46s/it]2017-06-01 18:29:48,021 root  INFO     step 843.000000 - time: 1.360378, loss: 1.379175, perplexity: 3.971623, precision: 0.000000, batch_len: 131.000000
Train, loss=1.37917495: 844it [20:17,  1.46s/it]2017-06-01 18:29:50,194 root  INFO     step 844.000000 - time: 1.973665, loss: 2.004805, perplexity: 7.424647, precision: 0.000000, batch_len: 152.000000
Train, loss=2.00480509: 845it [20:20,  1.67s/it]2017-06-01 18:29:51,801 root  INFO     step 845.000000 - time: 1.586852, loss: 1.767358, perplexity: 5.855365, precision: 0.000000, batch_len: 142.000000
Train, loss=1.76735830: 846it [20:21,  1.65s/it]2017-06-01 18:29:53,230 root  INFO     step 846.000000 - time: 1.362268, loss: 1.655128, perplexity: 5.233747, precision: 0.000000, batch_len: 134.000000
Train, loss=1.65512753: 847it [20:23,  1.59s/it]2017-06-01 18:29:54,639 root  INFO     step 847.000000 - time: 1.350661, loss: 1.693826, perplexity: 5.440253, precision: 0.000000, batch_len: 132.000000
Train, loss=1.69382560: 848it [20:24,  1.53s/it]2017-06-01 18:29:55,648 root  INFO     step 848.000000 - time: 0.830226, loss: 1.285370, perplexity: 3.616005, precision: 0.015625, batch_len: 77.000000
Train, loss=1.28536987: 849it [20:25,  1.38s/it]2017-06-01 18:29:56,744 root  INFO     step 849.000000 - time: 1.048542, loss: 1.383173, perplexity: 3.987535, precision: 0.171875, batch_len: 76.000000
Train, loss=1.38317335: 850it [20:26,  1.29s/it]2017-06-01 18:29:58,786 root  INFO     step 850.000000 - time: 1.957155, loss: 1.570601, perplexity: 4.809540, precision: 0.000000, batch_len: 150.000000
Train, loss=1.57060146: 851it [20:28,  1.52s/it]2017-06-01 18:29:59,872 root  INFO     step 851.000000 - time: 0.937811, loss: 1.659018, perplexity: 5.254146, precision: 0.015625, batch_len: 71.000000
Train, loss=1.65901756: 852it [20:29,  1.39s/it]2017-06-01 18:29:59,961 root  INFO     Generating first batch)
2017-06-01 18:30:03,874 root  INFO     step 852.000000 - time: 1.107979, loss: 1.864995, perplexity: 6.455904, precision: 0.015625, batch_len: 110.000000
Train, loss=1.86499500: 853it [20:33,  2.17s/it]2017-06-01 18:30:05,072 root  INFO     step 853.000000 - time: 1.095753, loss: 1.263077, perplexity: 3.536286, precision: 0.062500, batch_len: 96.000000
Train, loss=1.26307690: 854it [20:34,  1.88s/it]2017-06-01 18:30:06,388 root  INFO     step 854.000000 - time: 1.065081, loss: 1.573156, perplexity: 4.821840, precision: 0.015625, batch_len: 93.000000
Train, loss=1.57315552: 855it [20:36,  1.71s/it]2017-06-01 18:30:08,279 root  INFO     step 855.000000 - time: 1.776197, loss: 1.484290, perplexity: 4.411833, precision: 0.000000, batch_len: 90.000000
Train, loss=1.48429036: 856it [20:38,  1.76s/it]2017-06-01 18:30:09,544 root  INFO     step 856.000000 - time: 1.210312, loss: 1.294622, perplexity: 3.649616, precision: 0.046875, batch_len: 104.000000
Train, loss=1.29462194: 857it [20:39,  1.61s/it]2017-06-01 18:30:10,657 root  INFO     step 857.000000 - time: 1.049721, loss: 1.244583, perplexity: 3.471487, precision: 0.062500, batch_len: 92.000000
Train, loss=1.24458313: 858it [20:40,  1.46s/it]2017-06-01 18:30:11,686 root  INFO     step 858.000000 - time: 0.977649, loss: 1.327601, perplexity: 3.771983, precision: 0.078125, batch_len: 101.000000
Train, loss=1.32760096: 859it [20:41,  1.33s/it]2017-06-01 18:30:12,724 root  INFO     step 859.000000 - time: 1.018783, loss: 1.298603, perplexity: 3.664174, precision: 0.015625, batch_len: 100.000000
Train, loss=1.29860282: 860it [20:42,  1.24s/it]2017-06-01 18:30:13,897 root  INFO     step 860.000000 - time: 1.146598, loss: 1.383285, perplexity: 3.987979, precision: 0.031250, batch_len: 91.000000
Train, loss=1.38328469: 861it [20:43,  1.22s/it]2017-06-01 18:30:15,529 root  INFO     step 861.000000 - time: 1.625237, loss: 1.274816, perplexity: 3.578043, precision: 0.015625, batch_len: 123.000000
Train, loss=1.27481604: 862it [20:45,  1.35s/it]2017-06-01 18:30:17,162 root  INFO     step 862.000000 - time: 1.599178, loss: 1.358384, perplexity: 3.889902, precision: 0.046875, batch_len: 121.000000
Train, loss=1.35838389: 863it [20:47,  1.43s/it]2017-06-01 18:30:18,416 root  INFO     step 863.000000 - time: 1.055057, loss: 1.318168, perplexity: 3.736570, precision: 0.078125, batch_len: 89.000000
Train, loss=1.31816804: 864it [20:48,  1.38s/it]2017-06-01 18:30:19,457 root  INFO     step 864.000000 - time: 1.006510, loss: 1.248259, perplexity: 3.484273, precision: 0.078125, batch_len: 88.000000
Train, loss=1.24825943: 865it [20:49,  1.28s/it]2017-06-01 18:30:20,799 root  INFO     step 865.000000 - time: 1.274248, loss: 1.221490, perplexity: 3.392239, precision: 0.046875, batch_len: 120.000000
Train, loss=1.22149014: 866it [20:50,  1.30s/it]2017-06-01 18:30:22,144 root  INFO     step 866.000000 - time: 1.169617, loss: 1.344494, perplexity: 3.836246, precision: 0.031250, batch_len: 85.000000
Train, loss=1.34449434: 867it [20:52,  1.31s/it]2017-06-01 18:30:23,371 root  INFO     step 867.000000 - time: 1.210203, loss: 1.368893, perplexity: 3.930995, precision: 0.031250, batch_len: 109.000000
Train, loss=1.36889267: 868it [20:53,  1.29s/it]2017-06-01 18:30:24,478 root  INFO     step 868.000000 - time: 1.090389, loss: 1.360541, perplexity: 3.898301, precision: 0.031250, batch_len: 113.000000
Train, loss=1.36054075: 869it [20:54,  1.23s/it]2017-06-01 18:30:25,591 root  INFO     step 869.000000 - time: 1.055231, loss: 1.369855, perplexity: 3.934779, precision: 0.015625, batch_len: 105.000000
Train, loss=1.36985469: 870it [20:55,  1.20s/it]2017-06-01 18:30:26,756 root  INFO     step 870.000000 - time: 1.112819, loss: 1.530048, perplexity: 4.618400, precision: 0.062500, batch_len: 106.000000
Train, loss=1.53004825: 871it [20:56,  1.19s/it]2017-06-01 18:30:28,023 root  INFO     step 871.000000 - time: 1.258748, loss: 1.772388, perplexity: 5.884888, precision: 0.000000, batch_len: 108.000000
Train, loss=1.77238774: 872it [20:57,  1.21s/it]2017-06-01 18:30:29,070 root  INFO     step 872.000000 - time: 1.005718, loss: 1.879525, perplexity: 6.550391, precision: 0.031250, batch_len: 102.000000
Train, loss=1.87952471: 873it [20:58,  1.16s/it]2017-06-01 18:30:30,563 root  INFO     step 873.000000 - time: 1.456520, loss: 1.298067, perplexity: 3.662212, precision: 0.015625, batch_len: 128.000000
Train, loss=1.29806745: 874it [21:00,  1.26s/it]2017-06-01 18:30:31,839 root  INFO     step 874.000000 - time: 1.245933, loss: 1.262427, perplexity: 3.533988, precision: 0.015625, batch_len: 97.000000
Train, loss=1.26242709: 875it [21:01,  1.27s/it]2017-06-01 18:30:33,198 root  INFO     step 875.000000 - time: 1.303706, loss: 1.428123, perplexity: 4.170862, precision: 0.015625, batch_len: 125.000000
Train, loss=1.42812276: 876it [21:03,  1.29s/it]2017-06-01 18:30:34,546 root  INFO     step 876.000000 - time: 1.315478, loss: 1.289958, perplexity: 3.632634, precision: 0.000000, batch_len: 124.000000
Train, loss=1.28995800: 877it [21:04,  1.31s/it]2017-06-01 18:30:35,514 root  INFO     step 877.000000 - time: 0.948450, loss: 1.205328, perplexity: 3.337854, precision: 0.078125, batch_len: 103.000000
Train, loss=1.20532823: 878it [21:05,  1.21s/it]2017-06-01 18:30:36,940 root  INFO     step 878.000000 - time: 1.332569, loss: 1.230513, perplexity: 3.422985, precision: 0.093750, batch_len: 112.000000
Train, loss=1.23051298: 879it [21:06,  1.27s/it]2017-06-01 18:30:38,119 root  INFO     step 879.000000 - time: 1.127382, loss: 1.075539, perplexity: 2.931572, precision: 0.078125, batch_len: 80.000000
Train, loss=1.07553887: 880it [21:07,  1.24s/it]2017-06-01 18:30:39,543 root  INFO     step 880.000000 - time: 1.356296, loss: 1.240740, perplexity: 3.458170, precision: 0.046875, batch_len: 117.000000
Train, loss=1.24073958: 881it [21:09,  1.30s/it]2017-06-01 18:30:40,583 root  INFO     step 881.000000 - time: 0.988191, loss: 1.565347, perplexity: 4.784337, precision: 0.000000, batch_len: 86.000000
Train, loss=1.56534743: 882it [21:10,  1.22s/it]2017-06-01 18:30:41,673 root  INFO     step 882.000000 - time: 0.956325, loss: 1.572303, perplexity: 4.817733, precision: 0.062500, batch_len: 79.000000
Train, loss=1.57230341: 883it [21:11,  1.18s/it]2017-06-01 18:30:42,695 root  INFO     step 883.000000 - time: 0.982068, loss: 1.390900, perplexity: 4.018465, precision: 0.015625, batch_len: 114.000000
Train, loss=1.39089990: 884it [21:12,  1.13s/it]2017-06-01 18:30:43,918 root  INFO     step 884.000000 - time: 1.216750, loss: 1.422659, perplexity: 4.148134, precision: 0.000000, batch_len: 98.000000
Train, loss=1.42265868: 885it [21:13,  1.16s/it]2017-06-01 18:30:45,039 root  INFO     step 885.000000 - time: 1.096423, loss: 1.523974, perplexity: 4.590433, precision: 0.046875, batch_len: 81.000000
Train, loss=1.52397442: 886it [21:14,  1.15s/it]2017-06-01 18:30:46,415 root  INFO     step 886.000000 - time: 1.121474, loss: 1.274120, perplexity: 3.575553, precision: 0.062500, batch_len: 115.000000
Train, loss=1.27411985: 887it [21:16,  1.22s/it]2017-06-01 18:30:47,514 root  INFO     step 887.000000 - time: 1.094162, loss: 1.314127, perplexity: 3.721501, precision: 0.046875, batch_len: 107.000000
Train, loss=1.31412709: 888it [21:17,  1.18s/it]2017-06-01 18:30:48,486 root  INFO     step 888.000000 - time: 0.956294, loss: 1.328217, perplexity: 3.774306, precision: 0.046875, batch_len: 94.000000
Train, loss=1.32821655: 889it [21:18,  1.12s/it]2017-06-01 18:30:49,466 root  INFO     step 889.000000 - time: 0.906902, loss: 1.248454, perplexity: 3.484951, precision: 0.156250, batch_len: 83.000000
Train, loss=1.24845386: 890it [21:19,  1.08s/it]2017-06-01 18:30:51,033 root  INFO     step 890.000000 - time: 1.558407, loss: 1.441143, perplexity: 4.225522, precision: 0.046875, batch_len: 116.000000
Train, loss=1.44114280: 891it [21:20,  1.22s/it]2017-06-01 18:30:52,630 root  INFO     step 891.000000 - time: 1.463946, loss: 1.449322, perplexity: 4.260225, precision: 0.000000, batch_len: 137.000000
Train, loss=1.44932199: 892it [21:22,  1.34s/it]2017-06-01 18:30:54,015 root  INFO     step 892.000000 - time: 1.351853, loss: 1.501011, perplexity: 4.486224, precision: 0.015625, batch_len: 119.000000
Train, loss=1.50101137: 893it [21:23,  1.35s/it]2017-06-01 18:30:55,115 root  INFO     step 893.000000 - time: 1.089089, loss: 1.784595, perplexity: 5.957166, precision: 0.000000, batch_len: 111.000000
Train, loss=1.78459489: 894it [21:24,  1.28s/it]2017-06-01 18:30:56,695 root  INFO     step 894.000000 - time: 1.463864, loss: 1.795712, perplexity: 6.023765, precision: 0.000000, batch_len: 136.000000
Train, loss=1.79571247: 895it [21:26,  1.37s/it]2017-06-01 18:30:58,351 root  INFO     step 895.000000 - time: 1.621108, loss: 1.325770, perplexity: 3.765083, precision: 0.015625, batch_len: 118.000000
Train, loss=1.32576978: 896it [21:28,  1.45s/it]2017-06-01 18:30:59,489 root  INFO     step 896.000000 - time: 1.102777, loss: 1.266160, perplexity: 3.547207, precision: 0.031250, batch_len: 99.000000
Train, loss=1.26616049: 897it [21:29,  1.36s/it]2017-06-01 18:31:00,479 root  INFO     step 897.000000 - time: 0.955316, loss: 1.331257, perplexity: 3.785801, precision: 0.031250, batch_len: 87.000000
Train, loss=1.33125746: 898it [21:30,  1.25s/it]2017-06-01 18:31:01,571 root  INFO     step 898.000000 - time: 0.998023, loss: 1.484007, perplexity: 4.410585, precision: 0.078125, batch_len: 84.000000
Train, loss=1.48400736: 899it [21:31,  1.20s/it]2017-06-01 18:31:02,876 root  INFO     step 899.000000 - time: 1.275597, loss: 1.148064, perplexity: 3.152085, precision: 0.078125, batch_len: 135.000000
Train, loss=1.14806414: 900it [21:32,  1.23s/it]2017-06-01 18:31:04,654 root  INFO     step 900.000000 - time: 1.726418, loss: 1.243039, perplexity: 3.466132, precision: 0.015625, batch_len: 144.000000
Train, loss=1.24303937: 901it [21:34,  1.40s/it]2017-06-01 18:31:06,121 root  INFO     step 901.000000 - time: 1.318489, loss: 1.177881, perplexity: 3.247487, precision: 0.062500, batch_len: 133.000000
Train, loss=1.17788148: 902it [21:35,  1.42s/it]2017-06-01 18:31:07,509 root  INFO     step 902.000000 - time: 1.360493, loss: 1.305062, perplexity: 3.687916, precision: 0.015625, batch_len: 126.000000
Train, loss=1.30506158: 903it [21:37,  1.41s/it]2017-06-01 18:31:08,943 root  INFO     step 903.000000 - time: 1.233887, loss: 1.575378, perplexity: 4.832569, precision: 0.015625, batch_len: 130.000000
Train, loss=1.57537818: 904it [21:38,  1.42s/it]2017-06-01 18:31:10,484 root  INFO     step 904.000000 - time: 1.463489, loss: 1.589574, perplexity: 4.901659, precision: 0.046875, batch_len: 122.000000
Train, loss=1.58957362: 905it [21:40,  1.45s/it]2017-06-01 18:31:12,118 root  INFO     step 905.000000 - time: 1.484224, loss: 1.376136, perplexity: 3.959572, precision: 0.000000, batch_len: 129.000000
Train, loss=1.37613606: 906it [21:41,  1.51s/it]2017-06-01 18:31:13,087 root  INFO     step 906.000000 - time: 0.922527, loss: 1.243823, perplexity: 3.468850, precision: 0.046875, batch_len: 82.000000
Train, loss=1.24382305: 907it [21:42,  1.35s/it]2017-06-01 18:31:14,485 root  INFO     step 907.000000 - time: 1.070922, loss: 1.135745, perplexity: 3.113493, precision: 0.078125, batch_len: 96.000000
Train, loss=1.13574529: 908it [21:44,  1.36s/it]2017-06-01 18:31:15,356 root  INFO     step 908.000000 - time: 0.857341, loss: 1.213745, perplexity: 3.366067, precision: 0.031250, batch_len: 78.000000
Train, loss=1.21374500: 909it [21:45,  1.21s/it]2017-06-01 18:31:17,006 root  INFO     step 909.000000 - time: 1.628090, loss: 1.690586, perplexity: 5.422659, precision: 0.000000, batch_len: 138.000000
Train, loss=1.69058621: 910it [21:46,  1.35s/it]2017-06-01 18:31:18,641 root  INFO     step 910.000000 - time: 1.483277, loss: 1.364330, perplexity: 3.913100, precision: 0.000000, batch_len: 132.000000
Train, loss=1.36432981: 911it [21:48,  1.43s/it]2017-06-01 18:31:20,235 root  INFO     step 911.000000 - time: 1.426231, loss: 1.498255, perplexity: 4.473876, precision: 0.000000, batch_len: 141.000000
Train, loss=1.49825525: 912it [21:50,  1.48s/it]2017-06-01 18:31:21,200 root  INFO     step 912.000000 - time: 0.942800, loss: 1.467812, perplexity: 4.339729, precision: 0.000000, batch_len: 74.000000
Train, loss=1.46781194: 913it [21:51,  1.33s/it]2017-06-01 18:31:22,543 root  INFO     step 913.000000 - time: 1.338351, loss: 1.956086, perplexity: 7.071592, precision: 0.000000, batch_len: 134.000000
Train, loss=1.95608568: 914it [21:52,  1.33s/it]2017-06-01 18:31:24,346 root  INFO     step 914.000000 - time: 1.741809, loss: 1.287551, perplexity: 3.623900, precision: 0.000000, batch_len: 139.000000
Train, loss=1.28755069: 915it [21:54,  1.47s/it]2017-06-01 18:31:25,592 root  INFO     step 915.000000 - time: 1.168417, loss: 0.977128, perplexity: 2.656816, precision: 0.140625, batch_len: 72.000000
Train, loss=0.97712845: 916it [21:55,  1.40s/it]2017-06-01 18:31:27,599 root  INFO     step 916.000000 - time: 1.957080, loss: 1.665627, perplexity: 5.288990, precision: 0.000000, batch_len: 152.000000
Train, loss=1.66562724: 917it [21:57,  1.59s/it]2017-06-01 18:31:28,651 root  INFO     step 917.000000 - time: 0.908257, loss: 1.349730, perplexity: 3.856385, precision: 0.062500, batch_len: 76.000000
Train, loss=1.34973013: 918it [21:58,  1.43s/it]2017-06-01 18:31:30,176 root  INFO     step 918.000000 - time: 1.520084, loss: 1.306244, perplexity: 3.692281, precision: 0.000000, batch_len: 131.000000
Train, loss=1.30624449: 919it [22:00,  1.46s/it]2017-06-01 18:31:32,321 root  INFO     step 919.000000 - time: 1.943793, loss: 1.245026, perplexity: 3.473025, precision: 0.015625, batch_len: 150.000000
Train, loss=1.24502587: 920it [22:02,  1.66s/it]2017-06-01 18:31:33,534 root  INFO     step 920.000000 - time: 1.124665, loss: 1.093260, perplexity: 2.983987, precision: 0.109375, batch_len: 77.000000
Train, loss=1.09326029: 921it [22:03,  1.53s/it]2017-06-01 18:31:35,211 root  INFO     step 921.000000 - time: 1.422627, loss: 1.507885, perplexity: 4.517165, precision: 0.000000, batch_len: 142.000000
Train, loss=1.50788462: 922it [22:05,  1.57s/it]2017-06-01 18:31:36,160 root  INFO     step 922.000000 - time: 0.944641, loss: 1.101564, perplexity: 3.008867, precision: 0.062500, batch_len: 71.000000
Train, loss=1.10156357: 923it [22:06,  1.39s/it]2017-06-01 18:31:36,257 root  INFO     Generating first batch)
2017-06-01 18:31:39,954 root  INFO     step 923.000000 - time: 1.014439, loss: 0.970303, perplexity: 2.638744, precision: 0.046875, batch_len: 101.000000
Train, loss=0.97030318: 924it [22:09,  2.11s/it]2017-06-01 18:31:41,153 root  INFO     step 924.000000 - time: 1.066249, loss: 1.032617, perplexity: 2.808405, precision: 0.078125, batch_len: 96.000000
Train, loss=1.03261662: 925it [22:11,  1.84s/it]2017-06-01 18:31:42,589 root  INFO     step 925.000000 - time: 1.199033, loss: 1.099230, perplexity: 3.001854, precision: 0.031250, batch_len: 128.000000
Train, loss=1.09923017: 926it [22:12,  1.72s/it]2017-06-01 18:31:44,414 root  INFO     step 926.000000 - time: 1.690418, loss: 1.264540, perplexity: 3.541464, precision: 0.031250, batch_len: 120.000000
Train, loss=1.26454031: 927it [22:14,  1.75s/it]2017-06-01 18:31:45,582 root  INFO     step 927.000000 - time: 1.045350, loss: 1.685824, perplexity: 5.396898, precision: 0.000000, batch_len: 92.000000
Train, loss=1.68582439: 928it [22:15,  1.57s/it]2017-06-01 18:31:46,721 root  INFO     step 928.000000 - time: 1.110130, loss: 1.919527, perplexity: 6.817732, precision: 0.015625, batch_len: 110.000000
Train, loss=1.91952682: 929it [22:16,  1.44s/it]2017-06-01 18:31:47,769 root  INFO     step 929.000000 - time: 1.024476, loss: 1.282005, perplexity: 3.603858, precision: 0.078125, batch_len: 88.000000
Train, loss=1.28200483: 930it [22:17,  1.33s/it]2017-06-01 18:31:49,031 root  INFO     step 930.000000 - time: 0.966666, loss: 1.224266, perplexity: 3.401669, precision: 0.093750, batch_len: 102.000000
Train, loss=1.22426617: 931it [22:18,  1.31s/it]2017-06-01 18:31:50,273 root  INFO     step 931.000000 - time: 1.188053, loss: 1.155760, perplexity: 3.176435, precision: 0.093750, batch_len: 108.000000
Train, loss=1.15575957: 932it [22:20,  1.29s/it]2017-06-01 18:31:51,592 root  INFO     step 932.000000 - time: 1.279308, loss: 1.314733, perplexity: 3.723757, precision: 0.093750, batch_len: 113.000000
Train, loss=1.31473303: 933it [22:21,  1.30s/it]2017-06-01 18:31:52,690 root  INFO     step 933.000000 - time: 1.086419, loss: 1.529022, perplexity: 4.613662, precision: 0.031250, batch_len: 93.000000
Train, loss=1.52902198: 934it [22:22,  1.24s/it]2017-06-01 18:31:53,732 root  INFO     step 934.000000 - time: 1.009454, loss: 1.188787, perplexity: 3.283096, precision: 0.109375, batch_len: 105.000000
Train, loss=1.18878675: 935it [22:23,  1.18s/it]2017-06-01 18:31:54,945 root  INFO     step 935.000000 - time: 1.033393, loss: 1.349149, perplexity: 3.854143, precision: 0.078125, batch_len: 106.000000
Train, loss=1.34914875: 936it [22:24,  1.19s/it]2017-06-01 18:31:56,156 root  INFO     step 936.000000 - time: 1.175065, loss: 1.362565, perplexity: 3.906201, precision: 0.015625, batch_len: 103.000000
Train, loss=1.36256528: 937it [22:26,  1.20s/it]2017-06-01 18:31:57,599 root  INFO     step 937.000000 - time: 1.401722, loss: 1.423187, perplexity: 4.150327, precision: 0.000000, batch_len: 104.000000
Train, loss=1.42318702: 938it [22:27,  1.27s/it]2017-06-01 18:31:58,700 root  INFO     step 938.000000 - time: 1.034521, loss: 1.722653, perplexity: 5.599362, precision: 0.015625, batch_len: 100.000000
Train, loss=1.72265267: 939it [22:28,  1.22s/it]2017-06-01 18:32:00,056 root  INFO     step 939.000000 - time: 1.336953, loss: 1.142292, perplexity: 3.133945, precision: 0.046875, batch_len: 112.000000
Train, loss=1.14229250: 940it [22:29,  1.26s/it]2017-06-01 18:32:01,414 root  INFO     step 940.000000 - time: 1.323786, loss: 1.404528, perplexity: 4.073602, precision: 0.031250, batch_len: 121.000000
Train, loss=1.40452754: 941it [22:31,  1.29s/it]2017-06-01 18:32:02,370 root  INFO     step 941.000000 - time: 0.899922, loss: 1.463606, perplexity: 4.321516, precision: 0.000000, batch_len: 94.000000
Train, loss=1.46360636: 942it [22:32,  1.19s/it]2017-06-01 18:32:03,581 root  INFO     step 942.000000 - time: 1.204314, loss: 1.232293, perplexity: 3.429084, precision: 0.062500, batch_len: 97.000000
Train, loss=1.23229313: 943it [22:33,  1.20s/it]2017-06-01 18:32:04,918 root  INFO     step 943.000000 - time: 1.292591, loss: 1.377911, perplexity: 3.966605, precision: 0.015625, batch_len: 111.000000
Train, loss=1.37791061: 944it [22:34,  1.24s/it]2017-06-01 18:32:06,597 root  INFO     step 944.000000 - time: 1.663578, loss: 1.471467, perplexity: 4.355622, precision: 0.031250, batch_len: 90.000000
Train, loss=1.47146738: 945it [22:36,  1.37s/it]2017-06-01 18:32:07,612 root  INFO     step 945.000000 - time: 0.995212, loss: 1.408234, perplexity: 4.088727, precision: 0.015625, batch_len: 81.000000
Train, loss=1.40823364: 946it [22:37,  1.26s/it]2017-06-01 18:32:08,908 root  INFO     step 946.000000 - time: 1.208235, loss: 1.303391, perplexity: 3.681760, precision: 0.031250, batch_len: 124.000000
Train, loss=1.30339098: 947it [22:38,  1.27s/it]2017-06-01 18:32:10,508 root  INFO     step 947.000000 - time: 1.566019, loss: 1.385218, perplexity: 3.995698, precision: 0.015625, batch_len: 117.000000
Train, loss=1.38521838: 948it [22:40,  1.37s/it]2017-06-01 18:32:11,715 root  INFO     step 948.000000 - time: 1.128691, loss: 1.279328, perplexity: 3.594224, precision: 0.093750, batch_len: 86.000000
Train, loss=1.27932811: 949it [22:41,  1.32s/it]2017-06-01 18:32:12,885 root  INFO     step 949.000000 - time: 1.132961, loss: 1.163616, perplexity: 3.201490, precision: 0.062500, batch_len: 114.000000
Train, loss=1.16361630: 950it [22:42,  1.28s/it]2017-06-01 18:32:13,984 root  INFO     step 950.000000 - time: 1.090371, loss: 1.248196, perplexity: 3.484053, precision: 0.062500, batch_len: 109.000000
Train, loss=1.24819636: 951it [22:43,  1.22s/it]2017-06-01 18:32:15,091 root  INFO     step 951.000000 - time: 0.983424, loss: 1.433782, perplexity: 4.194534, precision: 0.046875, batch_len: 107.000000
Train, loss=1.43378234: 952it [22:44,  1.19s/it]2017-06-01 18:32:16,634 root  INFO     step 952.000000 - time: 1.392675, loss: 1.233747, perplexity: 3.434071, precision: 0.046875, batch_len: 91.000000
Train, loss=1.23374653: 953it [22:46,  1.29s/it]2017-06-01 18:32:17,862 root  INFO     step 953.000000 - time: 1.184158, loss: 1.256872, perplexity: 3.514412, precision: 0.203125, batch_len: 89.000000
Train, loss=1.25687218: 954it [22:47,  1.27s/it]2017-06-01 18:32:19,043 root  INFO     step 954.000000 - time: 1.125378, loss: 1.150053, perplexity: 3.158360, precision: 0.093750, batch_len: 115.000000
Train, loss=1.15005279: 955it [22:48,  1.25s/it]2017-06-01 18:32:20,363 root  INFO     step 955.000000 - time: 1.286464, loss: 1.668761, perplexity: 5.305593, precision: 0.000000, batch_len: 129.000000
Train, loss=1.66876149: 956it [22:50,  1.27s/it]2017-06-01 18:32:21,426 root  INFO     step 956.000000 - time: 1.006989, loss: 1.550086, perplexity: 4.711873, precision: 0.000000, batch_len: 79.000000
Train, loss=1.55008554: 957it [22:51,  1.21s/it]2017-06-01 18:32:22,658 root  INFO     step 957.000000 - time: 1.215167, loss: 1.217351, perplexity: 3.378228, precision: 0.031250, batch_len: 125.000000
Train, loss=1.21735144: 958it [22:52,  1.21s/it]2017-06-01 18:32:23,953 root  INFO     step 958.000000 - time: 1.152881, loss: 1.081438, perplexity: 2.948918, precision: 0.109375, batch_len: 80.000000
Train, loss=1.08143842: 959it [22:53,  1.24s/it]2017-06-01 18:32:25,137 root  INFO     step 959.000000 - time: 1.103456, loss: 1.447877, perplexity: 4.254075, precision: 0.093750, batch_len: 85.000000
Train, loss=1.44787741: 960it [22:55,  1.22s/it]2017-06-01 18:32:26,405 root  INFO     step 960.000000 - time: 1.165918, loss: 1.113817, perplexity: 3.045963, precision: 0.062500, batch_len: 99.000000
Train, loss=1.11381698: 961it [22:56,  1.24s/it]2017-06-01 18:32:27,810 root  INFO     step 961.000000 - time: 1.384530, loss: 1.125795, perplexity: 3.082666, precision: 0.015625, batch_len: 118.000000
Train, loss=1.12579489: 962it [22:57,  1.29s/it]2017-06-01 18:32:29,203 root  INFO     step 962.000000 - time: 1.368383, loss: 1.089780, perplexity: 2.973621, precision: 0.046875, batch_len: 119.000000
Train, loss=1.08978033: 963it [22:59,  1.32s/it]2017-06-01 18:32:30,456 root  INFO     step 963.000000 - time: 1.249092, loss: 0.959444, perplexity: 2.610244, precision: 0.125000, batch_len: 98.000000
Train, loss=0.95944357: 964it [23:00,  1.30s/it]2017-06-01 18:32:32,006 root  INFO     step 964.000000 - time: 1.469012, loss: 1.204141, perplexity: 3.333894, precision: 0.078125, batch_len: 116.000000
Train, loss=1.20414114: 965it [23:01,  1.37s/it]2017-06-01 18:32:33,685 root  INFO     step 965.000000 - time: 1.624260, loss: 1.054034, perplexity: 2.869202, precision: 0.000000, batch_len: 123.000000
Train, loss=1.05403399: 966it [23:03,  1.47s/it]2017-06-01 18:32:34,686 root  INFO     step 966.000000 - time: 0.964818, loss: 1.064509, perplexity: 2.899414, precision: 0.062500, batch_len: 87.000000
Train, loss=1.06450880: 967it [23:04,  1.33s/it]2017-06-01 18:32:36,116 root  INFO     step 967.000000 - time: 1.401878, loss: 1.088983, perplexity: 2.971252, precision: 0.031250, batch_len: 126.000000
Train, loss=1.08898342: 968it [23:05,  1.36s/it]2017-06-01 18:32:37,195 root  INFO     step 968.000000 - time: 0.903973, loss: 1.150804, perplexity: 3.160732, precision: 0.062500, batch_len: 83.000000
Train, loss=1.15080357: 969it [23:07,  1.27s/it]2017-06-01 18:32:38,330 root  INFO     step 969.000000 - time: 1.057173, loss: 1.124042, perplexity: 3.077268, precision: 0.187500, batch_len: 82.000000
Train, loss=1.12404227: 970it [23:08,  1.23s/it]2017-06-01 18:32:39,658 root  INFO     step 970.000000 - time: 1.184218, loss: 1.161849, perplexity: 3.195839, precision: 0.140625, batch_len: 84.000000
Train, loss=1.16184950: 971it [23:09,  1.26s/it]2017-06-01 18:32:41,301 root  INFO     step 971.000000 - time: 1.408505, loss: 1.214905, perplexity: 3.369975, precision: 0.062500, batch_len: 130.000000
Train, loss=1.21490526: 972it [23:11,  1.38s/it]2017-06-01 18:32:42,896 root  INFO     step 972.000000 - time: 1.443202, loss: 1.137649, perplexity: 3.119425, precision: 0.046875, batch_len: 136.000000
Train, loss=1.13764858: 973it [23:12,  1.44s/it]2017-06-01 18:32:44,173 root  INFO     step 973.000000 - time: 1.235523, loss: 1.229949, perplexity: 3.421054, precision: 0.078125, batch_len: 135.000000
Train, loss=1.22994876: 974it [23:14,  1.39s/it]2017-06-01 18:32:45,875 root  INFO     step 974.000000 - time: 1.651732, loss: 1.327573, perplexity: 3.771877, precision: 0.015625, batch_len: 133.000000
Train, loss=1.32757282: 975it [23:15,  1.49s/it]2017-06-01 18:32:47,360 root  INFO     step 975.000000 - time: 1.385256, loss: 1.393903, perplexity: 4.030550, precision: 0.093750, batch_len: 122.000000
Train, loss=1.39390290: 976it [23:17,  1.48s/it]2017-06-01 18:32:48,844 root  INFO     step 976.000000 - time: 1.416162, loss: 1.367954, perplexity: 3.927305, precision: 0.000000, batch_len: 144.000000
Train, loss=1.36795354: 977it [23:18,  1.48s/it]2017-06-01 18:32:50,200 root  INFO     step 977.000000 - time: 1.330633, loss: 1.783105, perplexity: 5.948298, precision: 0.000000, batch_len: 141.000000
Train, loss=1.78310513: 978it [23:20,  1.45s/it]2017-06-01 18:32:51,782 root  INFO     step 978.000000 - time: 1.570332, loss: 1.342361, perplexity: 3.828072, precision: 0.062500, batch_len: 137.000000
Train, loss=1.34236121: 979it [23:21,  1.49s/it]2017-06-01 18:32:53,138 root  INFO     step 979.000000 - time: 1.170918, loss: 1.187574, perplexity: 3.279117, precision: 0.078125, batch_len: 78.000000
Train, loss=1.18757415: 980it [23:23,  1.45s/it]2017-06-01 18:32:54,409 root  INFO     step 980.000000 - time: 1.089638, loss: 1.048130, perplexity: 2.852313, precision: 0.156250, batch_len: 96.000000
Train, loss=1.04813015: 981it [23:24,  1.39s/it]2017-06-01 18:32:55,873 root  INFO     step 981.000000 - time: 1.432223, loss: 1.033143, perplexity: 2.809882, precision: 0.046875, batch_len: 139.000000
Train, loss=1.03314257: 982it [23:25,  1.42s/it]2017-06-01 18:32:57,327 root  INFO     step 982.000000 - time: 1.368021, loss: 1.053221, perplexity: 2.866872, precision: 0.015625, batch_len: 138.000000
Train, loss=1.05322146: 983it [23:27,  1.43s/it]2017-06-01 18:32:59,173 root  INFO     step 983.000000 - time: 1.668150, loss: 1.069788, perplexity: 2.914761, precision: 0.046875, batch_len: 134.000000
Train, loss=1.06978798: 984it [23:29,  1.55s/it]2017-06-01 18:33:00,224 root  INFO     step 984.000000 - time: 0.948638, loss: 1.470868, perplexity: 4.353011, precision: 0.015625, batch_len: 77.000000
Train, loss=1.47086775: 985it [23:30,  1.40s/it]2017-06-01 18:33:01,722 root  INFO     step 985.000000 - time: 1.404558, loss: 2.433401, perplexity: 11.397575, precision: 0.000000, batch_len: 142.000000
Train, loss=2.43340063: 986it [23:31,  1.43s/it]2017-06-01 18:33:03,424 root  INFO     step 986.000000 - time: 1.578257, loss: 1.349379, perplexity: 3.855032, precision: 0.000000, batch_len: 150.000000
Train, loss=1.34937930: 987it [23:33,  1.51s/it]2017-06-01 18:33:04,993 root  INFO     step 987.000000 - time: 1.521772, loss: 1.167812, perplexity: 3.214952, precision: 0.015625, batch_len: 132.000000
Train, loss=1.16781235: 988it [23:34,  1.53s/it]2017-06-01 18:33:07,097 root  INFO     step 988.000000 - time: 2.045135, loss: 1.249827, perplexity: 3.489739, precision: 0.015625, batch_len: 152.000000
Train, loss=1.24982691: 989it [23:36,  1.70s/it]2017-06-01 18:33:08,057 root  INFO     step 989.000000 - time: 0.930771, loss: 0.972348, perplexity: 2.644145, precision: 0.078125, batch_len: 74.000000
Train, loss=0.97234768: 990it [23:37,  1.48s/it]2017-06-01 18:33:09,359 root  INFO     step 990.000000 - time: 1.156457, loss: 0.825416, perplexity: 2.282831, precision: 0.234375, batch_len: 72.000000
Train, loss=0.82541621: 991it [23:39,  1.43s/it]2017-06-01 18:33:10,276 root  INFO     step 991.000000 - time: 0.861010, loss: 1.055650, perplexity: 2.873841, precision: 0.265625, batch_len: 76.000000
Train, loss=1.05564952: 992it [23:40,  1.27s/it]2017-06-01 18:33:11,951 root  INFO     step 992.000000 - time: 1.632580, loss: 1.052701, perplexity: 2.865381, precision: 0.031250, batch_len: 131.000000
Train, loss=1.05270123: 993it [23:41,  1.39s/it]2017-06-01 18:33:13,119 root  INFO     step 993.000000 - time: 1.103611, loss: 0.873773, perplexity: 2.395933, precision: 0.234375, batch_len: 71.000000
Train, loss=0.87377262: 994it [23:42,  1.33s/it]2017-06-01 18:33:13,345 root  INFO     Generating first batch)
2017-06-01 18:33:17,235 root  INFO     step 994.000000 - time: 1.359470, loss: 0.893295, perplexity: 2.443166, precision: 0.140625, batch_len: 96.000000
Train, loss=0.89329475: 995it [23:47,  2.16s/it]2017-06-01 18:33:18,567 root  INFO     step 995.000000 - time: 1.070699, loss: 1.048119, perplexity: 2.852281, precision: 0.109375, batch_len: 110.000000
Train, loss=1.04811895: 996it [23:48,  1.91s/it]2017-06-01 18:33:19,956 root  INFO     step 996.000000 - time: 1.082080, loss: 1.018505, perplexity: 2.769053, precision: 0.078125, batch_len: 113.000000
Train, loss=1.01850522: 997it [23:49,  1.76s/it]2017-06-01 18:33:21,082 root  INFO     step 997.000000 - time: 1.044328, loss: 1.053345, perplexity: 2.867227, precision: 0.093750, batch_len: 108.000000
Train, loss=1.05334520: 998it [23:50,  1.57s/it]2017-06-01 18:33:22,126 root  INFO     step 998.000000 - time: 1.010453, loss: 1.114918, perplexity: 3.049318, precision: 0.046875, batch_len: 88.000000
Train, loss=1.11491799: 999it [23:52,  1.41s/it]2017-06-01 18:33:23,415 root  INFO     step 999.000000 - time: 1.235418, loss: 1.201857, perplexity: 3.326287, precision: 0.109375, batch_len: 92.000000
Train, loss=1.20185673: 1000it [23:53,  1.37s/it]2017-06-01 18:33:24,852 root  INFO     step 1000.000000 - time: 1.415339, loss: 1.100241, perplexity: 3.004889, precision: 0.031250, batch_len: 128.000000
Train, loss=1.10024059: 1001it [23:54,  1.39s/it]2017-06-01 18:33:25,975 root  INFO     step 1001.000000 - time: 1.061291, loss: 1.076528, perplexity: 2.934474, precision: 0.062500, batch_len: 91.000000
Train, loss=1.07652831: 1002it [23:55,  1.31s/it]2017-06-01 18:33:27,088 root  INFO     step 1002.000000 - time: 1.108550, loss: 1.156731, perplexity: 3.179521, precision: 0.046875, batch_len: 90.000000
Train, loss=1.15673065: 1003it [23:56,  1.25s/it]2017-06-01 18:33:28,142 root  INFO     step 1003.000000 - time: 1.026995, loss: 1.164289, perplexity: 3.203645, precision: 0.015625, batch_len: 105.000000
Train, loss=1.16428912: 1004it [23:58,  1.19s/it]2017-06-01 18:33:29,541 root  INFO     step 1004.000000 - time: 1.289144, loss: 1.640661, perplexity: 5.158576, precision: 0.031250, batch_len: 93.000000
Train, loss=1.64066052: 1005it [23:59,  1.25s/it]2017-06-01 18:33:30,781 root  INFO     step 1005.000000 - time: 1.232213, loss: 1.247911, perplexity: 3.483061, precision: 0.046875, batch_len: 120.000000
Train, loss=1.24791145: 1006it [24:00,  1.25s/it]2017-06-01 18:33:32,002 root  INFO     step 1006.000000 - time: 1.213359, loss: 1.327888, perplexity: 3.773067, precision: 0.046875, batch_len: 79.000000
Train, loss=1.32788813: 1007it [24:01,  1.24s/it]2017-06-01 18:33:33,217 root  INFO     step 1007.000000 - time: 1.178623, loss: 1.143560, perplexity: 3.137919, precision: 0.093750, batch_len: 104.000000
Train, loss=1.14355969: 1008it [24:03,  1.23s/it]2017-06-01 18:33:34,605 root  INFO     step 1008.000000 - time: 1.284730, loss: 1.031526, perplexity: 2.805344, precision: 0.109375, batch_len: 124.000000
Train, loss=1.03152633: 1009it [24:04,  1.28s/it]2017-06-01 18:33:35,763 root  INFO     step 1009.000000 - time: 1.119661, loss: 1.210324, perplexity: 3.354573, precision: 0.015625, batch_len: 114.000000
Train, loss=1.21032441: 1010it [24:05,  1.24s/it]2017-06-01 18:33:36,812 root  INFO     step 1010.000000 - time: 0.990232, loss: 1.033581, perplexity: 2.811115, precision: 0.062500, batch_len: 97.000000
Train, loss=1.03358138: 1011it [24:06,  1.18s/it]2017-06-01 18:33:38,029 root  INFO     step 1011.000000 - time: 1.202673, loss: 1.058283, perplexity: 2.881420, precision: 0.078125, batch_len: 107.000000
Train, loss=1.05828309: 1012it [24:07,  1.19s/it]2017-06-01 18:33:39,527 root  INFO     step 1012.000000 - time: 1.461195, loss: 1.166145, perplexity: 3.209596, precision: 0.078125, batch_len: 112.000000
Train, loss=1.16614509: 1013it [24:09,  1.29s/it]2017-06-01 18:33:40,547 root  INFO     step 1013.000000 - time: 1.008876, loss: 1.534644, perplexity: 4.639673, precision: 0.046875, batch_len: 84.000000
Train, loss=1.53464389: 1014it [24:10,  1.21s/it]2017-06-01 18:33:41,679 root  INFO     step 1014.000000 - time: 1.080642, loss: 1.609533, perplexity: 5.000478, precision: 0.062500, batch_len: 109.000000
Train, loss=1.60953343: 1015it [24:11,  1.18s/it]2017-06-01 18:33:42,839 root  INFO     step 1015.000000 - time: 1.070208, loss: 1.261658, perplexity: 3.531273, precision: 0.062500, batch_len: 111.000000
Train, loss=1.26165843: 1016it [24:12,  1.18s/it]2017-06-01 18:33:43,905 root  INFO     step 1016.000000 - time: 0.984963, loss: 1.068735, perplexity: 2.911694, precision: 0.093750, batch_len: 115.000000
Train, loss=1.06873512: 1017it [24:13,  1.14s/it]2017-06-01 18:33:45,082 root  INFO     step 1017.000000 - time: 1.139734, loss: 1.062138, perplexity: 2.892548, precision: 0.109375, batch_len: 100.000000
Train, loss=1.06213760: 1018it [24:14,  1.15s/it]2017-06-01 18:33:46,240 root  INFO     step 1018.000000 - time: 1.142378, loss: 1.068935, perplexity: 2.912276, precision: 0.125000, batch_len: 81.000000
Train, loss=1.06893492: 1019it [24:16,  1.15s/it]2017-06-01 18:33:47,302 root  INFO     step 1019.000000 - time: 1.037153, loss: 0.938129, perplexity: 2.555196, precision: 0.093750, batch_len: 101.000000
Train, loss=0.93812895: 1020it [24:17,  1.13s/it]2017-06-01 18:33:48,351 root  INFO     step 1020.000000 - time: 1.032003, loss: 1.008519, perplexity: 2.741538, precision: 0.203125, batch_len: 89.000000
Train, loss=1.00851905: 1021it [24:18,  1.10s/it]2017-06-01 18:33:49,808 root  INFO     step 1021.000000 - time: 1.344552, loss: 1.039727, perplexity: 2.828446, precision: 0.093750, batch_len: 121.000000
Train, loss=1.03972733: 1022it [24:19,  1.21s/it]2017-06-01 18:33:50,880 root  INFO     step 1022.000000 - time: 1.045990, loss: 1.146918, perplexity: 3.148473, precision: 0.046875, batch_len: 103.000000
Train, loss=1.14691758: 1023it [24:20,  1.17s/it]2017-06-01 18:33:52,226 root  INFO     step 1023.000000 - time: 1.312461, loss: 1.523309, perplexity: 4.587379, precision: 0.031250, batch_len: 106.000000
Train, loss=1.52330875: 1024it [24:22,  1.22s/it]2017-06-01 18:33:53,714 root  INFO     step 1024.000000 - time: 1.474215, loss: 1.095423, perplexity: 2.990447, precision: 0.046875, batch_len: 117.000000
Train, loss=1.09542286: 1025it [24:23,  1.30s/it]2017-06-01 18:33:55,135 root  INFO     step 1025.000000 - time: 1.363019, loss: 1.429064, perplexity: 4.174790, precision: 0.031250, batch_len: 116.000000
Train, loss=1.42906404: 1026it [24:25,  1.34s/it]2017-06-01 18:33:56,184 root  INFO     step 1026.000000 - time: 0.995262, loss: 1.397656, perplexity: 4.045705, precision: 0.078125, batch_len: 85.000000
Train, loss=1.39765573: 1027it [24:26,  1.25s/it]2017-06-01 18:33:57,211 root  INFO     step 1027.000000 - time: 0.899066, loss: 1.338695, perplexity: 3.814063, precision: 0.000000, batch_len: 94.000000
Train, loss=1.33869505: 1028it [24:27,  1.18s/it]2017-06-01 18:33:58,405 root  INFO     step 1028.000000 - time: 1.188519, loss: 1.067165, perplexity: 2.907128, precision: 0.031250, batch_len: 102.000000
Train, loss=1.06716549: 1029it [24:28,  1.19s/it]2017-06-01 18:33:59,930 root  INFO     step 1029.000000 - time: 1.462730, loss: 1.028389, perplexity: 2.796558, precision: 0.093750, batch_len: 125.000000
Train, loss=1.02838922: 1030it [24:29,  1.29s/it]2017-06-01 18:34:01,004 root  INFO     step 1030.000000 - time: 0.951583, loss: 1.086281, perplexity: 2.963232, precision: 0.078125, batch_len: 86.000000
Train, loss=1.08628058: 1031it [24:30,  1.22s/it]2017-06-01 18:34:02,566 root  INFO     step 1031.000000 - time: 1.379129, loss: 1.393823, perplexity: 4.030227, precision: 0.000000, batch_len: 126.000000
Train, loss=1.39382267: 1032it [24:32,  1.33s/it]2017-06-01 18:34:03,636 root  INFO     step 1032.000000 - time: 0.988125, loss: 1.255832, perplexity: 3.510758, precision: 0.015625, batch_len: 98.000000
Train, loss=1.25583196: 1033it [24:33,  1.25s/it]2017-06-01 18:34:05,180 root  INFO     step 1033.000000 - time: 1.434840, loss: 1.259000, perplexity: 3.521896, precision: 0.015625, batch_len: 135.000000
Train, loss=1.25899959: 1034it [24:35,  1.34s/it]2017-06-01 18:34:06,444 root  INFO     step 1034.000000 - time: 1.089273, loss: 1.138777, perplexity: 3.122946, precision: 0.062500, batch_len: 83.000000
Train, loss=1.13877678: 1035it [24:36,  1.32s/it]2017-06-01 18:34:07,457 root  INFO     step 1035.000000 - time: 0.955857, loss: 0.951176, perplexity: 2.588751, precision: 0.234375, batch_len: 80.000000
Train, loss=0.95117563: 1036it [24:37,  1.22s/it]2017-06-01 18:34:08,684 root  INFO     step 1036.000000 - time: 1.102744, loss: 1.096548, perplexity: 2.993815, precision: 0.062500, batch_len: 99.000000
Train, loss=1.09654844: 1037it [24:38,  1.23s/it]2017-06-01 18:34:09,950 root  INFO     step 1037.000000 - time: 1.210794, loss: 1.106868, perplexity: 3.024869, precision: 0.015625, batch_len: 118.000000
Train, loss=1.10686779: 1038it [24:39,  1.24s/it]2017-06-01 18:34:11,594 root  INFO     step 1038.000000 - time: 1.559252, loss: 1.200907, perplexity: 3.323129, precision: 0.046875, batch_len: 119.000000
Train, loss=1.20090675: 1039it [24:41,  1.36s/it]2017-06-01 18:34:12,814 root  INFO     step 1039.000000 - time: 1.139334, loss: 1.155007, perplexity: 3.174047, precision: 0.062500, batch_len: 87.000000
Train, loss=1.15500736: 1040it [24:42,  1.32s/it]2017-06-01 18:34:14,232 root  INFO     step 1040.000000 - time: 1.400831, loss: 1.172424, perplexity: 3.229811, precision: 0.015625, batch_len: 123.000000
Train, loss=1.17242372: 1041it [24:44,  1.35s/it]2017-06-01 18:34:15,618 root  INFO     step 1041.000000 - time: 1.354562, loss: 1.255171, perplexity: 3.508438, precision: 0.015625, batch_len: 129.000000
Train, loss=1.25517106: 1042it [24:45,  1.36s/it]2017-06-01 18:34:16,933 root  INFO     step 1042.000000 - time: 1.303929, loss: 1.317043, perplexity: 3.732368, precision: 0.015625, batch_len: 137.000000
Train, loss=1.31704295: 1043it [24:46,  1.35s/it]2017-06-01 18:34:18,177 root  INFO     step 1043.000000 - time: 1.068844, loss: 1.059403, perplexity: 2.884649, precision: 0.062500, batch_len: 82.000000
Train, loss=1.05940330: 1044it [24:48,  1.32s/it]2017-06-01 18:34:19,896 root  INFO     step 1044.000000 - time: 1.610097, loss: 1.030310, perplexity: 2.801933, precision: 0.031250, batch_len: 136.000000
Train, loss=1.03030968: 1045it [24:49,  1.44s/it]2017-06-01 18:34:21,279 root  INFO     step 1045.000000 - time: 1.306596, loss: 0.991519, perplexity: 2.695327, precision: 0.046875, batch_len: 133.000000
Train, loss=0.99151945: 1046it [24:51,  1.42s/it]2017-06-01 18:34:22,827 root  INFO     step 1046.000000 - time: 1.415333, loss: 1.103609, perplexity: 3.015028, precision: 0.078125, batch_len: 130.000000
Train, loss=1.10360897: 1047it [24:52,  1.46s/it]2017-06-01 18:34:24,418 root  INFO     step 1047.000000 - time: 1.568503, loss: 1.003245, perplexity: 2.727118, precision: 0.031250, batch_len: 144.000000
Train, loss=1.00324535: 1048it [24:54,  1.50s/it]2017-06-01 18:34:26,203 root  INFO     step 1048.000000 - time: 1.612785, loss: 1.185419, perplexity: 3.272056, precision: 0.015625, batch_len: 122.000000
Train, loss=1.18541861: 1049it [24:56,  1.58s/it]2017-06-01 18:34:27,494 root  INFO     step 1049.000000 - time: 1.091270, loss: 1.177587, perplexity: 3.246532, precision: 0.140625, batch_len: 78.000000
Train, loss=1.17758727: 1050it [24:57,  1.50s/it]2017-06-01 18:34:28,780 root  INFO     step 1050.000000 - time: 1.154605, loss: 0.982668, perplexity: 2.671573, precision: 0.171875, batch_len: 96.000000
Train, loss=0.98266751: 1051it [24:58,  1.43s/it]2017-06-01 18:34:30,291 root  INFO     step 1051.000000 - time: 1.383655, loss: 1.315704, perplexity: 3.727375, precision: 0.015625, batch_len: 134.000000
Train, loss=1.31570411: 1052it [25:00,  1.46s/it]2017-06-01 18:34:31,752 root  INFO     step 1052.000000 - time: 1.206179, loss: 1.055671, perplexity: 2.873903, precision: 0.015625, batch_len: 72.000000
Train, loss=1.05567110: 1053it [25:01,  1.46s/it]2017-06-01 18:34:33,991 root  INFO     step 1053.000000 - time: 2.229116, loss: 1.684472, perplexity: 5.389605, precision: 0.000000, batch_len: 150.000000
Train, loss=1.68447208: 1054it [25:03,  1.69s/it]2017-06-01 18:34:35,486 root  INFO     step 1054.000000 - time: 1.438304, loss: 1.573435, perplexity: 4.823187, precision: 0.000000, batch_len: 138.000000
Train, loss=1.57343483: 1055it [25:05,  1.63s/it]2017-06-01 18:34:36,979 root  INFO     step 1055.000000 - time: 1.432933, loss: 1.444477, perplexity: 4.239633, precision: 0.015625, batch_len: 141.000000
Train, loss=1.44447660: 1056it [25:06,  1.59s/it]2017-06-01 18:34:38,857 root  INFO     step 1056.000000 - time: 1.752257, loss: 1.190367, perplexity: 3.288288, precision: 0.000000, batch_len: 152.000000
Train, loss=1.19036698: 1057it [25:08,  1.68s/it]2017-06-01 18:34:40,688 root  INFO     step 1057.000000 - time: 1.753596, loss: 1.094607, perplexity: 2.988009, precision: 0.000000, batch_len: 139.000000
Train, loss=1.09460735: 1058it [25:10,  1.72s/it]2017-06-01 18:34:41,678 root  INFO     step 1058.000000 - time: 0.957583, loss: 0.810724, perplexity: 2.249536, precision: 0.187500, batch_len: 74.000000
Train, loss=0.81072414: 1059it [25:11,  1.50s/it]2017-06-01 18:34:43,047 root  INFO     step 1059.000000 - time: 1.325687, loss: 1.113611, perplexity: 3.045334, precision: 0.015625, batch_len: 132.000000
Train, loss=1.11361063: 1060it [25:12,  1.46s/it]2017-06-01 18:34:43,996 root  INFO     step 1060.000000 - time: 0.937661, loss: 0.929380, perplexity: 2.532938, precision: 0.218750, batch_len: 77.000000
Train, loss=0.92937982: 1061it [25:13,  1.31s/it]2017-06-01 18:34:44,920 root  INFO     step 1061.000000 - time: 0.818183, loss: 0.812748, perplexity: 2.254093, precision: 0.281250, batch_len: 71.000000
Train, loss=0.81274784: 1062it [25:14,  1.19s/it]2017-06-01 18:34:45,968 root  INFO     step 1062.000000 - time: 1.027202, loss: 0.893774, perplexity: 2.444336, precision: 0.265625, batch_len: 76.000000
Train, loss=0.89377356: 1063it [25:15,  1.15s/it]2017-06-01 18:34:47,668 root  INFO     step 1063.000000 - time: 1.684994, loss: 1.158395, perplexity: 3.184818, precision: 0.015625, batch_len: 142.000000
Train, loss=1.15839517: 1064it [25:17,  1.31s/it]2017-06-01 18:34:49,420 root  INFO     step 1064.000000 - time: 1.392089, loss: 0.926051, perplexity: 2.524521, precision: 0.046875, batch_len: 131.000000
Train, loss=0.92605120: 1065it [25:19,  1.45s/it]2017-06-01 18:34:49,511 root  INFO     Generating first batch)
2017-06-01 18:34:53,317 root  INFO     step 1065.000000 - time: 1.007644, loss: 0.849589, perplexity: 2.338685, precision: 0.203125, batch_len: 101.000000
Train, loss=0.84958875: 1066it [25:23,  2.18s/it]2017-06-01 18:34:54,484 root  INFO     step 1066.000000 - time: 0.999132, loss: 0.925961, perplexity: 2.524293, precision: 0.171875, batch_len: 92.000000
Train, loss=0.92596108: 1067it [25:24,  1.88s/it]2017-06-01 18:34:55,824 root  INFO     step 1067.000000 - time: 1.138075, loss: 0.990542, perplexity: 2.692694, precision: 0.093750, batch_len: 113.000000
Train, loss=0.99054205: 1068it [25:25,  1.72s/it]2017-06-01 18:34:57,247 root  INFO     step 1068.000000 - time: 1.130636, loss: 0.768790, perplexity: 2.157154, precision: 0.093750, batch_len: 96.000000
Train, loss=0.76878971: 1069it [25:27,  1.63s/it]2017-06-01 18:34:58,717 root  INFO     step 1069.000000 - time: 1.361434, loss: 1.056494, perplexity: 2.876270, precision: 0.125000, batch_len: 110.000000
Train, loss=1.05649447: 1070it [25:28,  1.58s/it]2017-06-01 18:34:59,843 root  INFO     step 1070.000000 - time: 1.027681, loss: 0.901388, perplexity: 2.463018, precision: 0.156250, batch_len: 105.000000
Train, loss=0.90138751: 1071it [25:29,  1.44s/it]2017-06-01 18:35:01,057 root  INFO     step 1071.000000 - time: 1.075578, loss: 0.921040, perplexity: 2.511901, precision: 0.140625, batch_len: 102.000000
Train, loss=0.92103970: 1072it [25:30,  1.37s/it]2017-06-01 18:35:02,332 root  INFO     step 1072.000000 - time: 1.137760, loss: 1.270625, perplexity: 3.563080, precision: 0.125000, batch_len: 97.000000
Train, loss=1.27062523: 1073it [25:32,  1.35s/it]2017-06-01 18:35:03,320 root  INFO     step 1073.000000 - time: 0.965289, loss: 1.709384, perplexity: 5.525559, precision: 0.046875, batch_len: 104.000000
Train, loss=1.70938444: 1074it [25:33,  1.24s/it]2017-06-01 18:35:04,496 root  INFO     step 1074.000000 - time: 1.151436, loss: 1.176698, perplexity: 3.243645, precision: 0.031250, batch_len: 100.000000
Train, loss=1.17669761: 1075it [25:34,  1.22s/it]2017-06-01 18:35:05,744 root  INFO     step 1075.000000 - time: 1.229676, loss: 1.008606, perplexity: 2.741776, precision: 0.140625, batch_len: 111.000000
Train, loss=1.00860572: 1076it [25:35,  1.23s/it]2017-06-01 18:35:07,087 root  INFO     step 1076.000000 - time: 1.261559, loss: 1.167856, perplexity: 3.215093, precision: 0.031250, batch_len: 128.000000
Train, loss=1.16785622: 1077it [25:36,  1.26s/it]2017-06-01 18:35:08,151 root  INFO     step 1077.000000 - time: 1.038410, loss: 1.241360, perplexity: 3.460318, precision: 0.031250, batch_len: 88.000000
Train, loss=1.24136043: 1078it [25:38,  1.20s/it]2017-06-01 18:35:09,456 root  INFO     step 1078.000000 - time: 1.298261, loss: 1.054661, perplexity: 2.871001, precision: 0.078125, batch_len: 120.000000
Train, loss=1.05466092: 1079it [25:39,  1.23s/it]2017-06-01 18:35:10,673 root  INFO     step 1079.000000 - time: 1.146733, loss: 1.336441, perplexity: 3.805474, precision: 0.015625, batch_len: 93.000000
Train, loss=1.33644056: 1080it [25:40,  1.23s/it]2017-06-01 18:35:12,653 root  INFO     step 1080.000000 - time: 1.887005, loss: 1.110310, perplexity: 3.035299, precision: 0.125000, batch_len: 90.000000
Train, loss=1.11030984: 1081it [25:42,  1.45s/it]2017-06-01 18:35:13,741 root  INFO     step 1081.000000 - time: 1.065537, loss: 1.050668, perplexity: 2.859559, precision: 0.125000, batch_len: 108.000000
Train, loss=1.05066752: 1082it [25:43,  1.34s/it]2017-06-01 18:35:15,101 root  INFO     step 1082.000000 - time: 1.300536, loss: 0.949347, perplexity: 2.584022, precision: 0.171875, batch_len: 117.000000
Train, loss=0.94934714: 1083it [25:44,  1.35s/it]2017-06-01 18:35:16,132 root  INFO     step 1083.000000 - time: 0.994670, loss: 0.882408, perplexity: 2.416711, precision: 0.218750, batch_len: 103.000000
Train, loss=0.88240767: 1084it [25:46,  1.25s/it]2017-06-01 18:35:17,660 root  INFO     step 1084.000000 - time: 1.282817, loss: 0.795607, perplexity: 2.215785, precision: 0.171875, batch_len: 106.000000
Train, loss=0.79560685: 1085it [25:47,  1.34s/it]2017-06-01 18:35:18,959 root  INFO     step 1085.000000 - time: 1.230644, loss: 0.848498, perplexity: 2.336136, precision: 0.234375, batch_len: 91.000000
Train, loss=0.84849811: 1086it [25:48,  1.32s/it]2017-06-01 18:35:20,314 root  INFO     step 1086.000000 - time: 1.326919, loss: 0.833741, perplexity: 2.301915, precision: 0.156250, batch_len: 124.000000
Train, loss=0.83374131: 1087it [25:50,  1.33s/it]2017-06-01 18:35:21,489 root  INFO     step 1087.000000 - time: 1.112804, loss: 1.005450, perplexity: 2.733136, precision: 0.046875, batch_len: 114.000000
Train, loss=1.00544965: 1088it [25:51,  1.29s/it]2017-06-01 18:35:22,584 root  INFO     step 1088.000000 - time: 1.087140, loss: 0.838635, perplexity: 2.313207, precision: 0.156250, batch_len: 112.000000
Train, loss=0.83863485: 1089it [25:52,  1.23s/it]2017-06-01 18:35:23,777 root  INFO     step 1089.000000 - time: 1.109803, loss: 0.915474, perplexity: 2.497958, precision: 0.078125, batch_len: 109.000000
Train, loss=0.91547358: 1090it [25:53,  1.22s/it]2017-06-01 18:35:24,911 root  INFO     step 1090.000000 - time: 1.114161, loss: 0.966309, perplexity: 2.628225, precision: 0.203125, batch_len: 86.000000
Train, loss=0.96630883: 1091it [25:54,  1.19s/it]2017-06-01 18:35:26,538 root  INFO     step 1091.000000 - time: 1.462195, loss: 1.054139, perplexity: 2.869503, precision: 0.171875, batch_len: 125.000000
Train, loss=1.05413902: 1092it [25:56,  1.32s/it]2017-06-01 18:35:27,577 root  INFO     step 1092.000000 - time: 1.034147, loss: 0.830902, perplexity: 2.295388, precision: 0.093750, batch_len: 87.000000
Train, loss=0.83090174: 1093it [25:57,  1.24s/it]2017-06-01 18:35:28,802 root  INFO     step 1093.000000 - time: 1.145630, loss: 0.935013, perplexity: 2.547246, precision: 0.187500, batch_len: 115.000000
Train, loss=0.93501270: 1094it [25:58,  1.23s/it]2017-06-01 18:35:29,853 root  INFO     step 1094.000000 - time: 1.033879, loss: 0.964692, perplexity: 2.623980, precision: 0.234375, batch_len: 89.000000
Train, loss=0.96469235: 1095it [25:59,  1.18s/it]2017-06-01 18:35:31,169 root  INFO     step 1095.000000 - time: 1.220229, loss: 1.018264, perplexity: 2.768385, precision: 0.187500, batch_len: 94.000000
Train, loss=1.01826406: 1096it [26:01,  1.22s/it]2017-06-01 18:35:32,254 root  INFO     step 1096.000000 - time: 1.037493, loss: 1.135623, perplexity: 3.113113, precision: 0.109375, batch_len: 107.000000
Train, loss=1.13562322: 1097it [26:02,  1.18s/it]2017-06-01 18:35:33,939 root  INFO     step 1097.000000 - time: 1.659628, loss: 1.543680, perplexity: 4.681790, precision: 0.031250, batch_len: 119.000000
Train, loss=1.54368043: 1098it [26:03,  1.33s/it]2017-06-01 18:35:35,325 root  INFO     step 1098.000000 - time: 1.363772, loss: 1.463461, perplexity: 4.320889, precision: 0.031250, batch_len: 123.000000
Train, loss=1.46346128: 1099it [26:05,  1.35s/it]2017-06-01 18:35:36,300 root  INFO     step 1099.000000 - time: 0.944181, loss: 1.455282, perplexity: 4.285694, precision: 0.062500, batch_len: 81.000000
Train, loss=1.45528245: 1100it [26:06,  1.24s/it]2017-06-01 18:35:37,708 root  INFO     step 1100.000000 - time: 1.347954, loss: 1.209233, perplexity: 3.350912, precision: 0.031250, batch_len: 137.000000
Train, loss=1.20923257: 1101it [26:07,  1.29s/it]2017-06-01 18:35:38,577 root  INFO     step 1101.000000 - time: 0.837270, loss: 1.092057, perplexity: 2.980398, precision: 0.078125, batch_len: 80.000000
Train, loss=1.09205699: 1102it [26:08,  1.16s/it]2017-06-01 18:35:39,799 root  INFO     step 1102.000000 - time: 1.118900, loss: 1.094402, perplexity: 2.987394, precision: 0.140625, batch_len: 83.000000
Train, loss=1.09440160: 1103it [26:09,  1.18s/it]2017-06-01 18:35:41,130 root  INFO     step 1103.000000 - time: 1.198911, loss: 1.089473, perplexity: 2.972708, precision: 0.109375, batch_len: 85.000000
Train, loss=1.08947325: 1104it [26:11,  1.23s/it]2017-06-01 18:35:42,528 root  INFO     step 1104.000000 - time: 1.374391, loss: 1.013846, perplexity: 2.756182, precision: 0.156250, batch_len: 121.000000
Train, loss=1.01384640: 1105it [26:12,  1.28s/it]2017-06-01 18:35:43,960 root  INFO     step 1105.000000 - time: 1.311152, loss: 1.008171, perplexity: 2.740583, precision: 0.078125, batch_len: 116.000000
Train, loss=1.00817084: 1106it [26:13,  1.32s/it]2017-06-01 18:35:45,318 root  INFO     step 1106.000000 - time: 1.229103, loss: 1.050013, perplexity: 2.857688, precision: 0.031250, batch_len: 118.000000
Train, loss=1.05001307: 1107it [26:15,  1.33s/it]2017-06-01 18:35:46,713 root  INFO     step 1107.000000 - time: 1.272665, loss: 1.235365, perplexity: 3.439634, precision: 0.000000, batch_len: 98.000000
Train, loss=1.23536515: 1108it [26:16,  1.35s/it]2017-06-01 18:35:48,237 root  INFO     step 1108.000000 - time: 1.444107, loss: 1.692737, perplexity: 5.434333, precision: 0.015625, batch_len: 133.000000
Train, loss=1.69273686: 1109it [26:18,  1.40s/it]2017-06-01 18:35:49,680 root  INFO     step 1109.000000 - time: 1.389871, loss: 1.257014, perplexity: 3.514910, precision: 0.046875, batch_len: 130.000000
Train, loss=1.25701380: 1110it [26:19,  1.42s/it]2017-06-01 18:35:50,751 root  INFO     step 1110.000000 - time: 1.059620, loss: 1.092311, perplexity: 2.981156, precision: 0.062500, batch_len: 99.000000
Train, loss=1.09231114: 1111it [26:20,  1.31s/it]2017-06-01 18:35:51,755 root  INFO     step 1111.000000 - time: 0.885950, loss: 1.300056, perplexity: 3.669503, precision: 0.078125, batch_len: 79.000000
Train, loss=1.30005622: 1112it [26:21,  1.22s/it]2017-06-01 18:35:52,935 root  INFO     step 1112.000000 - time: 1.161082, loss: 0.983227, perplexity: 2.673068, precision: 0.140625, batch_len: 84.000000
Train, loss=0.98322701: 1113it [26:22,  1.21s/it]2017-06-01 18:35:54,565 root  INFO     step 1113.000000 - time: 1.490331, loss: 0.959237, perplexity: 2.609703, precision: 0.015625, batch_len: 129.000000
Train, loss=0.95923650: 1114it [26:24,  1.33s/it]2017-06-01 18:35:56,183 root  INFO     step 1114.000000 - time: 1.449346, loss: 1.006540, perplexity: 2.736117, precision: 0.000000, batch_len: 144.000000
Train, loss=1.00653958: 1115it [26:26,  1.42s/it]2017-06-01 18:35:57,654 root  INFO     step 1115.000000 - time: 1.307121, loss: 0.992695, perplexity: 2.698496, precision: 0.140625, batch_len: 122.000000
Train, loss=0.99269462: 1116it [26:27,  1.43s/it]2017-06-01 18:35:58,719 root  INFO     step 1116.000000 - time: 1.028341, loss: 0.990627, perplexity: 2.692922, precision: 0.125000, batch_len: 78.000000
Train, loss=0.99062681: 1117it [26:28,  1.32s/it]2017-06-01 18:36:00,611 root  INFO     step 1117.000000 - time: 1.787447, loss: 1.121768, perplexity: 3.070279, precision: 0.015625, batch_len: 138.000000
Train, loss=1.12176847: 1118it [26:30,  1.49s/it]2017-06-01 18:36:01,580 root  INFO     step 1118.000000 - time: 0.944067, loss: 1.121084, perplexity: 3.068178, precision: 0.125000, batch_len: 82.000000
Train, loss=1.12108397: 1119it [26:31,  1.34s/it]2017-06-01 18:36:02,903 root  INFO     step 1119.000000 - time: 1.312383, loss: 0.989083, perplexity: 2.688769, precision: 0.031250, batch_len: 135.000000
Train, loss=0.98908329: 1120it [26:32,  1.33s/it]2017-06-01 18:36:04,415 root  INFO     step 1120.000000 - time: 1.431338, loss: 0.927219, perplexity: 2.527471, precision: 0.031250, batch_len: 136.000000
Train, loss=0.92721903: 1121it [26:34,  1.39s/it]2017-06-01 18:36:05,944 root  INFO     step 1121.000000 - time: 1.458655, loss: 1.099048, perplexity: 3.001309, precision: 0.046875, batch_len: 126.000000
Train, loss=1.09904850: 1122it [26:35,  1.43s/it]2017-06-01 18:36:07,441 root  INFO     step 1122.000000 - time: 1.249803, loss: 1.308289, perplexity: 3.699839, precision: 0.015625, batch_len: 96.000000
Train, loss=1.30828929: 1123it [26:37,  1.45s/it]2017-06-01 18:36:08,588 root  INFO     step 1123.000000 - time: 0.967680, loss: 0.959929, perplexity: 2.611511, precision: 0.218750, batch_len: 74.000000
Train, loss=0.95992917: 1124it [26:38,  1.36s/it]2017-06-01 18:36:10,163 root  INFO     step 1124.000000 - time: 1.417796, loss: 0.859290, perplexity: 2.361484, precision: 0.078125, batch_len: 139.000000
Train, loss=0.85929018: 1125it [26:40,  1.42s/it]2017-06-01 18:36:11,178 root  INFO     step 1125.000000 - time: 0.882764, loss: 0.762289, perplexity: 2.143176, precision: 0.296875, batch_len: 77.000000
Train, loss=0.76228893: 1126it [26:41,  1.30s/it]2017-06-01 18:36:12,686 root  INFO     step 1126.000000 - time: 1.467441, loss: 0.950264, perplexity: 2.586391, precision: 0.031250, batch_len: 134.000000
Train, loss=0.95026356: 1127it [26:42,  1.36s/it]2017-06-01 18:36:14,349 root  INFO     step 1127.000000 - time: 1.655316, loss: 1.257912, perplexity: 3.518069, precision: 0.031250, batch_len: 141.000000
Train, loss=1.25791216: 1128it [26:44,  1.45s/it]2017-06-01 18:36:15,682 root  INFO     step 1128.000000 - time: 1.321814, loss: 1.416346, perplexity: 4.122030, precision: 0.031250, batch_len: 132.000000
Train, loss=1.41634583: 1129it [26:45,  1.42s/it]2017-06-01 18:36:17,585 root  INFO     step 1129.000000 - time: 1.849072, loss: 1.184356, perplexity: 3.268581, precision: 0.046875, batch_len: 152.000000
Train, loss=1.18435597: 1130it [26:47,  1.56s/it]2017-06-01 18:36:18,490 root  INFO     step 1130.000000 - time: 0.879563, loss: 0.715765, perplexity: 2.045752, precision: 0.265625, batch_len: 71.000000
Train, loss=0.71576548: 1131it [26:48,  1.37s/it]2017-06-01 18:36:19,694 root  INFO     step 1131.000000 - time: 1.087907, loss: 0.978846, perplexity: 2.661383, precision: 0.218750, batch_len: 76.000000
Train, loss=0.97884595: 1132it [26:49,  1.32s/it]2017-06-01 18:36:21,371 root  INFO     step 1132.000000 - time: 1.593316, loss: 1.013672, perplexity: 2.755700, precision: 0.078125, batch_len: 142.000000
Train, loss=1.01367164: 1133it [26:51,  1.43s/it]2017-06-01 18:36:22,688 root  INFO     step 1133.000000 - time: 1.156770, loss: 0.741123, perplexity: 2.098290, precision: 0.187500, batch_len: 72.000000
Train, loss=0.74112260: 1134it [26:52,  1.39s/it]2017-06-01 18:36:24,425 root  INFO     step 1134.000000 - time: 1.678839, loss: 1.174415, perplexity: 3.236250, precision: 0.015625, batch_len: 150.000000
Train, loss=1.17441535: 1135it [26:54,  1.50s/it]2017-06-01 18:36:26,112 root  INFO     step 1135.000000 - time: 1.468899, loss: 1.016169, perplexity: 2.762592, precision: 0.046875, batch_len: 131.000000
Train, loss=1.01616931: 1136it [26:55,  1.55s/it]2017-06-01 18:36:26,166 root  INFO     Generating first batch)
2017-06-01 18:36:29,971 root  INFO     step 1136.000000 - time: 1.106260, loss: 1.068548, perplexity: 2.911150, precision: 0.062500, batch_len: 96.000000
Train, loss=1.06854808: 1137it [26:59,  2.25s/it]2017-06-01 18:36:31,329 root  INFO     step 1137.000000 - time: 1.033117, loss: 0.930534, perplexity: 2.535864, precision: 0.046875, batch_len: 105.000000
Train, loss=0.93053448: 1138it [27:01,  1.98s/it]2017-06-01 18:36:32,938 root  INFO     step 1138.000000 - time: 1.544306, loss: 1.237252, perplexity: 3.446130, precision: 0.046875, batch_len: 112.000000
Train, loss=1.23725176: 1139it [27:02,  1.87s/it]2017-06-01 18:36:34,634 root  INFO     step 1139.000000 - time: 1.558197, loss: 1.029287, perplexity: 2.799068, precision: 0.062500, batch_len: 120.000000
Train, loss=1.02928650: 1140it [27:04,  1.82s/it]2017-06-01 18:36:35,816 root  INFO     step 1140.000000 - time: 1.069836, loss: 1.050910, perplexity: 2.860253, precision: 0.109375, batch_len: 110.000000
Train, loss=1.05091000: 1141it [27:05,  1.63s/it]2017-06-01 18:36:36,874 root  INFO     step 1141.000000 - time: 1.039549, loss: 1.078732, perplexity: 2.940949, precision: 0.093750, batch_len: 93.000000
Train, loss=1.07873225: 1142it [27:06,  1.46s/it]2017-06-01 18:36:37,985 root  INFO     step 1142.000000 - time: 0.990207, loss: 0.996058, perplexity: 2.707588, precision: 0.125000, batch_len: 108.000000
Train, loss=0.99605829: 1143it [27:07,  1.35s/it]2017-06-01 18:36:39,323 root  INFO     step 1143.000000 - time: 1.267361, loss: 1.130371, perplexity: 3.096807, precision: 0.140625, batch_len: 88.000000
Train, loss=1.13037145: 1144it [27:09,  1.35s/it]2017-06-01 18:36:40,545 root  INFO     step 1144.000000 - time: 1.193284, loss: 1.052366, perplexity: 2.864421, precision: 0.046875, batch_len: 101.000000
Train, loss=1.05236614: 1145it [27:10,  1.31s/it]2017-06-01 18:36:41,998 root  INFO     step 1145.000000 - time: 1.295422, loss: 1.029259, perplexity: 2.798991, precision: 0.078125, batch_len: 128.000000
Train, loss=1.02925909: 1146it [27:11,  1.35s/it]2017-06-01 18:36:43,279 root  INFO     step 1146.000000 - time: 1.065104, loss: 1.001013, perplexity: 2.721037, precision: 0.093750, batch_len: 102.000000
Train, loss=1.00101304: 1147it [27:13,  1.33s/it]2017-06-01 18:36:44,246 root  INFO     step 1147.000000 - time: 0.952502, loss: 0.905862, perplexity: 2.474064, precision: 0.187500, batch_len: 100.000000
Train, loss=0.90586203: 1148it [27:14,  1.22s/it]2017-06-01 18:36:45,525 root  INFO     step 1148.000000 - time: 1.225012, loss: 1.039213, perplexity: 2.826992, precision: 0.140625, batch_len: 111.000000
Train, loss=1.03921330: 1149it [27:15,  1.24s/it]2017-06-01 18:36:47,099 root  INFO     step 1149.000000 - time: 1.535724, loss: 0.843544, perplexity: 2.324591, precision: 0.140625, batch_len: 117.000000
Train, loss=0.84354424: 1150it [27:16,  1.34s/it]2017-06-01 18:36:48,223 root  INFO     step 1150.000000 - time: 1.088678, loss: 0.836265, perplexity: 2.307730, precision: 0.140625, batch_len: 109.000000
Train, loss=0.83626455: 1151it [27:18,  1.27s/it]2017-06-01 18:36:49,361 root  INFO     step 1151.000000 - time: 1.115387, loss: 1.029281, perplexity: 2.799053, precision: 0.062500, batch_len: 113.000000
Train, loss=1.02928102: 1152it [27:19,  1.23s/it]2017-06-01 18:36:50,509 root  INFO     step 1152.000000 - time: 1.085689, loss: 1.466695, perplexity: 4.334884, precision: 0.000000, batch_len: 106.000000
Train, loss=1.46669495: 1153it [27:20,  1.21s/it]2017-06-01 18:36:51,557 root  INFO     step 1153.000000 - time: 0.961020, loss: 1.248479, perplexity: 3.485037, precision: 0.078125, batch_len: 104.000000
Train, loss=1.24847865: 1154it [27:21,  1.16s/it]2017-06-01 18:36:52,724 root  INFO     step 1154.000000 - time: 1.093555, loss: 1.101516, perplexity: 3.008723, precision: 0.078125, batch_len: 81.000000
Train, loss=1.10151589: 1155it [27:22,  1.16s/it]2017-06-01 18:36:54,236 root  INFO     step 1155.000000 - time: 1.456863, loss: 0.989157, perplexity: 2.688967, precision: 0.125000, batch_len: 124.000000
Train, loss=0.98915708: 1156it [27:24,  1.27s/it]2017-06-01 18:36:55,322 root  INFO     step 1156.000000 - time: 1.074234, loss: 0.924327, perplexity: 2.520172, precision: 0.109375, batch_len: 92.000000
Train, loss=0.92432708: 1157it [27:25,  1.21s/it]2017-06-01 18:36:56,425 root  INFO     step 1157.000000 - time: 1.089440, loss: 1.420546, perplexity: 4.139378, precision: 0.031250, batch_len: 97.000000
Train, loss=1.42054558: 1158it [27:26,  1.18s/it]2017-06-01 18:36:57,510 root  INFO     step 1158.000000 - time: 0.920064, loss: 1.220787, perplexity: 3.389854, precision: 0.015625, batch_len: 91.000000
Train, loss=1.22078693: 1159it [27:27,  1.15s/it]2017-06-01 18:36:58,751 root  INFO     step 1159.000000 - time: 1.150025, loss: 1.019620, perplexity: 2.772141, precision: 0.281250, batch_len: 85.000000
Train, loss=1.01961994: 1160it [27:28,  1.18s/it]2017-06-01 18:37:00,076 root  INFO     step 1160.000000 - time: 1.280372, loss: 0.984325, perplexity: 2.676005, precision: 0.203125, batch_len: 89.000000
Train, loss=0.98432487: 1161it [27:29,  1.22s/it]2017-06-01 18:37:01,207 root  INFO     step 1161.000000 - time: 1.111307, loss: 0.926638, perplexity: 2.526003, precision: 0.109375, batch_len: 114.000000
Train, loss=0.92663831: 1162it [27:31,  1.19s/it]2017-06-01 18:37:02,653 root  INFO     step 1162.000000 - time: 1.326981, loss: 0.861109, perplexity: 2.365784, precision: 0.078125, batch_len: 123.000000
Train, loss=0.86110950: 1163it [27:32,  1.27s/it]2017-06-01 18:37:03,731 root  INFO     step 1163.000000 - time: 1.066439, loss: 1.068943, perplexity: 2.912300, precision: 0.031250, batch_len: 90.000000
Train, loss=1.06894302: 1164it [27:33,  1.21s/it]2017-06-01 18:37:04,731 root  INFO     step 1164.000000 - time: 0.937675, loss: 1.104651, perplexity: 3.018172, precision: 0.250000, batch_len: 79.000000
Train, loss=1.10465133: 1165it [27:34,  1.15s/it]2017-06-01 18:37:06,130 root  INFO     step 1165.000000 - time: 1.286814, loss: 0.889758, perplexity: 2.434541, precision: 0.140625, batch_len: 107.000000
Train, loss=0.88975835: 1166it [27:36,  1.22s/it]2017-06-01 18:37:07,330 root  INFO     step 1166.000000 - time: 1.102760, loss: 0.875282, perplexity: 2.399552, precision: 0.281250, batch_len: 86.000000
Train, loss=0.87528217: 1167it [27:37,  1.22s/it]2017-06-01 18:37:08,281 root  INFO     step 1167.000000 - time: 0.945035, loss: 0.934011, perplexity: 2.544695, precision: 0.265625, batch_len: 83.000000
Train, loss=0.93401074: 1168it [27:38,  1.14s/it]2017-06-01 18:37:09,676 root  INFO     step 1168.000000 - time: 1.304052, loss: 0.938938, perplexity: 2.557264, precision: 0.171875, batch_len: 121.000000
Train, loss=0.93893802: 1169it [27:39,  1.21s/it]2017-06-01 18:37:10,759 root  INFO     step 1169.000000 - time: 1.049253, loss: 0.908695, perplexity: 2.481083, precision: 0.187500, batch_len: 115.000000
Train, loss=0.90869498: 1170it [27:40,  1.17s/it]2017-06-01 18:37:11,969 root  INFO     step 1170.000000 - time: 1.174310, loss: 0.763148, perplexity: 2.145019, precision: 0.218750, batch_len: 103.000000
Train, loss=0.76314843: 1171it [27:41,  1.19s/it]2017-06-01 18:37:13,637 root  INFO     step 1171.000000 - time: 1.523188, loss: 0.816983, perplexity: 2.263661, precision: 0.140625, batch_len: 125.000000
Train, loss=0.81698334: 1172it [27:43,  1.33s/it]2017-06-01 18:37:14,613 root  INFO     step 1172.000000 - time: 0.965032, loss: 0.651370, perplexity: 1.918167, precision: 0.375000, batch_len: 80.000000
Train, loss=0.65137029: 1173it [27:44,  1.22s/it]2017-06-01 18:37:16,024 root  INFO     step 1173.000000 - time: 1.315693, loss: 1.080041, perplexity: 2.944800, precision: 0.062500, batch_len: 116.000000
Train, loss=1.08004105: 1174it [27:45,  1.28s/it]2017-06-01 18:37:17,108 root  INFO     step 1174.000000 - time: 0.964962, loss: 1.138968, perplexity: 3.123542, precision: 0.015625, batch_len: 94.000000
Train, loss=1.13896775: 1175it [27:46,  1.22s/it]2017-06-01 18:37:18,544 root  INFO     step 1175.000000 - time: 1.374524, loss: 0.992805, perplexity: 2.698795, precision: 0.062500, batch_len: 129.000000
Train, loss=0.99280548: 1176it [27:48,  1.29s/it]2017-06-01 18:37:20,243 root  INFO     step 1176.000000 - time: 1.593898, loss: 0.962284, perplexity: 2.617669, precision: 0.046875, batch_len: 118.000000
Train, loss=0.96228427: 1177it [27:50,  1.41s/it]2017-06-01 18:37:21,478 root  INFO     step 1177.000000 - time: 1.096472, loss: 0.769066, perplexity: 2.157751, precision: 0.250000, batch_len: 98.000000
Train, loss=0.76906645: 1178it [27:51,  1.36s/it]2017-06-01 18:37:22,549 root  INFO     step 1178.000000 - time: 0.947424, loss: 0.821550, perplexity: 2.274022, precision: 0.234375, batch_len: 87.000000
Train, loss=0.82155019: 1179it [27:52,  1.27s/it]2017-06-01 18:37:23,990 root  INFO     step 1179.000000 - time: 1.323864, loss: 0.894916, perplexity: 2.447131, precision: 0.187500, batch_len: 137.000000
Train, loss=0.89491612: 1180it [27:53,  1.32s/it]2017-06-01 18:37:25,855 root  INFO     step 1180.000000 - time: 1.849700, loss: 0.728825, perplexity: 2.072644, precision: 0.140625, batch_len: 136.000000
Train, loss=0.72882509: 1181it [27:55,  1.49s/it]2017-06-01 18:37:26,763 root  INFO     step 1181.000000 - time: 0.880706, loss: 0.796558, perplexity: 2.217893, precision: 0.250000, batch_len: 82.000000
Train, loss=0.79655766: 1182it [27:56,  1.31s/it]2017-06-01 18:37:27,968 root  INFO     step 1182.000000 - time: 1.120599, loss: 1.002090, perplexity: 2.723969, precision: 0.171875, batch_len: 84.000000
Train, loss=1.00208998: 1183it [27:57,  1.28s/it]2017-06-01 18:37:29,563 root  INFO     step 1183.000000 - time: 1.557233, loss: 0.916720, perplexity: 2.501074, precision: 0.093750, batch_len: 126.000000
Train, loss=0.91672039: 1184it [27:59,  1.37s/it]2017-06-01 18:37:30,760 root  INFO     step 1184.000000 - time: 1.108405, loss: 0.915227, perplexity: 2.497341, precision: 0.187500, batch_len: 99.000000
Train, loss=0.91522670: 1185it [28:00,  1.32s/it]2017-06-01 18:37:32,183 root  INFO     step 1185.000000 - time: 1.344040, loss: 0.997776, perplexity: 2.712244, precision: 0.125000, batch_len: 130.000000
Train, loss=0.99777639: 1186it [28:02,  1.35s/it]2017-06-01 18:37:33,404 root  INFO     step 1186.000000 - time: 1.098957, loss: 0.847485, perplexity: 2.333770, precision: 0.140625, batch_len: 135.000000
Train, loss=0.84748507: 1187it [28:03,  1.31s/it]2017-06-01 18:37:35,265 root  INFO     step 1187.000000 - time: 1.769198, loss: 1.455878, perplexity: 4.288246, precision: 0.046875, batch_len: 144.000000
Train, loss=1.45587778: 1188it [28:05,  1.48s/it]2017-06-01 18:37:36,655 root  INFO     step 1188.000000 - time: 1.366293, loss: 1.299581, perplexity: 3.667761, precision: 0.031250, batch_len: 122.000000
Train, loss=1.29958129: 1189it [28:06,  1.45s/it]2017-06-01 18:37:38,187 root  INFO     step 1189.000000 - time: 1.381223, loss: 1.655317, perplexity: 5.234737, precision: 0.000000, batch_len: 119.000000
Train, loss=1.65531659: 1190it [28:08,  1.48s/it]2017-06-01 18:37:39,291 root  INFO     step 1190.000000 - time: 0.890749, loss: 1.013693, perplexity: 2.755758, precision: 0.078125, batch_len: 78.000000
Train, loss=1.01369262: 1191it [28:09,  1.36s/it]2017-06-01 18:37:40,947 root  INFO     step 1191.000000 - time: 1.637632, loss: 1.032756, perplexity: 2.808797, precision: 0.046875, batch_len: 141.000000
Train, loss=1.03275621: 1192it [28:10,  1.45s/it]2017-06-01 18:37:42,254 root  INFO     step 1192.000000 - time: 1.256031, loss: 0.828264, perplexity: 2.289341, precision: 0.140625, batch_len: 96.000000
Train, loss=0.82826394: 1193it [28:12,  1.41s/it]2017-06-01 18:37:43,720 root  INFO     step 1193.000000 - time: 1.308812, loss: 0.882084, perplexity: 2.415930, precision: 0.062500, batch_len: 133.000000
Train, loss=0.88208425: 1194it [28:13,  1.43s/it]2017-06-01 18:37:45,174 root  INFO     step 1194.000000 - time: 1.351209, loss: 0.839502, perplexity: 2.315214, precision: 0.156250, batch_len: 132.000000
Train, loss=0.83950233: 1195it [28:15,  1.43s/it]2017-06-01 18:37:46,522 root  INFO     step 1195.000000 - time: 1.257435, loss: 0.868093, perplexity: 2.382362, precision: 0.109375, batch_len: 134.000000
Train, loss=0.86809260: 1196it [28:16,  1.41s/it]2017-06-01 18:37:48,450 root  INFO     step 1196.000000 - time: 1.762662, loss: 0.977905, perplexity: 2.658880, precision: 0.015625, batch_len: 138.000000
Train, loss=0.97790515: 1197it [28:18,  1.56s/it]2017-06-01 18:37:50,335 root  INFO     step 1197.000000 - time: 1.878752, loss: 1.422359, perplexity: 4.146891, precision: 0.031250, batch_len: 152.000000
Train, loss=1.42235899: 1198it [28:20,  1.66s/it]2017-06-01 18:37:51,356 root  INFO     step 1198.000000 - time: 0.937440, loss: 0.943534, perplexity: 2.569045, precision: 0.109375, batch_len: 77.000000
Train, loss=0.94353426: 1199it [28:21,  1.47s/it]2017-06-01 18:37:52,329 root  INFO     step 1199.000000 - time: 0.894133, loss: 0.695731, perplexity: 2.005175, precision: 0.281250, batch_len: 74.000000
Train, loss=0.69573140: 1200it [28:22,  1.32s/it]2017-06-01 18:37:54,190 root  INFO     step 1200.000000 - time: 1.846605, loss: 0.906166, perplexity: 2.474815, precision: 0.046875, batch_len: 150.000000
Train, loss=0.90616554: 1201it [28:24,  1.48s/it]2017-06-01 18:37:55,767 root  INFO     step 1201.000000 - time: 1.565194, loss: 1.145563, perplexity: 3.144210, precision: 0.015625, batch_len: 142.000000
Train, loss=1.14556265: 1202it [28:25,  1.51s/it]2017-06-01 18:37:57,374 root  INFO     step 1202.000000 - time: 1.449410, loss: 0.859178, perplexity: 2.361218, precision: 0.031250, batch_len: 139.000000
Train, loss=0.85917777: 1203it [28:27,  1.54s/it]2017-06-01 18:37:58,424 root  INFO     step 1203.000000 - time: 0.890380, loss: 0.619942, perplexity: 1.858820, precision: 0.296875, batch_len: 71.000000
Train, loss=0.61994207: 1204it [28:28,  1.39s/it]2017-06-01 18:37:59,523 root  INFO     step 1204.000000 - time: 1.094861, loss: 0.566440, perplexity: 1.761982, precision: 0.375000, batch_len: 72.000000
Train, loss=0.56643951: 1205it [28:29,  1.30s/it]2017-06-01 18:38:00,769 root  INFO     step 1205.000000 - time: 1.087639, loss: 0.758077, perplexity: 2.134167, precision: 0.343750, batch_len: 76.000000
Train, loss=0.75807655: 1206it [28:30,  1.29s/it]2017-06-01 18:38:02,433 root  INFO     step 1206.000000 - time: 1.539801, loss: 0.822645, perplexity: 2.276514, precision: 0.093750, batch_len: 131.000000
Train, loss=0.82264513: 1207it [28:32,  1.40s/it]2017-06-01 18:38:02,549 root  INFO     Generating first batch)
2017-06-01 18:38:06,667 root  INFO     step 1207.000000 - time: 1.232612, loss: 0.822417, perplexity: 2.275994, precision: 0.156250, batch_len: 96.000000
Train, loss=0.82241678: 1208it [28:36,  2.25s/it]2017-06-01 18:38:08,016 root  INFO     step 1208.000000 - time: 1.117490, loss: 1.041208, perplexity: 2.832636, precision: 0.140625, batch_len: 111.000000
Train, loss=1.04120767: 1209it [28:37,  1.98s/it]2017-06-01 18:38:09,221 root  INFO     step 1209.000000 - time: 1.134104, loss: 1.044469, perplexity: 2.841889, precision: 0.140625, batch_len: 113.000000
Train, loss=1.04446912: 1210it [28:39,  1.75s/it]2017-06-01 18:38:10,272 root  INFO     step 1210.000000 - time: 1.031994, loss: 1.089796, perplexity: 2.973667, precision: 0.171875, batch_len: 93.000000
Train, loss=1.08979583: 1211it [28:40,  1.54s/it]2017-06-01 18:38:11,316 root  INFO     step 1211.000000 - time: 1.012751, loss: 1.140447, perplexity: 3.128165, precision: 0.093750, batch_len: 104.000000
Train, loss=1.14044666: 1212it [28:41,  1.39s/it]2017-06-01 18:38:13,111 root  INFO     step 1212.000000 - time: 1.679431, loss: 1.218340, perplexity: 3.381569, precision: 0.046875, batch_len: 128.000000
Train, loss=1.21833992: 1213it [28:42,  1.51s/it]2017-06-01 18:38:14,237 root  INFO     step 1213.000000 - time: 1.026749, loss: 0.826995, perplexity: 2.286437, precision: 0.078125, batch_len: 100.000000
Train, loss=0.82699460: 1214it [28:44,  1.40s/it]2017-06-01 18:38:15,621 root  INFO     step 1214.000000 - time: 1.368207, loss: 0.733600, perplexity: 2.082564, precision: 0.250000, batch_len: 120.000000
Train, loss=0.73359990: 1215it [28:45,  1.39s/it]2017-06-01 18:38:17,000 root  INFO     step 1215.000000 - time: 1.230368, loss: 0.756327, perplexity: 2.130437, precision: 0.203125, batch_len: 117.000000
Train, loss=0.75632727: 1216it [28:46,  1.39s/it]2017-06-01 18:38:18,006 root  INFO     step 1216.000000 - time: 0.992970, loss: 0.662551, perplexity: 1.939735, precision: 0.328125, batch_len: 88.000000
Train, loss=0.66255116: 1217it [28:47,  1.27s/it]2017-06-01 18:38:19,186 root  INFO     step 1217.000000 - time: 1.090792, loss: 0.795336, perplexity: 2.215184, precision: 0.312500, batch_len: 80.000000
Train, loss=0.79533565: 1218it [28:49,  1.25s/it]2017-06-01 18:38:20,419 root  INFO     step 1218.000000 - time: 1.147436, loss: 0.842054, perplexity: 2.321130, precision: 0.218750, batch_len: 105.000000
Train, loss=0.84205413: 1219it [28:50,  1.24s/it]2017-06-01 18:38:21,478 root  INFO     step 1219.000000 - time: 1.046459, loss: 0.969273, perplexity: 2.636029, precision: 0.218750, batch_len: 92.000000
Train, loss=0.96927345: 1220it [28:51,  1.19s/it]2017-06-01 18:38:23,242 root  INFO     step 1220.000000 - time: 1.675940, loss: 0.905802, perplexity: 2.473915, precision: 0.203125, batch_len: 90.000000
Train, loss=0.90580207: 1221it [28:53,  1.36s/it]2017-06-01 18:38:24,227 root  INFO     step 1221.000000 - time: 0.971026, loss: 0.845797, perplexity: 2.329834, precision: 0.234375, batch_len: 103.000000
Train, loss=0.84579694: 1222it [28:54,  1.25s/it]2017-06-01 18:38:25,563 root  INFO     step 1222.000000 - time: 1.212594, loss: 0.822667, perplexity: 2.276562, precision: 0.250000, batch_len: 108.000000
Train, loss=0.82266653: 1223it [28:55,  1.27s/it]2017-06-01 18:38:26,721 root  INFO     step 1223.000000 - time: 1.124389, loss: 0.980785, perplexity: 2.666550, precision: 0.125000, batch_len: 110.000000
Train, loss=0.98078549: 1224it [28:56,  1.24s/it]2017-06-01 18:38:28,106 root  INFO     step 1224.000000 - time: 1.378423, loss: 0.859795, perplexity: 2.362677, precision: 0.203125, batch_len: 101.000000
Train, loss=0.85979533: 1225it [28:57,  1.28s/it]2017-06-01 18:38:29,225 root  INFO     step 1225.000000 - time: 1.066644, loss: 0.704882, perplexity: 2.023607, precision: 0.281250, batch_len: 86.000000
Train, loss=0.70488167: 1226it [28:59,  1.23s/it]2017-06-01 18:38:30,837 root  INFO     step 1226.000000 - time: 1.478256, loss: 0.880849, perplexity: 2.412947, precision: 0.203125, batch_len: 137.000000
Train, loss=0.88084871: 1227it [29:00,  1.35s/it]2017-06-01 18:38:32,023 root  INFO     step 1227.000000 - time: 1.142623, loss: 0.887490, perplexity: 2.429024, precision: 0.156250, batch_len: 102.000000
Train, loss=0.88748974: 1228it [29:01,  1.30s/it]2017-06-01 18:38:33,266 root  INFO     step 1228.000000 - time: 1.215348, loss: 0.765583, perplexity: 2.150248, precision: 0.203125, batch_len: 124.000000
Train, loss=0.76558328: 1229it [29:03,  1.28s/it]2017-06-01 18:38:34,960 root  INFO     step 1229.000000 - time: 1.675034, loss: 0.916586, perplexity: 2.500737, precision: 0.171875, batch_len: 121.000000
Train, loss=0.91658551: 1230it [29:04,  1.41s/it]2017-06-01 18:38:35,907 root  INFO     step 1230.000000 - time: 0.936817, loss: 0.861705, perplexity: 2.367193, precision: 0.265625, batch_len: 83.000000
Train, loss=0.86170501: 1231it [29:05,  1.27s/it]2017-06-01 18:38:37,042 root  INFO     step 1231.000000 - time: 1.101137, loss: 0.837696, perplexity: 2.311037, precision: 0.140625, batch_len: 114.000000
Train, loss=0.83769619: 1232it [29:06,  1.23s/it]2017-06-01 18:38:38,221 root  INFO     step 1232.000000 - time: 1.091824, loss: 1.192220, perplexity: 3.294386, precision: 0.156250, batch_len: 97.000000
Train, loss=1.19221973: 1233it [29:08,  1.21s/it]2017-06-01 18:38:39,234 root  INFO     step 1233.000000 - time: 1.007074, loss: 1.397069, perplexity: 4.043331, precision: 0.000000, batch_len: 112.000000
Train, loss=1.39706898: 1234it [29:09,  1.15s/it]2017-06-01 18:38:40,515 root  INFO     step 1234.000000 - time: 1.209829, loss: 1.603912, perplexity: 4.972448, precision: 0.015625, batch_len: 109.000000
Train, loss=1.60391235: 1235it [29:10,  1.19s/it]2017-06-01 18:38:41,850 root  INFO     step 1235.000000 - time: 1.324554, loss: 1.020635, perplexity: 2.774958, precision: 0.140625, batch_len: 106.000000
Train, loss=1.02063549: 1236it [29:11,  1.23s/it]2017-06-01 18:38:42,886 root  INFO     step 1236.000000 - time: 1.004027, loss: 0.800328, perplexity: 2.226271, precision: 0.234375, batch_len: 85.000000
Train, loss=0.80032778: 1237it [29:12,  1.17s/it]2017-06-01 18:38:44,006 root  INFO     step 1237.000000 - time: 1.084099, loss: 0.855991, perplexity: 2.353705, precision: 0.234375, batch_len: 107.000000
Train, loss=0.85599083: 1238it [29:13,  1.16s/it]2017-06-01 18:38:45,070 root  INFO     step 1238.000000 - time: 0.920605, loss: 0.757275, perplexity: 2.132458, precision: 0.171875, batch_len: 81.000000
Train, loss=0.75727522: 1239it [29:14,  1.13s/it]2017-06-01 18:38:46,177 root  INFO     step 1239.000000 - time: 1.046618, loss: 0.977653, perplexity: 2.658210, precision: 0.203125, batch_len: 115.000000
Train, loss=0.97765279: 1240it [29:16,  1.12s/it]2017-06-01 18:38:47,482 root  INFO     step 1240.000000 - time: 1.262847, loss: 0.740285, perplexity: 2.096533, precision: 0.281250, batch_len: 89.000000
Train, loss=0.74028492: 1241it [29:17,  1.18s/it]2017-06-01 18:38:49,097 root  INFO     step 1241.000000 - time: 1.465209, loss: 0.864139, perplexity: 2.372962, precision: 0.109375, batch_len: 125.000000
Train, loss=0.86413914: 1242it [29:18,  1.31s/it]2017-06-01 18:38:50,230 root  INFO     step 1242.000000 - time: 1.008052, loss: 0.858242, perplexity: 2.359010, precision: 0.203125, batch_len: 91.000000
Train, loss=0.85824203: 1243it [29:20,  1.26s/it]2017-06-01 18:38:51,493 root  INFO     step 1243.000000 - time: 1.090394, loss: 0.697036, perplexity: 2.007792, precision: 0.281250, batch_len: 98.000000
Train, loss=0.69703573: 1244it [29:21,  1.26s/it]2017-06-01 18:38:52,525 root  INFO     step 1244.000000 - time: 0.950595, loss: 0.802589, perplexity: 2.231311, precision: 0.265625, batch_len: 94.000000
Train, loss=0.80258924: 1245it [29:22,  1.19s/it]2017-06-01 18:38:53,789 root  INFO     step 1245.000000 - time: 1.173236, loss: 0.708058, perplexity: 2.030045, precision: 0.281250, batch_len: 87.000000
Train, loss=0.70805800: 1246it [29:23,  1.21s/it]2017-06-01 18:38:54,948 root  INFO     step 1246.000000 - time: 1.153762, loss: 0.675947, perplexity: 1.965893, precision: 0.265625, batch_len: 82.000000
Train, loss=0.67594671: 1247it [29:24,  1.20s/it]2017-06-01 18:38:55,997 root  INFO     step 1247.000000 - time: 0.995310, loss: 0.814211, perplexity: 2.257394, precision: 0.359375, batch_len: 79.000000
Train, loss=0.81421089: 1248it [29:25,  1.15s/it]2017-06-01 18:38:57,234 root  INFO     step 1248.000000 - time: 1.099105, loss: 0.723424, perplexity: 2.061480, precision: 0.203125, batch_len: 99.000000
Train, loss=0.72342432: 1249it [29:27,  1.18s/it]2017-06-01 18:38:58,700 root  INFO     step 1249.000000 - time: 1.436141, loss: 0.837290, perplexity: 2.310098, precision: 0.109375, batch_len: 136.000000
Train, loss=0.83728993: 1250it [29:28,  1.26s/it]2017-06-01 18:39:00,260 root  INFO     step 1250.000000 - time: 1.459086, loss: 1.159487, perplexity: 3.188296, precision: 0.000000, batch_len: 123.000000
Train, loss=1.15948653: 1251it [29:30,  1.35s/it]2017-06-01 18:39:01,919 root  INFO     step 1251.000000 - time: 1.528927, loss: 1.390762, perplexity: 4.017910, precision: 0.000000, batch_len: 118.000000
Train, loss=1.39076185: 1252it [29:31,  1.44s/it]2017-06-01 18:39:02,998 root  INFO     step 1252.000000 - time: 1.039877, loss: 1.004703, perplexity: 2.731095, precision: 0.218750, batch_len: 84.000000
Train, loss=1.00470257: 1253it [29:32,  1.34s/it]2017-06-01 18:39:04,490 root  INFO     step 1253.000000 - time: 1.400812, loss: 0.886008, perplexity: 2.425427, precision: 0.140625, batch_len: 144.000000
Train, loss=0.88600755: 1254it [29:34,  1.38s/it]2017-06-01 18:39:05,789 root  INFO     step 1254.000000 - time: 1.202912, loss: 0.823031, perplexity: 2.277392, precision: 0.140625, batch_len: 126.000000
Train, loss=0.82303083: 1255it [29:35,  1.36s/it]2017-06-01 18:39:07,281 root  INFO     step 1255.000000 - time: 1.446516, loss: 0.669127, perplexity: 1.952532, precision: 0.265625, batch_len: 116.000000
Train, loss=0.66912687: 1256it [29:37,  1.40s/it]2017-06-01 18:39:08,767 root  INFO     step 1256.000000 - time: 1.457253, loss: 0.746688, perplexity: 2.109999, precision: 0.109375, batch_len: 129.000000
Train, loss=0.74668765: 1257it [29:38,  1.42s/it]2017-06-01 18:39:10,180 root  INFO     step 1257.000000 - time: 1.346717, loss: 0.733081, perplexity: 2.081483, precision: 0.187500, batch_len: 133.000000
Train, loss=0.73308069: 1258it [29:40,  1.42s/it]2017-06-01 18:39:12,116 root  INFO     step 1258.000000 - time: 1.851653, loss: 0.814267, perplexity: 2.257519, precision: 0.125000, batch_len: 152.000000
Train, loss=0.81426662: 1259it [29:41,  1.58s/it]2017-06-01 18:39:13,440 root  INFO     step 1259.000000 - time: 1.317263, loss: 0.867821, perplexity: 2.381714, precision: 0.125000, batch_len: 119.000000
Train, loss=0.86782050: 1260it [29:43,  1.50s/it]2017-06-01 18:39:15,159 root  INFO     step 1260.000000 - time: 1.590754, loss: 0.862178, perplexity: 2.368313, precision: 0.171875, batch_len: 135.000000
Train, loss=0.86217773: 1261it [29:45,  1.57s/it]2017-06-01 18:39:16,265 root  INFO     step 1261.000000 - time: 1.101507, loss: 0.696566, perplexity: 2.006848, precision: 0.140625, batch_len: 96.000000
Train, loss=0.69656551: 1262it [29:46,  1.43s/it]2017-06-01 18:39:17,753 root  INFO     step 1262.000000 - time: 1.436958, loss: 1.075561, perplexity: 2.931638, precision: 0.109375, batch_len: 122.000000
Train, loss=1.07556129: 1263it [29:47,  1.45s/it]2017-06-01 18:39:19,187 root  INFO     step 1263.000000 - time: 1.259799, loss: 1.391621, perplexity: 4.021362, precision: 0.015625, batch_len: 130.000000
Train, loss=1.39162064: 1264it [29:49,  1.44s/it]2017-06-01 18:39:20,504 root  INFO     step 1264.000000 - time: 1.190983, loss: 0.878277, perplexity: 2.406749, precision: 0.296875, batch_len: 78.000000
Train, loss=0.87827682: 1265it [29:50,  1.40s/it]2017-06-01 18:39:22,271 root  INFO     step 1265.000000 - time: 1.596922, loss: 1.013637, perplexity: 2.755606, precision: 0.031250, batch_len: 138.000000
Train, loss=1.01363730: 1266it [29:52,  1.51s/it]2017-06-01 18:39:23,883 root  INFO     step 1266.000000 - time: 1.408985, loss: 1.086225, perplexity: 2.963066, precision: 0.046875, batch_len: 141.000000
Train, loss=1.08622468: 1267it [29:53,  1.54s/it]2017-06-01 18:39:25,239 root  INFO     step 1267.000000 - time: 1.318979, loss: 1.029175, perplexity: 2.798757, precision: 0.078125, batch_len: 132.000000
Train, loss=1.02917528: 1268it [29:55,  1.49s/it]2017-06-01 18:39:26,634 root  INFO     step 1268.000000 - time: 1.345898, loss: 0.801906, perplexity: 2.229786, precision: 0.109375, batch_len: 139.000000
Train, loss=0.80190551: 1269it [29:56,  1.46s/it]2017-06-01 18:39:27,706 root  INFO     step 1269.000000 - time: 1.037471, loss: 0.748810, perplexity: 2.114483, precision: 0.140625, batch_len: 74.000000
Train, loss=0.74881035: 1270it [29:57,  1.34s/it]2017-06-01 18:39:28,921 root  INFO     step 1270.000000 - time: 1.099190, loss: 1.159998, perplexity: 3.189927, precision: 0.234375, batch_len: 76.000000
Train, loss=1.15999818: 1271it [29:58,  1.30s/it]2017-06-01 18:39:30,908 root  INFO     step 1271.000000 - time: 1.975463, loss: 0.890016, perplexity: 2.435169, precision: 0.093750, batch_len: 150.000000
Train, loss=0.89001632: 1272it [30:00,  1.51s/it]2017-06-01 18:39:32,472 root  INFO     step 1272.000000 - time: 1.437266, loss: 1.012926, perplexity: 2.753646, precision: 0.078125, batch_len: 142.000000
Train, loss=1.01292574: 1273it [30:02,  1.53s/it]2017-06-01 18:39:33,846 root  INFO     step 1273.000000 - time: 1.319772, loss: 0.759845, perplexity: 2.137944, precision: 0.093750, batch_len: 134.000000
Train, loss=0.75984484: 1274it [30:03,  1.48s/it]2017-06-01 18:39:34,900 root  INFO     step 1274.000000 - time: 0.887043, loss: 0.763744, perplexity: 2.146296, precision: 0.187500, batch_len: 77.000000
Train, loss=0.76374376: 1275it [30:04,  1.35s/it]2017-06-01 18:39:36,295 root  INFO     step 1275.000000 - time: 1.316220, loss: 0.485572, perplexity: 1.625104, precision: 0.546875, batch_len: 72.000000
Train, loss=0.48557168: 1276it [30:06,  1.36s/it]2017-06-01 18:39:37,415 root  INFO     step 1276.000000 - time: 0.968163, loss: 0.478335, perplexity: 1.613385, precision: 0.500000, batch_len: 71.000000
Train, loss=0.47833455: 1277it [30:07,  1.29s/it]2017-06-01 18:39:38,843 root  INFO     step 1277.000000 - time: 1.369375, loss: 0.841316, perplexity: 2.319418, precision: 0.093750, batch_len: 131.000000
Train, loss=0.84131610: 1278it [30:08,  1.33s/it]2017-06-01 18:39:38,984 root  INFO     Generating first batch)
2017-06-01 18:39:42,723 root  INFO     step 1278.000000 - time: 1.093628, loss: 0.632779, perplexity: 1.882835, precision: 0.125000, batch_len: 96.000000
Train, loss=0.63277853: 1279it [30:12,  2.10s/it]2017-06-01 18:39:44,025 root  INFO     step 1279.000000 - time: 1.085685, loss: 0.984292, perplexity: 2.675916, precision: 0.078125, batch_len: 113.000000
Train, loss=0.98429179: 1280it [30:13,  1.86s/it]2017-06-01 18:39:45,494 root  INFO     step 1280.000000 - time: 1.354170, loss: 0.945507, perplexity: 2.574118, precision: 0.062500, batch_len: 120.000000
Train, loss=0.94550687: 1281it [30:15,  1.74s/it]2017-06-01 18:39:46,882 root  INFO     step 1281.000000 - time: 1.224828, loss: 0.758845, perplexity: 2.135808, precision: 0.171875, batch_len: 110.000000
Train, loss=0.75884515: 1282it [30:16,  1.64s/it]2017-06-01 18:39:48,342 root  INFO     step 1282.000000 - time: 1.327324, loss: 0.765604, perplexity: 2.150294, precision: 0.250000, batch_len: 108.000000
Train, loss=0.76560450: 1283it [30:18,  1.58s/it]2017-06-01 18:39:49,403 root  INFO     step 1283.000000 - time: 1.051808, loss: 0.695508, perplexity: 2.004727, precision: 0.375000, batch_len: 105.000000
Train, loss=0.69550765: 1284it [30:19,  1.43s/it]2017-06-01 18:39:50,570 root  INFO     step 1284.000000 - time: 1.091260, loss: 0.868531, perplexity: 2.383406, precision: 0.265625, batch_len: 111.000000
Train, loss=0.86853063: 1285it [30:20,  1.35s/it]2017-06-01 18:39:51,985 root  INFO     step 1285.000000 - time: 1.279879, loss: 0.795088, perplexity: 2.214637, precision: 0.062500, batch_len: 124.000000
Train, loss=0.79508829: 1286it [30:21,  1.37s/it]2017-06-01 18:39:53,186 root  INFO     step 1286.000000 - time: 1.007747, loss: 1.069118, perplexity: 2.912808, precision: 0.062500, batch_len: 92.000000
Train, loss=1.06911755: 1287it [30:23,  1.32s/it]2017-06-01 18:39:54,869 root  INFO     step 1287.000000 - time: 1.600946, loss: 1.137390, perplexity: 3.118617, precision: 0.046875, batch_len: 112.000000
Train, loss=1.13738966: 1288it [30:24,  1.43s/it]2017-06-01 18:39:55,849 root  INFO     step 1288.000000 - time: 0.972067, loss: 0.746771, perplexity: 2.110176, precision: 0.062500, batch_len: 101.000000
Train, loss=0.74677134: 1289it [30:25,  1.29s/it]2017-06-01 18:39:57,211 root  INFO     step 1289.000000 - time: 1.291948, loss: 0.942890, perplexity: 2.567389, precision: 0.062500, batch_len: 128.000000
Train, loss=0.94288957: 1290it [30:27,  1.31s/it]2017-06-01 18:39:58,380 root  INFO     step 1290.000000 - time: 1.054709, loss: 0.770642, perplexity: 2.161154, precision: 0.171875, batch_len: 102.000000
Train, loss=0.77064234: 1291it [30:28,  1.27s/it]2017-06-01 18:39:59,408 root  INFO     step 1291.000000 - time: 0.967091, loss: 0.772909, perplexity: 2.166058, precision: 0.343750, batch_len: 88.000000
Train, loss=0.77290910: 1292it [30:29,  1.20s/it]2017-06-01 18:40:00,801 root  INFO     step 1292.000000 - time: 1.243863, loss: 0.825861, perplexity: 2.283847, precision: 0.265625, batch_len: 106.000000
Train, loss=0.82586110: 1293it [30:30,  1.26s/it]2017-06-01 18:40:02,042 root  INFO     step 1293.000000 - time: 1.203561, loss: 0.918382, perplexity: 2.505233, precision: 0.218750, batch_len: 93.000000
Train, loss=0.91838169: 1294it [30:31,  1.25s/it]2017-06-01 18:40:03,101 root  INFO     step 1294.000000 - time: 1.010683, loss: 0.812490, perplexity: 2.253513, precision: 0.234375, batch_len: 100.000000
Train, loss=0.81249022: 1295it [30:32,  1.19s/it]2017-06-01 18:40:04,191 root  INFO     step 1295.000000 - time: 0.969495, loss: 0.692427, perplexity: 1.998560, precision: 0.375000, batch_len: 86.000000
Train, loss=0.69242686: 1296it [30:34,  1.16s/it]2017-06-01 18:40:05,480 root  INFO     step 1296.000000 - time: 1.179546, loss: 0.883362, perplexity: 2.419019, precision: 0.250000, batch_len: 91.000000
Train, loss=0.88336200: 1297it [30:35,  1.20s/it]2017-06-01 18:40:06,498 root  INFO     step 1297.000000 - time: 1.001321, loss: 0.746360, perplexity: 2.109308, precision: 0.234375, batch_len: 80.000000
Train, loss=0.74635977: 1298it [30:36,  1.15s/it]2017-06-01 18:40:07,826 root  INFO     step 1298.000000 - time: 1.323303, loss: 1.028953, perplexity: 2.798136, precision: 0.234375, batch_len: 85.000000
Train, loss=1.02895331: 1299it [30:37,  1.20s/it]2017-06-01 18:40:08,871 root  INFO     step 1299.000000 - time: 1.023202, loss: 0.783606, perplexity: 2.189352, precision: 0.156250, batch_len: 81.000000
Train, loss=0.78360569: 1300it [30:38,  1.15s/it]2017-06-01 18:40:09,845 root  INFO     step 1300.000000 - time: 0.962373, loss: 0.775135, perplexity: 2.170885, precision: 0.296875, batch_len: 83.000000
Train, loss=0.77513486: 1301it [30:39,  1.10s/it]2017-06-01 18:40:10,934 root  INFO     step 1301.000000 - time: 1.059113, loss: 0.862699, perplexity: 2.369548, precision: 0.250000, batch_len: 89.000000
Train, loss=0.86269927: 1302it [30:40,  1.10s/it]2017-06-01 18:40:11,931 root  INFO     step 1302.000000 - time: 0.967596, loss: 0.950338, perplexity: 2.586583, precision: 0.296875, batch_len: 79.000000
Train, loss=0.95033753: 1303it [30:41,  1.07s/it]2017-06-01 18:40:13,041 root  INFO     step 1303.000000 - time: 1.028750, loss: 0.764446, perplexity: 2.147803, precision: 0.265625, batch_len: 109.000000
Train, loss=0.76444560: 1304it [30:42,  1.08s/it]2017-06-01 18:40:14,772 root  INFO     step 1304.000000 - time: 1.699809, loss: 0.882495, perplexity: 2.416923, precision: 0.093750, batch_len: 123.000000
Train, loss=0.88249528: 1305it [30:44,  1.28s/it]2017-06-01 18:40:15,916 root  INFO     step 1305.000000 - time: 1.124374, loss: 0.979137, perplexity: 2.662159, precision: 0.093750, batch_len: 97.000000
Train, loss=0.97913742: 1306it [30:45,  1.24s/it]2017-06-01 18:40:17,049 root  INFO     step 1306.000000 - time: 1.114406, loss: 0.870542, perplexity: 2.388205, precision: 0.203125, batch_len: 115.000000
Train, loss=0.87054223: 1307it [30:46,  1.20s/it]2017-06-01 18:40:18,173 root  INFO     step 1307.000000 - time: 1.086746, loss: 0.738895, perplexity: 2.093620, precision: 0.203125, batch_len: 114.000000
Train, loss=0.73889482: 1308it [30:48,  1.18s/it]2017-06-01 18:40:19,132 root  INFO     step 1308.000000 - time: 0.949160, loss: 0.738052, perplexity: 2.091856, precision: 0.312500, batch_len: 107.000000
Train, loss=0.73805159: 1309it [30:49,  1.11s/it]2017-06-01 18:40:20,319 root  INFO     step 1309.000000 - time: 1.170475, loss: 0.786449, perplexity: 2.195587, precision: 0.234375, batch_len: 103.000000
Train, loss=0.78644937: 1310it [30:50,  1.14s/it]2017-06-01 18:40:22,248 root  INFO     step 1310.000000 - time: 1.908869, loss: 0.848881, perplexity: 2.337031, precision: 0.312500, batch_len: 90.000000
Train, loss=0.84888136: 1311it [30:52,  1.37s/it]2017-06-01 18:40:23,632 root  INFO     step 1311.000000 - time: 1.355366, loss: 0.660194, perplexity: 1.935168, precision: 0.203125, batch_len: 116.000000
Train, loss=0.66019422: 1312it [30:53,  1.38s/it]2017-06-01 18:40:24,667 root  INFO     step 1312.000000 - time: 1.022166, loss: 0.637146, perplexity: 1.891076, precision: 0.218750, batch_len: 104.000000
Train, loss=0.63714576: 1313it [30:54,  1.27s/it]2017-06-01 18:40:25,919 root  INFO     step 1313.000000 - time: 1.241831, loss: 0.895395, perplexity: 2.448303, precision: 0.156250, batch_len: 117.000000
Train, loss=0.89539528: 1314it [30:55,  1.27s/it]2017-06-01 18:40:27,480 root  INFO     step 1314.000000 - time: 1.551948, loss: 0.865264, perplexity: 2.375633, precision: 0.093750, batch_len: 125.000000
Train, loss=0.86526394: 1315it [30:57,  1.36s/it]2017-06-01 18:40:29,267 root  INFO     step 1315.000000 - time: 1.556371, loss: 1.347521, perplexity: 3.847875, precision: 0.093750, batch_len: 121.000000
Train, loss=1.34752107: 1316it [30:59,  1.49s/it]2017-06-01 18:40:30,462 root  INFO     step 1316.000000 - time: 1.136687, loss: 1.192976, perplexity: 3.296879, precision: 0.078125, batch_len: 87.000000
Train, loss=1.19297624: 1317it [31:00,  1.40s/it]2017-06-01 18:40:31,495 root  INFO     step 1317.000000 - time: 1.010256, loss: 1.246882, perplexity: 3.479476, precision: 0.031250, batch_len: 94.000000
Train, loss=1.24688160: 1318it [31:01,  1.29s/it]2017-06-01 18:40:32,968 root  INFO     step 1318.000000 - time: 1.424863, loss: 1.073158, perplexity: 2.924601, precision: 0.046875, batch_len: 137.000000
Train, loss=1.07315791: 1319it [31:02,  1.34s/it]2017-06-01 18:40:34,302 root  INFO     step 1319.000000 - time: 1.238261, loss: 0.803426, perplexity: 2.233178, precision: 0.156250, batch_len: 119.000000
Train, loss=0.80342555: 1320it [31:04,  1.34s/it]2017-06-01 18:40:35,877 root  INFO     step 1320.000000 - time: 1.450129, loss: 0.659787, perplexity: 1.934380, precision: 0.265625, batch_len: 135.000000
Train, loss=0.65978700: 1321it [31:05,  1.41s/it]2017-06-01 18:40:37,647 root  INFO     step 1321.000000 - time: 1.607018, loss: 0.794489, perplexity: 2.213309, precision: 0.156250, batch_len: 144.000000
Train, loss=0.79448855: 1322it [31:07,  1.52s/it]2017-06-01 18:40:38,759 root  INFO     step 1322.000000 - time: 1.071433, loss: 0.744008, perplexity: 2.104353, precision: 0.265625, batch_len: 98.000000
Train, loss=0.74400783: 1323it [31:08,  1.40s/it]2017-06-01 18:40:40,112 root  INFO     step 1323.000000 - time: 1.314605, loss: 0.791208, perplexity: 2.206059, precision: 0.140625, batch_len: 118.000000
Train, loss=0.79120779: 1324it [31:09,  1.38s/it]2017-06-01 18:40:41,051 root  INFO     step 1324.000000 - time: 0.800132, loss: 0.757447, perplexity: 2.132825, precision: 0.140625, batch_len: 82.000000
Train, loss=0.75744736: 1325it [31:10,  1.25s/it]2017-06-01 18:40:42,445 root  INFO     step 1325.000000 - time: 1.321792, loss: 0.825136, perplexity: 2.282192, precision: 0.140625, batch_len: 99.000000
Train, loss=0.82513636: 1326it [31:12,  1.29s/it]2017-06-01 18:40:44,139 root  INFO     step 1326.000000 - time: 1.495275, loss: 0.841131, perplexity: 2.318988, precision: 0.109375, batch_len: 133.000000
Train, loss=0.84113073: 1327it [31:14,  1.41s/it]2017-06-01 18:40:45,472 root  INFO     step 1327.000000 - time: 1.294218, loss: 0.940932, perplexity: 2.562368, precision: 0.140625, batch_len: 129.000000
Train, loss=0.94093198: 1328it [31:15,  1.39s/it]2017-06-01 18:40:46,967 root  INFO     step 1328.000000 - time: 1.316575, loss: 0.889866, perplexity: 2.434803, precision: 0.062500, batch_len: 126.000000
Train, loss=0.88986588: 1329it [31:16,  1.42s/it]2017-06-01 18:40:48,602 root  INFO     step 1329.000000 - time: 1.410285, loss: 1.075827, perplexity: 2.932416, precision: 0.125000, batch_len: 130.000000
Train, loss=1.07582664: 1330it [31:18,  1.49s/it]2017-06-01 18:40:50,058 root  INFO     step 1330.000000 - time: 1.334339, loss: 0.602062, perplexity: 1.825880, precision: 0.187500, batch_len: 96.000000
Train, loss=0.60206187: 1331it [31:19,  1.48s/it]2017-06-01 18:40:51,165 root  INFO     step 1331.000000 - time: 1.021520, loss: 0.780001, perplexity: 2.181475, precision: 0.312500, batch_len: 84.000000
Train, loss=0.78000122: 1332it [31:21,  1.37s/it]2017-06-01 18:40:52,745 root  INFO     step 1332.000000 - time: 1.472907, loss: 0.689120, perplexity: 1.991963, precision: 0.171875, batch_len: 138.000000
Train, loss=0.68912035: 1333it [31:22,  1.43s/it]2017-06-01 18:40:54,175 root  INFO     step 1333.000000 - time: 1.278565, loss: 0.814468, perplexity: 2.257974, precision: 0.093750, batch_len: 136.000000
Train, loss=0.81446791: 1334it [31:24,  1.43s/it]2017-06-01 18:40:56,552 root  INFO     step 1334.000000 - time: 2.253556, loss: 0.915908, perplexity: 2.499043, precision: 0.125000, batch_len: 152.000000
Train, loss=0.91590804: 1335it [31:26,  1.71s/it]2017-06-01 18:40:58,063 root  INFO     step 1335.000000 - time: 1.448050, loss: 0.722807, perplexity: 2.060208, precision: 0.093750, batch_len: 139.000000
Train, loss=0.72280681: 1336it [31:27,  1.65s/it]2017-06-01 18:40:59,480 root  INFO     step 1336.000000 - time: 1.369921, loss: 0.722456, perplexity: 2.059485, precision: 0.234375, batch_len: 122.000000
Train, loss=0.72245580: 1337it [31:29,  1.58s/it]2017-06-01 18:41:00,443 root  INFO     step 1337.000000 - time: 0.871137, loss: 0.709834, perplexity: 2.033654, precision: 0.296875, batch_len: 78.000000
Train, loss=0.70983440: 1338it [31:30,  1.40s/it]2017-06-01 18:41:01,454 root  INFO     step 1338.000000 - time: 0.982114, loss: 0.425691, perplexity: 1.530648, precision: 0.562500, batch_len: 74.000000
Train, loss=0.42569089: 1339it [31:31,  1.28s/it]2017-06-01 18:41:03,143 root  INFO     step 1339.000000 - time: 1.506363, loss: 0.414287, perplexity: 1.513292, precision: 0.625000, batch_len: 72.000000
Train, loss=0.41428745: 1340it [31:33,  1.40s/it]2017-06-01 18:41:04,151 root  INFO     step 1340.000000 - time: 0.966746, loss: 0.494817, perplexity: 1.640198, precision: 0.484375, batch_len: 77.000000
Train, loss=0.49481726: 1341it [31:34,  1.28s/it]2017-06-01 18:41:05,606 root  INFO     step 1341.000000 - time: 1.442609, loss: 0.863508, perplexity: 2.371465, precision: 0.140625, batch_len: 141.000000
Train, loss=0.86350811: 1342it [31:35,  1.34s/it]2017-06-01 18:41:07,041 root  INFO     step 1342.000000 - time: 1.286807, loss: 0.661523, perplexity: 1.937741, precision: 0.187500, batch_len: 134.000000
Train, loss=0.66152298: 1343it [31:36,  1.37s/it]2017-06-01 18:41:08,034 root  INFO     step 1343.000000 - time: 0.928136, loss: 0.639426, perplexity: 1.895393, precision: 0.375000, batch_len: 76.000000
Train, loss=0.63942599: 1344it [31:37,  1.25s/it]2017-06-01 18:41:10,178 root  INFO     step 1344.000000 - time: 2.127209, loss: 0.615393, perplexity: 1.850384, precision: 0.234375, batch_len: 150.000000
Train, loss=0.61539316: 1345it [31:40,  1.52s/it]2017-06-01 18:41:11,165 root  INFO     step 1345.000000 - time: 0.944564, loss: 0.454339, perplexity: 1.575132, precision: 0.453125, batch_len: 71.000000
Train, loss=0.45433915: 1346it [31:41,  1.36s/it]2017-06-01 18:41:12,656 root  INFO     step 1346.000000 - time: 1.316500, loss: 0.647697, perplexity: 1.911135, precision: 0.265625, batch_len: 132.000000
Train, loss=0.64769727: 1347it [31:42,  1.40s/it]2017-06-01 18:41:13,997 root  INFO     step 1347.000000 - time: 1.309014, loss: 0.797708, perplexity: 2.220446, precision: 0.109375, batch_len: 142.000000
Train, loss=0.79770803: 1348it [31:43,  1.38s/it]2017-06-01 18:41:15,702 root  INFO     step 1348.000000 - time: 1.542828, loss: 0.651593, perplexity: 1.918594, precision: 0.250000, batch_len: 131.000000
Train, loss=0.65159261: 1349it [31:45,  1.48s/it]2017-06-01 18:41:15,926 root  INFO     Generating first batch)
2017-06-01 18:41:19,655 root  INFO     step 1349.000000 - time: 1.041532, loss: 0.760069, perplexity: 2.138424, precision: 0.265625, batch_len: 113.000000
Train, loss=0.76006931: 1350it [31:49,  2.22s/it]2017-06-01 18:41:20,956 root  INFO     step 1350.000000 - time: 1.269238, loss: 0.618072, perplexity: 1.855348, precision: 0.312500, batch_len: 96.000000
Train, loss=0.61807215: 1351it [31:50,  1.95s/it]2017-06-01 18:41:22,371 root  INFO     step 1351.000000 - time: 1.107631, loss: 0.864390, perplexity: 2.373558, precision: 0.078125, batch_len: 110.000000
Train, loss=0.86439025: 1352it [31:52,  1.79s/it]2017-06-01 18:41:23,834 root  INFO     step 1352.000000 - time: 1.309804, loss: 1.185350, perplexity: 3.271831, precision: 0.140625, batch_len: 124.000000
Train, loss=1.18534970: 1353it [31:53,  1.69s/it]2017-06-01 18:41:25,125 root  INFO     step 1353.000000 - time: 1.283195, loss: 1.325246, perplexity: 3.763111, precision: 0.015625, batch_len: 128.000000
Train, loss=1.32524610: 1354it [31:55,  1.57s/it]2017-06-01 18:41:26,124 root  INFO     step 1354.000000 - time: 0.983093, loss: 1.305631, perplexity: 3.690016, precision: 0.000000, batch_len: 108.000000
Train, loss=1.30563092: 1355it [31:56,  1.40s/it]2017-06-01 18:41:27,380 root  INFO     step 1355.000000 - time: 1.125846, loss: 0.872730, perplexity: 2.393436, precision: 0.156250, batch_len: 105.000000
Train, loss=0.87273020: 1356it [31:57,  1.36s/it]2017-06-01 18:41:28,619 root  INFO     step 1356.000000 - time: 1.205165, loss: 0.706840, perplexity: 2.027573, precision: 0.281250, batch_len: 92.000000
Train, loss=0.70683956: 1357it [31:58,  1.32s/it]2017-06-01 18:41:29,993 root  INFO     step 1357.000000 - time: 1.321767, loss: 0.759233, perplexity: 2.136637, precision: 0.328125, batch_len: 88.000000
Train, loss=0.75923318: 1358it [31:59,  1.34s/it]2017-06-01 18:41:31,162 root  INFO     step 1358.000000 - time: 1.118517, loss: 0.656642, perplexity: 1.928305, precision: 0.328125, batch_len: 104.000000
Train, loss=0.65664154: 1359it [32:01,  1.29s/it]2017-06-01 18:41:32,367 root  INFO     step 1359.000000 - time: 1.120029, loss: 0.838781, perplexity: 2.313546, precision: 0.218750, batch_len: 93.000000
Train, loss=0.83878142: 1360it [32:02,  1.26s/it]2017-06-01 18:41:33,428 root  INFO     step 1360.000000 - time: 1.047670, loss: 0.776079, perplexity: 2.172935, precision: 0.265625, batch_len: 90.000000
Train, loss=0.77607858: 1361it [32:03,  1.20s/it]2017-06-01 18:41:34,678 root  INFO     step 1361.000000 - time: 1.216688, loss: 0.722496, perplexity: 2.059568, precision: 0.250000, batch_len: 120.000000
Train, loss=0.72249633: 1362it [32:04,  1.22s/it]2017-06-01 18:41:35,954 root  INFO     step 1362.000000 - time: 1.239462, loss: 0.708524, perplexity: 2.030991, precision: 0.312500, batch_len: 97.000000
Train, loss=0.70852387: 1363it [32:05,  1.23s/it]2017-06-01 18:41:37,473 root  INFO     step 1363.000000 - time: 1.493955, loss: 0.673344, perplexity: 1.960783, precision: 0.296875, batch_len: 117.000000
Train, loss=0.67334414: 1364it [32:07,  1.32s/it]2017-06-01 18:41:38,872 root  INFO     step 1364.000000 - time: 1.367145, loss: 0.806700, perplexity: 2.240502, precision: 0.265625, batch_len: 121.000000
Train, loss=0.80669981: 1365it [32:08,  1.34s/it]2017-06-01 18:41:39,995 root  INFO     step 1365.000000 - time: 1.007332, loss: 0.540030, perplexity: 1.716059, precision: 0.281250, batch_len: 101.000000
Train, loss=0.54003036: 1366it [32:09,  1.28s/it]2017-06-01 18:41:41,090 root  INFO     step 1366.000000 - time: 1.028144, loss: 0.667226, perplexity: 1.948824, precision: 0.296875, batch_len: 112.000000
Train, loss=0.66722596: 1367it [32:10,  1.22s/it]2017-06-01 18:41:42,396 root  INFO     step 1367.000000 - time: 1.302952, loss: 0.706595, perplexity: 2.027077, precision: 0.328125, batch_len: 114.000000
Train, loss=0.70659494: 1368it [32:12,  1.25s/it]2017-06-01 18:41:43,707 root  INFO     step 1368.000000 - time: 1.267715, loss: 0.777761, perplexity: 2.176593, precision: 0.218750, batch_len: 109.000000
Train, loss=0.77776062: 1369it [32:13,  1.27s/it]2017-06-01 18:41:44,813 root  INFO     step 1369.000000 - time: 1.079046, loss: 0.751119, perplexity: 2.119369, precision: 0.437500, batch_len: 106.000000
Train, loss=0.75111854: 1370it [32:14,  1.22s/it]2017-06-01 18:41:45,939 root  INFO     step 1370.000000 - time: 1.066164, loss: 0.695643, perplexity: 2.004998, precision: 0.328125, batch_len: 85.000000
Train, loss=0.69564295: 1371it [32:15,  1.19s/it]2017-06-01 18:41:47,054 root  INFO     step 1371.000000 - time: 0.925679, loss: 0.689966, perplexity: 1.993647, precision: 0.546875, batch_len: 86.000000
Train, loss=0.68996572: 1372it [32:16,  1.17s/it]2017-06-01 18:41:47,987 root  INFO     step 1372.000000 - time: 0.909745, loss: 0.688631, perplexity: 1.990989, precision: 0.343750, batch_len: 100.000000
Train, loss=0.68863136: 1373it [32:17,  1.10s/it]2017-06-01 18:41:49,304 root  INFO     step 1373.000000 - time: 1.188385, loss: 0.773332, perplexity: 2.166974, precision: 0.390625, batch_len: 81.000000
Train, loss=0.77333182: 1374it [32:19,  1.16s/it]2017-06-01 18:41:50,726 root  INFO     step 1374.000000 - time: 1.404418, loss: 0.822420, perplexity: 2.276000, precision: 0.281250, batch_len: 91.000000
Train, loss=0.82241958: 1375it [32:20,  1.24s/it]2017-06-01 18:41:51,975 root  INFO     step 1375.000000 - time: 1.099557, loss: 0.850816, perplexity: 2.341557, precision: 0.250000, batch_len: 111.000000
Train, loss=0.85081595: 1376it [32:21,  1.24s/it]2017-06-01 18:41:53,107 root  INFO     step 1376.000000 - time: 1.034563, loss: 0.733840, perplexity: 2.083065, precision: 0.281250, batch_len: 89.000000
Train, loss=0.73384023: 1377it [32:22,  1.21s/it]2017-06-01 18:41:54,091 root  INFO     step 1377.000000 - time: 0.948082, loss: 0.668875, perplexity: 1.952040, precision: 0.125000, batch_len: 103.000000
Train, loss=0.66887498: 1378it [32:23,  1.14s/it]2017-06-01 18:41:55,170 root  INFO     step 1378.000000 - time: 1.038343, loss: 0.920308, perplexity: 2.510062, precision: 0.171875, batch_len: 83.000000
Train, loss=0.92030764: 1379it [32:25,  1.12s/it]2017-06-01 18:41:56,449 root  INFO     step 1379.000000 - time: 1.225542, loss: 0.817362, perplexity: 2.264518, precision: 0.390625, batch_len: 79.000000
Train, loss=0.81736201: 1380it [32:26,  1.17s/it]2017-06-01 18:41:57,940 root  INFO     step 1380.000000 - time: 1.303901, loss: 0.614388, perplexity: 1.848524, precision: 0.140625, batch_len: 129.000000
Train, loss=0.61438751: 1381it [32:27,  1.27s/it]2017-06-01 18:41:59,066 root  INFO     step 1381.000000 - time: 1.098981, loss: 0.694078, perplexity: 2.001862, precision: 0.218750, batch_len: 107.000000
Train, loss=0.69407761: 1382it [32:28,  1.22s/it]2017-06-01 18:42:00,421 root  INFO     step 1382.000000 - time: 1.298404, loss: 0.622304, perplexity: 1.863215, precision: 0.375000, batch_len: 116.000000
Train, loss=0.62230372: 1383it [32:30,  1.26s/it]2017-06-01 18:42:01,540 root  INFO     step 1383.000000 - time: 1.108571, loss: 0.793504, perplexity: 2.211130, precision: 0.187500, batch_len: 115.000000
Train, loss=0.79350388: 1384it [32:31,  1.22s/it]2017-06-01 18:42:02,781 root  INFO     step 1384.000000 - time: 1.070932, loss: 0.736320, perplexity: 2.088238, precision: 0.140625, batch_len: 87.000000
Train, loss=0.73632050: 1385it [32:32,  1.23s/it]2017-06-01 18:42:04,298 root  INFO     step 1385.000000 - time: 1.486963, loss: 0.608902, perplexity: 1.838411, precision: 0.187500, batch_len: 123.000000
Train, loss=0.60890174: 1386it [32:34,  1.31s/it]2017-06-01 18:42:05,421 root  INFO     step 1386.000000 - time: 1.091108, loss: 0.696070, perplexity: 2.005854, precision: 0.218750, batch_len: 102.000000
Train, loss=0.69606984: 1387it [32:35,  1.26s/it]2017-06-01 18:42:06,716 root  INFO     step 1387.000000 - time: 1.285653, loss: 0.516869, perplexity: 1.676769, precision: 0.390625, batch_len: 125.000000
Train, loss=0.51686865: 1388it [32:36,  1.27s/it]2017-06-01 18:42:07,708 root  INFO     step 1388.000000 - time: 0.899610, loss: 0.691351, perplexity: 1.996411, precision: 0.343750, batch_len: 94.000000
Train, loss=0.69135106: 1389it [32:37,  1.19s/it]2017-06-01 18:42:09,193 root  INFO     step 1389.000000 - time: 1.474304, loss: 0.601925, perplexity: 1.825630, precision: 0.328125, batch_len: 118.000000
Train, loss=0.60192525: 1390it [32:39,  1.28s/it]2017-06-01 18:42:10,783 root  INFO     step 1390.000000 - time: 1.567685, loss: 0.604903, perplexity: 1.831074, precision: 0.375000, batch_len: 144.000000
Train, loss=0.60490263: 1391it [32:40,  1.37s/it]2017-06-01 18:42:12,261 root  INFO     step 1391.000000 - time: 1.396506, loss: 0.960112, perplexity: 2.611988, precision: 0.140625, batch_len: 130.000000
Train, loss=0.96011174: 1392it [32:42,  1.40s/it]2017-06-01 18:42:13,271 root  INFO     step 1392.000000 - time: 0.936043, loss: 0.584487, perplexity: 1.794070, precision: 0.484375, batch_len: 82.000000
Train, loss=0.58448696: 1393it [32:43,  1.28s/it]2017-06-01 18:42:14,302 root  INFO     step 1393.000000 - time: 0.914277, loss: 0.647770, perplexity: 1.911274, precision: 0.421875, batch_len: 80.000000
Train, loss=0.64777023: 1394it [32:44,  1.21s/it]2017-06-01 18:42:15,610 root  INFO     step 1394.000000 - time: 1.284883, loss: 0.814175, perplexity: 2.257312, precision: 0.234375, batch_len: 99.000000
Train, loss=0.81417459: 1395it [32:45,  1.24s/it]2017-06-01 18:42:17,267 root  INFO     step 1395.000000 - time: 1.613540, loss: 0.870404, perplexity: 2.387876, precision: 0.203125, batch_len: 137.000000
Train, loss=0.87040442: 1396it [32:47,  1.36s/it]2017-06-01 18:42:18,686 root  INFO     step 1396.000000 - time: 1.398901, loss: 0.825008, perplexity: 2.281899, precision: 0.250000, batch_len: 126.000000
Train, loss=0.82500792: 1397it [32:48,  1.38s/it]2017-06-01 18:42:19,870 root  INFO     step 1397.000000 - time: 1.049768, loss: 1.295723, perplexity: 3.653636, precision: 0.046875, batch_len: 98.000000
Train, loss=1.29572296: 1398it [32:49,  1.32s/it]2017-06-01 18:42:21,306 root  INFO     step 1398.000000 - time: 1.285187, loss: 2.165133, perplexity: 8.715761, precision: 0.015625, batch_len: 133.000000
Train, loss=2.16513300: 1399it [32:51,  1.36s/it]2017-06-01 18:42:23,178 root  INFO     step 1399.000000 - time: 1.695729, loss: 0.896223, perplexity: 2.450331, precision: 0.062500, batch_len: 119.000000
Train, loss=0.89622307: 1400it [32:53,  1.51s/it]2017-06-01 18:42:24,227 root  INFO     step 1400.000000 - time: 0.996779, loss: 0.886973, perplexity: 2.427769, precision: 0.328125, batch_len: 84.000000
Train, loss=0.88697255: 1401it [32:54,  1.37s/it]2017-06-01 18:42:25,776 root  INFO     step 1401.000000 - time: 1.340025, loss: 0.808766, perplexity: 2.245135, precision: 0.109375, batch_len: 132.000000
Train, loss=0.80876577: 1402it [32:55,  1.43s/it]2017-06-01 18:42:26,901 root  INFO     step 1402.000000 - time: 1.056817, loss: 0.770290, perplexity: 2.160393, precision: 0.203125, batch_len: 78.000000
Train, loss=0.77029002: 1403it [32:56,  1.34s/it]2017-06-01 18:42:28,877 root  INFO     step 1403.000000 - time: 1.930877, loss: 0.822036, perplexity: 2.275127, precision: 0.078125, batch_len: 152.000000
Train, loss=0.82203609: 1404it [32:58,  1.53s/it]2017-06-01 18:42:30,340 root  INFO     step 1404.000000 - time: 1.420679, loss: 0.814640, perplexity: 2.258362, precision: 0.031250, batch_len: 122.000000
Train, loss=0.81463993: 1405it [33:00,  1.51s/it]2017-06-01 18:42:31,950 root  INFO     step 1405.000000 - time: 1.553518, loss: 0.807515, perplexity: 2.242329, precision: 0.125000, batch_len: 135.000000
Train, loss=0.80751503: 1406it [33:01,  1.54s/it]2017-06-01 18:42:33,478 root  INFO     step 1406.000000 - time: 1.467890, loss: 0.623495, perplexity: 1.865437, precision: 0.234375, batch_len: 136.000000
Train, loss=0.62349510: 1407it [33:03,  1.54s/it]2017-06-01 18:42:34,735 root  INFO     step 1407.000000 - time: 1.062967, loss: 0.503284, perplexity: 1.654144, precision: 0.375000, batch_len: 96.000000
Train, loss=0.50328356: 1408it [33:04,  1.45s/it]2017-06-01 18:42:35,705 root  INFO     step 1408.000000 - time: 0.870836, loss: 0.366928, perplexity: 1.443294, precision: 0.437500, batch_len: 74.000000
Train, loss=0.36692795: 1409it [33:05,  1.31s/it]2017-06-01 18:42:37,487 root  INFO     step 1409.000000 - time: 1.574192, loss: 0.704822, perplexity: 2.023486, precision: 0.109375, batch_len: 139.000000
Train, loss=0.70482177: 1410it [33:07,  1.45s/it]2017-06-01 18:42:38,655 root  INFO     step 1410.000000 - time: 1.114239, loss: 0.663779, perplexity: 1.942118, precision: 0.359375, batch_len: 76.000000
Train, loss=0.66377890: 1411it [33:08,  1.37s/it]2017-06-01 18:42:40,080 root  INFO     step 1411.000000 - time: 1.343386, loss: 0.559966, perplexity: 1.750613, precision: 0.203125, batch_len: 134.000000
Train, loss=0.55996615: 1412it [33:09,  1.38s/it]2017-06-01 18:42:41,540 root  INFO     step 1412.000000 - time: 1.402282, loss: 0.622450, perplexity: 1.863489, precision: 0.312500, batch_len: 141.000000
Train, loss=0.62245029: 1413it [33:11,  1.41s/it]2017-06-01 18:42:42,982 root  INFO     step 1413.000000 - time: 1.321841, loss: 0.552296, perplexity: 1.737237, precision: 0.234375, batch_len: 138.000000
Train, loss=0.55229574: 1414it [33:12,  1.42s/it]2017-06-01 18:42:45,104 root  INFO     step 1414.000000 - time: 2.111939, loss: 0.908844, perplexity: 2.481452, precision: 0.062500, batch_len: 150.000000
Train, loss=0.90884387: 1415it [33:14,  1.63s/it]2017-06-01 18:42:46,038 root  INFO     step 1415.000000 - time: 0.925460, loss: 0.838553, perplexity: 2.313017, precision: 0.296875, batch_len: 77.000000
Train, loss=0.83855289: 1416it [33:15,  1.42s/it]2017-06-01 18:42:47,229 root  INFO     step 1416.000000 - time: 1.165407, loss: 0.423156, perplexity: 1.526772, precision: 0.484375, batch_len: 72.000000
Train, loss=0.42315599: 1417it [33:17,  1.35s/it]2017-06-01 18:42:48,304 root  INFO     step 1417.000000 - time: 0.855785, loss: 0.425486, perplexity: 1.530333, precision: 0.484375, batch_len: 71.000000
Train, loss=0.42548552: 1418it [33:18,  1.27s/it]2017-06-01 18:42:49,904 root  INFO     step 1418.000000 - time: 1.380561, loss: 0.727848, perplexity: 2.070620, precision: 0.109375, batch_len: 131.000000
Train, loss=0.72784805: 1419it [33:19,  1.37s/it]2017-06-01 18:42:51,558 root  INFO     step 1419.000000 - time: 1.648713, loss: 0.809889, perplexity: 2.247658, precision: 0.093750, batch_len: 142.000000
Train, loss=0.80988884: 1420it [33:21,  1.45s/it]2017-06-01 18:42:51,700 root  INFO     Generating first batch)
2017-06-01 18:42:55,125 root  INFO     step 1420.000000 - time: 1.196792, loss: 0.659121, perplexity: 1.933092, precision: 0.359375, batch_len: 96.000000
Train, loss=0.65912056: 1421it [33:25,  2.09s/it]2017-06-01 18:42:56,905 root  INFO     step 1421.000000 - time: 1.285044, loss: 0.927920, perplexity: 2.529242, precision: 0.218750, batch_len: 111.000000
Train, loss=0.92791963: 1422it [33:26,  2.00s/it]2017-06-01 18:42:58,184 root  INFO     step 1422.000000 - time: 1.273542, loss: 0.764764, perplexity: 2.148487, precision: 0.203125, batch_len: 128.000000
Train, loss=0.76476395: 1423it [33:28,  1.78s/it]2017-06-01 18:42:59,243 root  INFO     step 1423.000000 - time: 1.012272, loss: 0.791604, perplexity: 2.206933, precision: 0.187500, batch_len: 105.000000
Train, loss=0.79160392: 1424it [33:29,  1.56s/it]2017-06-01 18:43:00,281 root  INFO     step 1424.000000 - time: 0.938951, loss: 0.526491, perplexity: 1.692982, precision: 0.328125, batch_len: 101.000000
Train, loss=0.52649140: 1425it [33:30,  1.41s/it]2017-06-01 18:43:01,412 root  INFO     step 1425.000000 - time: 1.085579, loss: 0.587338, perplexity: 1.799192, precision: 0.343750, batch_len: 110.000000
Train, loss=0.58733767: 1426it [33:31,  1.32s/it]2017-06-01 18:43:02,907 root  INFO     step 1426.000000 - time: 1.306298, loss: 0.671641, perplexity: 1.957446, precision: 0.343750, batch_len: 93.000000
Train, loss=0.67164063: 1427it [33:32,  1.38s/it]2017-06-01 18:43:03,986 root  INFO     step 1427.000000 - time: 1.012265, loss: 0.532157, perplexity: 1.702602, precision: 0.375000, batch_len: 104.000000
Train, loss=0.53215742: 1428it [33:33,  1.29s/it]2017-06-01 18:43:05,124 root  INFO     step 1428.000000 - time: 1.092178, loss: 0.550270, perplexity: 1.733721, precision: 0.375000, batch_len: 92.000000
Train, loss=0.55027008: 1429it [33:35,  1.24s/it]2017-06-01 18:43:06,302 root  INFO     step 1429.000000 - time: 1.075820, loss: 1.129417, perplexity: 3.093851, precision: 0.218750, batch_len: 113.000000
Train, loss=1.12941670: 1430it [33:36,  1.22s/it]2017-06-01 18:43:07,520 root  INFO     step 1430.000000 - time: 1.078431, loss: 1.418693, perplexity: 4.131717, precision: 0.031250, batch_len: 112.000000
Train, loss=1.41869295: 1431it [33:37,  1.22s/it]2017-06-01 18:43:08,775 root  INFO     step 1431.000000 - time: 1.223473, loss: 0.848061, perplexity: 2.335114, precision: 0.234375, batch_len: 91.000000
Train, loss=0.84806073: 1432it [33:38,  1.23s/it]2017-06-01 18:43:10,037 root  INFO     step 1432.000000 - time: 1.208557, loss: 0.743500, perplexity: 2.103284, precision: 0.359375, batch_len: 89.000000
Train, loss=0.74350011: 1433it [33:39,  1.24s/it]2017-06-01 18:43:11,183 root  INFO     step 1433.000000 - time: 1.130418, loss: 0.541086, perplexity: 1.717871, precision: 0.375000, batch_len: 97.000000
Train, loss=0.54108560: 1434it [33:41,  1.21s/it]2017-06-01 18:43:12,317 root  INFO     step 1434.000000 - time: 1.067729, loss: 0.630416, perplexity: 1.878391, precision: 0.359375, batch_len: 109.000000
Train, loss=0.63041556: 1435it [33:42,  1.19s/it]2017-06-01 18:43:13,466 root  INFO     step 1435.000000 - time: 1.040961, loss: 0.508134, perplexity: 1.662186, precision: 0.437500, batch_len: 103.000000
Train, loss=0.50813377: 1436it [33:43,  1.18s/it]2017-06-01 18:43:14,486 root  INFO     step 1436.000000 - time: 0.988637, loss: 0.612756, perplexity: 1.845511, precision: 0.515625, batch_len: 88.000000
Train, loss=0.61275637: 1437it [33:44,  1.13s/it]2017-06-01 18:43:15,644 root  INFO     step 1437.000000 - time: 1.127434, loss: 0.496073, perplexity: 1.642260, precision: 0.515625, batch_len: 86.000000
Train, loss=0.49607313: 1438it [33:45,  1.14s/it]2017-06-01 18:43:16,897 root  INFO     step 1438.000000 - time: 1.175710, loss: 0.646062, perplexity: 1.908011, precision: 0.390625, batch_len: 100.000000
Train, loss=0.64606154: 1439it [33:46,  1.17s/it]2017-06-01 18:43:18,308 root  INFO     step 1439.000000 - time: 1.362695, loss: 0.555542, perplexity: 1.742885, precision: 0.359375, batch_len: 120.000000
Train, loss=0.55554205: 1440it [33:48,  1.24s/it]2017-06-01 18:43:19,444 root  INFO     step 1440.000000 - time: 1.068521, loss: 0.483519, perplexity: 1.621771, precision: 0.484375, batch_len: 108.000000
Train, loss=0.48351863: 1441it [33:49,  1.21s/it]2017-06-01 18:43:21,139 root  INFO     step 1441.000000 - time: 1.551142, loss: 0.576023, perplexity: 1.778950, precision: 0.468750, batch_len: 90.000000
Train, loss=0.57602334: 1442it [33:51,  1.36s/it]2017-06-01 18:43:22,245 root  INFO     step 1442.000000 - time: 1.091474, loss: 0.507517, perplexity: 1.661161, precision: 0.437500, batch_len: 80.000000
Train, loss=0.50751686: 1443it [33:52,  1.28s/it]2017-06-01 18:43:23,559 root  INFO     step 1443.000000 - time: 1.281124, loss: 0.833738, perplexity: 2.301907, precision: 0.218750, batch_len: 114.000000
Train, loss=0.83373809: 1444it [33:53,  1.29s/it]2017-06-01 18:43:24,875 root  INFO     step 1444.000000 - time: 1.291359, loss: 0.882214, perplexity: 2.416243, precision: 0.109375, batch_len: 124.000000
Train, loss=0.88221371: 1445it [33:54,  1.30s/it]2017-06-01 18:43:25,939 root  INFO     step 1445.000000 - time: 1.043005, loss: 1.142279, perplexity: 3.133904, precision: 0.250000, batch_len: 85.000000
Train, loss=1.14227939: 1446it [33:55,  1.23s/it]2017-06-01 18:43:27,041 root  INFO     step 1446.000000 - time: 1.087177, loss: 0.618208, perplexity: 1.855599, precision: 0.281250, batch_len: 102.000000
Train, loss=0.61820751: 1447it [33:56,  1.19s/it]2017-06-01 18:43:28,566 root  INFO     step 1447.000000 - time: 1.488885, loss: 0.579470, perplexity: 1.785093, precision: 0.421875, batch_len: 117.000000
Train, loss=0.57947028: 1448it [33:58,  1.29s/it]2017-06-01 18:43:29,614 root  INFO     step 1448.000000 - time: 0.978248, loss: 0.668506, perplexity: 1.951320, precision: 0.328125, batch_len: 115.000000
Train, loss=0.66850615: 1449it [33:59,  1.22s/it]2017-06-01 18:43:30,770 root  INFO     step 1449.000000 - time: 1.142512, loss: 0.705315, perplexity: 2.024484, precision: 0.328125, batch_len: 79.000000
Train, loss=0.70531476: 1450it [34:00,  1.20s/it]2017-06-01 18:43:32,019 root  INFO     step 1450.000000 - time: 1.243272, loss: 0.924255, perplexity: 2.519990, precision: 0.156250, batch_len: 107.000000
Train, loss=0.92425478: 1451it [34:01,  1.21s/it]2017-06-01 18:43:33,013 root  INFO     step 1451.000000 - time: 0.979219, loss: 0.751975, perplexity: 2.121186, precision: 0.250000, batch_len: 81.000000
Train, loss=0.75197518: 1452it [34:02,  1.15s/it]2017-06-01 18:43:34,536 root  INFO     step 1452.000000 - time: 1.430531, loss: 0.843156, perplexity: 2.323688, precision: 0.250000, batch_len: 121.000000
Train, loss=0.84315562: 1453it [34:04,  1.26s/it]2017-06-01 18:43:35,625 root  INFO     step 1453.000000 - time: 0.918227, loss: 0.610673, perplexity: 1.841670, precision: 0.390625, batch_len: 94.000000
Train, loss=0.61067253: 1454it [34:05,  1.21s/it]2017-06-01 18:43:37,038 root  INFO     step 1454.000000 - time: 1.397737, loss: 0.761587, perplexity: 2.141673, precision: 0.140625, batch_len: 125.000000
Train, loss=0.76158738: 1455it [34:06,  1.27s/it]2017-06-01 18:43:38,348 root  INFO     step 1455.000000 - time: 1.269702, loss: 0.772094, perplexity: 2.164294, precision: 0.093750, batch_len: 106.000000
Train, loss=0.77209425: 1456it [34:08,  1.28s/it]2017-06-01 18:43:39,945 root  INFO     step 1456.000000 - time: 1.310547, loss: 0.614330, perplexity: 1.848418, precision: 0.296875, batch_len: 116.000000
Train, loss=0.61433017: 1457it [34:09,  1.38s/it]2017-06-01 18:43:41,019 root  INFO     step 1457.000000 - time: 0.957788, loss: 0.576869, perplexity: 1.780454, precision: 0.328125, batch_len: 87.000000
Train, loss=0.57686853: 1458it [34:10,  1.29s/it]2017-06-01 18:43:42,255 root  INFO     step 1458.000000 - time: 1.220146, loss: 0.639420, perplexity: 1.895382, precision: 0.171875, batch_len: 123.000000
Train, loss=0.63942039: 1459it [34:12,  1.27s/it]2017-06-01 18:43:43,823 root  INFO     step 1459.000000 - time: 1.526646, loss: 0.652910, perplexity: 1.921123, precision: 0.171875, batch_len: 126.000000
Train, loss=0.65290993: 1460it [34:13,  1.36s/it]2017-06-01 18:43:44,946 root  INFO     step 1460.000000 - time: 1.110374, loss: 0.696284, perplexity: 2.006284, precision: 0.437500, batch_len: 83.000000
Train, loss=0.69628417: 1461it [34:14,  1.29s/it]2017-06-01 18:43:46,591 root  INFO     step 1461.000000 - time: 1.339358, loss: 0.471450, perplexity: 1.602316, precision: 0.375000, batch_len: 118.000000
Train, loss=0.47144988: 1462it [34:16,  1.40s/it]2017-06-01 18:43:47,525 root  INFO     step 1462.000000 - time: 0.925742, loss: 0.611512, perplexity: 1.843216, precision: 0.312500, batch_len: 82.000000
Train, loss=0.61151189: 1463it [34:17,  1.26s/it]2017-06-01 18:43:48,634 root  INFO     step 1463.000000 - time: 1.080920, loss: 0.483233, perplexity: 1.621307, precision: 0.468750, batch_len: 98.000000
Train, loss=0.48323277: 1464it [34:18,  1.21s/it]2017-06-01 18:43:50,188 root  INFO     step 1464.000000 - time: 1.537232, loss: 0.486307, perplexity: 1.626300, precision: 0.375000, batch_len: 119.000000
Train, loss=0.48630744: 1465it [34:20,  1.32s/it]2017-06-01 18:43:51,459 root  INFO     step 1465.000000 - time: 1.250803, loss: 0.578210, perplexity: 1.782845, precision: 0.437500, batch_len: 84.000000
Train, loss=0.57821041: 1466it [34:21,  1.30s/it]2017-06-01 18:43:52,792 root  INFO     step 1466.000000 - time: 1.295867, loss: 0.529470, perplexity: 1.698033, precision: 0.328125, batch_len: 129.000000
Train, loss=0.52947044: 1467it [34:22,  1.31s/it]2017-06-01 18:43:54,367 root  INFO     step 1467.000000 - time: 1.429324, loss: 0.541969, perplexity: 1.719389, precision: 0.281250, batch_len: 144.000000
Train, loss=0.54196888: 1468it [34:24,  1.39s/it]2017-06-01 18:43:55,480 root  INFO     step 1468.000000 - time: 0.990195, loss: 0.566628, perplexity: 1.762315, precision: 0.296875, batch_len: 99.000000
Train, loss=0.56662804: 1469it [34:25,  1.31s/it]2017-06-01 18:43:57,055 root  INFO     step 1469.000000 - time: 1.544827, loss: 0.523530, perplexity: 1.687975, precision: 0.265625, batch_len: 133.000000
Train, loss=0.52352977: 1470it [34:26,  1.39s/it]2017-06-01 18:43:58,679 root  INFO     step 1470.000000 - time: 1.584818, loss: 0.597107, perplexity: 1.816854, precision: 0.250000, batch_len: 136.000000
Train, loss=0.59710658: 1471it [34:28,  1.46s/it]2017-06-01 18:44:00,035 root  INFO     step 1471.000000 - time: 1.344560, loss: 0.646778, perplexity: 1.909379, precision: 0.140625, batch_len: 135.000000
Train, loss=0.64677787: 1472it [34:29,  1.43s/it]2017-06-01 18:44:01,415 root  INFO     step 1472.000000 - time: 1.352352, loss: 0.720949, perplexity: 2.056383, precision: 0.093750, batch_len: 130.000000
Train, loss=0.72094858: 1473it [34:31,  1.41s/it]2017-06-01 18:44:03,163 root  INFO     step 1473.000000 - time: 1.391031, loss: 0.751644, perplexity: 2.120484, precision: 0.203125, batch_len: 137.000000
Train, loss=0.75164425: 1474it [34:33,  1.51s/it]2017-06-01 18:44:04,898 root  INFO     step 1474.000000 - time: 1.659875, loss: 0.698239, perplexity: 2.010210, precision: 0.109375, batch_len: 138.000000
Train, loss=0.69823897: 1475it [34:34,  1.58s/it]2017-06-01 18:44:06,028 root  INFO     step 1475.000000 - time: 1.007634, loss: 0.690052, perplexity: 1.993819, precision: 0.406250, batch_len: 78.000000
Train, loss=0.69005203: 1476it [34:35,  1.44s/it]2017-06-01 18:44:07,297 root  INFO     step 1476.000000 - time: 1.074416, loss: 0.595053, perplexity: 1.813127, precision: 0.343750, batch_len: 96.000000
Train, loss=0.59505308: 1477it [34:37,  1.39s/it]2017-06-01 18:44:08,636 root  INFO     step 1477.000000 - time: 1.223095, loss: 0.767392, perplexity: 2.154142, precision: 0.296875, batch_len: 122.000000
Train, loss=0.76739240: 1478it [34:38,  1.38s/it]2017-06-01 18:44:10,142 root  INFO     step 1478.000000 - time: 1.453509, loss: 0.996099, perplexity: 2.707698, precision: 0.156250, batch_len: 132.000000
Train, loss=0.99609876: 1479it [34:40,  1.42s/it]2017-06-01 18:44:11,659 root  INFO     step 1479.000000 - time: 1.495376, loss: 1.477759, perplexity: 4.383112, precision: 0.062500, batch_len: 134.000000
Train, loss=1.47775888: 1480it [34:41,  1.45s/it]2017-06-01 18:44:13,592 root  INFO     step 1480.000000 - time: 1.785581, loss: 1.199330, perplexity: 3.317893, precision: 0.015625, batch_len: 150.000000
Train, loss=1.19932997: 1481it [34:43,  1.59s/it]2017-06-01 18:44:14,825 root  INFO     step 1481.000000 - time: 1.137256, loss: 0.571661, perplexity: 1.771207, precision: 0.343750, batch_len: 72.000000
Train, loss=0.57166111: 1482it [34:44,  1.48s/it]2017-06-01 18:44:15,709 root  INFO     step 1482.000000 - time: 0.859045, loss: 0.675028, perplexity: 1.964089, precision: 0.375000, batch_len: 76.000000
Train, loss=0.67502832: 1483it [34:45,  1.30s/it]2017-06-01 18:44:17,896 root  INFO     step 1483.000000 - time: 2.179812, loss: 1.014280, perplexity: 2.757377, precision: 0.062500, batch_len: 152.000000
Train, loss=1.01427984: 1484it [34:47,  1.57s/it]2017-06-01 18:44:19,431 root  INFO     step 1484.000000 - time: 1.420136, loss: 0.961017, perplexity: 2.614354, precision: 0.015625, batch_len: 139.000000
Train, loss=0.96101701: 1485it [34:49,  1.56s/it]2017-06-01 18:44:20,882 root  INFO     step 1485.000000 - time: 1.434957, loss: 0.737711, perplexity: 2.091144, precision: 0.109375, batch_len: 142.000000
Train, loss=0.73771143: 1486it [34:50,  1.53s/it]2017-06-01 18:44:22,217 root  INFO     step 1486.000000 - time: 1.298658, loss: 0.825494, perplexity: 2.283008, precision: 0.156250, batch_len: 141.000000
Train, loss=0.82549405: 1487it [34:52,  1.47s/it]2017-06-01 18:44:23,342 root  INFO     step 1487.000000 - time: 1.065082, loss: 0.499173, perplexity: 1.647359, precision: 0.421875, batch_len: 71.000000
Train, loss=0.49917319: 1488it [34:53,  1.37s/it]2017-06-01 18:44:24,633 root  INFO     step 1488.000000 - time: 1.233185, loss: 0.404519, perplexity: 1.498582, precision: 0.546875, batch_len: 77.000000
Train, loss=0.40451920: 1489it [34:54,  1.34s/it]2017-06-01 18:44:25,696 root  INFO     step 1489.000000 - time: 0.951712, loss: 0.263678, perplexity: 1.301709, precision: 0.734375, batch_len: 74.000000
Train, loss=0.26367813: 1490it [34:55,  1.26s/it]2017-06-01 18:44:27,216 root  INFO     step 1490.000000 - time: 1.468574, loss: 0.692884, perplexity: 1.999474, precision: 0.093750, batch_len: 131.000000
Train, loss=0.69288403: 1491it [34:57,  1.34s/it]2017-06-01 18:44:27,437 root  INFO     Generating first batch)
2017-06-01 18:44:31,763 root  INFO     step 1491.000000 - time: 1.213118, loss: 0.635534, perplexity: 1.888031, precision: 0.218750, batch_len: 92.000000
Train, loss=0.63553435: 1492it [35:01,  2.30s/it]2017-06-01 18:44:32,959 root  INFO     step 1492.000000 - time: 1.147044, loss: 0.460681, perplexity: 1.585153, precision: 0.328125, batch_len: 96.000000
Train, loss=0.46068072: 1493it [35:02,  1.97s/it]2017-06-01 18:44:34,341 root  INFO     step 1493.000000 - time: 1.292359, loss: 0.592061, perplexity: 1.807710, precision: 0.281250, batch_len: 112.000000
Train, loss=0.59206104: 1494it [35:04,  1.79s/it]2017-06-01 18:44:35,433 root  INFO     step 1494.000000 - time: 0.946224, loss: 0.621511, perplexity: 1.861739, precision: 0.437500, batch_len: 88.000000
Train, loss=0.62151086: 1495it [35:05,  1.58s/it]2017-06-01 18:44:36,749 root  INFO     step 1495.000000 - time: 1.172698, loss: 0.734490, perplexity: 2.084418, precision: 0.265625, batch_len: 111.000000
Train, loss=0.73448956: 1496it [35:06,  1.50s/it]2017-06-01 18:44:38,257 root  INFO     step 1496.000000 - time: 1.456609, loss: 0.577136, perplexity: 1.780930, precision: 0.328125, batch_len: 128.000000
Train, loss=0.57713580: 1497it [35:08,  1.50s/it]2017-06-01 18:44:39,614 root  INFO     step 1497.000000 - time: 1.351765, loss: 0.549713, perplexity: 1.732756, precision: 0.343750, batch_len: 120.000000
Train, loss=0.54971331: 1498it [35:09,  1.46s/it]2017-06-01 18:44:40,712 root  INFO     step 1498.000000 - time: 1.014680, loss: 0.631601, perplexity: 1.880619, precision: 0.468750, batch_len: 90.000000
Train, loss=0.63160086: 1499it [35:10,  1.35s/it]2017-06-01 18:44:41,710 root  INFO     step 1499.000000 - time: 0.983773, loss: 0.525844, perplexity: 1.691887, precision: 0.468750, batch_len: 101.000000
Train, loss=0.52584434: 1500it [35:11,  1.25s/it]2017-06-01 18:44:42,816 root  INFO     step 1500.000000 - time: 1.080124, loss: 0.549661, perplexity: 1.732666, precision: 0.375000, batch_len: 113.000000
Train, loss=0.54966140: 1501it [35:12,  1.20s/it]2017-06-01 18:44:44,193 root  INFO     step 1501.000000 - time: 1.260758, loss: 0.563688, perplexity: 1.757141, precision: 0.359375, batch_len: 108.000000
Train, loss=0.56368828: 1502it [35:14,  1.26s/it]2017-06-01 18:44:45,373 root  INFO     step 1502.000000 - time: 1.101716, loss: 0.718180, perplexity: 2.050698, precision: 0.296875, batch_len: 93.000000
Train, loss=0.71818006: 1503it [35:15,  1.23s/it]2017-06-01 18:44:46,437 root  INFO     step 1503.000000 - time: 0.943282, loss: 0.925460, perplexity: 2.523027, precision: 0.171875, batch_len: 86.000000
Train, loss=0.92545950: 1504it [35:16,  1.18s/it]2017-06-01 18:44:47,528 root  INFO     step 1504.000000 - time: 1.010951, loss: 0.778000, perplexity: 2.177114, precision: 0.250000, batch_len: 100.000000
Train, loss=0.77800000: 1505it [35:17,  1.15s/it]2017-06-01 18:44:48,541 root  INFO     step 1505.000000 - time: 0.971432, loss: 1.206695, perplexity: 3.342421, precision: 0.140625, batch_len: 89.000000
Train, loss=1.20669544: 1506it [35:18,  1.11s/it]2017-06-01 18:44:49,722 root  INFO     step 1506.000000 - time: 1.162809, loss: 1.121033, perplexity: 3.068023, precision: 0.078125, batch_len: 102.000000
Train, loss=1.12103343: 1507it [35:19,  1.13s/it]2017-06-01 18:44:51,151 root  INFO     step 1507.000000 - time: 1.342824, loss: 1.116971, perplexity: 3.055584, precision: 0.062500, batch_len: 104.000000
Train, loss=1.11697078: 1508it [35:21,  1.22s/it]2017-06-01 18:44:52,204 root  INFO     step 1508.000000 - time: 1.042134, loss: 0.816922, perplexity: 2.263522, precision: 0.296875, batch_len: 91.000000
Train, loss=0.81692207: 1509it [35:22,  1.17s/it]2017-06-01 18:44:53,551 root  INFO     step 1509.000000 - time: 1.328987, loss: 0.515522, perplexity: 1.674512, precision: 0.437500, batch_len: 117.000000
Train, loss=0.51552153: 1510it [35:23,  1.22s/it]2017-06-01 18:44:54,667 root  INFO     step 1510.000000 - time: 1.053179, loss: 0.577336, perplexity: 1.781287, precision: 0.421875, batch_len: 97.000000
Train, loss=0.57733619: 1511it [35:24,  1.19s/it]2017-06-01 18:44:55,651 root  INFO     step 1511.000000 - time: 0.918048, loss: 0.711746, perplexity: 2.037545, precision: 0.265625, batch_len: 79.000000
Train, loss=0.71174556: 1512it [35:25,  1.13s/it]2017-06-01 18:44:56,976 root  INFO     step 1512.000000 - time: 1.310469, loss: 0.828038, perplexity: 2.288823, precision: 0.187500, batch_len: 110.000000
Train, loss=0.82803792: 1513it [35:26,  1.19s/it]2017-06-01 18:44:58,312 root  INFO     step 1513.000000 - time: 1.269303, loss: 0.673725, perplexity: 1.961531, precision: 0.281250, batch_len: 114.000000
Train, loss=0.67372537: 1514it [35:28,  1.23s/it]2017-06-01 18:44:59,753 root  INFO     step 1514.000000 - time: 1.417876, loss: 0.520761, perplexity: 1.683308, precision: 0.296875, batch_len: 121.000000
Train, loss=0.52076066: 1515it [35:29,  1.29s/it]2017-06-01 18:45:00,754 root  INFO     step 1515.000000 - time: 0.972199, loss: 0.534842, perplexity: 1.707178, precision: 0.468750, batch_len: 81.000000
Train, loss=0.53484190: 1516it [35:30,  1.21s/it]2017-06-01 18:45:01,843 root  INFO     step 1516.000000 - time: 1.054872, loss: 0.555893, perplexity: 1.743497, precision: 0.359375, batch_len: 106.000000
Train, loss=0.55589283: 1517it [35:31,  1.17s/it]2017-06-01 18:45:03,452 root  INFO     step 1517.000000 - time: 1.522615, loss: 0.568829, perplexity: 1.766198, precision: 0.203125, batch_len: 123.000000
Train, loss=0.56882942: 1518it [35:33,  1.30s/it]2017-06-01 18:45:04,930 root  INFO     step 1518.000000 - time: 1.284061, loss: 0.797471, perplexity: 2.219920, precision: 0.046875, batch_len: 115.000000
Train, loss=0.79747105: 1519it [35:34,  1.36s/it]2017-06-01 18:45:06,084 root  INFO     step 1519.000000 - time: 1.119764, loss: 0.965245, perplexity: 2.625430, precision: 0.140625, batch_len: 109.000000
Train, loss=0.96524477: 1520it [35:35,  1.29s/it]2017-06-01 18:45:07,157 root  INFO     step 1520.000000 - time: 1.067997, loss: 0.720830, perplexity: 2.056140, precision: 0.093750, batch_len: 103.000000
Train, loss=0.72083044: 1521it [35:37,  1.23s/it]2017-06-01 18:45:08,321 root  INFO     step 1521.000000 - time: 1.061899, loss: 0.676500, perplexity: 1.966981, precision: 0.406250, batch_len: 107.000000
Train, loss=0.67649972: 1522it [35:38,  1.21s/it]2017-06-01 18:45:09,297 root  INFO     step 1522.000000 - time: 0.874303, loss: 0.676361, perplexity: 1.966708, precision: 0.406250, batch_len: 85.000000
Train, loss=0.67636108: 1523it [35:39,  1.14s/it]2017-06-01 18:45:10,422 root  INFO     step 1523.000000 - time: 1.095997, loss: 0.548389, perplexity: 1.730463, precision: 0.375000, batch_len: 83.000000
Train, loss=0.54838890: 1524it [35:40,  1.13s/it]2017-06-01 18:45:11,725 root  INFO     step 1524.000000 - time: 1.291706, loss: 0.503555, perplexity: 1.654593, precision: 0.406250, batch_len: 98.000000
Train, loss=0.50355506: 1525it [35:41,  1.19s/it]2017-06-01 18:45:12,856 root  INFO     step 1525.000000 - time: 1.012675, loss: 0.474877, perplexity: 1.607817, precision: 0.453125, batch_len: 105.000000
Train, loss=0.47487715: 1526it [35:42,  1.17s/it]2017-06-01 18:45:14,182 root  INFO     step 1526.000000 - time: 1.267727, loss: 0.534335, perplexity: 1.706314, precision: 0.296875, batch_len: 124.000000
Train, loss=0.53433549: 1527it [35:44,  1.22s/it]2017-06-01 18:45:15,441 root  INFO     step 1527.000000 - time: 1.241741, loss: 0.416162, perplexity: 1.516131, precision: 0.421875, batch_len: 118.000000
Train, loss=0.41616151: 1528it [35:45,  1.23s/it]2017-06-01 18:45:16,485 root  INFO     step 1528.000000 - time: 1.019817, loss: 0.381157, perplexity: 1.463977, precision: 0.593750, batch_len: 87.000000
Train, loss=0.38115686: 1529it [35:46,  1.17s/it]2017-06-01 18:45:18,117 root  INFO     step 1529.000000 - time: 1.551024, loss: 0.428850, perplexity: 1.535491, precision: 0.468750, batch_len: 116.000000
Train, loss=0.42884988: 1530it [35:47,  1.31s/it]2017-06-01 18:45:19,528 root  INFO     step 1530.000000 - time: 1.279931, loss: 0.450857, perplexity: 1.569657, precision: 0.421875, batch_len: 125.000000
Train, loss=0.45085692: 1531it [35:49,  1.34s/it]2017-06-01 18:45:20,972 root  INFO     step 1531.000000 - time: 1.412680, loss: 0.685174, perplexity: 1.984117, precision: 0.093750, batch_len: 144.000000
Train, loss=0.68517423: 1532it [35:50,  1.37s/it]2017-06-01 18:45:22,015 root  INFO     step 1532.000000 - time: 0.921025, loss: 0.810797, perplexity: 2.249700, precision: 0.109375, batch_len: 94.000000
Train, loss=0.81079692: 1533it [35:51,  1.27s/it]2017-06-01 18:45:23,626 root  INFO     step 1533.000000 - time: 1.581581, loss: 0.550810, perplexity: 1.734658, precision: 0.265625, batch_len: 137.000000
Train, loss=0.55081046: 1534it [35:53,  1.37s/it]2017-06-01 18:45:24,971 root  INFO     step 1534.000000 - time: 1.247881, loss: 0.416853, perplexity: 1.517180, precision: 0.531250, batch_len: 99.000000
Train, loss=0.41685304: 1535it [35:54,  1.37s/it]2017-06-01 18:45:26,401 root  INFO     step 1535.000000 - time: 1.409800, loss: 0.548616, perplexity: 1.730855, precision: 0.296875, batch_len: 126.000000
Train, loss=0.54861557: 1536it [35:56,  1.39s/it]2017-06-01 18:45:27,845 root  INFO     step 1536.000000 - time: 1.350730, loss: 0.768422, perplexity: 2.156362, precision: 0.125000, batch_len: 129.000000
Train, loss=0.76842242: 1537it [35:57,  1.40s/it]2017-06-01 18:45:28,880 root  INFO     step 1537.000000 - time: 0.955246, loss: 0.635039, perplexity: 1.887096, precision: 0.343750, batch_len: 80.000000
Train, loss=0.63503891: 1538it [35:58,  1.29s/it]2017-06-01 18:45:30,491 root  INFO     step 1538.000000 - time: 1.599760, loss: 0.656148, perplexity: 1.927354, precision: 0.359375, batch_len: 119.000000
Train, loss=0.65614820: 1539it [36:00,  1.39s/it]2017-06-01 18:45:31,668 root  INFO     step 1539.000000 - time: 1.001769, loss: 0.457015, perplexity: 1.579353, precision: 0.500000, batch_len: 82.000000
Train, loss=0.45701545: 1540it [36:01,  1.32s/it]2017-06-01 18:45:32,990 root  INFO     step 1540.000000 - time: 1.292754, loss: 0.526204, perplexity: 1.692495, precision: 0.453125, batch_len: 84.000000
Train, loss=0.52620399: 1541it [36:02,  1.32s/it]2017-06-01 18:45:34,438 root  INFO     step 1541.000000 - time: 1.350645, loss: 0.509127, perplexity: 1.663838, precision: 0.406250, batch_len: 135.000000
Train, loss=0.50912684: 1542it [36:04,  1.36s/it]2017-06-01 18:45:35,904 root  INFO     step 1542.000000 - time: 1.315514, loss: 0.507889, perplexity: 1.661779, precision: 0.359375, batch_len: 133.000000
Train, loss=0.50788856: 1543it [36:05,  1.39s/it]2017-06-01 18:45:36,885 root  INFO     step 1543.000000 - time: 0.970037, loss: 0.440233, perplexity: 1.553069, precision: 0.515625, batch_len: 78.000000
Train, loss=0.44023308: 1544it [36:06,  1.27s/it]2017-06-01 18:45:38,574 root  INFO     step 1544.000000 - time: 1.508741, loss: 0.456029, perplexity: 1.577795, precision: 0.343750, batch_len: 136.000000
Train, loss=0.45602855: 1545it [36:08,  1.39s/it]2017-06-01 18:45:40,298 root  INFO     step 1545.000000 - time: 1.586497, loss: 0.453290, perplexity: 1.573480, precision: 0.296875, batch_len: 130.000000
Train, loss=0.45328963: 1546it [36:10,  1.49s/it]2017-06-01 18:45:41,757 root  INFO     step 1546.000000 - time: 1.352258, loss: 0.493858, perplexity: 1.638626, precision: 0.390625, batch_len: 132.000000
Train, loss=0.49385780: 1547it [36:11,  1.48s/it]2017-06-01 18:45:43,694 root  INFO     step 1547.000000 - time: 1.895063, loss: 0.596134, perplexity: 1.815089, precision: 0.328125, batch_len: 152.000000
Train, loss=0.59613442: 1548it [36:13,  1.62s/it]2017-06-01 18:45:45,193 root  INFO     step 1548.000000 - time: 1.241885, loss: 0.570108, perplexity: 1.768458, precision: 0.390625, batch_len: 96.000000
Train, loss=0.57010770: 1549it [36:15,  1.58s/it]2017-06-01 18:45:46,847 root  INFO     step 1549.000000 - time: 1.606492, loss: 0.505553, perplexity: 1.657901, precision: 0.375000, batch_len: 138.000000
Train, loss=0.50555253: 1550it [36:16,  1.60s/it]2017-06-01 18:45:48,237 root  INFO     step 1550.000000 - time: 1.336193, loss: 0.457214, perplexity: 1.579666, precision: 0.406250, batch_len: 122.000000
Train, loss=0.45721352: 1551it [36:18,  1.54s/it]2017-06-01 18:45:49,802 root  INFO     step 1551.000000 - time: 1.401738, loss: 0.574139, perplexity: 1.775600, precision: 0.390625, batch_len: 141.000000
Train, loss=0.57413864: 1552it [36:19,  1.55s/it]2017-06-01 18:45:51,251 root  INFO     step 1552.000000 - time: 1.398439, loss: 0.456333, perplexity: 1.578276, precision: 0.390625, batch_len: 139.000000
Train, loss=0.45633325: 1553it [36:21,  1.52s/it]2017-06-01 18:45:52,850 root  INFO     step 1553.000000 - time: 1.543398, loss: 0.277470, perplexity: 1.319787, precision: 0.750000, batch_len: 72.000000
Train, loss=0.27746999: 1554it [36:22,  1.54s/it]2017-06-01 18:45:54,393 root  INFO     step 1554.000000 - time: 1.444719, loss: 0.850290, perplexity: 2.340325, precision: 0.109375, batch_len: 142.000000
Train, loss=0.85029000: 1555it [36:24,  1.54s/it]2017-06-01 18:45:55,795 root  INFO     step 1555.000000 - time: 1.271164, loss: 1.730255, perplexity: 5.642090, precision: 0.000000, batch_len: 134.000000
Train, loss=1.73025453: 1556it [36:25,  1.50s/it]2017-06-01 18:45:56,701 root  INFO     step 1556.000000 - time: 0.865087, loss: 0.609653, perplexity: 1.839793, precision: 0.453125, batch_len: 76.000000
Train, loss=0.60965288: 1557it [36:26,  1.32s/it]2017-06-01 18:45:57,535 root  INFO     step 1557.000000 - time: 0.755902, loss: 0.418585, perplexity: 1.519809, precision: 0.500000, batch_len: 71.000000
Train, loss=0.41858491: 1558it [36:27,  1.18s/it]2017-06-01 18:45:58,684 root  INFO     step 1558.000000 - time: 1.066768, loss: 0.375461, perplexity: 1.455662, precision: 0.593750, batch_len: 77.000000
Train, loss=0.37546080: 1559it [36:28,  1.17s/it]2017-06-01 18:45:59,861 root  INFO     step 1559.000000 - time: 1.132305, loss: 0.285201, perplexity: 1.330029, precision: 0.640625, batch_len: 74.000000
Train, loss=0.28520110: 1560it [36:29,  1.17s/it]2017-06-01 18:46:01,842 root  INFO     step 1560.000000 - time: 1.812086, loss: 1.480035, perplexity: 4.393098, precision: 0.000000, batch_len: 150.000000
Train, loss=1.48003459: 1561it [36:31,  1.41s/it]2017-06-01 18:46:03,275 root  INFO     step 1561.000000 - time: 1.385505, loss: 1.215527, perplexity: 3.372070, precision: 0.000000, batch_len: 131.000000
Train, loss=1.21552694: 1562it [36:33,  1.42s/it]2017-06-01 18:46:03,488 root  INFO     Generating first batch)
2017-06-01 18:46:07,056 root  INFO     step 1562.000000 - time: 1.119590, loss: 0.689028, perplexity: 1.991779, precision: 0.250000, batch_len: 96.000000
Train, loss=0.68902826: 1563it [36:36,  2.13s/it]2017-06-01 18:46:08,358 root  INFO     step 1563.000000 - time: 1.076218, loss: 0.629004, perplexity: 1.875742, precision: 0.296875, batch_len: 110.000000
Train, loss=0.62900442: 1564it [36:38,  1.88s/it]2017-06-01 18:46:09,797 root  INFO     step 1564.000000 - time: 1.221135, loss: 0.503830, perplexity: 1.655048, precision: 0.390625, batch_len: 92.000000
Train, loss=0.50383008: 1565it [36:39,  1.75s/it]2017-06-01 18:46:11,231 root  INFO     step 1565.000000 - time: 1.376132, loss: 0.572657, perplexity: 1.772971, precision: 0.281250, batch_len: 113.000000
Train, loss=0.57265675: 1566it [36:41,  1.65s/it]2017-06-01 18:46:12,556 root  INFO     step 1566.000000 - time: 1.096508, loss: 0.579659, perplexity: 1.785430, precision: 0.453125, batch_len: 97.000000
Train, loss=0.57965910: 1567it [36:42,  1.56s/it]2017-06-01 18:46:13,873 root  INFO     step 1567.000000 - time: 1.235028, loss: 0.610359, perplexity: 1.841093, precision: 0.375000, batch_len: 91.000000
Train, loss=0.61035919: 1568it [36:43,  1.48s/it]2017-06-01 18:46:14,976 root  INFO     step 1568.000000 - time: 0.908468, loss: 0.646678, perplexity: 1.909188, precision: 0.421875, batch_len: 88.000000
Train, loss=0.64667797: 1569it [36:44,  1.37s/it]2017-06-01 18:46:16,433 root  INFO     step 1569.000000 - time: 1.402800, loss: 0.512768, perplexity: 1.669907, precision: 0.328125, batch_len: 128.000000
Train, loss=0.51276767: 1570it [36:46,  1.40s/it]2017-06-01 18:46:17,747 root  INFO     step 1570.000000 - time: 1.209300, loss: 0.483705, perplexity: 1.622074, precision: 0.359375, batch_len: 104.000000
Train, loss=0.48370549: 1571it [36:47,  1.37s/it]2017-06-01 18:46:19,098 root  INFO     step 1571.000000 - time: 1.331103, loss: 0.455025, perplexity: 1.576213, precision: 0.421875, batch_len: 117.000000
Train, loss=0.45502496: 1572it [36:48,  1.37s/it]2017-06-01 18:46:20,147 root  INFO     step 1572.000000 - time: 1.035582, loss: 0.405370, perplexity: 1.499858, precision: 0.515625, batch_len: 101.000000
Train, loss=0.40537035: 1573it [36:50,  1.27s/it]2017-06-01 18:46:21,138 root  INFO     step 1573.000000 - time: 0.986006, loss: 0.530468, perplexity: 1.699728, precision: 0.468750, batch_len: 93.000000
Train, loss=0.53046834: 1574it [36:51,  1.19s/it]2017-06-01 18:46:22,172 root  INFO     step 1574.000000 - time: 0.999630, loss: 0.702108, perplexity: 2.018002, precision: 0.328125, batch_len: 111.000000
Train, loss=0.70210779: 1575it [36:52,  1.14s/it]2017-06-01 18:46:23,641 root  INFO     step 1575.000000 - time: 1.463700, loss: 0.979695, perplexity: 2.663645, precision: 0.046875, batch_len: 120.000000
Train, loss=0.97969532: 1576it [36:53,  1.24s/it]2017-06-01 18:46:24,942 root  INFO     step 1576.000000 - time: 1.297119, loss: 1.053272, perplexity: 2.867018, precision: 0.062500, batch_len: 108.000000
Train, loss=1.05327237: 1577it [36:54,  1.26s/it]2017-06-01 18:46:26,263 root  INFO     step 1577.000000 - time: 1.157036, loss: 1.062062, perplexity: 2.892328, precision: 0.062500, batch_len: 114.000000
Train, loss=1.06206167: 1578it [36:56,  1.28s/it]2017-06-01 18:46:27,430 root  INFO     step 1578.000000 - time: 1.162210, loss: 0.482553, perplexity: 1.620205, precision: 0.343750, batch_len: 106.000000
Train, loss=0.48255259: 1579it [36:57,  1.24s/it]2017-06-01 18:46:28,546 root  INFO     step 1579.000000 - time: 1.036508, loss: 0.557924, perplexity: 1.747041, precision: 0.437500, batch_len: 90.000000
Train, loss=0.55792367: 1580it [36:58,  1.21s/it]2017-06-01 18:46:30,149 root  INFO     step 1580.000000 - time: 1.586856, loss: 0.534290, perplexity: 1.706236, precision: 0.390625, batch_len: 121.000000
Train, loss=0.53428972: 1581it [37:00,  1.32s/it]2017-06-01 18:46:31,218 root  INFO     step 1581.000000 - time: 1.033760, loss: 0.678447, perplexity: 1.970816, precision: 0.234375, batch_len: 112.000000
Train, loss=0.67844748: 1582it [37:01,  1.25s/it]2017-06-01 18:46:32,490 root  INFO     step 1582.000000 - time: 1.224004, loss: 0.513902, perplexity: 1.671801, precision: 0.468750, batch_len: 103.000000
Train, loss=0.51390159: 1583it [37:02,  1.26s/it]2017-06-01 18:46:34,017 root  INFO     step 1583.000000 - time: 1.491370, loss: 0.500714, perplexity: 1.649899, precision: 0.343750, batch_len: 116.000000
Train, loss=0.50071388: 1584it [37:03,  1.34s/it]2017-06-01 18:46:35,060 root  INFO     step 1584.000000 - time: 1.034940, loss: 0.516734, perplexity: 1.676543, precision: 0.453125, batch_len: 105.000000
Train, loss=0.51673383: 1585it [37:04,  1.25s/it]2017-06-01 18:46:36,073 root  INFO     step 1585.000000 - time: 0.978364, loss: 0.485218, perplexity: 1.624529, precision: 0.453125, batch_len: 94.000000
Train, loss=0.48521817: 1586it [37:05,  1.18s/it]2017-06-01 18:46:37,101 root  INFO     step 1586.000000 - time: 0.941266, loss: 0.446086, perplexity: 1.562186, precision: 0.437500, batch_len: 102.000000
Train, loss=0.44608581: 1587it [37:06,  1.13s/it]2017-06-01 18:46:38,289 root  INFO     step 1587.000000 - time: 1.110930, loss: 0.595533, perplexity: 1.813998, precision: 0.328125, batch_len: 109.000000
Train, loss=0.59553301: 1588it [37:08,  1.15s/it]2017-06-01 18:46:39,498 root  INFO     step 1588.000000 - time: 1.144815, loss: 0.594303, perplexity: 1.811768, precision: 0.468750, batch_len: 79.000000
Train, loss=0.59430337: 1589it [37:09,  1.17s/it]2017-06-01 18:46:40,648 root  INFO     step 1589.000000 - time: 1.063082, loss: 0.478103, perplexity: 1.613012, precision: 0.359375, batch_len: 100.000000
Train, loss=0.47810325: 1590it [37:10,  1.16s/it]2017-06-01 18:46:41,684 root  INFO     step 1590.000000 - time: 0.963801, loss: 0.510765, perplexity: 1.666565, precision: 0.406250, batch_len: 86.000000
Train, loss=0.51076484: 1591it [37:11,  1.12s/it]2017-06-01 18:46:42,668 root  INFO     step 1591.000000 - time: 0.950627, loss: 0.596546, perplexity: 1.815836, precision: 0.484375, batch_len: 81.000000
Train, loss=0.59654617: 1592it [37:12,  1.08s/it]2017-06-01 18:46:43,795 root  INFO     step 1592.000000 - time: 1.036073, loss: 0.580390, perplexity: 1.786735, precision: 0.359375, batch_len: 107.000000
Train, loss=0.58039010: 1593it [37:13,  1.10s/it]2017-06-01 18:46:45,485 root  INFO     step 1593.000000 - time: 1.486465, loss: 0.689666, perplexity: 1.993050, precision: 0.140625, batch_len: 126.000000
Train, loss=0.68966627: 1594it [37:15,  1.27s/it]2017-06-01 18:46:46,986 root  INFO     step 1594.000000 - time: 1.463912, loss: 1.034260, perplexity: 2.813024, precision: 0.125000, batch_len: 124.000000
Train, loss=1.03426003: 1595it [37:16,  1.34s/it]2017-06-01 18:46:48,115 root  INFO     step 1595.000000 - time: 1.111494, loss: 0.651005, perplexity: 1.917466, precision: 0.343750, batch_len: 115.000000
Train, loss=0.65100467: 1596it [37:17,  1.28s/it]2017-06-01 18:46:49,435 root  INFO     step 1596.000000 - time: 1.280651, loss: 0.363734, perplexity: 1.438692, precision: 0.593750, batch_len: 125.000000
Train, loss=0.36373410: 1597it [37:19,  1.29s/it]2017-06-01 18:46:50,483 root  INFO     step 1597.000000 - time: 1.001473, loss: 0.479197, perplexity: 1.614777, precision: 0.625000, batch_len: 89.000000
Train, loss=0.47919688: 1598it [37:20,  1.22s/it]2017-06-01 18:46:51,499 root  INFO     step 1598.000000 - time: 1.005254, loss: 0.585016, perplexity: 1.795020, precision: 0.390625, batch_len: 80.000000
Train, loss=0.58501637: 1599it [37:21,  1.16s/it]2017-06-01 18:46:52,806 root  INFO     step 1599.000000 - time: 1.231011, loss: 0.788144, perplexity: 2.199312, precision: 0.312500, batch_len: 85.000000
Train, loss=0.78814441: 1600it [37:22,  1.20s/it]2017-06-01 18:46:53,949 root  INFO     step 1600.000000 - time: 1.068046, loss: 0.502922, perplexity: 1.653546, precision: 0.453125, batch_len: 87.000000
Train, loss=0.50292188: 1601it [37:23,  1.18s/it]2017-06-01 18:46:55,033 root  INFO     step 1601.000000 - time: 1.021064, loss: 0.574147, perplexity: 1.775615, precision: 0.531250, batch_len: 84.000000
Train, loss=0.57414699: 1602it [37:24,  1.15s/it]2017-06-01 18:46:56,679 root  INFO     step 1602.000000 - time: 1.449697, loss: 0.590044, perplexity: 1.804068, precision: 0.281250, batch_len: 137.000000
Train, loss=0.59004390: 1603it [37:26,  1.30s/it]2017-06-01 18:46:57,670 root  INFO     step 1603.000000 - time: 0.872217, loss: 0.510786, perplexity: 1.666601, precision: 0.390625, batch_len: 83.000000
Train, loss=0.51078594: 1604it [37:27,  1.21s/it]2017-06-01 18:46:59,366 root  INFO     step 1604.000000 - time: 1.544126, loss: 0.647624, perplexity: 1.910996, precision: 0.234375, batch_len: 129.000000
Train, loss=0.64762437: 1605it [37:29,  1.35s/it]2017-06-01 18:47:00,857 root  INFO     step 1605.000000 - time: 1.466000, loss: 0.560699, perplexity: 1.751896, precision: 0.156250, batch_len: 123.000000
Train, loss=0.56069851: 1606it [37:30,  1.40s/it]2017-06-01 18:47:02,289 root  INFO     step 1606.000000 - time: 1.371393, loss: 0.508594, perplexity: 1.662952, precision: 0.250000, batch_len: 144.000000
Train, loss=0.50859439: 1607it [37:32,  1.41s/it]2017-06-01 18:47:03,454 root  INFO     step 1607.000000 - time: 1.103689, loss: 0.546439, perplexity: 1.727091, precision: 0.484375, batch_len: 99.000000
Train, loss=0.54643869: 1608it [37:33,  1.33s/it]2017-06-01 18:47:04,986 root  INFO     step 1608.000000 - time: 1.372254, loss: 0.749477, perplexity: 2.115893, precision: 0.265625, batch_len: 135.000000
Train, loss=0.74947673: 1609it [37:34,  1.39s/it]2017-06-01 18:47:06,584 root  INFO     step 1609.000000 - time: 1.567833, loss: 0.875503, perplexity: 2.400083, precision: 0.109375, batch_len: 118.000000
Train, loss=0.87550330: 1610it [37:36,  1.45s/it]2017-06-01 18:47:07,736 root  INFO     step 1610.000000 - time: 1.085185, loss: 0.474006, perplexity: 1.606416, precision: 0.234375, batch_len: 98.000000
Train, loss=0.47400585: 1611it [37:37,  1.36s/it]2017-06-01 18:47:09,278 root  INFO     step 1611.000000 - time: 1.313262, loss: 0.837810, perplexity: 2.311299, precision: 0.203125, batch_len: 119.000000
Train, loss=0.83780956: 1612it [37:39,  1.42s/it]2017-06-01 18:47:10,528 root  INFO     step 1612.000000 - time: 1.175754, loss: 0.579378, perplexity: 1.784929, precision: 0.218750, batch_len: 133.000000
Train, loss=0.57937837: 1613it [37:40,  1.37s/it]2017-06-01 18:47:12,190 root  INFO     step 1613.000000 - time: 1.581541, loss: 0.730994, perplexity: 2.077144, precision: 0.265625, batch_len: 136.000000
Train, loss=0.73099393: 1614it [37:42,  1.46s/it]2017-06-01 18:47:13,825 root  INFO     step 1614.000000 - time: 1.561706, loss: 0.725488, perplexity: 2.065738, precision: 0.171875, batch_len: 130.000000
Train, loss=0.72548771: 1615it [37:43,  1.51s/it]2017-06-01 18:47:14,793 root  INFO     step 1615.000000 - time: 0.963882, loss: 0.522101, perplexity: 1.685564, precision: 0.500000, batch_len: 82.000000
Train, loss=0.52210051: 1616it [37:44,  1.35s/it]2017-06-01 18:47:16,418 root  INFO     step 1616.000000 - time: 1.401449, loss: 0.733107, perplexity: 2.081538, precision: 0.156250, batch_len: 138.000000
Train, loss=0.73310697: 1617it [37:46,  1.43s/it]2017-06-01 18:47:18,552 root  INFO     step 1617.000000 - time: 1.945909, loss: 0.750534, perplexity: 2.118131, precision: 0.093750, batch_len: 152.000000
Train, loss=0.75053406: 1618it [37:48,  1.64s/it]2017-06-01 18:47:20,221 root  INFO     step 1618.000000 - time: 1.585997, loss: 0.635629, perplexity: 1.888209, precision: 0.218750, batch_len: 122.000000
Train, loss=0.63562900: 1619it [37:50,  1.65s/it]2017-06-01 18:47:21,264 root  INFO     step 1619.000000 - time: 0.993112, loss: 0.493340, perplexity: 1.637777, precision: 0.296875, batch_len: 78.000000
Train, loss=0.49333990: 1620it [37:51,  1.47s/it]2017-06-01 18:47:22,419 root  INFO     step 1620.000000 - time: 1.072978, loss: 0.432145, perplexity: 1.540558, precision: 0.437500, batch_len: 96.000000
Train, loss=0.43214479: 1621it [37:52,  1.37s/it]2017-06-01 18:47:23,372 root  INFO     step 1621.000000 - time: 0.902073, loss: 0.648969, perplexity: 1.913567, precision: 0.203125, batch_len: 74.000000
Train, loss=0.64896929: 1622it [37:53,  1.25s/it]2017-06-01 18:47:25,126 root  INFO     step 1622.000000 - time: 1.596358, loss: 1.004835, perplexity: 2.731455, precision: 0.140625, batch_len: 139.000000
Train, loss=1.00483453: 1623it [37:55,  1.40s/it]2017-06-01 18:47:26,686 root  INFO     step 1623.000000 - time: 1.537401, loss: 1.064112, perplexity: 2.898264, precision: 0.078125, batch_len: 132.000000
Train, loss=1.06411195: 1624it [37:56,  1.45s/it]2017-06-01 18:47:27,754 root  INFO     step 1624.000000 - time: 1.042622, loss: 1.000208, perplexity: 2.718849, precision: 0.218750, batch_len: 76.000000
Train, loss=1.00020850: 1625it [37:57,  1.33s/it]2017-06-01 18:47:29,046 root  INFO     step 1625.000000 - time: 1.272071, loss: 0.411616, perplexity: 1.509254, precision: 0.375000, batch_len: 72.000000
Train, loss=0.41161576: 1626it [37:58,  1.32s/it]2017-06-01 18:47:30,628 root  INFO     step 1626.000000 - time: 1.373397, loss: 0.572015, perplexity: 1.771833, precision: 0.171875, batch_len: 131.000000
Train, loss=0.57201481: 1627it [38:00,  1.40s/it]2017-06-01 18:47:31,800 root  INFO     step 1627.000000 - time: 1.121095, loss: 0.475712, perplexity: 1.609160, precision: 0.421875, batch_len: 77.000000
Train, loss=0.47571206: 1628it [38:01,  1.33s/it]2017-06-01 18:47:33,232 root  INFO     step 1628.000000 - time: 1.381316, loss: 0.604656, perplexity: 1.830623, precision: 0.171875, batch_len: 134.000000
Train, loss=0.60465622: 1629it [38:03,  1.36s/it]2017-06-01 18:47:34,327 root  INFO     step 1629.000000 - time: 1.075025, loss: 0.314144, perplexity: 1.369087, precision: 0.656250, batch_len: 71.000000
Train, loss=0.31414431: 1630it [38:04,  1.28s/it]2017-06-01 18:47:35,864 root  INFO     step 1630.000000 - time: 1.516041, loss: 0.805576, perplexity: 2.237985, precision: 0.125000, batch_len: 141.000000
Train, loss=0.80557609: 1631it [38:05,  1.36s/it]2017-06-01 18:47:37,763 root  INFO     step 1631.000000 - time: 1.798146, loss: 0.935671, perplexity: 2.548922, precision: 0.078125, batch_len: 150.000000
Train, loss=0.93567061: 1632it [38:07,  1.52s/it]2017-06-01 18:47:39,115 root  INFO     step 1632.000000 - time: 1.293250, loss: 0.643657, perplexity: 1.903429, precision: 0.093750, batch_len: 142.000000
Train, loss=0.64365697: 1633it [38:08,  1.47s/it]2017-06-01 18:47:39,369 root  INFO     Generating first batch)
2017-06-01 18:47:43,029 root  INFO     step 1633.000000 - time: 1.033053, loss: 0.598792, perplexity: 1.819919, precision: 0.265625, batch_len: 104.000000
Train, loss=0.59879184: 1634it [38:12,  2.20s/it]2017-06-01 18:47:44,134 root  INFO     step 1634.000000 - time: 1.006833, loss: 0.348705, perplexity: 1.417231, precision: 0.500000, batch_len: 96.000000
Train, loss=0.34870470: 1635it [38:14,  1.87s/it]2017-06-01 18:47:45,843 root  INFO     step 1635.000000 - time: 1.399772, loss: 0.477850, perplexity: 1.612604, precision: 0.437500, batch_len: 110.000000
Train, loss=0.47785041: 1636it [38:15,  1.82s/it]2017-06-01 18:47:47,056 root  INFO     step 1636.000000 - time: 1.165324, loss: 0.442805, perplexity: 1.557069, precision: 0.437500, batch_len: 109.000000
Train, loss=0.44280535: 1637it [38:16,  1.64s/it]2017-06-01 18:47:48,504 root  INFO     step 1637.000000 - time: 1.342796, loss: 0.463334, perplexity: 1.589365, precision: 0.406250, batch_len: 120.000000
Train, loss=0.46333432: 1638it [38:18,  1.58s/it]2017-06-01 18:47:49,580 root  INFO     step 1638.000000 - time: 1.053371, loss: 0.432769, perplexity: 1.541520, precision: 0.437500, batch_len: 92.000000
Train, loss=0.43276900: 1639it [38:19,  1.43s/it]2017-06-01 18:47:50,694 root  INFO     step 1639.000000 - time: 1.008547, loss: 0.740725, perplexity: 2.097455, precision: 0.296875, batch_len: 97.000000
Train, loss=0.74072468: 1640it [38:20,  1.34s/it]2017-06-01 18:47:51,832 root  INFO     step 1640.000000 - time: 1.125815, loss: 0.541831, perplexity: 1.719151, precision: 0.250000, batch_len: 101.000000
Train, loss=0.54183072: 1641it [38:21,  1.28s/it]2017-06-01 18:47:53,075 root  INFO     step 1641.000000 - time: 1.209930, loss: 0.753608, perplexity: 2.124652, precision: 0.265625, batch_len: 88.000000
Train, loss=0.75360793: 1642it [38:22,  1.27s/it]2017-06-01 18:47:54,200 root  INFO     step 1642.000000 - time: 1.061364, loss: 0.707961, perplexity: 2.029849, precision: 0.171875, batch_len: 93.000000
Train, loss=0.70796120: 1643it [38:24,  1.22s/it]2017-06-01 18:47:55,279 root  INFO     step 1643.000000 - time: 0.972072, loss: 0.802169, perplexity: 2.230373, precision: 0.390625, batch_len: 81.000000
Train, loss=0.80216897: 1644it [38:25,  1.18s/it]2017-06-01 18:47:56,404 root  INFO     step 1644.000000 - time: 1.061956, loss: 0.548253, perplexity: 1.730228, precision: 0.500000, batch_len: 106.000000
Train, loss=0.54825318: 1645it [38:26,  1.16s/it]2017-06-01 18:47:57,511 root  INFO     step 1645.000000 - time: 1.011964, loss: 0.538275, perplexity: 1.713050, precision: 0.421875, batch_len: 108.000000
Train, loss=0.53827536: 1646it [38:27,  1.15s/it]2017-06-01 18:47:58,905 root  INFO     step 1646.000000 - time: 1.317697, loss: 0.485877, perplexity: 1.625600, precision: 0.343750, batch_len: 114.000000
Train, loss=0.48587716: 1647it [38:28,  1.22s/it]2017-06-01 18:48:00,137 root  INFO     step 1647.000000 - time: 1.140660, loss: 0.435994, perplexity: 1.546499, precision: 0.500000, batch_len: 100.000000
Train, loss=0.43599373: 1648it [38:30,  1.22s/it]2017-06-01 18:48:01,180 root  INFO     step 1648.000000 - time: 1.025953, loss: 0.377451, perplexity: 1.458561, precision: 0.546875, batch_len: 105.000000
Train, loss=0.37745053: 1649it [38:31,  1.17s/it]2017-06-01 18:48:02,411 root  INFO     step 1649.000000 - time: 1.097315, loss: 0.407213, perplexity: 1.502624, precision: 0.531250, batch_len: 112.000000
Train, loss=0.40721279: 1650it [38:32,  1.19s/it]2017-06-01 18:48:03,461 root  INFO     step 1650.000000 - time: 1.000418, loss: 0.582076, perplexity: 1.789750, precision: 0.515625, batch_len: 111.000000
Train, loss=0.58207583: 1651it [38:33,  1.15s/it]2017-06-01 18:48:05,098 root  INFO     step 1651.000000 - time: 1.558979, loss: 0.365274, perplexity: 1.440908, precision: 0.593750, batch_len: 117.000000
Train, loss=0.36527359: 1652it [38:34,  1.29s/it]2017-06-01 18:48:06,414 root  INFO     step 1652.000000 - time: 1.277262, loss: 0.528673, perplexity: 1.696679, precision: 0.421875, batch_len: 113.000000
Train, loss=0.52867287: 1653it [38:36,  1.30s/it]2017-06-01 18:48:07,532 root  INFO     step 1653.000000 - time: 1.071775, loss: 0.534906, perplexity: 1.707288, precision: 0.500000, batch_len: 102.000000
Train, loss=0.53490603: 1654it [38:37,  1.25s/it]2017-06-01 18:48:08,631 root  INFO     step 1654.000000 - time: 1.022457, loss: 0.493315, perplexity: 1.637736, precision: 0.421875, batch_len: 90.000000
Train, loss=0.49331498: 1655it [38:38,  1.20s/it]2017-06-01 18:48:09,739 root  INFO     step 1655.000000 - time: 1.048962, loss: 0.628744, perplexity: 1.875253, precision: 0.421875, batch_len: 89.000000
Train, loss=0.62874377: 1656it [38:39,  1.17s/it]2017-06-01 18:48:10,977 root  INFO     step 1656.000000 - time: 1.180486, loss: 0.372490, perplexity: 1.451344, precision: 0.468750, batch_len: 128.000000
Train, loss=0.37249008: 1657it [38:40,  1.19s/it]2017-06-01 18:48:12,639 root  INFO     step 1657.000000 - time: 1.557902, loss: 0.623835, perplexity: 1.866071, precision: 0.500000, batch_len: 91.000000
Train, loss=0.62383491: 1658it [38:42,  1.33s/it]2017-06-01 18:48:13,764 root  INFO     step 1658.000000 - time: 1.090385, loss: 0.525259, perplexity: 1.690897, precision: 0.406250, batch_len: 115.000000
Train, loss=0.52525902: 1659it [38:43,  1.27s/it]2017-06-01 18:48:15,313 root  INFO     step 1659.000000 - time: 1.364958, loss: 0.729696, perplexity: 2.074449, precision: 0.187500, batch_len: 121.000000
Train, loss=0.72969556: 1660it [38:45,  1.35s/it]2017-06-01 18:48:16,258 root  INFO     step 1660.000000 - time: 0.916283, loss: 0.968653, perplexity: 2.634393, precision: 0.171875, batch_len: 80.000000
Train, loss=0.96865273: 1661it [38:46,  1.23s/it]2017-06-01 18:48:17,324 root  INFO     step 1661.000000 - time: 0.961071, loss: 1.166248, perplexity: 3.209927, precision: 0.171875, batch_len: 85.000000
Train, loss=1.16624832: 1662it [38:47,  1.18s/it]2017-06-01 18:48:18,675 root  INFO     step 1662.000000 - time: 1.260868, loss: 0.611392, perplexity: 1.842994, precision: 0.281250, batch_len: 103.000000
Train, loss=0.61139154: 1663it [38:48,  1.23s/it]2017-06-01 18:48:20,241 root  INFO     step 1663.000000 - time: 1.457278, loss: 0.623235, perplexity: 1.864952, precision: 0.203125, batch_len: 124.000000
Train, loss=0.62323546: 1664it [38:50,  1.33s/it]2017-06-01 18:48:21,236 root  INFO     step 1664.000000 - time: 0.980063, loss: 0.861549, perplexity: 2.366824, precision: 0.218750, batch_len: 86.000000
Train, loss=0.86154896: 1665it [38:51,  1.23s/it]2017-06-01 18:48:22,318 root  INFO     step 1665.000000 - time: 1.070793, loss: 0.935445, perplexity: 2.548347, precision: 0.140625, batch_len: 107.000000
Train, loss=0.93544489: 1666it [38:52,  1.19s/it]2017-06-01 18:48:23,549 root  INFO     step 1666.000000 - time: 1.215419, loss: 0.580276, perplexity: 1.786532, precision: 0.328125, batch_len: 125.000000
Train, loss=0.58027649: 1667it [38:53,  1.20s/it]2017-06-01 18:48:24,851 root  INFO     step 1667.000000 - time: 1.169694, loss: 0.573500, perplexity: 1.774466, precision: 0.421875, batch_len: 94.000000
Train, loss=0.57349950: 1668it [38:54,  1.23s/it]2017-06-01 18:48:26,492 root  INFO     step 1668.000000 - time: 1.517668, loss: 0.550706, perplexity: 1.734477, precision: 0.312500, batch_len: 116.000000
Train, loss=0.55070615: 1669it [38:56,  1.35s/it]2017-06-01 18:48:27,898 root  INFO     step 1669.000000 - time: 1.380259, loss: 0.701367, perplexity: 2.016507, precision: 0.234375, batch_len: 129.000000
Train, loss=0.70136666: 1670it [38:57,  1.37s/it]2017-06-01 18:48:29,341 root  INFO     step 1670.000000 - time: 1.408303, loss: 0.424677, perplexity: 1.529096, precision: 0.390625, batch_len: 118.000000
Train, loss=0.42467701: 1671it [38:59,  1.39s/it]2017-06-01 18:48:30,342 root  INFO     step 1671.000000 - time: 0.946186, loss: 0.358252, perplexity: 1.430827, precision: 0.515625, batch_len: 87.000000
Train, loss=0.35825235: 1672it [39:00,  1.27s/it]2017-06-01 18:48:31,894 root  INFO     step 1672.000000 - time: 1.464532, loss: 0.353352, perplexity: 1.423833, precision: 0.515625, batch_len: 119.000000
Train, loss=0.35335225: 1673it [39:01,  1.36s/it]2017-06-01 18:48:33,058 root  INFO     step 1673.000000 - time: 1.072789, loss: 0.470380, perplexity: 1.600603, precision: 0.546875, batch_len: 79.000000
Train, loss=0.47038025: 1674it [39:02,  1.30s/it]2017-06-01 18:48:34,780 root  INFO     step 1674.000000 - time: 1.644350, loss: 0.562353, perplexity: 1.754797, precision: 0.343750, batch_len: 123.000000
Train, loss=0.56235319: 1675it [39:04,  1.43s/it]2017-06-01 18:48:36,298 root  INFO     step 1675.000000 - time: 1.469037, loss: 0.808646, perplexity: 2.244867, precision: 0.140625, batch_len: 137.000000
Train, loss=0.80864632: 1676it [39:06,  1.45s/it]2017-06-01 18:48:37,845 root  INFO     step 1676.000000 - time: 1.387944, loss: 1.132446, perplexity: 3.103237, precision: 0.093750, batch_len: 136.000000
Train, loss=1.13244569: 1677it [39:07,  1.48s/it]2017-06-01 18:48:39,251 root  INFO     step 1677.000000 - time: 1.365997, loss: 1.008692, perplexity: 2.742012, precision: 0.078125, batch_len: 144.000000
Train, loss=1.00869203: 1678it [39:09,  1.46s/it]2017-06-01 18:48:41,173 root  INFO     step 1678.000000 - time: 1.748487, loss: 0.870829, perplexity: 2.388889, precision: 0.125000, batch_len: 135.000000
Train, loss=0.87082851: 1679it [39:11,  1.60s/it]2017-06-01 18:48:42,536 root  INFO     step 1679.000000 - time: 1.348760, loss: 0.560343, perplexity: 1.751274, precision: 0.171875, batch_len: 133.000000
Train, loss=0.56034333: 1680it [39:12,  1.53s/it]2017-06-01 18:48:43,537 root  INFO     step 1680.000000 - time: 0.954599, loss: 0.551580, perplexity: 1.735994, precision: 0.546875, batch_len: 83.000000
Train, loss=0.55158007: 1681it [39:13,  1.37s/it]2017-06-01 18:48:44,679 root  INFO     step 1681.000000 - time: 1.096340, loss: 0.389086, perplexity: 1.475632, precision: 0.359375, batch_len: 98.000000
Train, loss=0.38908613: 1682it [39:14,  1.30s/it]2017-06-01 18:48:46,031 root  INFO     step 1682.000000 - time: 1.223246, loss: 0.677481, perplexity: 1.968912, precision: 0.218750, batch_len: 126.000000
Train, loss=0.67748117: 1683it [39:15,  1.32s/it]2017-06-01 18:48:47,161 root  INFO     step 1683.000000 - time: 1.115033, loss: 0.461770, perplexity: 1.586881, precision: 0.515625, batch_len: 82.000000
Train, loss=0.46177047: 1684it [39:17,  1.26s/it]2017-06-01 18:48:48,354 root  INFO     step 1684.000000 - time: 1.184692, loss: 0.541117, perplexity: 1.717925, precision: 0.515625, batch_len: 84.000000
Train, loss=0.54111725: 1685it [39:18,  1.24s/it]2017-06-01 18:48:49,648 root  INFO     step 1685.000000 - time: 1.098010, loss: 0.465827, perplexity: 1.593331, precision: 0.375000, batch_len: 99.000000
Train, loss=0.46582699: 1686it [39:19,  1.26s/it]2017-06-01 18:48:50,741 root  INFO     step 1686.000000 - time: 1.086116, loss: 0.370402, perplexity: 1.448317, precision: 0.484375, batch_len: 96.000000
Train, loss=0.37040240: 1687it [39:20,  1.21s/it]2017-06-01 18:48:51,796 root  INFO     step 1687.000000 - time: 0.885296, loss: 0.331435, perplexity: 1.392965, precision: 0.640625, batch_len: 78.000000
Train, loss=0.33143485: 1688it [39:21,  1.16s/it]2017-06-01 18:48:53,554 root  INFO     step 1688.000000 - time: 1.561411, loss: 0.433642, perplexity: 1.542866, precision: 0.437500, batch_len: 139.000000
Train, loss=0.43364197: 1689it [39:23,  1.34s/it]2017-06-01 18:48:55,143 root  INFO     step 1689.000000 - time: 1.551138, loss: 0.455844, perplexity: 1.577505, precision: 0.437500, batch_len: 130.000000
Train, loss=0.45584437: 1690it [39:25,  1.42s/it]2017-06-01 18:48:57,146 root  INFO     step 1690.000000 - time: 1.905195, loss: 0.586896, perplexity: 1.798398, precision: 0.203125, batch_len: 152.000000
Train, loss=0.58689642: 1691it [39:27,  1.59s/it]2017-06-01 18:48:58,504 root  INFO     step 1691.000000 - time: 1.271282, loss: 0.878664, perplexity: 2.407680, precision: 0.062500, batch_len: 138.000000
Train, loss=0.87866354: 1692it [39:28,  1.52s/it]2017-06-01 18:49:00,078 root  INFO     step 1692.000000 - time: 1.568782, loss: 0.757315, perplexity: 2.132542, precision: 0.156250, batch_len: 122.000000
Train, loss=0.75731450: 1693it [39:29,  1.54s/it]2017-06-01 18:49:01,354 root  INFO     step 1693.000000 - time: 1.084684, loss: 0.456630, perplexity: 1.578744, precision: 0.281250, batch_len: 74.000000
Train, loss=0.45662981: 1694it [39:31,  1.46s/it]2017-06-01 18:49:02,797 root  INFO     step 1694.000000 - time: 1.337612, loss: 0.561225, perplexity: 1.752818, precision: 0.281250, batch_len: 132.000000
Train, loss=0.56122494: 1695it [39:32,  1.45s/it]2017-06-01 18:49:04,354 root  INFO     step 1695.000000 - time: 1.420878, loss: 0.526984, perplexity: 1.693816, precision: 0.281250, batch_len: 142.000000
Train, loss=0.52698392: 1696it [39:34,  1.49s/it]2017-06-01 18:49:05,268 root  INFO     step 1696.000000 - time: 0.872213, loss: 0.296630, perplexity: 1.345318, precision: 0.562500, batch_len: 71.000000
Train, loss=0.29663014: 1697it [39:35,  1.31s/it]2017-06-01 18:49:07,556 root  INFO     step 1697.000000 - time: 2.236029, loss: 0.564031, perplexity: 1.757744, precision: 0.265625, batch_len: 150.000000
Train, loss=0.56403106: 1698it [39:37,  1.61s/it]2017-06-01 18:49:09,013 root  INFO     step 1698.000000 - time: 1.444701, loss: 0.692132, perplexity: 1.997971, precision: 0.203125, batch_len: 141.000000
Train, loss=0.69213200: 1699it [39:38,  1.56s/it]2017-06-01 18:49:10,012 root  INFO     step 1699.000000 - time: 0.960175, loss: 0.611244, perplexity: 1.842723, precision: 0.421875, batch_len: 76.000000
Train, loss=0.61124438: 1700it [39:39,  1.39s/it]2017-06-01 18:49:10,998 root  INFO     step 1700.000000 - time: 0.930287, loss: 0.431736, perplexity: 1.539929, precision: 0.531250, batch_len: 72.000000
Train, loss=0.43173629: 1701it [39:40,  1.27s/it]2017-06-01 18:49:12,313 root  INFO     step 1701.000000 - time: 1.253408, loss: 0.436257, perplexity: 1.546906, precision: 0.468750, batch_len: 134.000000
Train, loss=0.43625677: 1702it [39:42,  1.28s/it]2017-06-01 18:49:14,252 root  INFO     step 1702.000000 - time: 1.765029, loss: 0.425751, perplexity: 1.530740, precision: 0.406250, batch_len: 131.000000
Train, loss=0.42575097: 1703it [39:44,  1.48s/it]2017-06-01 18:49:15,296 root  INFO     step 1703.000000 - time: 0.956937, loss: 0.374623, perplexity: 1.454444, precision: 0.640625, batch_len: 77.000000
Train, loss=0.37462348: 1704it [39:45,  1.35s/it]2017-06-01 18:49:15,423 root  INFO     Generating first batch)
2017-06-01 18:49:19,284 root  INFO     step 1704.000000 - time: 1.241732, loss: 0.262578, perplexity: 1.300277, precision: 0.640625, batch_len: 96.000000
Train, loss=0.26257756: 1705it [39:49,  2.14s/it]2017-06-01 18:49:21,075 root  INFO     step 1705.000000 - time: 1.664059, loss: 0.525540, perplexity: 1.691372, precision: 0.484375, batch_len: 120.000000
Train, loss=0.52554029: 1706it [39:50,  2.04s/it]2017-06-01 18:49:22,328 root  INFO     step 1706.000000 - time: 1.028784, loss: 0.398517, perplexity: 1.489615, precision: 0.468750, batch_len: 92.000000
Train, loss=0.39851740: 1707it [39:52,  1.80s/it]2017-06-01 18:49:23,580 root  INFO     step 1707.000000 - time: 1.175782, loss: 0.922505, perplexity: 2.515583, precision: 0.156250, batch_len: 128.000000
Train, loss=0.92250460: 1708it [39:53,  1.64s/it]2017-06-01 18:49:24,774 root  INFO     step 1708.000000 - time: 1.131862, loss: 0.983225, perplexity: 2.673064, precision: 0.109375, batch_len: 101.000000
Train, loss=0.98322529: 1709it [39:54,  1.50s/it]2017-06-01 18:49:26,227 root  INFO     step 1709.000000 - time: 1.305333, loss: 1.017293, perplexity: 2.765699, precision: 0.062500, batch_len: 114.000000
Train, loss=1.01729333: 1710it [39:56,  1.49s/it]2017-06-01 18:49:27,432 root  INFO     step 1710.000000 - time: 1.171030, loss: 0.863887, perplexity: 2.372364, precision: 0.203125, batch_len: 110.000000
Train, loss=0.86388707: 1711it [39:57,  1.40s/it]2017-06-01 18:49:28,737 root  INFO     step 1711.000000 - time: 1.101878, loss: 0.596927, perplexity: 1.816528, precision: 0.421875, batch_len: 105.000000
Train, loss=0.59692705: 1712it [39:58,  1.37s/it]2017-06-01 18:49:29,868 root  INFO     step 1712.000000 - time: 1.126113, loss: 0.510768, perplexity: 1.666570, precision: 0.453125, batch_len: 111.000000
Train, loss=0.51076758: 1713it [39:59,  1.30s/it]2017-06-01 18:49:31,000 root  INFO     step 1713.000000 - time: 1.102242, loss: 0.362248, perplexity: 1.436555, precision: 0.500000, batch_len: 103.000000
Train, loss=0.36224762: 1714it [40:00,  1.25s/it]2017-06-01 18:49:32,179 root  INFO     step 1714.000000 - time: 1.168732, loss: 0.673961, perplexity: 1.961993, precision: 0.234375, batch_len: 93.000000
Train, loss=0.67396092: 1715it [40:02,  1.23s/it]2017-06-01 18:49:33,500 root  INFO     step 1715.000000 - time: 1.242663, loss: 0.547358, perplexity: 1.728680, precision: 0.156250, batch_len: 113.000000
Train, loss=0.54735833: 1716it [40:03,  1.26s/it]2017-06-01 18:49:34,731 root  INFO     step 1716.000000 - time: 1.206305, loss: 0.439623, perplexity: 1.552122, precision: 0.468750, batch_len: 90.000000
Train, loss=0.43962318: 1717it [40:04,  1.25s/it]2017-06-01 18:49:36,036 root  INFO     step 1717.000000 - time: 1.273765, loss: 0.382083, perplexity: 1.465333, precision: 0.609375, batch_len: 125.000000
Train, loss=0.38208255: 1718it [40:05,  1.27s/it]2017-06-01 18:49:37,183 root  INFO     step 1718.000000 - time: 1.038977, loss: 0.375201, perplexity: 1.455284, precision: 0.578125, batch_len: 85.000000
Train, loss=0.37520105: 1719it [40:07,  1.23s/it]2017-06-01 18:49:38,342 root  INFO     step 1719.000000 - time: 1.055326, loss: 0.500268, perplexity: 1.649164, precision: 0.484375, batch_len: 106.000000
Train, loss=0.50026828: 1720it [40:08,  1.21s/it]2017-06-01 18:49:39,926 root  INFO     step 1720.000000 - time: 1.512401, loss: 0.422479, perplexity: 1.525739, precision: 0.500000, batch_len: 117.000000
Train, loss=0.42247897: 1721it [40:09,  1.32s/it]2017-06-01 18:49:41,200 root  INFO     step 1721.000000 - time: 1.186000, loss: 0.417516, perplexity: 1.518185, precision: 0.609375, batch_len: 79.000000
Train, loss=0.41751575: 1722it [40:11,  1.31s/it]2017-06-01 18:49:42,393 root  INFO     step 1722.000000 - time: 1.103744, loss: 0.426969, perplexity: 1.532606, precision: 0.515625, batch_len: 112.000000
Train, loss=0.42696929: 1723it [40:12,  1.27s/it]2017-06-01 18:49:43,450 root  INFO     step 1723.000000 - time: 1.039118, loss: 0.514882, perplexity: 1.673442, precision: 0.468750, batch_len: 89.000000
Train, loss=0.51488233: 1724it [40:13,  1.21s/it]2017-06-01 18:49:44,520 root  INFO     step 1724.000000 - time: 1.058648, loss: 0.514503, perplexity: 1.672806, precision: 0.453125, batch_len: 108.000000
Train, loss=0.51450264: 1725it [40:14,  1.17s/it]2017-06-01 18:49:45,489 root  INFO     step 1725.000000 - time: 0.901904, loss: 0.357131, perplexity: 1.429223, precision: 0.625000, batch_len: 87.000000
Train, loss=0.35713100: 1726it [40:15,  1.11s/it]2017-06-01 18:49:46,601 root  INFO     step 1726.000000 - time: 1.096743, loss: 0.496778, perplexity: 1.643418, precision: 0.500000, batch_len: 94.000000
Train, loss=0.49677807: 1727it [40:16,  1.11s/it]2017-06-01 18:49:47,874 root  INFO     step 1727.000000 - time: 1.183118, loss: 0.433831, perplexity: 1.543158, precision: 0.578125, batch_len: 81.000000
Train, loss=0.43383080: 1728it [40:17,  1.16s/it]2017-06-01 18:49:49,050 root  INFO     step 1728.000000 - time: 1.144872, loss: 0.433241, perplexity: 1.542248, precision: 0.500000, batch_len: 115.000000
Train, loss=0.43324125: 1729it [40:18,  1.16s/it]2017-06-01 18:49:50,020 root  INFO     step 1729.000000 - time: 0.950505, loss: 0.344225, perplexity: 1.410896, precision: 0.609375, batch_len: 86.000000
Train, loss=0.34422511: 1730it [40:19,  1.11s/it]2017-06-01 18:49:51,102 root  INFO     step 1730.000000 - time: 1.075702, loss: 0.347685, perplexity: 1.415786, precision: 0.484375, batch_len: 102.000000
Train, loss=0.34768516: 1731it [40:20,  1.10s/it]2017-06-01 18:49:52,121 root  INFO     step 1731.000000 - time: 0.974171, loss: 0.423942, perplexity: 1.527972, precision: 0.468750, batch_len: 109.000000
Train, loss=0.42394155: 1732it [40:21,  1.07s/it]2017-06-01 18:49:53,321 root  INFO     step 1732.000000 - time: 1.180378, loss: 0.512019, perplexity: 1.668656, precision: 0.375000, batch_len: 100.000000
Train, loss=0.51201856: 1733it [40:23,  1.11s/it]2017-06-01 18:49:54,688 root  INFO     step 1733.000000 - time: 1.363865, loss: 0.410946, perplexity: 1.508244, precision: 0.515625, batch_len: 97.000000
Train, loss=0.41094625: 1734it [40:24,  1.19s/it]2017-06-01 18:49:55,807 root  INFO     step 1734.000000 - time: 1.035879, loss: 0.366857, perplexity: 1.443192, precision: 0.625000, batch_len: 104.000000
Train, loss=0.36685723: 1735it [40:25,  1.17s/it]2017-06-01 18:49:56,892 root  INFO     step 1735.000000 - time: 1.067963, loss: 0.433629, perplexity: 1.542846, precision: 0.593750, batch_len: 88.000000
Train, loss=0.43362892: 1736it [40:26,  1.14s/it]2017-06-01 18:49:58,145 root  INFO     step 1736.000000 - time: 1.226099, loss: 0.488260, perplexity: 1.629478, precision: 0.562500, batch_len: 91.000000
Train, loss=0.48826000: 1737it [40:28,  1.18s/it]2017-06-01 18:49:59,411 root  INFO     step 1737.000000 - time: 1.134315, loss: 0.369885, perplexity: 1.447568, precision: 0.515625, batch_len: 124.000000
Train, loss=0.36988470: 1738it [40:29,  1.20s/it]2017-06-01 18:50:00,769 root  INFO     step 1738.000000 - time: 1.233061, loss: 0.480371, perplexity: 1.616675, precision: 0.515625, batch_len: 107.000000
Train, loss=0.48037133: 1739it [40:30,  1.25s/it]2017-06-01 18:50:02,352 root  INFO     step 1739.000000 - time: 1.488750, loss: 0.569481, perplexity: 1.767349, precision: 0.250000, batch_len: 123.000000
Train, loss=0.56948084: 1740it [40:32,  1.35s/it]2017-06-01 18:50:03,832 root  INFO     step 1740.000000 - time: 1.369874, loss: 1.024448, perplexity: 2.785558, precision: 0.171875, batch_len: 121.000000
Train, loss=1.02444816: 1741it [40:33,  1.39s/it]2017-06-01 18:50:04,812 root  INFO     step 1741.000000 - time: 0.917304, loss: 0.553617, perplexity: 1.739533, precision: 0.500000, batch_len: 80.000000
Train, loss=0.55361670: 1742it [40:34,  1.27s/it]2017-06-01 18:50:06,077 root  INFO     step 1742.000000 - time: 1.253185, loss: 0.431021, perplexity: 1.538828, precision: 0.406250, batch_len: 118.000000
Train, loss=0.43102083: 1743it [40:35,  1.27s/it]2017-06-01 18:50:07,622 root  INFO     step 1743.000000 - time: 1.525801, loss: 0.330415, perplexity: 1.391545, precision: 0.640625, batch_len: 116.000000
Train, loss=0.33041489: 1744it [40:37,  1.35s/it]2017-06-01 18:50:08,697 root  INFO     step 1744.000000 - time: 1.036935, loss: 0.384511, perplexity: 1.468897, precision: 0.609375, batch_len: 83.000000
Train, loss=0.38451147: 1745it [40:38,  1.27s/it]2017-06-01 18:50:09,803 root  INFO     step 1745.000000 - time: 1.010469, loss: 0.411651, perplexity: 1.509307, precision: 0.500000, batch_len: 84.000000
Train, loss=0.41165057: 1746it [40:39,  1.22s/it]2017-06-01 18:50:11,281 root  INFO     step 1746.000000 - time: 1.335152, loss: 0.345168, perplexity: 1.412227, precision: 0.484375, batch_len: 129.000000
Train, loss=0.34516817: 1747it [40:41,  1.30s/it]2017-06-01 18:50:12,667 root  INFO     step 1747.000000 - time: 1.314538, loss: 0.523298, perplexity: 1.687584, precision: 0.406250, batch_len: 126.000000
Train, loss=0.52329803: 1748it [40:42,  1.32s/it]2017-06-01 18:50:14,699 root  INFO     step 1748.000000 - time: 1.809743, loss: 0.548637, perplexity: 1.730892, precision: 0.343750, batch_len: 137.000000
Train, loss=0.54863703: 1749it [40:44,  1.54s/it]2017-06-01 18:50:15,862 root  INFO     step 1749.000000 - time: 1.080076, loss: 0.207247, perplexity: 1.230286, precision: 0.671875, batch_len: 98.000000
Train, loss=0.20724705: 1750it [40:45,  1.42s/it]2017-06-01 18:50:17,337 root  INFO     step 1750.000000 - time: 1.347354, loss: 0.369787, perplexity: 1.447427, precision: 0.421875, batch_len: 119.000000
Train, loss=0.36978734: 1751it [40:47,  1.44s/it]2017-06-01 18:50:18,338 root  INFO     step 1751.000000 - time: 0.888508, loss: 0.426227, perplexity: 1.531468, precision: 0.453125, batch_len: 82.000000
Train, loss=0.42622691: 1752it [40:48,  1.31s/it]2017-06-01 18:50:19,584 root  INFO     step 1752.000000 - time: 1.113369, loss: 0.361236, perplexity: 1.435102, precision: 0.578125, batch_len: 99.000000
Train, loss=0.36123610: 1753it [40:49,  1.29s/it]2017-06-01 18:50:20,866 root  INFO     step 1753.000000 - time: 1.194348, loss: 0.262472, perplexity: 1.300140, precision: 0.718750, batch_len: 78.000000
Train, loss=0.26247168: 1754it [40:50,  1.29s/it]2017-06-01 18:50:22,349 root  INFO     step 1754.000000 - time: 1.475208, loss: 0.305311, perplexity: 1.357047, precision: 0.531250, batch_len: 136.000000
Train, loss=0.30531138: 1755it [40:52,  1.35s/it]2017-06-01 18:50:23,735 root  INFO     step 1755.000000 - time: 1.332755, loss: 0.300898, perplexity: 1.351072, precision: 0.531250, batch_len: 133.000000
Train, loss=0.30089825: 1756it [40:53,  1.36s/it]2017-06-01 18:50:25,099 root  INFO     step 1756.000000 - time: 1.357063, loss: 0.356710, perplexity: 1.428622, precision: 0.531250, batch_len: 130.000000
Train, loss=0.35671049: 1757it [40:54,  1.36s/it]2017-06-01 18:50:26,789 root  INFO     step 1757.000000 - time: 1.558625, loss: 0.391798, perplexity: 1.479638, precision: 0.531250, batch_len: 135.000000
Train, loss=0.39179760: 1758it [40:56,  1.46s/it]2017-06-01 18:50:28,518 root  INFO     step 1758.000000 - time: 1.578356, loss: 0.364233, perplexity: 1.439410, precision: 0.531250, batch_len: 144.000000
Train, loss=0.36423343: 1759it [40:58,  1.54s/it]2017-06-01 18:50:29,989 root  INFO     step 1759.000000 - time: 1.424969, loss: 0.282563, perplexity: 1.326526, precision: 0.718750, batch_len: 72.000000
Train, loss=0.28256327: 1760it [40:59,  1.52s/it]2017-06-01 18:50:31,679 root  INFO     step 1760.000000 - time: 1.591995, loss: 0.658062, perplexity: 1.931047, precision: 0.250000, batch_len: 138.000000
Train, loss=0.65806222: 1761it [41:01,  1.57s/it]2017-06-01 18:50:32,990 root  INFO     step 1761.000000 - time: 1.187587, loss: 1.207167, perplexity: 3.343997, precision: 0.125000, batch_len: 96.000000
Train, loss=1.20716679: 1762it [41:02,  1.49s/it]2017-06-01 18:50:34,402 root  INFO     step 1762.000000 - time: 1.302549, loss: 0.763887, perplexity: 2.146604, precision: 0.234375, batch_len: 141.000000
Train, loss=0.76388693: 1763it [41:04,  1.47s/it]2017-06-01 18:50:36,338 root  INFO     step 1763.000000 - time: 1.734404, loss: 0.573282, perplexity: 1.774080, precision: 0.312500, batch_len: 122.000000
Train, loss=0.57328182: 1764it [41:06,  1.61s/it]2017-06-01 18:50:37,850 root  INFO     step 1764.000000 - time: 1.449036, loss: 0.407443, perplexity: 1.502969, precision: 0.375000, batch_len: 139.000000
Train, loss=0.40744275: 1765it [41:07,  1.58s/it]2017-06-01 18:50:38,801 root  INFO     step 1765.000000 - time: 0.932679, loss: 0.366447, perplexity: 1.442599, precision: 0.484375, batch_len: 77.000000
Train, loss=0.36644655: 1766it [41:08,  1.39s/it]2017-06-01 18:50:40,582 root  INFO     step 1766.000000 - time: 1.700265, loss: 0.409743, perplexity: 1.506430, precision: 0.406250, batch_len: 152.000000
Train, loss=0.40974268: 1767it [41:10,  1.51s/it]2017-06-01 18:50:42,079 root  INFO     step 1767.000000 - time: 1.452992, loss: 0.443479, perplexity: 1.558119, precision: 0.343750, batch_len: 134.000000
Train, loss=0.44347924: 1768it [41:11,  1.50s/it]2017-06-01 18:50:43,647 root  INFO     step 1768.000000 - time: 1.459783, loss: 0.420371, perplexity: 1.522526, precision: 0.390625, batch_len: 132.000000
Train, loss=0.42037052: 1769it [41:13,  1.52s/it]2017-06-01 18:50:45,501 root  INFO     step 1769.000000 - time: 1.807625, loss: 0.376022, perplexity: 1.456479, precision: 0.375000, batch_len: 150.000000
Train, loss=0.37602174: 1770it [41:15,  1.62s/it]2017-06-01 18:50:46,476 root  INFO     step 1770.000000 - time: 0.932690, loss: 0.454167, perplexity: 1.574862, precision: 0.640625, batch_len: 76.000000
Train, loss=0.45416740: 1771it [41:16,  1.43s/it]2017-06-01 18:50:47,826 root  INFO     step 1771.000000 - time: 1.286525, loss: 0.387796, perplexity: 1.473730, precision: 0.406250, batch_len: 131.000000
Train, loss=0.38779646: 1772it [41:17,  1.41s/it]2017-06-01 18:50:49,107 root  INFO     step 1772.000000 - time: 1.111122, loss: 0.317898, perplexity: 1.374236, precision: 0.718750, batch_len: 74.000000
Train, loss=0.31789815: 1773it [41:18,  1.37s/it]2017-06-01 18:50:50,717 root  INFO     step 1773.000000 - time: 1.555223, loss: 0.476534, perplexity: 1.610482, precision: 0.250000, batch_len: 142.000000
Train, loss=0.47653350: 1774it [41:20,  1.44s/it]2017-06-01 18:50:51,806 root  INFO     step 1774.000000 - time: 0.919245, loss: 0.295349, perplexity: 1.343595, precision: 0.531250, batch_len: 71.000000
Train, loss=0.29534894: 1775it [41:21,  1.33s/it]2017-06-01 18:50:51,889 root  INFO     Generating first batch)
2017-06-01 18:50:55,455 root  INFO     step 1775.000000 - time: 1.241788, loss: 0.302188, perplexity: 1.352816, precision: 0.578125, batch_len: 96.000000
Train, loss=0.30218840: 1776it [41:25,  2.03s/it]2017-06-01 18:50:57,002 root  INFO     step 1776.000000 - time: 1.088030, loss: 0.548556, perplexity: 1.730752, precision: 0.343750, batch_len: 111.000000
Train, loss=0.54855579: 1777it [41:26,  1.88s/it]2017-06-01 18:50:58,436 root  INFO     step 1777.000000 - time: 1.282202, loss: 0.590263, perplexity: 1.804463, precision: 0.281250, batch_len: 128.000000
Train, loss=0.59026289: 1778it [41:28,  1.75s/it]2017-06-01 18:50:59,846 root  INFO     step 1778.000000 - time: 1.223291, loss: 0.636627, perplexity: 1.890095, precision: 0.250000, batch_len: 110.000000
Train, loss=0.63662720: 1779it [41:29,  1.65s/it]2017-06-01 18:51:01,723 root  INFO     step 1779.000000 - time: 1.617383, loss: 0.584661, perplexity: 1.794383, precision: 0.390625, batch_len: 120.000000
Train, loss=0.58466113: 1780it [41:31,  1.72s/it]2017-06-01 18:51:02,762 root  INFO     step 1780.000000 - time: 1.028798, loss: 0.437116, perplexity: 1.548236, precision: 0.437500, batch_len: 104.000000
Train, loss=0.43711624: 1781it [41:32,  1.51s/it]2017-06-01 18:51:03,857 root  INFO     step 1781.000000 - time: 1.019576, loss: 0.325831, perplexity: 1.385182, precision: 0.546875, batch_len: 101.000000
Train, loss=0.32583135: 1782it [41:33,  1.39s/it]2017-06-01 18:51:04,862 root  INFO     step 1782.000000 - time: 0.968544, loss: 0.473437, perplexity: 1.605503, precision: 0.578125, batch_len: 105.000000
Train, loss=0.47343734: 1783it [41:34,  1.27s/it]2017-06-01 18:51:05,960 root  INFO     step 1783.000000 - time: 1.010265, loss: 0.495535, perplexity: 1.641377, precision: 0.406250, batch_len: 109.000000
Train, loss=0.49553549: 1784it [41:35,  1.22s/it]2017-06-01 18:51:07,402 root  INFO     step 1784.000000 - time: 1.312416, loss: 0.413945, perplexity: 1.512774, precision: 0.437500, batch_len: 108.000000
Train, loss=0.41394535: 1785it [41:37,  1.29s/it]2017-06-01 18:51:08,652 root  INFO     step 1785.000000 - time: 1.170763, loss: 0.441409, perplexity: 1.554896, precision: 0.390625, batch_len: 113.000000
Train, loss=0.44140893: 1786it [41:38,  1.28s/it]2017-06-01 18:51:10,040 root  INFO     step 1786.000000 - time: 1.326560, loss: 0.403388, perplexity: 1.496887, precision: 0.468750, batch_len: 112.000000
Train, loss=0.40338758: 1787it [41:39,  1.31s/it]2017-06-01 18:51:11,059 root  INFO     step 1787.000000 - time: 1.013162, loss: 0.429869, perplexity: 1.537057, precision: 0.562500, batch_len: 90.000000
Train, loss=0.42986932: 1788it [41:40,  1.22s/it]2017-06-01 18:51:12,186 root  INFO     step 1788.000000 - time: 0.960675, loss: 0.418151, perplexity: 1.519150, precision: 0.500000, batch_len: 102.000000
Train, loss=0.41815081: 1789it [41:42,  1.19s/it]2017-06-01 18:51:13,451 root  INFO     step 1789.000000 - time: 1.250714, loss: 0.317233, perplexity: 1.373322, precision: 0.640625, batch_len: 97.000000
Train, loss=0.31723285: 1790it [41:43,  1.22s/it]2017-06-01 18:51:14,771 root  INFO     step 1790.000000 - time: 1.262132, loss: 0.367749, perplexity: 1.444480, precision: 0.546875, batch_len: 114.000000
Train, loss=0.36774939: 1791it [41:44,  1.25s/it]2017-06-01 18:51:15,771 root  INFO     step 1791.000000 - time: 0.971211, loss: 0.592778, perplexity: 1.809006, precision: 0.390625, batch_len: 81.000000
Train, loss=0.59277773: 1792it [41:45,  1.17s/it]2017-06-01 18:51:16,909 root  INFO     step 1792.000000 - time: 1.084985, loss: 0.530877, perplexity: 1.700423, precision: 0.218750, batch_len: 106.000000
Train, loss=0.53087723: 1793it [41:46,  1.16s/it]2017-06-01 18:51:18,246 root  INFO     step 1793.000000 - time: 1.288236, loss: 0.561850, perplexity: 1.753914, precision: 0.203125, batch_len: 124.000000
Train, loss=0.56185001: 1794it [41:48,  1.21s/it]2017-06-01 18:51:19,218 root  INFO     step 1794.000000 - time: 0.945318, loss: 0.526449, perplexity: 1.692910, precision: 0.328125, batch_len: 93.000000
Train, loss=0.52644897: 1795it [41:49,  1.14s/it]2017-06-01 18:51:20,433 root  INFO     step 1795.000000 - time: 1.201121, loss: 0.343644, perplexity: 1.410076, precision: 0.562500, batch_len: 100.000000
Train, loss=0.34364361: 1796it [41:50,  1.16s/it]2017-06-01 18:51:21,997 root  INFO     step 1796.000000 - time: 1.513335, loss: 0.266908, perplexity: 1.305921, precision: 0.593750, batch_len: 117.000000
Train, loss=0.26690847: 1797it [41:51,  1.28s/it]2017-06-01 18:51:23,175 root  INFO     step 1797.000000 - time: 1.093531, loss: 0.341223, perplexity: 1.406667, precision: 0.578125, batch_len: 92.000000
Train, loss=0.34122318: 1798it [41:53,  1.25s/it]2017-06-01 18:51:24,268 root  INFO     step 1798.000000 - time: 1.073015, loss: 0.331165, perplexity: 1.392589, precision: 0.593750, batch_len: 103.000000
Train, loss=0.33116478: 1799it [41:54,  1.20s/it]2017-06-01 18:51:25,355 root  INFO     step 1799.000000 - time: 0.964918, loss: 0.480430, perplexity: 1.616769, precision: 0.609375, batch_len: 88.000000
Train, loss=0.48042980: 1800it [41:55,  1.17s/it]2017-06-01 18:51:26,549 root  INFO     step 1800.000000 - time: 1.187205, loss: 0.647946, perplexity: 1.911610, precision: 0.468750, batch_len: 94.000000
Train, loss=0.64794600: 1801it [41:56,  1.18s/it]2017-06-01 18:51:27,949 root  INFO     step 1801.000000 - time: 1.387526, loss: 0.369693, perplexity: 1.447291, precision: 0.375000, batch_len: 121.000000
Train, loss=0.36969322: 1802it [41:57,  1.24s/it]2017-06-01 18:51:29,276 root  INFO     step 1802.000000 - time: 1.253385, loss: 0.353783, perplexity: 1.424446, precision: 0.640625, batch_len: 91.000000
Train, loss=0.35378289: 1803it [41:59,  1.27s/it]2017-06-01 18:51:30,451 root  INFO     step 1803.000000 - time: 1.091330, loss: 0.322134, perplexity: 1.380070, precision: 0.625000, batch_len: 89.000000
Train, loss=0.32213417: 1804it [42:00,  1.24s/it]2017-06-01 18:51:31,619 root  INFO     step 1804.000000 - time: 1.054043, loss: 0.462166, perplexity: 1.587508, precision: 0.593750, batch_len: 85.000000
Train, loss=0.46216553: 1805it [42:01,  1.22s/it]2017-06-01 18:51:33,080 root  INFO     step 1805.000000 - time: 1.353032, loss: 0.431315, perplexity: 1.539280, precision: 0.453125, batch_len: 123.000000
Train, loss=0.43131506: 1806it [42:02,  1.29s/it]2017-06-01 18:51:34,238 root  INFO     step 1806.000000 - time: 1.037422, loss: 0.391919, perplexity: 1.479819, precision: 0.500000, batch_len: 107.000000
Train, loss=0.39191949: 1807it [42:04,  1.25s/it]2017-06-01 18:51:35,365 root  INFO     step 1807.000000 - time: 1.072471, loss: 0.466482, perplexity: 1.594375, precision: 0.562500, batch_len: 83.000000
Train, loss=0.46648169: 1808it [42:05,  1.21s/it]2017-06-01 18:51:36,608 root  INFO     step 1808.000000 - time: 1.125156, loss: 0.356465, perplexity: 1.428271, precision: 0.609375, batch_len: 80.000000
Train, loss=0.35646456: 1809it [42:06,  1.22s/it]2017-06-01 18:51:37,751 root  INFO     step 1809.000000 - time: 1.125796, loss: 0.414383, perplexity: 1.513437, precision: 0.546875, batch_len: 115.000000
Train, loss=0.41438329: 1810it [42:07,  1.20s/it]2017-06-01 18:51:39,125 root  INFO     step 1810.000000 - time: 1.310890, loss: 0.272125, perplexity: 1.312751, precision: 0.546875, batch_len: 125.000000
Train, loss=0.27212477: 1811it [42:09,  1.25s/it]2017-06-01 18:51:40,117 root  INFO     step 1811.000000 - time: 0.887381, loss: 0.357109, perplexity: 1.429192, precision: 0.671875, batch_len: 86.000000
Train, loss=0.35710907: 1812it [42:09,  1.17s/it]2017-06-01 18:51:41,093 root  INFO     step 1812.000000 - time: 0.894571, loss: 0.327820, perplexity: 1.387939, precision: 0.593750, batch_len: 87.000000
Train, loss=0.32781956: 1813it [42:10,  1.11s/it]2017-06-01 18:51:42,677 root  INFO     step 1813.000000 - time: 1.556469, loss: 0.257016, perplexity: 1.293066, precision: 0.609375, batch_len: 116.000000
Train, loss=0.25701582: 1814it [42:12,  1.26s/it]2017-06-01 18:51:44,278 root  INFO     step 1814.000000 - time: 1.472779, loss: 0.369467, perplexity: 1.446963, precision: 0.531250, batch_len: 136.000000
Train, loss=0.36946708: 1815it [42:14,  1.36s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4659957 get requests, put_count=4659963 evicted_count=3000 eviction_rate=0.000643782 and unsatisfied allocation rate=0.000727904
2017-06-01 18:51:45,327 root  INFO     step 1815.000000 - time: 0.975846, loss: 0.418272, perplexity: 1.519333, precision: 0.500000, batch_len: 79.000000
Train, loss=0.41827154: 1816it [42:15,  1.27s/it]2017-06-01 18:51:46,492 root  INFO     step 1816.000000 - time: 1.092324, loss: 0.260032, perplexity: 1.296972, precision: 0.609375, batch_len: 98.000000
Train, loss=0.26003197: 1817it [42:16,  1.24s/it]2017-06-01 18:51:47,834 root  INFO     step 1817.000000 - time: 1.183865, loss: 0.328371, perplexity: 1.388704, precision: 0.515625, batch_len: 129.000000
Train, loss=0.32837081: 1818it [42:17,  1.27s/it]2017-06-01 18:51:49,015 root  INFO     step 1818.000000 - time: 1.105855, loss: 0.323762, perplexity: 1.382319, precision: 0.656250, batch_len: 82.000000
Train, loss=0.32376245: 1819it [42:18,  1.24s/it]2017-06-01 18:51:50,618 root  INFO     step 1819.000000 - time: 1.480997, loss: 0.587655, perplexity: 1.799763, precision: 0.359375, batch_len: 135.000000
Train, loss=0.58765501: 1820it [42:20,  1.35s/it]2017-06-01 18:51:52,008 root  INFO     step 1820.000000 - time: 1.348354, loss: 1.153291, perplexity: 3.168602, precision: 0.109375, batch_len: 119.000000
Train, loss=1.15329051: 1821it [42:21,  1.36s/it]2017-06-01 18:51:53,525 root  INFO     step 1821.000000 - time: 1.481064, loss: 0.637845, perplexity: 1.892399, precision: 0.187500, batch_len: 137.000000
Train, loss=0.63784546: 1822it [42:23,  1.41s/it]2017-06-01 18:51:54,858 root  INFO     step 1822.000000 - time: 1.304324, loss: 0.885833, perplexity: 2.425004, precision: 0.218750, batch_len: 118.000000
Train, loss=0.88583332: 1823it [42:24,  1.39s/it]2017-06-01 18:51:56,331 root  INFO     step 1823.000000 - time: 1.352992, loss: 0.352128, perplexity: 1.422090, precision: 0.546875, batch_len: 99.000000
Train, loss=0.35212770: 1824it [42:26,  1.41s/it]2017-06-01 18:51:57,390 root  INFO     step 1824.000000 - time: 1.021216, loss: 0.464557, perplexity: 1.591309, precision: 0.562500, batch_len: 84.000000
Train, loss=0.46455681: 1825it [42:27,  1.31s/it]2017-06-01 18:51:58,614 root  INFO     step 1825.000000 - time: 1.099011, loss: 0.327130, perplexity: 1.386982, precision: 0.578125, batch_len: 96.000000
Train, loss=0.32713023: 1826it [42:28,  1.28s/it]2017-06-01 18:52:00,100 root  INFO     step 1826.000000 - time: 1.324321, loss: 0.402484, perplexity: 1.495535, precision: 0.421875, batch_len: 126.000000
Train, loss=0.40248379: 1827it [42:29,  1.34s/it]2017-06-01 18:52:01,848 root  INFO     step 1827.000000 - time: 1.641369, loss: 0.363666, perplexity: 1.438593, precision: 0.406250, batch_len: 144.000000
Train, loss=0.36366576: 1828it [42:31,  1.46s/it]2017-06-01 18:52:03,488 root  INFO     step 1828.000000 - time: 1.514635, loss: 0.403787, perplexity: 1.497485, precision: 0.390625, batch_len: 122.000000
Train, loss=0.40378690: 1829it [42:33,  1.52s/it]2017-06-01 18:52:04,552 root  INFO     step 1829.000000 - time: 1.003789, loss: 0.341437, perplexity: 1.406967, precision: 0.609375, batch_len: 78.000000
Train, loss=0.34143656: 1830it [42:34,  1.38s/it]2017-06-01 18:52:06,026 root  INFO     step 1830.000000 - time: 1.375559, loss: 0.363269, perplexity: 1.438022, precision: 0.484375, batch_len: 130.000000
Train, loss=0.36326885: 1831it [42:35,  1.41s/it]2017-06-01 18:52:07,908 root  INFO     step 1831.000000 - time: 1.872116, loss: 0.438674, perplexity: 1.550649, precision: 0.296875, batch_len: 152.000000
Train, loss=0.43867362: 1832it [42:37,  1.55s/it]2017-06-01 18:52:09,622 root  INFO     step 1832.000000 - time: 1.596584, loss: 0.652797, perplexity: 1.920906, precision: 0.062500, batch_len: 133.000000
Train, loss=0.65279675: 1833it [42:39,  1.60s/it]2017-06-01 18:52:11,068 root  INFO     step 1833.000000 - time: 1.361451, loss: 0.965071, perplexity: 2.624974, precision: 0.093750, batch_len: 134.000000
Train, loss=0.96507090: 1834it [42:40,  1.55s/it]2017-06-01 18:52:12,318 root  INFO     step 1834.000000 - time: 1.157143, loss: 0.617614, perplexity: 1.854499, precision: 0.250000, batch_len: 72.000000
Train, loss=0.61761439: 1835it [42:42,  1.46s/it]2017-06-01 18:52:13,236 root  INFO     step 1835.000000 - time: 0.904295, loss: 1.014461, perplexity: 2.757876, precision: 0.265625, batch_len: 74.000000
Train, loss=1.01446080: 1836it [42:43,  1.30s/it]2017-06-01 18:52:14,319 root  INFO     step 1836.000000 - time: 0.920381, loss: 0.597588, perplexity: 1.817729, precision: 0.390625, batch_len: 76.000000
Train, loss=0.59758818: 1837it [42:44,  1.23s/it]2017-06-01 18:52:16,107 root  INFO     step 1837.000000 - time: 1.747595, loss: 0.585745, perplexity: 1.796328, precision: 0.296875, batch_len: 138.000000
Train, loss=0.58574474: 1838it [42:45,  1.40s/it]2017-06-01 18:52:17,505 root  INFO     step 1838.000000 - time: 1.323122, loss: 0.466942, perplexity: 1.595109, precision: 0.359375, batch_len: 132.000000
Train, loss=0.46694198: 1839it [42:47,  1.40s/it]2017-06-01 18:52:18,957 root  INFO     step 1839.000000 - time: 1.420962, loss: 0.531319, perplexity: 1.701175, precision: 0.343750, batch_len: 139.000000
Train, loss=0.53131932: 1840it [42:48,  1.42s/it]2017-06-01 18:52:20,317 root  INFO     step 1840.000000 - time: 1.323331, loss: 0.497997, perplexity: 1.645422, precision: 0.250000, batch_len: 141.000000
Train, loss=0.49799716: 1841it [42:50,  1.40s/it]2017-06-01 18:52:22,266 root  INFO     step 1841.000000 - time: 1.931146, loss: 0.542576, perplexity: 1.720433, precision: 0.312500, batch_len: 150.000000
Train, loss=0.54257584: 1842it [42:52,  1.56s/it]2017-06-01 18:52:23,540 root  INFO     step 1842.000000 - time: 1.080960, loss: 0.363323, perplexity: 1.438100, precision: 0.343750, batch_len: 77.000000
Train, loss=0.36332279: 1843it [42:53,  1.48s/it]2017-06-01 18:52:24,449 root  INFO     step 1843.000000 - time: 0.899341, loss: 0.329032, perplexity: 1.389623, precision: 0.578125, batch_len: 71.000000
Train, loss=0.32903242: 1844it [42:54,  1.31s/it]2017-06-01 18:52:26,081 root  INFO     step 1844.000000 - time: 1.473333, loss: 0.374267, perplexity: 1.453926, precision: 0.265625, batch_len: 142.000000
Train, loss=0.37426728: 1845it [42:55,  1.40s/it]2017-06-01 18:52:27,540 root  INFO     step 1845.000000 - time: 1.417070, loss: 0.428398, perplexity: 1.534797, precision: 0.250000, batch_len: 131.000000
Train, loss=0.42839822: 1846it [42:57,  1.42s/it]2017-06-01 18:52:27,691 root  INFO     Generating first batch)
2017-06-01 18:52:31,788 root  INFO     step 1846.000000 - time: 1.108691, loss: 0.349218, perplexity: 1.417959, precision: 0.500000, batch_len: 96.000000
Train, loss=0.34921825: 1847it [43:01,  2.27s/it]2017-06-01 18:52:32,904 root  INFO     step 1847.000000 - time: 1.101738, loss: 0.361853, perplexity: 1.435988, precision: 0.484375, batch_len: 113.000000
Train, loss=0.36185318: 1848it [43:02,  1.92s/it]2017-06-01 18:52:34,049 root  INFO     step 1848.000000 - time: 0.889860, loss: 0.262941, perplexity: 1.300750, precision: 0.593750, batch_len: 101.000000
Train, loss=0.26294127: 1849it [43:03,  1.69s/it]2017-06-01 18:52:35,434 root  INFO     step 1849.000000 - time: 1.248491, loss: 0.331221, perplexity: 1.392667, precision: 0.656250, batch_len: 105.000000
Train, loss=0.33122054: 1850it [43:05,  1.60s/it]2017-06-01 18:52:36,782 root  INFO     step 1850.000000 - time: 1.278743, loss: 0.328447, perplexity: 1.388810, precision: 0.640625, batch_len: 108.000000
Train, loss=0.32844716: 1851it [43:06,  1.52s/it]2017-06-01 18:52:37,841 root  INFO     step 1851.000000 - time: 1.027405, loss: 0.372829, perplexity: 1.451836, precision: 0.671875, batch_len: 104.000000
Train, loss=0.37282881: 1852it [43:07,  1.38s/it]2017-06-01 18:52:38,896 root  INFO     step 1852.000000 - time: 1.045068, loss: 0.350865, perplexity: 1.420296, precision: 0.531250, batch_len: 100.000000
Train, loss=0.35086513: 1853it [43:08,  1.29s/it]2017-06-01 18:52:40,217 root  INFO     step 1853.000000 - time: 1.243355, loss: 0.454367, perplexity: 1.575175, precision: 0.296875, batch_len: 128.000000
Train, loss=0.45436668: 1854it [43:10,  1.30s/it]2017-06-01 18:52:41,549 root  INFO     step 1854.000000 - time: 1.201139, loss: 0.394840, perplexity: 1.484147, precision: 0.343750, batch_len: 110.000000
Train, loss=0.39484021: 1855it [43:11,  1.31s/it]2017-06-01 18:52:42,958 root  INFO     step 1855.000000 - time: 1.346180, loss: 0.243161, perplexity: 1.275274, precision: 0.625000, batch_len: 112.000000
Train, loss=0.24316120: 1856it [43:12,  1.34s/it]2017-06-01 18:52:44,394 root  INFO     step 1856.000000 - time: 1.333075, loss: 0.250889, perplexity: 1.285168, precision: 0.750000, batch_len: 117.000000
Train, loss=0.25088924: 1857it [43:14,  1.37s/it]2017-06-01 18:52:45,575 root  INFO     step 1857.000000 - time: 1.074868, loss: 0.436583, perplexity: 1.547411, precision: 0.562500, batch_len: 111.000000
Train, loss=0.43658346: 1858it [43:15,  1.31s/it]2017-06-01 18:52:46,706 root  INFO     step 1858.000000 - time: 1.085352, loss: 0.320732, perplexity: 1.378136, precision: 0.593750, batch_len: 92.000000
Train, loss=0.32073182: 1859it [43:16,  1.26s/it]2017-06-01 18:52:47,743 root  INFO     step 1859.000000 - time: 1.008087, loss: 0.678076, perplexity: 1.970083, precision: 0.296875, batch_len: 102.000000
Train, loss=0.67807573: 1860it [43:17,  1.19s/it]2017-06-01 18:52:49,417 root  INFO     step 1860.000000 - time: 1.653870, loss: 0.669651, perplexity: 1.953556, precision: 0.281250, batch_len: 121.000000
Train, loss=0.66965115: 1861it [43:19,  1.34s/it]2017-06-01 18:52:50,447 root  INFO     step 1861.000000 - time: 0.971921, loss: 0.458486, perplexity: 1.581677, precision: 0.593750, batch_len: 81.000000
Train, loss=0.45848554: 1862it [43:20,  1.24s/it]2017-06-01 18:52:51,654 root  INFO     step 1862.000000 - time: 1.092622, loss: 0.361392, perplexity: 1.435326, precision: 0.562500, batch_len: 97.000000
Train, loss=0.36139181: 1863it [43:21,  1.23s/it]2017-06-01 18:52:53,021 root  INFO     step 1863.000000 - time: 1.337688, loss: 0.324489, perplexity: 1.383324, precision: 0.593750, batch_len: 120.000000
Train, loss=0.32448944: 1864it [43:22,  1.27s/it]2017-06-01 18:52:54,310 root  INFO     step 1864.000000 - time: 1.216607, loss: 0.316363, perplexity: 1.372129, precision: 0.515625, batch_len: 124.000000
Train, loss=0.31636330: 1865it [43:24,  1.28s/it]2017-06-01 18:52:55,769 root  INFO     step 1865.000000 - time: 1.411502, loss: 0.427791, perplexity: 1.533865, precision: 0.531250, batch_len: 91.000000
Train, loss=0.42779097: 1866it [43:25,  1.33s/it]2017-06-01 18:52:56,970 root  INFO     step 1866.000000 - time: 1.137277, loss: 0.440123, perplexity: 1.552899, precision: 0.609375, batch_len: 88.000000
Train, loss=0.44012332: 1867it [43:26,  1.29s/it]2017-06-01 18:52:58,317 root  INFO     step 1867.000000 - time: 1.321870, loss: 0.370146, perplexity: 1.447946, precision: 0.625000, batch_len: 125.000000
Train, loss=0.37014619: 1868it [43:28,  1.31s/it]2017-06-01 18:52:59,329 root  INFO     step 1868.000000 - time: 1.006950, loss: 0.392854, perplexity: 1.481201, precision: 0.578125, batch_len: 90.000000
Train, loss=0.39285356: 1869it [43:29,  1.22s/it]2017-06-01 18:53:00,401 root  INFO     step 1869.000000 - time: 1.033610, loss: 0.339616, perplexity: 1.404408, precision: 0.437500, batch_len: 114.000000
Train, loss=0.33961573: 1870it [43:30,  1.18s/it]2017-06-01 18:53:02,062 root  INFO     step 1870.000000 - time: 1.549347, loss: 0.518529, perplexity: 1.679555, precision: 0.375000, batch_len: 123.000000
Train, loss=0.51852906: 1871it [43:31,  1.32s/it]2017-06-01 18:53:03,332 root  INFO     step 1871.000000 - time: 1.224876, loss: 0.448631, perplexity: 1.566166, precision: 0.437500, batch_len: 93.000000
Train, loss=0.44863066: 1872it [43:33,  1.31s/it]2017-06-01 18:53:04,589 root  INFO     step 1872.000000 - time: 1.104589, loss: 0.377118, perplexity: 1.458076, precision: 0.609375, batch_len: 109.000000
Train, loss=0.37711775: 1873it [43:34,  1.29s/it]2017-06-01 18:53:05,757 root  INFO     step 1873.000000 - time: 1.098237, loss: 0.371131, perplexity: 1.449373, precision: 0.562500, batch_len: 106.000000
Train, loss=0.37113124: 1874it [43:35,  1.25s/it]2017-06-01 18:53:06,828 root  INFO     step 1874.000000 - time: 1.052128, loss: 0.440460, perplexity: 1.553421, precision: 0.531250, batch_len: 89.000000
Train, loss=0.44045991: 1875it [43:36,  1.20s/it]2017-06-01 18:53:07,975 root  INFO     step 1875.000000 - time: 1.095929, loss: 0.554950, perplexity: 1.741855, precision: 0.531250, batch_len: 85.000000
Train, loss=0.55495048: 1876it [43:37,  1.18s/it]2017-06-01 18:53:09,439 root  INFO     step 1876.000000 - time: 1.391233, loss: 0.394276, perplexity: 1.483310, precision: 0.546875, batch_len: 107.000000
Train, loss=0.39427620: 1877it [43:39,  1.27s/it]2017-06-01 18:53:10,528 root  INFO     step 1877.000000 - time: 0.986289, loss: 0.292432, perplexity: 1.339681, precision: 0.718750, batch_len: 86.000000
Train, loss=0.29243180: 1878it [43:40,  1.21s/it]2017-06-01 18:53:11,711 root  INFO     step 1878.000000 - time: 1.116843, loss: 0.357175, perplexity: 1.429286, precision: 0.500000, batch_len: 115.000000
Train, loss=0.35717487: 1879it [43:41,  1.20s/it]2017-06-01 18:53:12,828 root  INFO     step 1879.000000 - time: 1.065462, loss: 0.242410, perplexity: 1.274316, precision: 0.687500, batch_len: 98.000000
Train, loss=0.24240962: 1880it [43:42,  1.18s/it]2017-06-01 18:53:13,907 root  INFO     step 1880.000000 - time: 1.033466, loss: 0.606134, perplexity: 1.833330, precision: 0.453125, batch_len: 99.000000
Train, loss=0.60613400: 1881it [43:43,  1.15s/it]2017-06-01 18:53:15,461 root  INFO     step 1881.000000 - time: 1.548369, loss: 0.345573, perplexity: 1.412799, precision: 0.593750, batch_len: 116.000000
Train, loss=0.34557265: 1882it [43:45,  1.27s/it]2017-06-01 18:53:16,656 root  INFO     step 1882.000000 - time: 1.127067, loss: 0.471197, perplexity: 1.601910, precision: 0.515625, batch_len: 83.000000
Train, loss=0.47119668: 1883it [43:46,  1.25s/it]2017-06-01 18:53:17,785 root  INFO     step 1883.000000 - time: 1.011835, loss: 0.427089, perplexity: 1.532790, precision: 0.578125, batch_len: 79.000000
Train, loss=0.42708948: 1884it [43:47,  1.21s/it]2017-06-01 18:53:18,863 root  INFO     step 1884.000000 - time: 1.014604, loss: 0.367018, perplexity: 1.443424, precision: 0.593750, batch_len: 94.000000
Train, loss=0.36701778: 1885it [43:48,  1.17s/it]2017-06-01 18:53:20,250 root  INFO     step 1885.000000 - time: 1.333310, loss: 0.473986, perplexity: 1.606385, precision: 0.375000, batch_len: 137.000000
Train, loss=0.47398645: 1886it [43:50,  1.24s/it]2017-06-01 18:53:21,403 root  INFO     step 1886.000000 - time: 1.046929, loss: 0.569550, perplexity: 1.767471, precision: 0.406250, batch_len: 80.000000
Train, loss=0.56954998: 1887it [43:51,  1.21s/it]2017-06-01 18:53:22,577 root  INFO     step 1887.000000 - time: 1.116807, loss: 0.578424, perplexity: 1.783226, precision: 0.375000, batch_len: 87.000000
Train, loss=0.57842410: 1888it [43:52,  1.20s/it]2017-06-01 18:53:23,712 root  INFO     step 1888.000000 - time: 1.103223, loss: 0.549264, perplexity: 1.731978, precision: 0.437500, batch_len: 103.000000
Train, loss=0.54926425: 1889it [43:53,  1.18s/it]2017-06-01 18:53:25,102 root  INFO     step 1889.000000 - time: 1.327305, loss: 0.327724, perplexity: 1.387806, precision: 0.515625, batch_len: 118.000000
Train, loss=0.32772410: 1890it [43:54,  1.24s/it]2017-06-01 18:53:26,529 root  INFO     step 1890.000000 - time: 1.378518, loss: 0.297165, perplexity: 1.346038, precision: 0.531250, batch_len: 129.000000
Train, loss=0.29716539: 1891it [43:56,  1.30s/it]2017-06-01 18:53:27,612 root  INFO     step 1891.000000 - time: 0.997217, loss: 0.431933, perplexity: 1.540232, precision: 0.593750, batch_len: 84.000000
Train, loss=0.43193299: 1892it [43:57,  1.23s/it]2017-06-01 18:53:29,189 root  INFO     step 1892.000000 - time: 1.453635, loss: 0.311711, perplexity: 1.365760, precision: 0.562500, batch_len: 119.000000
Train, loss=0.31171098: 1893it [43:59,  1.34s/it]2017-06-01 18:53:31,261 root  INFO     step 1893.000000 - time: 1.821471, loss: 0.427545, perplexity: 1.533488, precision: 0.328125, batch_len: 138.000000
Train, loss=0.42754465: 1894it [44:01,  1.56s/it]2017-06-01 18:53:32,675 root  INFO     step 1894.000000 - time: 1.395972, loss: 0.487168, perplexity: 1.627700, precision: 0.234375, batch_len: 130.000000
Train, loss=0.48716819: 1895it [44:02,  1.51s/it]2017-06-01 18:53:34,100 root  INFO     step 1895.000000 - time: 1.408481, loss: 0.711642, perplexity: 2.037333, precision: 0.078125, batch_len: 126.000000
Train, loss=0.71164179: 1896it [44:03,  1.49s/it]2017-06-01 18:53:35,002 root  INFO     step 1896.000000 - time: 0.885276, loss: 0.446054, perplexity: 1.562137, precision: 0.328125, batch_len: 82.000000
Train, loss=0.44605449: 1897it [44:04,  1.31s/it]2017-06-01 18:53:36,677 root  INFO     step 1897.000000 - time: 1.499886, loss: 0.476476, perplexity: 1.610390, precision: 0.312500, batch_len: 144.000000
Train, loss=0.47647637: 1898it [44:06,  1.42s/it]2017-06-01 18:53:38,244 root  INFO     step 1898.000000 - time: 1.541559, loss: 1.066304, perplexity: 2.904625, precision: 0.171875, batch_len: 133.000000
Train, loss=1.06630445: 1899it [44:08,  1.46s/it]2017-06-01 18:53:39,697 root  INFO     step 1899.000000 - time: 1.326858, loss: 0.685291, perplexity: 1.984350, precision: 0.187500, batch_len: 135.000000
Train, loss=0.68529147: 1900it [44:09,  1.46s/it]2017-06-01 18:53:41,096 root  INFO     step 1900.000000 - time: 1.384130, loss: 0.701367, perplexity: 2.016508, precision: 0.250000, batch_len: 122.000000
Train, loss=0.70136714: 1901it [44:10,  1.44s/it]2017-06-01 18:53:42,500 root  INFO     step 1901.000000 - time: 1.310808, loss: 0.376152, perplexity: 1.456669, precision: 0.437500, batch_len: 136.000000
Train, loss=0.37615204: 1902it [44:12,  1.43s/it]2017-06-01 18:53:44,821 root  INFO     step 1902.000000 - time: 2.250636, loss: 0.413883, perplexity: 1.512680, precision: 0.390625, batch_len: 152.000000
Train, loss=0.41388309: 1903it [44:14,  1.70s/it]2017-06-01 18:53:45,913 root  INFO     step 1903.000000 - time: 1.076692, loss: 0.301738, perplexity: 1.352207, precision: 0.531250, batch_len: 96.000000
Train, loss=0.30173779: 1904it [44:15,  1.52s/it]2017-06-01 18:53:46,991 root  INFO     step 1904.000000 - time: 0.982332, loss: 0.293388, perplexity: 1.340963, precision: 0.640625, batch_len: 78.000000
Train, loss=0.29338813: 1905it [44:16,  1.38s/it]2017-06-01 18:53:48,124 root  INFO     step 1905.000000 - time: 1.122165, loss: 0.248797, perplexity: 1.282481, precision: 0.656250, batch_len: 72.000000
Train, loss=0.24879652: 1906it [44:18,  1.31s/it]2017-06-01 18:53:50,204 root  INFO     step 1906.000000 - time: 1.851212, loss: 0.313752, perplexity: 1.368550, precision: 0.468750, batch_len: 150.000000
Train, loss=0.31375206: 1907it [44:20,  1.54s/it]2017-06-01 18:53:51,452 root  INFO     step 1907.000000 - time: 1.110258, loss: 0.396764, perplexity: 1.487005, precision: 0.609375, batch_len: 76.000000
Train, loss=0.39676422: 1908it [44:21,  1.45s/it]2017-06-01 18:53:52,974 root  INFO     step 1908.000000 - time: 1.443903, loss: 0.296778, perplexity: 1.345517, precision: 0.593750, batch_len: 139.000000
Train, loss=0.29677802: 1909it [44:22,  1.47s/it]2017-06-01 18:53:54,502 root  INFO     step 1909.000000 - time: 1.404915, loss: 0.348995, perplexity: 1.417642, precision: 0.593750, batch_len: 141.000000
Train, loss=0.34899515: 1910it [44:24,  1.49s/it]2017-06-01 18:53:55,915 root  INFO     step 1910.000000 - time: 1.265115, loss: 0.251206, perplexity: 1.285575, precision: 0.656250, batch_len: 132.000000
Train, loss=0.25120574: 1911it [44:25,  1.47s/it]2017-06-01 18:53:57,047 root  INFO     step 1911.000000 - time: 1.109529, loss: 0.208570, perplexity: 1.231915, precision: 0.781250, batch_len: 74.000000
Train, loss=0.20857024: 1912it [44:26,  1.37s/it]2017-06-01 18:53:58,697 root  INFO     step 1912.000000 - time: 1.512489, loss: 0.298383, perplexity: 1.347678, precision: 0.500000, batch_len: 134.000000
Train, loss=0.29838341: 1913it [44:28,  1.45s/it]2017-06-01 18:53:59,761 root  INFO     step 1913.000000 - time: 0.927188, loss: 0.221028, perplexity: 1.247358, precision: 0.687500, batch_len: 77.000000
Train, loss=0.22102757: 1914it [44:29,  1.34s/it]2017-06-01 18:54:01,179 root  INFO     step 1914.000000 - time: 1.359892, loss: 0.202203, perplexity: 1.224097, precision: 0.671875, batch_len: 131.000000
Train, loss=0.20220342: 1915it [44:31,  1.36s/it]2017-06-01 18:54:02,615 root  INFO     step 1915.000000 - time: 1.329793, loss: 0.362717, perplexity: 1.437230, precision: 0.421875, batch_len: 142.000000
Train, loss=0.36271745: 1916it [44:32,  1.38s/it]2017-06-01 18:54:03,722 root  INFO     step 1916.000000 - time: 1.085243, loss: 0.193227, perplexity: 1.213158, precision: 0.765625, batch_len: 71.000000
Train, loss=0.19322684: 1917it [44:33,  1.30s/it]2017-06-01 18:54:03,906 root  INFO     Generating first batch)
2017-06-01 18:54:07,629 root  INFO     step 1917.000000 - time: 0.974123, loss: 0.294352, perplexity: 1.342257, precision: 0.640625, batch_len: 101.000000
Train, loss=0.29435244: 1918it [44:37,  2.08s/it]2017-06-01 18:54:09,127 root  INFO     step 1918.000000 - time: 1.310446, loss: 0.314715, perplexity: 1.369869, precision: 0.531250, batch_len: 113.000000
Train, loss=0.31471545: 1919it [44:39,  1.91s/it]2017-06-01 18:54:10,287 root  INFO     step 1919.000000 - time: 1.112074, loss: 0.202684, perplexity: 1.224685, precision: 0.656250, batch_len: 96.000000
Train, loss=0.20268407: 1920it [44:40,  1.68s/it]2017-06-01 18:54:11,594 root  INFO     step 1920.000000 - time: 1.290109, loss: 0.308252, perplexity: 1.361044, precision: 0.390625, batch_len: 128.000000
Train, loss=0.30825227: 1921it [44:41,  1.57s/it]2017-06-01 18:54:12,989 root  INFO     step 1921.000000 - time: 1.295020, loss: 0.341993, perplexity: 1.407751, precision: 0.593750, batch_len: 112.000000
Train, loss=0.34199348: 1922it [44:42,  1.52s/it]2017-06-01 18:54:14,010 root  INFO     step 1922.000000 - time: 0.906626, loss: 0.355448, perplexity: 1.426820, precision: 0.687500, batch_len: 88.000000
Train, loss=0.35544801: 1923it [44:43,  1.37s/it]2017-06-01 18:54:16,027 root  INFO     step 1923.000000 - time: 1.969264, loss: 0.473708, perplexity: 1.605938, precision: 0.484375, batch_len: 90.000000
Train, loss=0.47370797: 1924it [44:45,  1.56s/it]2017-06-01 18:54:17,145 root  INFO     step 1924.000000 - time: 1.099494, loss: 0.764206, perplexity: 2.147290, precision: 0.328125, batch_len: 110.000000
Train, loss=0.76420647: 1925it [44:47,  1.43s/it]2017-06-01 18:54:18,370 root  INFO     step 1925.000000 - time: 1.037252, loss: 0.370353, perplexity: 1.448246, precision: 0.484375, batch_len: 92.000000
Train, loss=0.37035298: 1926it [44:48,  1.37s/it]2017-06-01 18:54:19,731 root  INFO     step 1926.000000 - time: 1.323716, loss: 0.336093, perplexity: 1.399470, precision: 0.531250, batch_len: 120.000000
Train, loss=0.33609343: 1927it [44:49,  1.37s/it]2017-06-01 18:54:20,809 root  INFO     step 1927.000000 - time: 0.961708, loss: 0.392217, perplexity: 1.480259, precision: 0.593750, batch_len: 105.000000
Train, loss=0.39221692: 1928it [44:50,  1.28s/it]2017-06-01 18:54:22,132 root  INFO     step 1928.000000 - time: 1.259867, loss: 0.365568, perplexity: 1.441333, precision: 0.531250, batch_len: 115.000000
Train, loss=0.36556810: 1929it [44:52,  1.29s/it]2017-06-01 18:54:23,667 root  INFO     step 1929.000000 - time: 1.474899, loss: 0.262996, perplexity: 1.300821, precision: 0.625000, batch_len: 117.000000
Train, loss=0.26299587: 1930it [44:53,  1.37s/it]2017-06-01 18:54:24,771 root  INFO     step 1930.000000 - time: 1.005060, loss: 0.367572, perplexity: 1.444224, precision: 0.593750, batch_len: 104.000000
Train, loss=0.36757231: 1931it [44:54,  1.29s/it]2017-06-01 18:54:25,762 root  INFO     step 1931.000000 - time: 0.982855, loss: 0.374657, perplexity: 1.454493, precision: 0.609375, batch_len: 94.000000
Train, loss=0.37465721: 1932it [44:55,  1.20s/it]2017-06-01 18:54:26,802 root  INFO     step 1932.000000 - time: 1.034687, loss: 0.574020, perplexity: 1.775390, precision: 0.390625, batch_len: 93.000000
Train, loss=0.57402039: 1933it [44:56,  1.15s/it]2017-06-01 18:54:28,352 root  INFO     step 1933.000000 - time: 1.334449, loss: 0.720953, perplexity: 2.056392, precision: 0.281250, batch_len: 109.000000
Train, loss=0.72095275: 1934it [44:58,  1.27s/it]2017-06-01 18:54:29,361 root  INFO     step 1934.000000 - time: 0.985684, loss: 0.462058, perplexity: 1.587337, precision: 0.406250, batch_len: 97.000000
Train, loss=0.46205750: 1935it [44:59,  1.19s/it]2017-06-01 18:54:30,869 root  INFO     step 1935.000000 - time: 1.481119, loss: 0.369306, perplexity: 1.446731, precision: 0.578125, batch_len: 91.000000
Train, loss=0.36930636: 1936it [45:00,  1.29s/it]2017-06-01 18:54:32,054 root  INFO     step 1936.000000 - time: 1.176876, loss: 0.532717, perplexity: 1.703555, precision: 0.515625, batch_len: 111.000000
Train, loss=0.53271723: 1937it [45:01,  1.26s/it]2017-06-01 18:54:33,094 root  INFO     step 1937.000000 - time: 1.014754, loss: 0.375234, perplexity: 1.455332, precision: 0.625000, batch_len: 85.000000
Train, loss=0.37523437: 1938it [45:02,  1.19s/it]2017-06-01 18:54:34,208 root  INFO     step 1938.000000 - time: 1.095087, loss: 0.358246, perplexity: 1.430818, precision: 0.593750, batch_len: 108.000000
Train, loss=0.35824606: 1939it [45:04,  1.17s/it]2017-06-01 18:54:35,298 root  INFO     step 1939.000000 - time: 0.962020, loss: 0.341262, perplexity: 1.406722, precision: 0.593750, batch_len: 103.000000
Train, loss=0.34126189: 1940it [45:05,  1.14s/it]2017-06-01 18:54:36,315 root  INFO     step 1940.000000 - time: 0.962922, loss: 0.307051, perplexity: 1.359410, precision: 0.703125, batch_len: 81.000000
Train, loss=0.30705076: 1941it [45:06,  1.11s/it]2017-06-01 18:54:37,624 root  INFO     step 1941.000000 - time: 1.293297, loss: 0.400288, perplexity: 1.492255, precision: 0.640625, batch_len: 106.000000
Train, loss=0.40028816: 1942it [45:07,  1.17s/it]2017-06-01 18:54:39,281 root  INFO     step 1942.000000 - time: 1.594313, loss: 0.256500, perplexity: 1.292398, precision: 0.609375, batch_len: 123.000000
Train, loss=0.25649956: 1943it [45:09,  1.31s/it]2017-06-01 18:54:40,376 root  INFO     step 1943.000000 - time: 1.074333, loss: 0.228001, perplexity: 1.256086, precision: 0.656250, batch_len: 102.000000
Train, loss=0.22800051: 1944it [45:10,  1.25s/it]2017-06-01 18:54:41,723 root  INFO     step 1944.000000 - time: 1.298314, loss: 0.263351, perplexity: 1.301284, precision: 0.562500, batch_len: 129.000000
Train, loss=0.26335138: 1945it [45:11,  1.28s/it]2017-06-01 18:54:42,744 root  INFO     step 1945.000000 - time: 0.975595, loss: 0.256863, perplexity: 1.292869, precision: 0.609375, batch_len: 114.000000
Train, loss=0.25686347: 1946it [45:12,  1.20s/it]2017-06-01 18:54:43,952 root  INFO     step 1946.000000 - time: 1.129684, loss: 0.345502, perplexity: 1.412699, precision: 0.562500, batch_len: 100.000000
Train, loss=0.34550217: 1947it [45:13,  1.20s/it]2017-06-01 18:54:45,097 root  INFO     step 1947.000000 - time: 1.139303, loss: 0.306703, perplexity: 1.358938, precision: 0.656250, batch_len: 86.000000
Train, loss=0.30670342: 1948it [45:14,  1.19s/it]2017-06-01 18:54:46,458 root  INFO     step 1948.000000 - time: 1.337609, loss: 0.293420, perplexity: 1.341005, precision: 0.609375, batch_len: 116.000000
Train, loss=0.29341960: 1949it [45:16,  1.24s/it]2017-06-01 18:54:47,893 root  INFO     step 1949.000000 - time: 1.401362, loss: 0.322788, perplexity: 1.380972, precision: 0.578125, batch_len: 126.000000
Train, loss=0.32278767: 1950it [45:17,  1.30s/it]2017-06-01 18:54:49,214 root  INFO     step 1950.000000 - time: 1.228813, loss: 0.209913, perplexity: 1.233570, precision: 0.671875, batch_len: 124.000000
Train, loss=0.20991254: 1951it [45:19,  1.30s/it]2017-06-01 18:54:50,426 root  INFO     step 1951.000000 - time: 1.178814, loss: 0.290310, perplexity: 1.336842, precision: 0.671875, batch_len: 89.000000
Train, loss=0.29031020: 1952it [45:20,  1.28s/it]2017-06-01 18:54:51,939 root  INFO     step 1952.000000 - time: 1.484001, loss: 0.302931, perplexity: 1.353822, precision: 0.609375, batch_len: 125.000000
Train, loss=0.30293134: 1953it [45:21,  1.35s/it]2017-06-01 18:54:52,979 root  INFO     step 1953.000000 - time: 1.005907, loss: 0.259704, perplexity: 1.296546, precision: 0.703125, batch_len: 84.000000
Train, loss=0.25970393: 1954it [45:22,  1.26s/it]2017-06-01 18:54:54,104 root  INFO     step 1954.000000 - time: 1.101789, loss: 0.314064, perplexity: 1.368977, precision: 0.593750, batch_len: 107.000000
Train, loss=0.31406400: 1955it [45:23,  1.22s/it]2017-06-01 18:54:55,160 root  INFO     step 1955.000000 - time: 0.916466, loss: 0.279184, perplexity: 1.322050, precision: 0.640625, batch_len: 87.000000
Train, loss=0.27918351: 1956it [45:25,  1.17s/it]2017-06-01 18:54:56,047 root  INFO     step 1956.000000 - time: 0.822687, loss: 0.257118, perplexity: 1.293197, precision: 0.671875, batch_len: 80.000000
Train, loss=0.25711766: 1957it [45:25,  1.08s/it]2017-06-01 18:54:57,819 root  INFO     step 1957.000000 - time: 1.633769, loss: 0.269232, perplexity: 1.308959, precision: 0.625000, batch_len: 121.000000
Train, loss=0.26923239: 1958it [45:27,  1.29s/it]2017-06-01 18:54:58,907 root  INFO     step 1958.000000 - time: 0.980069, loss: 0.312708, perplexity: 1.367123, precision: 0.656250, batch_len: 83.000000
Train, loss=0.31270850: 1959it [45:28,  1.23s/it]2017-06-01 18:55:00,337 root  INFO     step 1959.000000 - time: 1.349757, loss: 0.271115, perplexity: 1.311425, precision: 0.500000, batch_len: 118.000000
Train, loss=0.27111453: 1960it [45:30,  1.29s/it]2017-06-01 18:55:02,095 root  INFO     step 1960.000000 - time: 1.564443, loss: 0.453194, perplexity: 1.573330, precision: 0.343750, batch_len: 144.000000
Train, loss=0.45319438: 1961it [45:31,  1.43s/it]2017-06-01 18:55:03,699 root  INFO     step 1961.000000 - time: 1.528311, loss: 0.700572, perplexity: 2.014905, precision: 0.109375, batch_len: 137.000000
Train, loss=0.70057225: 1962it [45:33,  1.48s/it]2017-06-01 18:55:05,009 root  INFO     step 1962.000000 - time: 1.236533, loss: 0.593167, perplexity: 1.809711, precision: 0.265625, batch_len: 99.000000
Train, loss=0.59316725: 1963it [45:34,  1.43s/it]2017-06-01 18:55:06,035 root  INFO     step 1963.000000 - time: 0.987047, loss: 0.411127, perplexity: 1.508518, precision: 0.390625, batch_len: 79.000000
Train, loss=0.41112739: 1964it [45:35,  1.31s/it]2017-06-01 18:55:07,375 root  INFO     step 1964.000000 - time: 1.327662, loss: 0.252165, perplexity: 1.286809, precision: 0.593750, batch_len: 119.000000
Train, loss=0.25216538: 1965it [45:37,  1.32s/it]2017-06-01 18:55:08,851 root  INFO     step 1965.000000 - time: 1.309392, loss: 0.310126, perplexity: 1.363596, precision: 0.515625, batch_len: 130.000000
Train, loss=0.31012565: 1966it [45:38,  1.37s/it]2017-06-01 18:55:09,986 root  INFO     step 1966.000000 - time: 1.030644, loss: 0.268632, perplexity: 1.308174, precision: 0.687500, batch_len: 82.000000
Train, loss=0.26863217: 1967it [45:39,  1.30s/it]2017-06-01 18:55:11,378 root  INFO     step 1967.000000 - time: 1.369559, loss: 0.160628, perplexity: 1.174248, precision: 0.703125, batch_len: 98.000000
Train, loss=0.16062787: 1968it [45:41,  1.33s/it]2017-06-01 18:55:12,694 root  INFO     step 1968.000000 - time: 1.309898, loss: 0.215591, perplexity: 1.240595, precision: 0.593750, batch_len: 133.000000
Train, loss=0.21559095: 1969it [45:42,  1.32s/it]2017-06-01 18:55:14,360 root  INFO     step 1969.000000 - time: 1.346234, loss: 0.229549, perplexity: 1.258033, precision: 0.593750, batch_len: 135.000000
Train, loss=0.22954947: 1970it [45:44,  1.43s/it]2017-06-01 18:55:15,400 root  INFO     step 1970.000000 - time: 0.915852, loss: 0.458793, perplexity: 1.582164, precision: 0.484375, batch_len: 78.000000
Train, loss=0.45879328: 1971it [45:45,  1.31s/it]2017-06-01 18:55:17,035 root  INFO     step 1971.000000 - time: 1.497463, loss: 0.316980, perplexity: 1.372975, precision: 0.515625, batch_len: 122.000000
Train, loss=0.31698012: 1972it [45:46,  1.41s/it]2017-06-01 18:55:18,689 root  INFO     step 1972.000000 - time: 1.596595, loss: 0.413681, perplexity: 1.512375, precision: 0.328125, batch_len: 138.000000
Train, loss=0.41368109: 1973it [45:48,  1.48s/it]2017-06-01 18:55:20,621 root  INFO     step 1973.000000 - time: 1.874229, loss: 0.354416, perplexity: 1.425348, precision: 0.375000, batch_len: 152.000000
Train, loss=0.35441589: 1974it [45:50,  1.62s/it]2017-06-01 18:55:21,969 root  INFO     step 1974.000000 - time: 1.262925, loss: 0.304958, perplexity: 1.356568, precision: 0.453125, batch_len: 132.000000
Train, loss=0.30495775: 1975it [45:51,  1.54s/it]2017-06-01 18:55:23,008 root  INFO     step 1975.000000 - time: 1.023527, loss: 0.237469, perplexity: 1.268036, precision: 0.750000, batch_len: 96.000000
Train, loss=0.23746905: 1976it [45:52,  1.39s/it]2017-06-01 18:55:24,147 root  INFO     step 1976.000000 - time: 1.122489, loss: 0.116985, perplexity: 1.124102, precision: 0.859375, batch_len: 74.000000
Train, loss=0.11698485: 1977it [45:54,  1.31s/it]2017-06-01 18:55:25,854 root  INFO     step 1977.000000 - time: 1.620501, loss: 0.312469, perplexity: 1.366796, precision: 0.500000, batch_len: 139.000000
Train, loss=0.31246901: 1978it [45:55,  1.43s/it]2017-06-01 18:55:27,141 root  INFO     step 1978.000000 - time: 1.238452, loss: 0.163629, perplexity: 1.177777, precision: 0.843750, batch_len: 72.000000
Train, loss=0.16362864: 1979it [45:57,  1.39s/it]2017-06-01 18:55:28,280 root  INFO     step 1979.000000 - time: 1.000209, loss: 0.275744, perplexity: 1.317510, precision: 0.687500, batch_len: 76.000000
Train, loss=0.27574363: 1980it [45:58,  1.31s/it]2017-06-01 18:55:29,229 root  INFO     step 1980.000000 - time: 0.911389, loss: 0.174887, perplexity: 1.191111, precision: 0.687500, batch_len: 77.000000
Train, loss=0.17488673: 1981it [45:59,  1.20s/it]2017-06-01 18:55:30,814 root  INFO     step 1981.000000 - time: 1.541745, loss: 0.196452, perplexity: 1.217077, precision: 0.703125, batch_len: 136.000000
Train, loss=0.19645195: 1982it [46:00,  1.32s/it]2017-06-01 18:55:32,452 root  INFO     step 1982.000000 - time: 1.626263, loss: 0.302679, perplexity: 1.353481, precision: 0.640625, batch_len: 141.000000
Train, loss=0.30267945: 1983it [46:02,  1.41s/it]2017-06-01 18:55:34,007 root  INFO     step 1983.000000 - time: 1.474989, loss: 0.237947, perplexity: 1.268642, precision: 0.593750, batch_len: 134.000000
Train, loss=0.23794705: 1984it [46:03,  1.46s/it]2017-06-01 18:55:35,651 root  INFO     step 1984.000000 - time: 1.378896, loss: 0.206633, perplexity: 1.229532, precision: 0.656250, batch_len: 142.000000
Train, loss=0.20663336: 1985it [46:05,  1.51s/it]2017-06-01 18:55:37,424 root  INFO     step 1985.000000 - time: 1.676073, loss: 0.265919, perplexity: 1.304629, precision: 0.546875, batch_len: 150.000000
Train, loss=0.26591897: 1986it [46:07,  1.59s/it]2017-06-01 18:55:39,274 root  INFO     step 1986.000000 - time: 1.655704, loss: 0.459487, perplexity: 1.583262, precision: 0.406250, batch_len: 131.000000
Train, loss=0.45948714: 1987it [46:09,  1.67s/it]2017-06-01 18:55:40,425 root  INFO     step 1987.000000 - time: 1.040297, loss: 0.286320, perplexity: 1.331519, precision: 0.671875, batch_len: 71.000000
Train, loss=0.28632036: 1988it [46:10,  1.51s/it]2017-06-01 18:55:40,504 root  INFO     Generating first batch)
2017-06-01 18:55:44,043 root  INFO     step 1988.000000 - time: 1.190600, loss: 0.243543, perplexity: 1.275761, precision: 0.625000, batch_len: 96.000000
Train, loss=0.24354255: 1989it [46:13,  2.14s/it]2017-06-01 18:55:45,791 root  INFO     step 1989.000000 - time: 1.361927, loss: 0.320142, perplexity: 1.377323, precision: 0.484375, batch_len: 128.000000
Train, loss=0.32014197: 1990it [46:15,  2.03s/it]2017-06-01 18:55:46,948 root  INFO     step 1990.000000 - time: 1.010847, loss: 0.471524, perplexity: 1.602435, precision: 0.578125, batch_len: 90.000000
Train, loss=0.47152436: 1991it [46:16,  1.77s/it]2017-06-01 18:55:48,235 root  INFO     step 1991.000000 - time: 1.069467, loss: 0.201596, perplexity: 1.223354, precision: 0.718750, batch_len: 92.000000
Train, loss=0.20159619: 1992it [46:18,  1.62s/it]2017-06-01 18:55:49,185 root  INFO     step 1992.000000 - time: 0.935608, loss: 0.215040, perplexity: 1.239911, precision: 0.656250, batch_len: 101.000000
Train, loss=0.21503994: 1993it [46:19,  1.42s/it]2017-06-01 18:55:50,562 root  INFO     step 1993.000000 - time: 1.223315, loss: 0.244484, perplexity: 1.276962, precision: 0.750000, batch_len: 106.000000
Train, loss=0.24448356: 1994it [46:20,  1.41s/it]2017-06-01 18:55:51,767 root  INFO     step 1994.000000 - time: 1.184919, loss: 0.230248, perplexity: 1.258912, precision: 0.750000, batch_len: 104.000000
Train, loss=0.23024788: 1995it [46:21,  1.35s/it]2017-06-01 18:55:52,840 root  INFO     step 1995.000000 - time: 1.057565, loss: 0.224468, perplexity: 1.251656, precision: 0.671875, batch_len: 105.000000
Train, loss=0.22446787: 1996it [46:22,  1.26s/it]2017-06-01 18:55:53,952 root  INFO     step 1996.000000 - time: 1.053937, loss: 0.346819, perplexity: 1.414561, precision: 0.656250, batch_len: 88.000000
Train, loss=0.34681925: 1997it [46:23,  1.22s/it]2017-06-01 18:55:55,032 root  INFO     step 1997.000000 - time: 1.076325, loss: 0.181825, perplexity: 1.199405, precision: 0.703125, batch_len: 113.000000
Train, loss=0.18182521: 1998it [46:24,  1.18s/it]2017-06-01 18:55:56,149 root  INFO     step 1998.000000 - time: 1.010911, loss: 0.340174, perplexity: 1.405191, precision: 0.640625, batch_len: 108.000000
Train, loss=0.34017354: 1999it [46:26,  1.16s/it]2017-06-01 18:55:57,888 root  INFO     step 1999.000000 - time: 1.639778, loss: 0.195179, perplexity: 1.215528, precision: 0.703125, batch_len: 117.000000
Train, loss=0.19517881: 2000it [46:27,  1.33s/it]2017-06-01 18:55:58,425 root  INFO     global step 2000 step-time 1.28 loss 1.471050  perplexity 4.35
2017-06-01 18:55:58,425 root  INFO     Saving model, current_step: 2000
2017-06-01 18:56:08,497 root  INFO     step 2000.000000 - time: 1.092805, loss: 0.271724, perplexity: 1.312225, precision: 0.687500, batch_len: 110.000000
Train, loss=0.27172396: 2001it [46:38,  4.12s/it]2017-06-01 18:56:09,721 root  INFO     step 2001.000000 - time: 1.184319, loss: 0.333995, perplexity: 1.396537, precision: 0.640625, batch_len: 112.000000
Train, loss=0.33399540: 2002it [46:39,  3.25s/it]2017-06-01 18:56:11,010 root  INFO     step 2002.000000 - time: 1.246517, loss: 0.272602, perplexity: 1.313378, precision: 0.687500, batch_len: 102.000000
Train, loss=0.27260220: 2003it [46:40,  2.66s/it]2017-06-01 18:56:12,324 root  INFO     step 2003.000000 - time: 1.238532, loss: 0.341703, perplexity: 1.407342, precision: 0.687500, batch_len: 93.000000
Train, loss=0.34170297: 2004it [46:42,  2.26s/it]2017-06-01 18:56:13,457 root  INFO     step 2004.000000 - time: 1.050924, loss: 0.220122, perplexity: 1.246229, precision: 0.750000, batch_len: 89.000000
Train, loss=0.22012234: 2005it [46:43,  1.92s/it]2017-06-01 18:56:14,905 root  INFO     step 2005.000000 - time: 1.398077, loss: 0.271245, perplexity: 1.311596, precision: 0.609375, batch_len: 120.000000
Train, loss=0.27124485: 2006it [46:44,  1.78s/it]2017-06-01 18:56:15,936 root  INFO     step 2006.000000 - time: 0.961119, loss: 0.253649, perplexity: 1.288719, precision: 0.656250, batch_len: 100.000000
Train, loss=0.25364867: 2007it [46:45,  1.55s/it]2017-06-01 18:56:16,866 root  INFO     step 2007.000000 - time: 0.894610, loss: 0.307455, perplexity: 1.359959, precision: 0.640625, batch_len: 81.000000
Train, loss=0.30745459: 2008it [46:46,  1.37s/it]2017-06-01 18:56:18,071 root  INFO     step 2008.000000 - time: 1.116273, loss: 0.285653, perplexity: 1.330631, precision: 0.796875, batch_len: 86.000000
Train, loss=0.28565320: 2009it [46:47,  1.32s/it]2017-06-01 18:56:19,403 root  INFO     step 2009.000000 - time: 1.247416, loss: 0.340251, perplexity: 1.405301, precision: 0.625000, batch_len: 97.000000
Train, loss=0.34025124: 2010it [46:49,  1.32s/it]2017-06-01 18:56:20,569 root  INFO     step 2010.000000 - time: 1.107422, loss: 0.301312, perplexity: 1.351631, precision: 0.640625, batch_len: 109.000000
Train, loss=0.30131185: 2011it [46:50,  1.28s/it]2017-06-01 18:56:21,756 root  INFO     step 2011.000000 - time: 1.063321, loss: 0.407660, perplexity: 1.503296, precision: 0.609375, batch_len: 111.000000
Train, loss=0.40765995: 2012it [46:51,  1.25s/it]2017-06-01 18:56:22,690 root  INFO     step 2012.000000 - time: 0.901622, loss: 0.274101, perplexity: 1.315348, precision: 0.640625, batch_len: 79.000000
Train, loss=0.27410150: 2013it [46:52,  1.15s/it]2017-06-01 18:56:23,632 root  INFO     step 2013.000000 - time: 0.930094, loss: 0.355296, perplexity: 1.426603, precision: 0.609375, batch_len: 80.000000
Train, loss=0.35529596: 2014it [46:53,  1.09s/it]2017-06-01 18:56:24,837 root  INFO     step 2014.000000 - time: 1.177432, loss: 0.376876, perplexity: 1.457724, precision: 0.640625, batch_len: 85.000000
Train, loss=0.37687647: 2015it [46:54,  1.12s/it]2017-06-01 18:56:26,384 root  INFO     step 2015.000000 - time: 1.507302, loss: 0.256553, perplexity: 1.292467, precision: 0.546875, batch_len: 124.000000
Train, loss=0.25655261: 2016it [46:56,  1.25s/it]2017-06-01 18:56:27,639 root  INFO     step 2016.000000 - time: 1.197950, loss: 0.393709, perplexity: 1.482469, precision: 0.453125, batch_len: 107.000000
Train, loss=0.39370865: 2017it [46:57,  1.25s/it]2017-06-01 18:56:28,766 root  INFO     step 2017.000000 - time: 1.072597, loss: 0.268862, perplexity: 1.308475, precision: 0.703125, batch_len: 91.000000
Train, loss=0.26886207: 2018it [46:58,  1.21s/it]2017-06-01 18:56:29,863 root  INFO     step 2018.000000 - time: 1.081793, loss: 0.205114, perplexity: 1.227665, precision: 0.703125, batch_len: 103.000000
Train, loss=0.20511389: 2019it [46:59,  1.18s/it]2017-06-01 18:56:31,178 root  INFO     step 2019.000000 - time: 1.258352, loss: 0.318516, perplexity: 1.375085, precision: 0.625000, batch_len: 114.000000
Train, loss=0.31851560: 2020it [47:01,  1.22s/it]2017-06-01 18:56:32,143 root  INFO     step 2020.000000 - time: 0.938089, loss: 0.279364, perplexity: 1.322288, precision: 0.593750, batch_len: 94.000000
Train, loss=0.27936363: 2021it [47:02,  1.14s/it]2017-06-01 18:56:33,984 root  INFO     step 2021.000000 - time: 1.716997, loss: 0.211761, perplexity: 1.235852, precision: 0.703125, batch_len: 125.000000
Train, loss=0.21176073: 2022it [47:03,  1.35s/it]2017-06-01 18:56:35,305 root  INFO     step 2022.000000 - time: 1.115351, loss: 0.172868, perplexity: 1.188710, precision: 0.703125, batch_len: 98.000000
Train, loss=0.17286849: 2023it [47:05,  1.34s/it]2017-06-01 18:56:36,679 root  INFO     step 2023.000000 - time: 1.312726, loss: 0.239165, perplexity: 1.270188, precision: 0.500000, batch_len: 129.000000
Train, loss=0.23916467: 2024it [47:06,  1.35s/it]2017-06-01 18:56:38,013 root  INFO     step 2024.000000 - time: 1.224084, loss: 0.357338, perplexity: 1.429519, precision: 0.390625, batch_len: 121.000000
Train, loss=0.35733777: 2025it [47:07,  1.35s/it]2017-06-01 18:56:39,830 root  INFO     step 2025.000000 - time: 1.572875, loss: 0.231409, perplexity: 1.260375, precision: 0.531250, batch_len: 116.000000
Train, loss=0.23140924: 2026it [47:09,  1.49s/it]2017-06-01 18:56:41,048 root  INFO     step 2026.000000 - time: 1.117737, loss: 0.293804, perplexity: 1.341521, precision: 0.578125, batch_len: 83.000000
Train, loss=0.29380369: 2027it [47:10,  1.41s/it]2017-06-01 18:56:42,205 root  INFO     step 2027.000000 - time: 1.089323, loss: 0.229547, perplexity: 1.258030, precision: 0.781250, batch_len: 99.000000
Train, loss=0.22954738: 2028it [47:12,  1.33s/it]2017-06-01 18:56:43,363 root  INFO     step 2028.000000 - time: 1.115507, loss: 0.217983, perplexity: 1.243566, precision: 0.703125, batch_len: 115.000000
Train, loss=0.21798334: 2029it [47:13,  1.28s/it]2017-06-01 18:56:44,335 root  INFO     step 2029.000000 - time: 0.888656, loss: 0.222027, perplexity: 1.248605, precision: 0.796875, batch_len: 84.000000
Train, loss=0.22202685: 2030it [47:14,  1.19s/it]2017-06-01 18:56:45,986 root  INFO     step 2030.000000 - time: 1.642812, loss: 0.212951, perplexity: 1.237325, precision: 0.750000, batch_len: 144.000000
Train, loss=0.21295141: 2031it [47:15,  1.33s/it]2017-06-01 18:56:47,642 root  INFO     step 2031.000000 - time: 1.482341, loss: 0.136037, perplexity: 1.145725, precision: 0.796875, batch_len: 135.000000
Train, loss=0.13603729: 2032it [47:17,  1.43s/it]2017-06-01 18:56:48,656 root  INFO     step 2032.000000 - time: 0.969855, loss: 0.134964, perplexity: 1.144495, precision: 0.781250, batch_len: 87.000000
Train, loss=0.13496377: 2033it [47:18,  1.30s/it]2017-06-01 18:56:50,138 root  INFO     step 2033.000000 - time: 1.387895, loss: 0.198456, perplexity: 1.219519, precision: 0.718750, batch_len: 137.000000
Train, loss=0.19845641: 2034it [47:20,  1.36s/it]2017-06-01 18:56:51,342 root  INFO     step 2034.000000 - time: 1.181565, loss: 0.136729, perplexity: 1.146518, precision: 0.781250, batch_len: 123.000000
Train, loss=0.13672933: 2035it [47:21,  1.31s/it]2017-06-01 18:56:52,388 root  INFO     step 2035.000000 - time: 1.043024, loss: 0.277557, perplexity: 1.319902, precision: 0.687500, batch_len: 82.000000
Train, loss=0.27755740: 2036it [47:22,  1.23s/it]2017-06-01 18:56:54,046 root  INFO     step 2036.000000 - time: 1.531629, loss: 0.243603, perplexity: 1.275837, precision: 0.640625, batch_len: 118.000000
Train, loss=0.24360275: 2037it [47:23,  1.36s/it]2017-06-01 18:56:55,485 root  INFO     step 2037.000000 - time: 1.352887, loss: 0.254988, perplexity: 1.290446, precision: 0.609375, batch_len: 119.000000
Train, loss=0.25498810: 2038it [47:25,  1.38s/it]2017-06-01 18:56:56,918 root  INFO     step 2038.000000 - time: 1.417546, loss: 0.182378, perplexity: 1.200068, precision: 0.734375, batch_len: 136.000000
Train, loss=0.18237787: 2039it [47:26,  1.40s/it]2017-06-01 18:56:58,274 root  INFO     step 2039.000000 - time: 1.266897, loss: 0.290575, perplexity: 1.337196, precision: 0.703125, batch_len: 126.000000
Train, loss=0.29057461: 2040it [47:28,  1.39s/it]2017-06-01 18:56:59,897 root  INFO     step 2040.000000 - time: 1.519133, loss: 0.308026, perplexity: 1.360737, precision: 0.546875, batch_len: 130.000000
Train, loss=0.30802634: 2041it [47:29,  1.46s/it]2017-06-01 18:57:01,531 root  INFO     step 2041.000000 - time: 1.326705, loss: 0.326335, perplexity: 1.385879, precision: 0.484375, batch_len: 133.000000
Train, loss=0.32633477: 2042it [47:31,  1.51s/it]2017-06-01 18:57:02,966 root  INFO     step 2042.000000 - time: 1.422474, loss: 0.610649, perplexity: 1.841626, precision: 0.343750, batch_len: 141.000000
Train, loss=0.61064899: 2043it [47:32,  1.49s/it]2017-06-01 18:57:04,244 root  INFO     step 2043.000000 - time: 1.160658, loss: 0.330819, perplexity: 1.392108, precision: 0.609375, batch_len: 96.000000
Train, loss=0.33081928: 2044it [47:34,  1.42s/it]2017-06-01 18:57:05,902 root  INFO     step 2044.000000 - time: 1.485342, loss: 0.267989, perplexity: 1.307333, precision: 0.609375, batch_len: 122.000000
Train, loss=0.26798898: 2045it [47:35,  1.49s/it]2017-06-01 18:57:07,458 root  INFO     step 2045.000000 - time: 1.478900, loss: 0.174388, perplexity: 1.190518, precision: 0.687500, batch_len: 132.000000
Train, loss=0.17438836: 2046it [47:37,  1.51s/it]2017-06-01 18:57:09,366 root  INFO     step 2046.000000 - time: 1.890005, loss: 0.228359, perplexity: 1.256536, precision: 0.625000, batch_len: 152.000000
Train, loss=0.22835900: 2047it [47:39,  1.63s/it]2017-06-01 18:57:10,955 root  INFO     step 2047.000000 - time: 1.460831, loss: 0.491529, perplexity: 1.634813, precision: 0.453125, batch_len: 138.000000
Train, loss=0.49152851: 2048it [47:40,  1.62s/it]2017-06-01 18:57:11,925 root  INFO     step 2048.000000 - time: 0.940968, loss: 0.317704, perplexity: 1.373970, precision: 0.625000, batch_len: 78.000000
Train, loss=0.31770447: 2049it [47:41,  1.42s/it]2017-06-01 18:57:13,764 root  INFO     step 2049.000000 - time: 1.749678, loss: 0.567927, perplexity: 1.764606, precision: 0.375000, batch_len: 139.000000
Train, loss=0.56792742: 2050it [47:43,  1.55s/it]2017-06-01 18:57:14,959 root  INFO     step 2050.000000 - time: 1.154740, loss: 1.077147, perplexity: 2.936292, precision: 0.328125, batch_len: 72.000000
Train, loss=1.07714748: 2051it [47:44,  1.44s/it]2017-06-01 18:57:16,091 root  INFO     step 2051.000000 - time: 0.954051, loss: 1.164120, perplexity: 3.203102, precision: 0.140625, batch_len: 76.000000
Train, loss=1.16411984: 2052it [47:45,  1.35s/it]2017-06-01 18:57:17,201 root  INFO     step 2052.000000 - time: 0.926270, loss: 0.538157, perplexity: 1.712848, precision: 0.468750, batch_len: 74.000000
Train, loss=0.53815722: 2053it [47:47,  1.28s/it]2017-06-01 18:57:19,016 root  INFO     step 2053.000000 - time: 1.785348, loss: 0.400751, perplexity: 1.492946, precision: 0.343750, batch_len: 150.000000
Train, loss=0.40075138: 2054it [47:48,  1.44s/it]2017-06-01 18:57:20,628 root  INFO     step 2054.000000 - time: 1.511078, loss: 0.257267, perplexity: 1.293390, precision: 0.515625, batch_len: 134.000000
Train, loss=0.25726670: 2055it [47:50,  1.49s/it]2017-06-01 18:57:21,645 root  INFO     step 2055.000000 - time: 0.981520, loss: 0.191581, perplexity: 1.211163, precision: 0.750000, batch_len: 77.000000
Train, loss=0.19158141: 2056it [47:51,  1.35s/it]2017-06-01 18:57:22,751 root  INFO     step 2056.000000 - time: 0.938984, loss: 0.126523, perplexity: 1.134876, precision: 0.875000, batch_len: 71.000000
Train, loss=0.12652317: 2057it [47:52,  1.28s/it]2017-06-01 18:57:24,205 root  INFO     step 2057.000000 - time: 1.418023, loss: 0.278583, perplexity: 1.321256, precision: 0.546875, batch_len: 142.000000
Train, loss=0.27858293: 2058it [47:54,  1.33s/it]2017-06-01 18:57:25,909 root  INFO     step 2058.000000 - time: 1.601105, loss: 0.315111, perplexity: 1.370411, precision: 0.468750, batch_len: 131.000000
Train, loss=0.31511071: 2059it [47:55,  1.44s/it]2017-06-01 18:57:26,133 root  INFO     Generating first batch)
2017-06-01 18:57:30,174 root  INFO     step 2059.000000 - time: 1.136111, loss: 0.137568, perplexity: 1.147479, precision: 0.718750, batch_len: 96.000000
Train, loss=0.13756764: 2060it [48:00,  2.29s/it]2017-06-01 18:57:32,047 root  INFO     step 2060.000000 - time: 1.641163, loss: 0.399614, perplexity: 1.491249, precision: 0.578125, batch_len: 90.000000
Train, loss=0.39961386: 2061it [48:01,  2.16s/it]2017-06-01 18:57:33,427 root  INFO     step 2061.000000 - time: 1.312640, loss: 0.239297, perplexity: 1.270356, precision: 0.703125, batch_len: 92.000000
Train, loss=0.23929694: 2062it [48:03,  1.93s/it]2017-06-01 18:57:34,580 root  INFO     step 2062.000000 - time: 1.075399, loss: 0.269061, perplexity: 1.308735, precision: 0.687500, batch_len: 102.000000
Train, loss=0.26906067: 2063it [48:04,  1.70s/it]2017-06-01 18:57:35,756 root  INFO     step 2063.000000 - time: 1.089312, loss: 0.244126, perplexity: 1.276505, precision: 0.656250, batch_len: 108.000000
Train, loss=0.24412604: 2064it [48:05,  1.54s/it]2017-06-01 18:57:36,993 root  INFO     step 2064.000000 - time: 1.095739, loss: 0.226131, perplexity: 1.253740, precision: 0.671875, batch_len: 110.000000
Train, loss=0.22613093: 2065it [48:06,  1.45s/it]2017-06-01 18:57:38,044 root  INFO     step 2065.000000 - time: 1.000351, loss: 0.285572, perplexity: 1.330522, precision: 0.609375, batch_len: 113.000000
Train, loss=0.28557166: 2066it [48:07,  1.33s/it]2017-06-01 18:57:39,304 root  INFO     step 2066.000000 - time: 1.221983, loss: 0.306273, perplexity: 1.358353, precision: 0.687500, batch_len: 97.000000
Train, loss=0.30627263: 2067it [48:09,  1.31s/it]2017-06-01 18:57:40,546 root  INFO     step 2067.000000 - time: 1.197920, loss: 0.344970, perplexity: 1.411948, precision: 0.609375, batch_len: 93.000000
Train, loss=0.34497017: 2068it [48:10,  1.29s/it]2017-06-01 18:57:41,698 root  INFO     step 2068.000000 - time: 1.052400, loss: 0.252860, perplexity: 1.287703, precision: 0.734375, batch_len: 91.000000
Train, loss=0.25285995: 2069it [48:11,  1.25s/it]2017-06-01 18:57:42,769 root  INFO     step 2069.000000 - time: 1.012407, loss: 0.183179, perplexity: 1.201029, precision: 0.796875, batch_len: 100.000000
Train, loss=0.18317910: 2070it [48:12,  1.19s/it]2017-06-01 18:57:44,006 root  INFO     step 2070.000000 - time: 1.200224, loss: 0.202910, perplexity: 1.224963, precision: 0.640625, batch_len: 128.000000
Train, loss=0.20291030: 2071it [48:13,  1.21s/it]2017-06-01 18:57:44,988 root  INFO     step 2071.000000 - time: 0.933196, loss: 0.226087, perplexity: 1.253685, precision: 0.781250, batch_len: 81.000000
Train, loss=0.22608741: 2072it [48:14,  1.14s/it]2017-06-01 18:57:46,206 root  INFO     step 2072.000000 - time: 1.182942, loss: 0.236264, perplexity: 1.266509, precision: 0.734375, batch_len: 104.000000
Train, loss=0.23626411: 2073it [48:16,  1.16s/it]2017-06-01 18:57:47,801 root  INFO     step 2073.000000 - time: 1.534941, loss: 0.215268, perplexity: 1.240195, precision: 0.703125, batch_len: 121.000000
Train, loss=0.21526843: 2074it [48:17,  1.29s/it]2017-06-01 18:57:48,834 root  INFO     step 2074.000000 - time: 1.021165, loss: 0.349131, perplexity: 1.417835, precision: 0.671875, batch_len: 105.000000
Train, loss=0.34913123: 2075it [48:18,  1.21s/it]2017-06-01 18:57:50,198 root  INFO     step 2075.000000 - time: 1.291459, loss: 0.202856, perplexity: 1.224896, precision: 0.734375, batch_len: 124.000000
Train, loss=0.20285594: 2076it [48:20,  1.26s/it]2017-06-01 18:57:51,178 root  INFO     step 2076.000000 - time: 0.973877, loss: 0.223507, perplexity: 1.250454, precision: 0.656250, batch_len: 88.000000
Train, loss=0.22350666: 2077it [48:21,  1.18s/it]2017-06-01 18:57:52,810 root  INFO     step 2077.000000 - time: 1.519461, loss: 0.211076, perplexity: 1.235006, precision: 0.656250, batch_len: 120.000000
Train, loss=0.21107560: 2078it [48:22,  1.31s/it]2017-06-01 18:57:54,100 root  INFO     step 2078.000000 - time: 1.238319, loss: 0.261983, perplexity: 1.299505, precision: 0.687500, batch_len: 89.000000
Train, loss=0.26198307: 2079it [48:23,  1.31s/it]2017-06-01 18:57:55,246 root  INFO     step 2079.000000 - time: 1.080413, loss: 0.220412, perplexity: 1.246591, precision: 0.656250, batch_len: 103.000000
Train, loss=0.22041228: 2080it [48:25,  1.26s/it]2017-06-01 18:57:56,262 root  INFO     step 2080.000000 - time: 0.971997, loss: 0.163081, perplexity: 1.177132, precision: 0.796875, batch_len: 80.000000
Train, loss=0.16308124: 2081it [48:26,  1.19s/it]2017-06-01 18:57:57,291 root  INFO     step 2081.000000 - time: 0.957743, loss: 0.153912, perplexity: 1.166389, precision: 0.812500, batch_len: 101.000000
Train, loss=0.15391237: 2082it [48:27,  1.14s/it]2017-06-01 18:57:58,596 root  INFO     step 2082.000000 - time: 1.295043, loss: 0.223207, perplexity: 1.250079, precision: 0.718750, batch_len: 112.000000
Train, loss=0.22320661: 2083it [48:28,  1.19s/it]2017-06-01 18:57:59,973 root  INFO     step 2083.000000 - time: 1.336917, loss: 0.368738, perplexity: 1.445908, precision: 0.656250, batch_len: 109.000000
Train, loss=0.36873767: 2084it [48:29,  1.24s/it]2017-06-01 18:58:01,101 root  INFO     step 2084.000000 - time: 1.084227, loss: 0.173673, perplexity: 1.189666, precision: 0.718750, batch_len: 106.000000
Train, loss=0.17367268: 2085it [48:30,  1.21s/it]2017-06-01 18:58:02,242 root  INFO     step 2085.000000 - time: 1.102876, loss: 0.268181, perplexity: 1.307583, precision: 0.718750, batch_len: 107.000000
Train, loss=0.26818073: 2086it [48:32,  1.19s/it]2017-06-01 18:58:03,579 root  INFO     step 2086.000000 - time: 1.266703, loss: 0.161140, perplexity: 1.174850, precision: 0.828125, batch_len: 117.000000
Train, loss=0.16114037: 2087it [48:33,  1.23s/it]2017-06-01 18:58:04,733 root  INFO     step 2087.000000 - time: 0.986803, loss: 0.292344, perplexity: 1.339563, precision: 0.656250, batch_len: 114.000000
Train, loss=0.29234374: 2088it [48:34,  1.21s/it]2017-06-01 18:58:05,975 root  INFO     step 2088.000000 - time: 1.228574, loss: 0.453081, perplexity: 1.573152, precision: 0.578125, batch_len: 111.000000
Train, loss=0.45308119: 2089it [48:35,  1.22s/it]2017-06-01 18:58:07,293 root  INFO     step 2089.000000 - time: 1.299928, loss: 0.292322, perplexity: 1.339535, precision: 0.593750, batch_len: 115.000000
Train, loss=0.29232234: 2090it [48:37,  1.25s/it]2017-06-01 18:58:08,338 root  INFO     step 2090.000000 - time: 1.011807, loss: 0.297348, perplexity: 1.346283, precision: 0.671875, batch_len: 85.000000
Train, loss=0.29734755: 2091it [48:38,  1.19s/it]2017-06-01 18:58:09,406 root  INFO     step 2091.000000 - time: 0.976790, loss: 0.273640, perplexity: 1.314742, precision: 0.781250, batch_len: 86.000000
Train, loss=0.27364022: 2092it [48:39,  1.15s/it]2017-06-01 18:58:10,489 root  INFO     step 2092.000000 - time: 1.005816, loss: 0.302407, perplexity: 1.353112, precision: 0.578125, batch_len: 94.000000
Train, loss=0.30240676: 2093it [48:40,  1.13s/it]2017-06-01 18:58:11,722 root  INFO     step 2093.000000 - time: 1.220414, loss: 0.253009, perplexity: 1.287895, precision: 0.515625, batch_len: 129.000000
Train, loss=0.25300893: 2094it [48:41,  1.16s/it]2017-06-01 18:58:13,520 root  INFO     step 2094.000000 - time: 1.760011, loss: 0.327344, perplexity: 1.387278, precision: 0.531250, batch_len: 123.000000
Train, loss=0.32734370: 2095it [48:43,  1.35s/it]2017-06-01 18:58:14,789 root  INFO     step 2095.000000 - time: 0.989451, loss: 0.505158, perplexity: 1.657248, precision: 0.546875, batch_len: 79.000000
Train, loss=0.50515825: 2096it [48:44,  1.33s/it]2017-06-01 18:58:16,227 root  INFO     step 2096.000000 - time: 1.317042, loss: 0.297727, perplexity: 1.346794, precision: 0.640625, batch_len: 125.000000
Train, loss=0.29772669: 2097it [48:46,  1.36s/it]2017-06-01 18:58:17,566 root  INFO     step 2097.000000 - time: 1.235583, loss: 0.282607, perplexity: 1.326583, precision: 0.656250, batch_len: 126.000000
Train, loss=0.28260669: 2098it [48:47,  1.35s/it]2017-06-01 18:58:19,166 root  INFO     step 2098.000000 - time: 1.546028, loss: 0.202127, perplexity: 1.224004, precision: 0.671875, batch_len: 116.000000
Train, loss=0.20212707: 2099it [48:49,  1.43s/it]2017-06-01 18:58:20,460 root  INFO     step 2099.000000 - time: 1.287063, loss: 0.134987, perplexity: 1.144522, precision: 0.796875, batch_len: 98.000000
Train, loss=0.13498732: 2100it [48:50,  1.39s/it]2017-06-01 18:58:21,586 root  INFO     step 2100.000000 - time: 0.971938, loss: 0.284060, perplexity: 1.328513, precision: 0.593750, batch_len: 83.000000
Train, loss=0.28406027: 2101it [48:51,  1.31s/it]2017-06-01 18:58:22,748 root  INFO     step 2101.000000 - time: 1.132758, loss: 0.213097, perplexity: 1.237505, precision: 0.796875, batch_len: 99.000000
Train, loss=0.21309733: 2102it [48:52,  1.26s/it]2017-06-01 18:58:23,867 root  INFO     step 2102.000000 - time: 0.993200, loss: 0.287963, perplexity: 1.333708, precision: 0.687500, batch_len: 84.000000
Train, loss=0.28796327: 2103it [48:53,  1.22s/it]2017-06-01 18:58:25,308 root  INFO     step 2103.000000 - time: 1.410400, loss: 0.193220, perplexity: 1.213150, precision: 0.781250, batch_len: 119.000000
Train, loss=0.19322000: 2104it [48:55,  1.29s/it]2017-06-01 18:58:26,396 root  INFO     step 2104.000000 - time: 0.979595, loss: 0.119400, perplexity: 1.126820, precision: 0.796875, batch_len: 87.000000
Train, loss=0.11939976: 2105it [48:56,  1.23s/it]2017-06-01 18:58:28,006 root  INFO     step 2105.000000 - time: 1.556654, loss: 0.192643, perplexity: 1.212449, precision: 0.671875, batch_len: 135.000000
Train, loss=0.19264261: 2106it [48:57,  1.34s/it]2017-06-01 18:58:29,521 root  INFO     step 2106.000000 - time: 1.493655, loss: 0.176684, perplexity: 1.193254, precision: 0.625000, batch_len: 118.000000
Train, loss=0.17668428: 2107it [48:59,  1.39s/it]2017-06-01 18:58:31,306 root  INFO     step 2107.000000 - time: 1.559631, loss: 0.258304, perplexity: 1.294733, precision: 0.546875, batch_len: 137.000000
Train, loss=0.25830436: 2108it [49:01,  1.51s/it]2017-06-01 18:58:32,298 root  INFO     step 2108.000000 - time: 0.961074, loss: 0.256618, perplexity: 1.292551, precision: 0.656250, batch_len: 82.000000
Train, loss=0.25661814: 2109it [49:02,  1.36s/it]2017-06-01 18:58:33,727 root  INFO     step 2109.000000 - time: 1.372885, loss: 0.178445, perplexity: 1.195358, precision: 0.656250, batch_len: 133.000000
Train, loss=0.17844546: 2110it [49:03,  1.38s/it]2017-06-01 18:58:35,461 root  INFO     step 2110.000000 - time: 1.669286, loss: 0.222246, perplexity: 1.248879, precision: 0.531250, batch_len: 130.000000
Train, loss=0.22224604: 2111it [49:05,  1.48s/it]2017-06-01 18:58:37,015 root  INFO     step 2111.000000 - time: 1.427764, loss: 0.188340, perplexity: 1.207243, precision: 0.656250, batch_len: 144.000000
Train, loss=0.18833965: 2112it [49:06,  1.51s/it]2017-06-01 18:58:38,190 root  INFO     step 2112.000000 - time: 1.000187, loss: 0.155468, perplexity: 1.168205, precision: 0.781250, batch_len: 78.000000
Train, loss=0.15546814: 2113it [49:08,  1.41s/it]2017-06-01 18:58:39,613 root  INFO     step 2113.000000 - time: 1.337700, loss: 0.151647, perplexity: 1.163749, precision: 0.703125, batch_len: 138.000000
Train, loss=0.15164672: 2114it [49:09,  1.41s/it]2017-06-01 18:58:40,950 root  INFO     step 2114.000000 - time: 1.227765, loss: 0.171945, perplexity: 1.187613, precision: 0.671875, batch_len: 96.000000
Train, loss=0.17194535: 2115it [49:10,  1.39s/it]2017-06-01 18:58:42,570 root  INFO     step 2115.000000 - time: 1.601197, loss: 0.214480, perplexity: 1.239217, precision: 0.609375, batch_len: 136.000000
Train, loss=0.21447960: 2116it [49:12,  1.46s/it]2017-06-01 18:58:44,657 root  INFO     step 2116.000000 - time: 1.802510, loss: 0.490844, perplexity: 1.633695, precision: 0.343750, batch_len: 150.000000
Train, loss=0.49084434: 2117it [49:14,  1.65s/it]2017-06-01 18:58:45,970 root  INFO     step 2117.000000 - time: 1.214516, loss: 1.011113, perplexity: 2.748659, precision: 0.281250, batch_len: 132.000000
Train, loss=1.01111317: 2118it [49:15,  1.55s/it]2017-06-01 18:58:47,763 root  INFO     step 2118.000000 - time: 1.675067, loss: 1.170744, perplexity: 3.224391, precision: 0.093750, batch_len: 141.000000
Train, loss=1.17074394: 2119it [49:17,  1.62s/it]2017-06-01 18:58:49,083 root  INFO     step 2119.000000 - time: 1.278666, loss: 0.449721, perplexity: 1.567875, precision: 0.484375, batch_len: 72.000000
Train, loss=0.44972116: 2120it [49:18,  1.53s/it]2017-06-01 18:58:50,112 root  INFO     step 2120.000000 - time: 0.980132, loss: 0.185765, perplexity: 1.204140, precision: 0.718750, batch_len: 74.000000
Train, loss=0.18576521: 2121it [49:19,  1.38s/it]2017-06-01 18:58:51,977 root  INFO     step 2121.000000 - time: 1.854794, loss: 0.785039, perplexity: 2.192492, precision: 0.187500, batch_len: 152.000000
Train, loss=0.78503895: 2122it [49:21,  1.53s/it]2017-06-01 18:58:53,430 root  INFO     step 2122.000000 - time: 1.393152, loss: 0.373027, perplexity: 1.452123, precision: 0.421875, batch_len: 134.000000
Train, loss=0.37302655: 2123it [49:23,  1.50s/it]2017-06-01 18:58:55,214 root  INFO     step 2123.000000 - time: 1.778184, loss: 0.227527, perplexity: 1.255491, precision: 0.609375, batch_len: 139.000000
Train, loss=0.22752658: 2124it [49:25,  1.59s/it]2017-06-01 18:58:56,623 root  INFO     step 2124.000000 - time: 1.402464, loss: 0.219928, perplexity: 1.245987, precision: 0.625000, batch_len: 122.000000
Train, loss=0.21992795: 2125it [49:26,  1.53s/it]2017-06-01 18:58:57,670 root  INFO     step 2125.000000 - time: 0.948869, loss: 0.291523, perplexity: 1.338464, precision: 0.718750, batch_len: 76.000000
Train, loss=0.29152262: 2126it [49:27,  1.39s/it]2017-06-01 18:58:59,030 root  INFO     step 2126.000000 - time: 1.315781, loss: 0.219347, perplexity: 1.245264, precision: 0.500000, batch_len: 142.000000
Train, loss=0.21934739: 2127it [49:28,  1.38s/it]2017-06-01 18:58:59,977 root  INFO     step 2127.000000 - time: 0.909973, loss: 0.148087, perplexity: 1.159613, precision: 0.734375, batch_len: 77.000000
Train, loss=0.14808661: 2128it [49:29,  1.25s/it]2017-06-01 18:59:01,176 root  INFO     step 2128.000000 - time: 1.077726, loss: 0.106575, perplexity: 1.112461, precision: 0.843750, batch_len: 71.000000
Train, loss=0.10657492: 2129it [49:31,  1.23s/it]2017-06-01 18:59:02,875 root  INFO     step 2129.000000 - time: 1.399128, loss: 0.160844, perplexity: 1.174501, precision: 0.671875, batch_len: 131.000000
Train, loss=0.16084373: 2130it [49:32,  1.37s/it]
input_tensor dim: (?, 1, 32, ?)
CNN outdim before squeeze: (?, 1, ?, 512)
CNN outdim: (?, ?, 512)
using GRU CELL in decoder
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:02:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x324b430
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:04:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x324f270
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:83:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3273b00
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40c, pci bus id: 0000:83:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40c, pci bus id: 0000:84:00.0)
2017-06-02 13:57:08,891 root  INFO     loading data
2017-06-02 13:57:08,974 root  INFO     phase: train
2017-06-02 13:57:08,974 root  INFO     model_dir: model_01_16
2017-06-02 13:57:08,974 root  INFO     load_model: False
2017-06-02 13:57:08,974 root  INFO     output_dir: results
2017-06-02 13:57:08,974 root  INFO     steps_per_checkpoint: 2000
2017-06-02 13:57:08,974 root  INFO     batch_size: 64
2017-06-02 13:57:08,975 root  INFO     num_epoch: 30
2017-06-02 13:57:08,975 root  INFO     learning_rate: 1
2017-06-02 13:57:08,975 root  INFO     reg_val: 0
2017-06-02 13:57:08,975 root  INFO     max_gradient_norm: 5.000000
2017-06-02 13:57:08,975 root  INFO     clip_gradients: True
2017-06-02 13:57:08,975 root  INFO     valid_target_length inf
2017-06-02 13:57:08,975 root  INFO     target_vocab_size: 39
2017-06-02 13:57:08,975 root  INFO     target_embedding_size: 10.000000
2017-06-02 13:57:08,975 root  INFO     attn_num_hidden: 256
2017-06-02 13:57:08,975 root  INFO     attn_num_layers: 2
2017-06-02 13:57:08,975 root  INFO     visualize: True
2017-06-02 13:57:08,975 root  INFO     buckets
2017-06-02 13:57:08,975 root  INFO     [(16, 11), (27, 17), (35, 19), (64, 22), (80, 32)]
2017-06-02 13:57:08,976 root  INFO     ues GRU in the decoder.
2017-06-02 13:58:16,245 root  INFO     Created model with fresh parameters.
Train: :   0%|          | 0/156 [00:00<?, ?it/s]2017-06-02 13:58:42,187 root  INFO     Generating first batch)
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.40GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.34GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=1325 evicted_count=1000 eviction_rate=0.754717 and unsatisfied allocation rate=0.913892
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.40GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.34GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2017-06-02 14:00:14,149 root  INFO     step 0.000000 - time: 7.651885, loss: 3.675009, perplexity: 39.449003, precision: 0.000000, batch_len: 101.000000
Train, loss=3.67500877:   1%|          | 1/156 [01:31<3:57:35, 91.97s/it]2017-06-02 14:00:20,421 root  INFO     step 1.000000 - time: 3.950993, loss: 3.438864, perplexity: 31.151542, precision: 0.000000, batch_len: 96.000000
Train, loss=3.43886375:   1%|1         | 2/156 [01:38<2:50:03, 66.26s/it]2017-06-02 14:00:27,667 root  INFO     step 2.000000 - time: 5.944557, loss: 3.248152, perplexity: 25.742718, precision: 0.000000, batch_len: 128.000000
Train, loss=3.24815178:   2%|1         | 3/156 [01:45<2:03:48, 48.55s/it]2017-06-02 14:00:29,345 root  INFO     step 3.000000 - time: 1.306492, loss: 3.110007, perplexity: 22.421208, precision: 0.000000, batch_len: 100.000000
Train, loss=3.11000729:   3%|2         | 4/156 [01:47<1:27:22, 34.49s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1014 evicted_count=1000 eviction_rate=0.986193 and unsatisfied allocation rate=0
2017-06-02 14:00:40,312 root  INFO     step 4.000000 - time: 3.916174, loss: 3.029313, perplexity: 20.683010, precision: 0.000000, batch_len: 111.000000
Train, loss=3.02931261:   3%|3         | 5/156 [01:58<1:09:02, 27.43s/it]2017-06-02 14:00:45,163 root  INFO     step 5.000000 - time: 4.262605, loss: 3.042944, perplexity: 20.966878, precision: 0.000000, batch_len: 92.000000
Train, loss=3.04294395:   4%|3         | 6/156 [02:02<51:38, 20.66s/it]  2017-06-02 14:00:47,060 root  INFO     step 6.000000 - time: 1.584312, loss: 2.991588, perplexity: 19.917288, precision: 0.000000, batch_len: 102.000000
Train, loss=2.99158812:   4%|4         | 7/156 [02:04<37:19, 15.03s/it]2017-06-02 14:00:50,502 root  INFO     step 7.000000 - time: 1.333591, loss: 2.949099, perplexity: 19.088748, precision: 0.000000, batch_len: 110.000000
Train, loss=2.94909906:   5%|5         | 8/156 [02:08<28:29, 11.55s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=2021 evicted_count=2000 eviction_rate=0.989609 and unsatisfied allocation rate=0
2017-06-02 14:00:55,704 root  INFO     step 8.000000 - time: 4.359021, loss: 2.935125, perplexity: 18.823854, precision: 0.000000, batch_len: 120.000000
Train, loss=2.93512487:   6%|5         | 9/156 [02:13<23:38,  9.65s/it]2017-06-02 14:01:01,252 root  INFO     step 9.000000 - time: 4.060675, loss: 2.976729, perplexity: 19.623526, precision: 0.000000, batch_len: 104.000000
Train, loss=2.97672915:   6%|6         | 10/156 [02:19<20:29,  8.42s/it]2017-06-02 14:01:05,100 root  INFO     step 10.000000 - time: 3.736158, loss: 2.923609, perplexity: 18.608329, precision: 0.000000, batch_len: 90.000000
Train, loss=2.92360926:   7%|7         | 11/156 [02:22<17:01,  7.05s/it]2017-06-02 14:01:09,573 root  INFO     step 11.000000 - time: 4.028393, loss: 2.863461, perplexity: 17.522066, precision: 0.000000, batch_len: 113.000000
Train, loss=2.86346102:   8%|7         | 12/156 [02:27<15:03,  6.27s/it]2017-06-02 14:01:12,955 root  INFO     step 12.000000 - time: 1.758825, loss: 2.940970, perplexity: 18.934212, precision: 0.000000, batch_len: 106.000000
Train, loss=2.94097042:   8%|8         | 13/156 [02:30<12:53,  5.41s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1033 evicted_count=1000 eviction_rate=0.968054 and unsatisfied allocation rate=0
2017-06-02 14:01:14,443 root  INFO     step 13.000000 - time: 1.430373, loss: 2.927585, perplexity: 18.682451, precision: 0.000000, batch_len: 105.000000
Train, loss=2.92758465:   9%|8         | 14/156 [02:32<10:00,  4.23s/it]2017-06-02 14:01:16,237 root  INFO     step 14.000000 - time: 1.593152, loss: 2.864041, perplexity: 17.532237, precision: 0.000000, batch_len: 112.000000
Train, loss=2.86404133:  10%|9         | 15/156 [02:34<08:13,  3.50s/it]2017-06-02 14:01:18,966 root  INFO     step 15.000000 - time: 1.413148, loss: 2.882286, perplexity: 17.855040, precision: 0.000000, batch_len: 93.000000
Train, loss=2.88228583:  10%|#         | 16/156 [02:36<07:37,  3.27s/it]2017-06-02 14:01:20,532 root  INFO     step 16.000000 - time: 1.435248, loss: 2.873621, perplexity: 17.700989, precision: 0.000000, batch_len: 97.000000
Train, loss=2.87362051:  11%|#         | 17/156 [02:38<06:23,  2.76s/it]2017-06-02 14:01:25,922 root  INFO     step 17.000000 - time: 3.988560, loss: 2.865954, perplexity: 17.565810, precision: 0.000000, batch_len: 81.000000
Train, loss=2.86595440:  12%|#1        | 18/156 [02:43<08:09,  3.55s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1054 evicted_count=1000 eviction_rate=0.948767 and unsatisfied allocation rate=0
2017-06-02 14:01:28,429 root  INFO     step 18.000000 - time: 1.683019, loss: 2.851675, perplexity: 17.316764, precision: 0.000000, batch_len: 108.000000
Train, loss=2.85167503:  12%|#2        | 19/156 [02:46<07:23,  3.24s/it]2017-06-02 14:01:30,671 root  INFO     step 19.000000 - time: 1.562189, loss: 2.877223, perplexity: 17.764876, precision: 0.000000, batch_len: 91.000000
Train, loss=2.87722325:  13%|#2        | 20/156 [02:48<06:39,  2.94s/it]2017-06-02 14:01:33,570 root  INFO     step 20.000000 - time: 1.643353, loss: 2.847770, perplexity: 17.249273, precision: 0.000000, batch_len: 88.000000
Train, loss=2.84776998:  13%|#3        | 21/156 [02:51<06:35,  2.93s/it]2017-06-02 14:01:39,657 root  INFO     step 21.000000 - time: 4.175681, loss: 2.871223, perplexity: 17.658601, precision: 0.000000, batch_len: 117.000000
Train, loss=2.87122297:  14%|#4        | 22/156 [02:57<08:39,  3.87s/it]2017-06-02 14:01:41,662 root  INFO     step 22.000000 - time: 1.390007, loss: 2.871274, perplexity: 17.659494, precision: 0.000000, batch_len: 109.000000
Train, loss=2.87127352:  15%|#4        | 23/156 [02:59<07:20,  3.31s/it]2017-06-02 14:01:42,799 root  INFO     step 23.000000 - time: 1.044823, loss: 2.839880, perplexity: 17.113703, precision: 0.000000, batch_len: 80.000000
Train, loss=2.83987951:  15%|#5        | 24/156 [03:00<05:51,  2.66s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=3270 evicted_count=2000 eviction_rate=0.611621 and unsatisfied allocation rate=0.547264
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 958 to 1053
2017-06-02 14:01:44,853 root  INFO     step 24.000000 - time: 1.405302, loss: 2.874258, perplexity: 17.712277, precision: 0.000000, batch_len: 89.000000
Train, loss=2.87425804:  16%|#6        | 25/156 [03:02<05:24,  2.48s/it]2017-06-02 14:01:46,710 root  INFO     step 25.000000 - time: 1.661028, loss: 2.860802, perplexity: 17.475540, precision: 0.000000, batch_len: 121.000000
Train, loss=2.86080217:  17%|#6        | 26/156 [03:04<04:57,  2.29s/it]2017-06-02 14:01:52,283 root  INFO     step 26.000000 - time: 4.345658, loss: 2.868788, perplexity: 17.615651, precision: 0.000000, batch_len: 124.000000
Train, loss=2.86878777:  17%|#7        | 27/156 [03:10<07:02,  3.28s/it]2017-06-02 14:01:54,033 root  INFO     step 27.000000 - time: 1.549776, loss: 2.836855, perplexity: 17.062028, precision: 0.000000, batch_len: 116.000000
Train, loss=2.83685541:  18%|#7        | 28/156 [03:11<06:00,  2.82s/it]2017-06-02 14:01:57,170 root  INFO     step 28.000000 - time: 2.381548, loss: 2.833862, perplexity: 17.011024, precision: 0.000000, batch_len: 123.000000
Train, loss=2.83386159:  19%|#8        | 29/156 [03:14<06:10,  2.91s/it]2017-06-02 14:02:01,707 root  INFO     step 29.000000 - time: 3.554701, loss: 2.838300, perplexity: 17.086689, precision: 0.000000, batch_len: 86.000000
Train, loss=2.83829975:  19%|#9        | 30/156 [03:19<07:08,  3.40s/it]2017-06-02 14:02:06,932 root  INFO     step 30.000000 - time: 3.657008, loss: 2.859445, perplexity: 17.451832, precision: 0.000000, batch_len: 79.000000
Train, loss=2.85944462:  20%|#9        | 31/156 [03:24<08:13,  3.95s/it]2017-06-02 14:02:09,059 root  INFO     step 31.000000 - time: 1.957560, loss: 2.843539, perplexity: 17.176450, precision: 0.000000, batch_len: 94.000000
Train, loss=2.84353924:  21%|##        | 32/156 [03:26<07:01,  3.40s/it]2017-06-02 14:02:12,009 root  INFO     step 32.000000 - time: 1.392065, loss: 2.850406, perplexity: 17.294801, precision: 0.000000, batch_len: 129.000000
Train, loss=2.85040593:  21%|##1       | 33/156 [03:29<06:41,  3.27s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3133 get requests, put_count=3484 evicted_count=1000 eviction_rate=0.287026 and unsatisfied allocation rate=0.266518
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 2049 to 2253
2017-06-02 14:02:14,516 root  INFO     step 33.000000 - time: 1.954360, loss: 2.858957, perplexity: 17.443329, precision: 0.000000, batch_len: 125.000000
Train, loss=2.85895729:  22%|##1       | 34/156 [03:32<06:10,  3.04s/it]2017-06-02 14:02:16,982 root  INFO     step 34.000000 - time: 1.579487, loss: 2.806347, perplexity: 16.549359, precision: 0.000000, batch_len: 115.000000
Train, loss=2.80634737:  22%|##2       | 35/156 [03:34<05:46,  2.87s/it]2017-06-02 14:02:21,023 root  INFO     step 35.000000 - time: 1.495965, loss: 2.896841, perplexity: 18.116825, precision: 0.000000, batch_len: 85.000000
Train, loss=2.89684105:  23%|##3       | 36/156 [03:38<06:26,  3.22s/it]2017-06-02 14:02:22,964 root  INFO     step 36.000000 - time: 1.081645, loss: 2.836253, perplexity: 17.051760, precision: 0.000000, batch_len: 87.000000
Train, loss=2.83625340:  24%|##3       | 37/156 [03:40<05:37,  2.84s/it]2017-06-02 14:02:24,987 root  INFO     step 37.000000 - time: 1.629742, loss: 2.818652, perplexity: 16.754245, precision: 0.000000, batch_len: 103.000000
Train, loss=2.81865168:  24%|##4       | 38/156 [03:42<05:05,  2.59s/it]2017-06-02 14:02:27,754 root  INFO     step 38.000000 - time: 1.411511, loss: 2.833464, perplexity: 17.004264, precision: 0.000000, batch_len: 114.000000
Train, loss=2.83346415:  25%|##5       | 39/156 [03:45<05:09,  2.64s/it]2017-06-02 14:02:30,592 root  INFO     step 39.000000 - time: 1.996983, loss: 2.877016, perplexity: 17.761196, precision: 0.000000, batch_len: 119.000000
Train, loss=2.87701607:  26%|##5       | 40/156 [03:48<05:13,  2.70s/it]2017-06-02 14:02:33,722 root  INFO     step 40.000000 - time: 1.616195, loss: 2.853556, perplexity: 17.349365, precision: 0.000000, batch_len: 118.000000
Train, loss=2.85355592:  26%|##6       | 41/156 [03:51<05:25,  2.83s/it]2017-06-02 14:02:38,306 root  INFO     step 41.000000 - time: 4.388960, loss: 2.793806, perplexity: 16.343097, precision: 0.000000, batch_len: 136.000000
Train, loss=2.79380560:  27%|##6       | 42/156 [03:56<06:22,  3.36s/it]2017-06-02 14:02:40,762 root  INFO     step 42.000000 - time: 1.491534, loss: 2.865354, perplexity: 17.555259, precision: 0.000000, batch_len: 83.000000
Train, loss=2.86535358:  28%|##7       | 43/156 [03:58<05:48,  3.09s/it]2017-06-02 14:02:42,355 root  INFO     step 43.000000 - time: 1.265598, loss: 2.796279, perplexity: 16.383577, precision: 0.000000, batch_len: 107.000000
Train, loss=2.79627943:  28%|##8       | 44/156 [04:00<04:55,  2.64s/it]2017-06-02 14:02:47,145 root  INFO     step 44.000000 - time: 4.495136, loss: 2.795809, perplexity: 16.375876, precision: 0.000000, batch_len: 135.000000
Train, loss=2.79580927:  29%|##8       | 45/156 [04:04<06:04,  3.28s/it]2017-06-02 14:02:48,771 root  INFO     step 45.000000 - time: 1.136889, loss: 2.849464, perplexity: 17.278517, precision: 0.000000, batch_len: 84.000000
Train, loss=2.84946394:  29%|##9       | 46/156 [04:06<05:06,  2.79s/it]2017-06-02 14:02:52,560 root  INFO     step 46.000000 - time: 1.592613, loss: 2.808298, perplexity: 16.581674, precision: 0.000000, batch_len: 137.000000
Train, loss=2.80829811:  30%|###       | 47/156 [04:10<05:36,  3.09s/it]2017-06-02 14:02:54,322 root  INFO     step 47.000000 - time: 1.672713, loss: 2.842690, perplexity: 17.161861, precision: 0.000000, batch_len: 126.000000
Train, loss=2.84268951:  31%|###       | 48/156 [04:12<04:50,  2.69s/it]2017-06-02 14:02:56,788 root  INFO     step 48.000000 - time: 1.174267, loss: 2.847563, perplexity: 17.245707, precision: 0.000000, batch_len: 78.000000
Train, loss=2.84756327:  31%|###1      | 49/156 [04:14<04:40,  2.62s/it]2017-06-02 14:03:01,163 root  INFO     step 49.000000 - time: 1.964367, loss: 2.838821, perplexity: 17.095601, precision: 0.000000, batch_len: 99.000000
Train, loss=2.83882117:  32%|###2      | 50/156 [04:18<05:33,  3.15s/it]2017-06-02 14:03:03,212 root  INFO     step 50.000000 - time: 1.285838, loss: 2.807564, perplexity: 16.569502, precision: 0.000000, batch_len: 98.000000
Train, loss=2.80756378:  33%|###2      | 51/156 [04:21<04:55,  2.82s/it]2017-06-02 14:03:07,952 root  INFO     step 51.000000 - time: 4.511945, loss: 2.845096, perplexity: 17.203204, precision: 0.000000, batch_len: 144.000000
Train, loss=2.84509563:  33%|###3      | 52/156 [04:25<05:53,  3.39s/it]2017-06-02 14:03:14,148 root  INFO     step 52.000000 - time: 1.728751, loss: 2.826387, perplexity: 16.884346, precision: 0.000000, batch_len: 130.000000
Train, loss=2.82638693:  34%|###3      | 53/156 [04:31<07:16,  4.24s/it]2017-06-02 14:03:19,007 root  INFO     step 53.000000 - time: 1.734266, loss: 2.817864, perplexity: 16.741061, precision: 0.000000, batch_len: 133.000000
Train, loss=2.81786442:  35%|###4      | 54/156 [04:36<07:31,  4.42s/it]2017-06-02 14:03:23,490 root  INFO     step 54.000000 - time: 1.009258, loss: 2.882051, perplexity: 17.850852, precision: 0.000000, batch_len: 96.000000
Train, loss=2.88205123:  35%|###5      | 55/156 [04:41<07:28,  4.44s/it]2017-06-02 14:03:25,148 root  INFO     step 55.000000 - time: 1.127045, loss: 2.844348, perplexity: 17.190346, precision: 0.000000, batch_len: 82.000000
Train, loss=2.84434795:  36%|###5      | 56/156 [04:42<06:00,  3.61s/it]2017-06-02 14:03:31,537 root  INFO     step 56.000000 - time: 4.435353, loss: 2.821084, perplexity: 16.795047, precision: 0.000000, batch_len: 141.000000
Train, loss=2.82108402:  37%|###6      | 57/156 [04:49<07:19,  4.44s/it]2017-06-02 14:03:33,936 root  INFO     step 57.000000 - time: 1.896140, loss: 2.791827, perplexity: 16.310796, precision: 0.000000, batch_len: 138.000000
Train, loss=2.79182720:  37%|###7      | 58/156 [04:51<06:15,  3.83s/it]2017-06-02 14:03:42,714 root  INFO     step 58.000000 - time: 7.540751, loss: 2.852835, perplexity: 17.336869, precision: 0.000000, batch_len: 152.000000
Train, loss=2.85283542:  38%|###7      | 59/156 [05:00<08:35,  5.31s/it]2017-06-02 14:03:45,848 root  INFO     step 59.000000 - time: 1.720505, loss: 2.906754, perplexity: 18.297318, precision: 0.000000, batch_len: 134.000000
Train, loss=2.90675449:  38%|###8      | 60/156 [05:03<07:27,  4.66s/it]2017-06-02 14:03:48,920 root  INFO     step 60.000000 - time: 1.520492, loss: 2.796139, perplexity: 16.381284, precision: 0.000000, batch_len: 139.000000
Train, loss=2.79613948:  39%|###9      | 61/156 [05:06<06:37,  4.18s/it]2017-06-02 14:03:53,228 root  INFO     step 61.000000 - time: 3.867282, loss: 2.832025, perplexity: 16.979803, precision: 0.000000, batch_len: 74.000000
Train, loss=2.83202457:  40%|###9      | 62/156 [05:11<06:36,  4.22s/it]2017-06-02 14:03:56,744 root  INFO     step 62.000000 - time: 1.847285, loss: 2.958169, perplexity: 19.262678, precision: 0.000000, batch_len: 132.000000
Train, loss=2.95816946:  40%|####      | 63/156 [05:14<06:12,  4.01s/it]2017-06-02 14:03:58,557 root  INFO     step 63.000000 - time: 1.461559, loss: 2.987369, perplexity: 19.833433, precision: 0.000000, batch_len: 76.000000
Train, loss=2.98736906:  41%|####1     | 64/156 [05:16<05:08,  3.35s/it]2017-06-02 14:03:59,756 root  INFO     step 64.000000 - time: 1.131531, loss: 2.805053, perplexity: 16.527960, precision: 0.000000, batch_len: 77.000000
Train, loss=2.80505347:  42%|####1     | 65/156 [05:17<04:06,  2.70s/it]2017-06-02 14:04:06,023 root  INFO     step 65.000000 - time: 1.756041, loss: 2.896042, perplexity: 18.102361, precision: 0.000000, batch_len: 122.000000
Train, loss=2.89604235:  42%|####2     | 66/156 [05:23<05:39,  3.77s/it]2017-06-02 14:04:07,883 root  INFO     step 66.000000 - time: 1.794850, loss: 2.907295, perplexity: 18.307210, precision: 0.000000, batch_len: 72.000000
Train, loss=2.90729499:  43%|####2     | 67/156 [05:25<04:44,  3.20s/it]2017-06-02 14:04:14,927 root  INFO     step 67.000000 - time: 5.097016, loss: 2.998593, perplexity: 20.057303, precision: 0.000000, batch_len: 150.000000
Train, loss=2.99859333:  44%|####3     | 68/156 [05:32<06:23,  4.35s/it]2017-06-02 14:04:21,288 root  INFO     step 68.000000 - time: 3.546768, loss: 3.002776, perplexity: 20.141375, precision: 0.000000, batch_len: 71.000000
Train, loss=3.00277615:  44%|####4     | 69/156 [05:39<07:11,  4.96s/it]2017-06-02 14:04:23,635 root  INFO     step 69.000000 - time: 1.899986, loss: 2.838581, perplexity: 17.091489, precision: 0.000000, batch_len: 142.000000
Train, loss=2.83858061:  45%|####4     | 70/156 [05:41<05:58,  4.17s/it]2017-06-02 14:04:26,805 root  INFO     step 70.000000 - time: 1.619572, loss: 2.784843, perplexity: 16.197274, precision: 0.000000, batch_len: 131.000000
Train, loss=2.78484297:  46%|####5     | 71/156 [05:44<05:29,  3.87s/it]2017-06-02 14:04:29,641 root  INFO     Generating first batch)
2017-06-02 14:04:33,302 root  INFO     step 71.000000 - time: 0.912334, loss: 2.845503, perplexity: 17.210207, precision: 0.000000, batch_len: 96.000000
Train, loss=2.84550261:  46%|####6     | 72/156 [05:51<06:31,  4.66s/it]2017-06-02 14:04:34,736 root  INFO     step 72.000000 - time: 1.203059, loss: 2.774652, perplexity: 16.033054, precision: 0.000000, batch_len: 101.000000
Train, loss=2.77465248:  47%|####6     | 73/156 [05:52<05:06,  3.69s/it]2017-06-02 14:04:36,039 root  INFO     step 73.000000 - time: 1.159343, loss: 2.801911, perplexity: 16.476097, precision: 0.000000, batch_len: 108.000000
Train, loss=2.80191064:  47%|####7     | 74/156 [05:53<04:03,  2.98s/it]2017-06-02 14:04:37,154 root  INFO     step 74.000000 - time: 1.036387, loss: 2.892092, perplexity: 18.030991, precision: 0.000000, batch_len: 105.000000
Train, loss=2.89209199:  48%|####8     | 75/156 [05:54<03:15,  2.42s/it]2017-06-02 14:04:38,214 root  INFO     step 75.000000 - time: 1.026986, loss: 2.850224, perplexity: 17.291655, precision: 0.000000, batch_len: 104.000000
Train, loss=2.85022402:  49%|####8     | 76/156 [05:56<02:40,  2.01s/it]2017-06-02 14:04:39,340 root  INFO     step 76.000000 - time: 0.980190, loss: 2.818651, perplexity: 16.754241, precision: 0.000000, batch_len: 113.000000
Train, loss=2.81865144:  49%|####9     | 77/156 [05:57<02:17,  1.74s/it]2017-06-02 14:04:40,489 root  INFO     step 77.000000 - time: 1.127282, loss: 2.826408, perplexity: 16.884708, precision: 0.000000, batch_len: 92.000000
Train, loss=2.82640839:  50%|#####     | 78/156 [05:58<02:02,  1.57s/it]2017-06-02 14:04:42,047 root  INFO     step 78.000000 - time: 1.473847, loss: 2.819202, perplexity: 16.763475, precision: 0.000000, batch_len: 91.000000
Train, loss=2.81920242:  51%|#####     | 79/156 [05:59<02:00,  1.56s/it]2017-06-02 14:04:43,434 root  INFO     step 79.000000 - time: 1.337457, loss: 2.765641, perplexity: 15.889229, precision: 0.000000, batch_len: 120.000000
Train, loss=2.76564145:  51%|#####1    | 80/156 [06:01<01:54,  1.51s/it]2017-06-02 14:04:44,755 root  INFO     step 80.000000 - time: 1.304204, loss: 2.826747, perplexity: 16.890426, precision: 0.000000, batch_len: 128.000000
Train, loss=2.82674694:  52%|#####1    | 81/156 [06:02<01:49,  1.45s/it]2017-06-02 14:04:45,726 root  INFO     step 81.000000 - time: 0.954693, loss: 2.824646, perplexity: 16.854973, precision: 0.000000, batch_len: 97.000000
Train, loss=2.82464576:  53%|#####2    | 82/156 [06:03<01:36,  1.31s/it]2017-06-02 14:04:46,879 root  INFO     step 82.000000 - time: 1.042289, loss: 2.777467, perplexity: 16.078247, precision: 0.000000, batch_len: 110.000000
Train, loss=2.77746725:  53%|#####3    | 83/156 [06:04<01:32,  1.26s/it]2017-06-02 14:04:48,542 root  INFO     step 83.000000 - time: 1.656646, loss: 2.921737, perplexity: 18.573525, precision: 0.000000, batch_len: 117.000000
Train, loss=2.92173719:  54%|#####3    | 84/156 [06:06<01:39,  1.38s/it]2017-06-02 14:04:49,647 root  INFO     step 84.000000 - time: 1.095747, loss: 2.811458, perplexity: 16.634147, precision: 0.000000, batch_len: 106.000000
Train, loss=2.81145763:  54%|#####4    | 85/156 [06:07<01:32,  1.30s/it]2017-06-02 14:04:50,997 root  INFO     step 85.000000 - time: 1.340450, loss: 2.842517, perplexity: 17.158907, precision: 0.000000, batch_len: 125.000000
Train, loss=2.84251738:  55%|#####5    | 86/156 [06:08<01:32,  1.31s/it]2017-06-02 14:04:52,090 root  INFO     step 86.000000 - time: 1.008664, loss: 2.799001, perplexity: 16.428230, precision: 0.000000, batch_len: 103.000000
Train, loss=2.79900122:  56%|#####5    | 87/156 [06:09<01:26,  1.25s/it]2017-06-02 14:04:53,146 root  INFO     step 87.000000 - time: 0.979695, loss: 2.808863, perplexity: 16.591046, precision: 0.000000, batch_len: 88.000000
Train, loss=2.80886316:  56%|#####6    | 88/156 [06:10<01:20,  1.19s/it]2017-06-02 14:04:54,348 root  INFO     step 88.000000 - time: 1.149186, loss: 2.827575, perplexity: 16.904425, precision: 0.000000, batch_len: 85.000000
Train, loss=2.82757545:  57%|#####7    | 89/156 [06:12<01:19,  1.19s/it]2017-06-02 14:04:55,538 root  INFO     step 89.000000 - time: 1.180183, loss: 2.773959, perplexity: 16.021934, precision: 0.000000, batch_len: 100.000000
Train, loss=2.77395868:  58%|#####7    | 90/156 [06:13<01:18,  1.19s/it]2017-06-02 14:04:56,853 root  INFO     step 90.000000 - time: 1.089650, loss: 2.759043, perplexity: 15.784737, precision: 0.000000, batch_len: 93.000000
Train, loss=2.75904346:  58%|#####8    | 91/156 [06:14<01:19,  1.23s/it]2017-06-02 14:04:58,210 root  INFO     step 91.000000 - time: 1.269792, loss: 2.832128, perplexity: 16.981552, precision: 0.000000, batch_len: 124.000000
Train, loss=2.83212757:  59%|#####8    | 92/156 [06:16<01:21,  1.27s/it]2017-06-02 14:04:59,162 root  INFO     step 92.000000 - time: 0.904751, loss: 2.840887, perplexity: 17.130947, precision: 0.000000, batch_len: 80.000000
Train, loss=2.84088659:  60%|#####9    | 93/156 [06:16<01:13,  1.17s/it]2017-06-02 14:05:00,293 root  INFO     step 93.000000 - time: 1.069062, loss: 2.794521, perplexity: 16.354787, precision: 0.000000, batch_len: 102.000000
Train, loss=2.79452062:  60%|######    | 94/156 [06:18<01:11,  1.16s/it]2017-06-02 14:05:01,723 root  INFO     step 94.000000 - time: 1.407003, loss: 2.862765, perplexity: 17.509872, precision: 0.000000, batch_len: 111.000000
Train, loss=2.86276484:  61%|######    | 95/156 [06:19<01:15,  1.24s/it]2017-06-02 14:05:03,242 root  INFO     step 95.000000 - time: 1.380675, loss: 2.864267, perplexity: 17.536201, precision: 0.000000, batch_len: 121.000000
Train, loss=2.86426735:  62%|######1   | 96/156 [06:21<01:19,  1.32s/it]2017-06-02 14:05:04,250 root  INFO     step 96.000000 - time: 0.963585, loss: 2.796920, perplexity: 16.394080, precision: 0.000000, batch_len: 81.000000
Train, loss=2.79692030:  62%|######2   | 97/156 [06:22<01:12,  1.23s/it]2017-06-02 14:05:05,631 root  INFO     step 97.000000 - time: 1.362246, loss: 2.760438, perplexity: 15.806768, precision: 0.000000, batch_len: 116.000000
Train, loss=2.76043820:  63%|######2   | 98/156 [06:23<01:13,  1.27s/it]2017-06-02 14:05:06,739 root  INFO     step 98.000000 - time: 1.022807, loss: 2.805734, perplexity: 16.539214, precision: 0.000000, batch_len: 114.000000
Train, loss=2.80573416:  63%|######3   | 99/156 [06:24<01:09,  1.22s/it]2017-06-02 14:05:08,066 root  INFO     step 99.000000 - time: 1.322842, loss: 2.799165, perplexity: 16.430929, precision: 0.000000, batch_len: 109.000000
Train, loss=2.79916549:  64%|######4   | 100/156 [06:25<01:10,  1.26s/it]2017-06-02 14:05:09,343 root  INFO     step 100.000000 - time: 1.191259, loss: 2.795384, perplexity: 16.368916, precision: 0.000000, batch_len: 89.000000
Train, loss=2.79538417:  65%|######4   | 101/156 [06:27<01:09,  1.26s/it]2017-06-02 14:05:10,788 root  INFO     step 101.000000 - time: 1.348256, loss: 2.771872, perplexity: 15.988530, precision: 0.000000, batch_len: 112.000000
Train, loss=2.77187157:  65%|######5   | 102/156 [06:28<01:11,  1.32s/it]2017-06-02 14:05:11,892 root  INFO     step 102.000000 - time: 1.089687, loss: 2.763980, perplexity: 15.862858, precision: 0.000000, batch_len: 107.000000
Train, loss=2.76398039:  66%|######6   | 103/156 [06:29<01:06,  1.25s/it]2017-06-02 14:05:12,916 root  INFO     step 103.000000 - time: 0.948007, loss: 2.801548, perplexity: 16.470115, precision: 0.000000, batch_len: 94.000000
Train, loss=2.80154753:  67%|######6   | 104/156 [06:30<01:01,  1.18s/it]2017-06-02 14:05:14,065 root  INFO     step 104.000000 - time: 1.058367, loss: 2.798601, perplexity: 16.421663, precision: 0.000000, batch_len: 87.000000
Train, loss=2.79860139:  67%|######7   | 105/156 [06:31<00:59,  1.17s/it]2017-06-02 14:05:15,307 root  INFO     step 105.000000 - time: 1.193215, loss: 2.803610, perplexity: 16.504125, precision: 0.000000, batch_len: 84.000000
Train, loss=2.80361032:  68%|######7   | 106/156 [06:33<00:59,  1.19s/it]2017-06-02 14:05:16,369 root  INFO     step 106.000000 - time: 1.038874, loss: 2.790344, perplexity: 16.286629, precision: 0.000000, batch_len: 90.000000
Train, loss=2.79034448:  69%|######8   | 107/156 [06:34<00:56,  1.15s/it]2017-06-02 14:05:17,584 root  INFO     step 107.000000 - time: 1.013285, loss: 2.792196, perplexity: 16.316817, precision: 0.000000, batch_len: 99.000000
Train, loss=2.79219627:  69%|######9   | 108/156 [06:35<00:56,  1.17s/it]2017-06-02 14:05:18,678 root  INFO     step 108.000000 - time: 1.084570, loss: 2.756305, perplexity: 15.741562, precision: 0.000000, batch_len: 115.000000
Train, loss=2.75630450:  70%|######9   | 109/156 [06:36<00:53,  1.15s/it]2017-06-02 14:05:20,080 root  INFO     step 109.000000 - time: 1.292575, loss: 2.805336, perplexity: 16.532638, precision: 0.000000, batch_len: 119.000000
Train, loss=2.80533648:  71%|#######   | 110/156 [06:37<00:56,  1.23s/it]2017-06-02 14:05:21,278 root  INFO     step 110.000000 - time: 1.191092, loss: 2.793335, perplexity: 16.335403, precision: 0.000000, batch_len: 86.000000
Train, loss=2.79333472:  71%|#######1  | 111/156 [06:39<00:54,  1.22s/it]2017-06-02 14:05:22,867 root  INFO     step 111.000000 - time: 1.507286, loss: 2.860514, perplexity: 17.470503, precision: 0.000000, batch_len: 123.000000
Train, loss=2.86051393:  72%|#######1  | 112/156 [06:40<00:58,  1.33s/it]2017-06-02 14:05:24,207 root  INFO     step 112.000000 - time: 1.306336, loss: 2.789952, perplexity: 16.280235, precision: 0.000000, batch_len: 129.000000
Train, loss=2.78995180:  72%|#######2  | 113/156 [06:42<00:57,  1.33s/it]2017-06-02 14:05:25,275 root  INFO     step 113.000000 - time: 0.962171, loss: 2.790013, perplexity: 16.281237, precision: 0.000000, batch_len: 98.000000
Train, loss=2.79001331:  73%|#######3  | 114/156 [06:43<00:52,  1.25s/it]2017-06-02 14:05:26,668 root  INFO     step 114.000000 - time: 1.357363, loss: 2.834855, perplexity: 17.027928, precision: 0.000000, batch_len: 126.000000
Train, loss=2.83485484:  74%|#######3  | 115/156 [06:44<00:53,  1.29s/it]2017-06-02 14:05:27,823 root  INFO     step 115.000000 - time: 1.143949, loss: 2.923883, perplexity: 18.613423, precision: 0.000000, batch_len: 79.000000
Train, loss=2.92388296:  74%|#######4  | 116/156 [06:45<00:50,  1.25s/it]2017-06-02 14:05:29,327 root  INFO     step 116.000000 - time: 1.468057, loss: 2.872488, perplexity: 17.680946, precision: 0.000000, batch_len: 118.000000
Train, loss=2.87248755:  75%|#######5  | 117/156 [06:47<00:51,  1.33s/it]2017-06-02 14:05:30,968 root  INFO     step 117.000000 - time: 1.540979, loss: 2.779505, perplexity: 16.111040, precision: 0.000000, batch_len: 133.000000
Train, loss=2.77950478:  76%|#######5  | 118/156 [06:48<00:54,  1.42s/it]2017-06-02 14:05:31,983 root  INFO     step 118.000000 - time: 0.964791, loss: 2.770537, perplexity: 15.967201, precision: 0.000000, batch_len: 83.000000
Train, loss=2.77053666:  76%|#######6  | 119/156 [06:49<00:48,  1.30s/it]2017-06-02 14:05:33,398 root  INFO     step 119.000000 - time: 1.403157, loss: 2.727171, perplexity: 15.289578, precision: 0.000000, batch_len: 136.000000
Train, loss=2.72717142:  77%|#######6  | 120/156 [06:51<00:48,  1.33s/it]2017-06-02 14:05:34,701 root  INFO     step 120.000000 - time: 1.280977, loss: 2.780104, perplexity: 16.120700, precision: 0.000000, batch_len: 144.000000
Train, loss=2.78010416:  78%|#######7  | 121/156 [06:52<00:46,  1.33s/it]2017-06-02 14:05:36,356 root  INFO     step 121.000000 - time: 1.622718, loss: 2.807380, perplexity: 16.566460, precision: 0.000000, batch_len: 137.000000
Train, loss=2.80738020:  78%|#######8  | 122/156 [06:54<00:48,  1.42s/it]2017-06-02 14:05:38,046 root  INFO     step 122.000000 - time: 1.555709, loss: 2.749283, perplexity: 15.631421, precision: 0.000000, batch_len: 135.000000
Train, loss=2.74928308:  79%|#######8  | 123/156 [06:55<00:49,  1.50s/it]2017-06-02 14:05:39,718 root  INFO     step 123.000000 - time: 1.375143, loss: 2.777062, perplexity: 16.071732, precision: 0.000000, batch_len: 130.000000
Train, loss=2.77706194:  79%|#######9  | 124/156 [06:57<00:49,  1.55s/it]2017-06-02 14:05:40,854 root  INFO     step 124.000000 - time: 0.922053, loss: 2.794981, perplexity: 16.362310, precision: 0.000000, batch_len: 82.000000
Train, loss=2.79498053:  80%|########  | 125/156 [06:58<00:44,  1.43s/it]2017-06-02 14:05:42,100 root  INFO     step 125.000000 - time: 1.055996, loss: 2.767704, perplexity: 15.922043, precision: 0.000000, batch_len: 78.000000
Train, loss=2.76770449:  81%|########  | 126/156 [06:59<00:41,  1.37s/it]2017-06-02 14:05:43,390 root  INFO     step 126.000000 - time: 1.158408, loss: 2.765856, perplexity: 15.892631, precision: 0.000000, batch_len: 76.000000
Train, loss=2.76585555:  81%|########1 | 127/156 [07:01<00:39,  1.35s/it]2017-06-02 14:05:44,849 root  INFO     step 127.000000 - time: 1.435885, loss: 2.820705, perplexity: 16.788689, precision: 0.000000, batch_len: 141.000000
Train, loss=2.82070541:  82%|########2 | 128/156 [07:02<00:38,  1.38s/it]2017-06-02 14:05:45,905 root  INFO     step 128.000000 - time: 1.002096, loss: 2.802915, perplexity: 16.492654, precision: 0.000000, batch_len: 96.000000
Train, loss=2.80291510:  83%|########2 | 129/156 [07:03<00:34,  1.28s/it]2017-06-02 14:05:47,291 root  INFO     step 129.000000 - time: 1.365241, loss: 2.859674, perplexity: 17.455831, precision: 0.000000, batch_len: 122.000000
Train, loss=2.85967374:  83%|########3 | 130/156 [07:05<00:34,  1.31s/it]2017-06-02 14:05:48,688 root  INFO     step 130.000000 - time: 1.376587, loss: 2.753440, perplexity: 15.696530, precision: 0.000000, batch_len: 134.000000
Train, loss=2.75343966:  84%|########3 | 131/156 [07:06<00:33,  1.34s/it]2017-06-02 14:05:50,240 root  INFO     step 131.000000 - time: 1.502069, loss: 2.804281, perplexity: 16.515193, precision: 0.000000, batch_len: 72.000000
Train, loss=2.80428076:  85%|########4 | 132/156 [07:08<00:33,  1.40s/it]2017-06-02 14:05:51,362 root  INFO     step 132.000000 - time: 0.966185, loss: 2.718411, perplexity: 15.156219, precision: 0.000000, batch_len: 77.000000
Train, loss=2.71841097:  85%|########5 | 133/156 [07:09<00:30,  1.32s/it]2017-06-02 14:05:52,787 root  INFO     step 133.000000 - time: 1.399765, loss: 2.827637, perplexity: 16.905469, precision: 0.000000, batch_len: 138.000000
Train, loss=2.82763720:  86%|########5 | 134/156 [07:10<00:29,  1.35s/it]2017-06-02 14:05:54,205 root  INFO     step 134.000000 - time: 1.329463, loss: 2.817510, perplexity: 16.735126, precision: 0.000000, batch_len: 139.000000
Train, loss=2.81750989:  87%|########6 | 135/156 [07:12<00:28,  1.37s/it]2017-06-02 14:05:55,444 root  INFO     step 135.000000 - time: 1.140291, loss: 2.723720, perplexity: 15.236899, precision: 0.000000, batch_len: 74.000000
Train, loss=2.72372007:  87%|########7 | 136/156 [07:13<00:26,  1.33s/it]2017-06-02 14:05:57,516 root  INFO     step 136.000000 - time: 1.997684, loss: 3.226257, perplexity: 25.185208, precision: 0.000000, batch_len: 152.000000
Train, loss=3.22625685:  88%|########7 | 137/156 [07:15<00:29,  1.55s/it]2017-06-02 14:05:58,965 root  INFO     step 137.000000 - time: 1.396799, loss: 2.944630, perplexity: 19.003633, precision: 0.000000, batch_len: 132.000000
Train, loss=2.94463015:  88%|########8 | 138/156 [07:16<00:27,  1.52s/it]2017-06-02 14:06:00,473 root  INFO     step 138.000000 - time: 1.410869, loss: 2.819970, perplexity: 16.776350, precision: 0.000000, batch_len: 142.000000
Train, loss=2.81997013:  89%|########9 | 139/156 [07:18<00:25,  1.52s/it]2017-06-02 14:06:02,120 root  INFO     step 139.000000 - time: 1.469883, loss: 2.751225, perplexity: 15.661798, precision: 0.000000, batch_len: 131.000000
Train, loss=2.75122452:  90%|########9 | 140/156 [07:19<00:24,  1.56s/it]2017-06-02 14:06:04,225 root  INFO     step 140.000000 - time: 2.033707, loss: 2.826994, perplexity: 16.894598, precision: 0.000000, batch_len: 150.000000
Train, loss=2.82699394:  90%|######### | 141/156 [07:22<00:25,  1.72s/it]2017-06-02 14:06:05,282 root  INFO     step 141.000000 - time: 0.933603, loss: 2.876359, perplexity: 17.749525, precision: 0.000000, batch_len: 71.000000
Train, loss=2.87635875:  91%|#########1| 142/156 [07:23<00:21,  1.52s/it]2017-06-02 14:06:05,430 root  INFO     Generating first batch)
2017-06-02 14:06:09,284 root  INFO     step 142.000000 - time: 1.117867, loss: 2.746596, perplexity: 15.589476, precision: 0.000000, batch_len: 110.000000
Train, loss=2.74659610:  92%|#########1| 143/156 [07:27<00:29,  2.27s/it]2017-06-02 14:06:10,772 root  INFO     step 143.000000 - time: 1.286985, loss: 2.778474, perplexity: 16.094436, precision: 0.000000, batch_len: 128.000000
Train, loss=2.77847362:  92%|#########2| 144/156 [07:28<00:24,  2.03s/it]2017-06-02 14:06:12,193 root  INFO     step 144.000000 - time: 1.365080, loss: 2.729417, perplexity: 15.323945, precision: 0.000000, batch_len: 120.000000
Train, loss=2.72941661:  93%|#########2| 145/156 [07:30<00:20,  1.85s/it]2017-06-02 14:06:13,226 root  INFO     step 145.000000 - time: 0.953881, loss: 2.788274, perplexity: 16.252944, precision: 0.000000, batch_len: 96.000000
Train, loss=2.78827405:  94%|#########3| 146/156 [07:31<00:16,  1.60s/it]2017-06-02 14:06:14,545 root  INFO     step 146.000000 - time: 1.250553, loss: 2.755099, perplexity: 15.722602, precision: 0.000000, batch_len: 92.000000
Train, loss=2.75509930:  94%|#########4| 147/156 [07:32<00:13,  1.52s/it]2017-06-02 14:06:15,833 root  INFO     step 147.000000 - time: 1.112562, loss: 2.733003, perplexity: 15.378999, precision: 0.000000, batch_len: 108.000000
Train, loss=2.73300290:  95%|#########4| 148/156 [07:33<00:11,  1.45s/it]2017-06-02 14:06:16,909 root  INFO     step 148.000000 - time: 1.005532, loss: 2.743528, perplexity: 15.541714, precision: 0.000000, batch_len: 105.000000
Train, loss=2.74352765:  96%|#########5| 149/156 [07:34<00:09,  1.34s/it]2017-06-02 14:06:18,072 root  INFO     step 149.000000 - time: 1.049995, loss: 2.787025, perplexity: 16.232663, precision: 0.000000, batch_len: 97.000000
Train, loss=2.78702545:  96%|#########6| 150/156 [07:35<00:07,  1.29s/it]2017-06-02 14:06:19,523 root  INFO     step 150.000000 - time: 1.298250, loss: 2.832361, perplexity: 16.985524, precision: 0.000000, batch_len: 117.000000
Train, loss=2.83236146:  97%|#########6| 151/156 [07:37<00:06,  1.33s/it]2017-06-02 14:06:20,774 root  INFO     step 151.000000 - time: 1.235864, loss: 2.872552, perplexity: 17.682080, precision: 0.000000, batch_len: 104.000000
Train, loss=2.87255168:  97%|#########7| 152/156 [07:38<00:05,  1.31s/it]2017-06-02 14:06:22,081 root  INFO     step 152.000000 - time: 1.234768, loss: 2.787683, perplexity: 16.243348, precision: 0.000000, batch_len: 113.000000
Train, loss=2.78768349:  98%|#########8| 153/156 [07:39<00:03,  1.31s/it]2017-06-02 14:06:23,227 root  INFO     step 153.000000 - time: 1.028717, loss: 2.784053, perplexity: 16.184485, precision: 0.000000, batch_len: 100.000000
Train, loss=2.78405309:  99%|#########8| 154/156 [07:41<00:02,  1.26s/it]2017-06-02 14:06:24,309 root  INFO     step 154.000000 - time: 1.061411, loss: 2.711117, perplexity: 15.046077, precision: 0.000000, batch_len: 103.000000
Train, loss=2.71111727:  99%|#########9| 155/156 [07:42<00:01,  1.21s/it]2017-06-02 14:06:25,923 root  INFO     step 155.000000 - time: 1.548001, loss: 2.817887, perplexity: 16.741444, precision: 0.000000, batch_len: 90.000000
Train, loss=2.81788731: 100%|##########| 156/156 [07:43<00:00,  1.33s/it]2017-06-02 14:06:27,113 root  INFO     step 156.000000 - time: 1.182083, loss: 2.870413, perplexity: 17.644301, precision: 0.000000, batch_len: 88.000000
Train, loss=2.87041283: 157it [07:44,  1.29s/it]                         2017-06-02 14:06:28,197 root  INFO     step 157.000000 - time: 1.059065, loss: 2.702597, perplexity: 14.918427, precision: 0.000000, batch_len: 106.000000
Train, loss=2.70259714: 158it [07:46,  1.23s/it]2017-06-02 14:06:29,566 root  INFO     step 158.000000 - time: 1.288317, loss: 2.749354, perplexity: 15.632528, precision: 0.000000, batch_len: 102.000000
Train, loss=2.74935389: 159it [07:47,  1.27s/it]2017-06-02 14:06:30,888 root  INFO     step 159.000000 - time: 1.211645, loss: 2.735415, perplexity: 15.416132, precision: 0.000000, batch_len: 93.000000
Train, loss=2.73541451: 160it [07:48,  1.28s/it]2017-06-02 14:06:32,081 root  INFO     step 160.000000 - time: 1.103966, loss: 2.791795, perplexity: 16.310275, precision: 0.000000, batch_len: 111.000000
Train, loss=2.79179525: 161it [07:49,  1.26s/it]2017-06-02 14:06:33,091 root  INFO     step 161.000000 - time: 0.940483, loss: 2.796304, perplexity: 16.383975, precision: 0.000000, batch_len: 85.000000
Train, loss=2.79630375: 162it [07:50,  1.18s/it]2017-06-02 14:06:34,021 root  INFO     step 162.000000 - time: 0.918646, loss: 2.694024, perplexity: 14.791080, precision: 0.000000, batch_len: 86.000000
Train, loss=2.69402432: 163it [07:51,  1.11s/it]2017-06-02 14:06:35,059 root  INFO     step 163.000000 - time: 1.016237, loss: 2.726128, perplexity: 15.273627, precision: 0.000000, batch_len: 89.000000
Train, loss=2.72612762: 164it [07:52,  1.09s/it]2017-06-02 14:06:36,351 root  INFO     step 164.000000 - time: 1.212902, loss: 2.701348, perplexity: 14.899808, precision: 0.000000, batch_len: 101.000000
Train, loss=2.70134830: 165it [07:54,  1.15s/it]2017-06-02 14:06:37,842 root  INFO     step 165.000000 - time: 1.395457, loss: 2.818684, perplexity: 16.754781, precision: 0.000000, batch_len: 124.000000
Train, loss=2.81868362: 166it [07:55,  1.25s/it]2017-06-02 14:06:38,972 root  INFO     step 166.000000 - time: 1.086030, loss: 2.838629, perplexity: 17.092316, precision: 0.000000, batch_len: 115.000000
Train, loss=2.83862901: 167it [07:56,  1.21s/it]2017-06-02 14:06:40,250 root  INFO     step 167.000000 - time: 1.219355, loss: 2.802666, perplexity: 16.488554, precision: 0.000000, batch_len: 91.000000
Train, loss=2.80266643: 168it [07:58,  1.23s/it]2017-06-02 14:06:41,238 root  INFO     step 168.000000 - time: 0.915674, loss: 2.788301, perplexity: 16.253389, precision: 0.000000, batch_len: 94.000000
Train, loss=2.78830147: 169it [07:59,  1.16s/it]2017-06-02 14:06:42,353 root  INFO     step 169.000000 - time: 1.048735, loss: 2.772317, perplexity: 15.995660, precision: 0.000000, batch_len: 81.000000
Train, loss=2.77231741: 170it [08:00,  1.15s/it]2017-06-02 14:06:43,618 root  INFO     step 170.000000 - time: 1.237320, loss: 2.672805, perplexity: 14.480535, precision: 0.000000, batch_len: 98.000000
Train, loss=2.67280531: 171it [08:01,  1.18s/it]2017-06-02 14:06:44,756 root  INFO     step 171.000000 - time: 1.109316, loss: 2.736969, perplexity: 15.440115, precision: 0.000000, batch_len: 107.000000
Train, loss=2.73696899: 172it [08:02,  1.17s/it]2017-06-02 14:06:45,904 root  INFO     step 172.000000 - time: 1.088066, loss: 2.742677, perplexity: 15.528503, precision: 0.000000, batch_len: 109.000000
Train, loss=2.74267721: 173it [08:03,  1.16s/it]2017-06-02 14:06:47,305 root  INFO     step 173.000000 - time: 1.383092, loss: 2.788597, perplexity: 16.258195, precision: 0.000000, batch_len: 123.000000
Train, loss=2.78859711: 174it [08:05,  1.23s/it]2017-06-02 14:06:48,771 root  INFO     step 174.000000 - time: 1.361414, loss: 2.754751, perplexity: 15.717126, precision: 0.000000, batch_len: 121.000000
Train, loss=2.75475097: 175it [08:06,  1.30s/it]2017-06-02 14:06:49,844 root  INFO     step 175.000000 - time: 1.066871, loss: 2.769590, perplexity: 15.952095, precision: 0.000000, batch_len: 80.000000
Train, loss=2.76959014: 176it [08:07,  1.23s/it]2017-06-02 14:06:50,945 root  INFO     step 176.000000 - time: 1.091427, loss: 2.761720, perplexity: 15.827049, precision: 0.000000, batch_len: 87.000000
Train, loss=2.76172042: 177it [08:08,  1.19s/it]2017-06-02 14:06:52,093 root  INFO     step 177.000000 - time: 1.128534, loss: 2.820845, perplexity: 16.791035, precision: 0.000000, batch_len: 114.000000
Train, loss=2.82084513: 178it [08:09,  1.18s/it]2017-06-02 14:06:53,558 root  INFO     step 178.000000 - time: 1.333027, loss: 2.737973, perplexity: 15.455624, precision: 0.000000, batch_len: 112.000000
Train, loss=2.73797297: 179it [08:11,  1.27s/it]2017-06-02 14:06:54,903 root  INFO     step 179.000000 - time: 1.337375, loss: 2.719276, perplexity: 15.169335, precision: 0.000000, batch_len: 116.000000
Train, loss=2.71927595: 180it [08:12,  1.29s/it]2017-06-02 14:06:56,604 root  INFO     step 180.000000 - time: 1.591155, loss: 2.767319, perplexity: 15.915906, precision: 0.000000, batch_len: 126.000000
Train, loss=2.76731896: 181it [08:14,  1.41s/it]2017-06-02 14:06:57,971 root  INFO     step 181.000000 - time: 1.300145, loss: 2.744133, perplexity: 15.551133, precision: 0.000000, batch_len: 125.000000
Train, loss=2.74413347: 182it [08:15,  1.40s/it]2017-06-02 14:06:59,466 root  INFO     step 182.000000 - time: 1.333732, loss: 2.749013, perplexity: 15.627207, precision: 0.000000, batch_len: 118.000000
Train, loss=2.74901342: 183it [08:17,  1.43s/it]2017-06-02 14:07:00,418 root  INFO     step 183.000000 - time: 0.936112, loss: 2.816134, perplexity: 16.712116, precision: 0.000000, batch_len: 84.000000
Train, loss=2.81613398: 184it [08:18,  1.29s/it]2017-06-02 14:07:01,728 root  INFO     step 184.000000 - time: 1.272268, loss: 2.824149, perplexity: 16.846601, precision: 0.000000, batch_len: 129.000000
Train, loss=2.82414889: 185it [08:19,  1.29s/it]2017-06-02 14:07:03,005 root  INFO     step 185.000000 - time: 1.137422, loss: 2.929772, perplexity: 18.723359, precision: 0.000000, batch_len: 79.000000
Train, loss=2.92977190: 186it [08:20,  1.29s/it]2017-06-02 14:07:04,181 root  INFO     step 186.000000 - time: 1.162064, loss: 2.798438, perplexity: 16.418985, precision: 0.000000, batch_len: 99.000000
Train, loss=2.79843831: 187it [08:21,  1.25s/it]2017-06-02 14:07:05,522 root  INFO     step 187.000000 - time: 1.317070, loss: 2.777740, perplexity: 16.082629, precision: 0.000000, batch_len: 119.000000
Train, loss=2.77773976: 188it [08:23,  1.28s/it]2017-06-02 14:07:06,947 root  INFO     step 188.000000 - time: 1.358779, loss: 2.781936, perplexity: 16.150260, precision: 0.000000, batch_len: 130.000000
Train, loss=2.78193617: 189it [08:24,  1.32s/it]2017-06-02 14:07:07,958 root  INFO     step 189.000000 - time: 0.905713, loss: 2.742805, perplexity: 15.530495, precision: 0.000000, batch_len: 83.000000
Train, loss=2.74280548: 190it [08:25,  1.23s/it]2017-06-02 14:07:09,630 root  INFO     step 190.000000 - time: 1.650287, loss: 2.754614, perplexity: 15.714968, precision: 0.000000, batch_len: 137.000000
Train, loss=2.75461364: 191it [08:27,  1.36s/it]train_demo.sh: 行 19: 70272 已终止               $py src/launcher.py --phase=train --data-path=sample/sample.txt --data-base-dir=sample --log-path=log_01_16.log --attn-num-hidden 256 --batch-size 64 --model-dir=model_01_16 --initial-learning-rate=1.0 --no-load-model --num-epoch=30 --gpu-id=0 --use-gru --steps-per-checkpoint=2000 --target-embedding-size=10
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:02:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3b23540
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:04:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3b273b0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:83:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3b4bc30
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40c, pci bus id: 0000:83:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40c, pci bus id: 0000:84:00.0)
2017-06-02 14:13:26,180 root  INFO     loading data
2017-06-02 14:13:26,228 root  INFO     phase: train
2017-06-02 14:13:26,228 root  INFO     model_dir: model_01_16
2017-06-02 14:13:26,228 root  INFO     load_model: True
2017-06-02 14:13:26,228 root  INFO     output_dir: results
2017-06-02 14:13:26,228 root  INFO     steps_per_checkpoint: 2000
2017-06-02 14:13:26,228 root  INFO     batch_size: 64
2017-06-02 14:13:26,228 root  INFO     num_epoch: 30
2017-06-02 14:13:26,228 root  INFO     learning_rate: 1
2017-06-02 14:13:26,229 root  INFO     reg_val: 0
2017-06-02 14:13:26,229 root  INFO     max_gradient_norm: 5.000000
2017-06-02 14:13:26,229 root  INFO     clip_gradients: True
2017-06-02 14:13:26,229 root  INFO     valid_target_length inf
2017-06-02 14:13:26,229 root  INFO     target_vocab_size: 39
2017-06-02 14:13:26,229 root  INFO     target_embedding_size: 10.000000
2017-06-02 14:13:26,229 root  INFO     attn_num_hidden: 256
2017-06-02 14:13:26,229 root  INFO     attn_num_layers: 2
2017-06-02 14:13:26,229 root  INFO     visualize: True
2017-06-02 14:13:26,229 root  INFO     buckets
2017-06-02 14:13:26,229 root  INFO     [(16, 11), (27, 17), (35, 19), (64, 22), (80, 32)]
2017-06-02 14:13:26,229 root  INFO     ues GRU in the decoder.
2017-06-02 14:14:34,819 root  INFO     Reading model parameters from model_01_16/translate.ckpt-2000
Train: :   0%|          | 0/156 [00:00<?, ?it/s]2017-06-02 14:14:58,029 root  INFO     Generating first batch)
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.40GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=1325 evicted_count=1000 eviction_rate=0.754717 and unsatisfied allocation rate=0.913892
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.40GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.34GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.07GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2017-06-02 14:15:07,473 root  INFO     step 0.000000 - time: 6.759811, loss: 0.173485, perplexity: 1.189443, precision: 0.781250, batch_len: 96.000000
Train, loss=0.17348540:   1%|          | 1/156 [00:09<24:25,  9.45s/it]2017-06-02 14:15:12,266 root  INFO     step 1.000000 - time: 4.074553, loss: 0.536789, perplexity: 1.710505, precision: 0.437500, batch_len: 90.000000
Train, loss=0.53678876:   1%|1         | 2/156 [00:14<20:40,  8.05s/it]2017-06-02 14:15:16,147 root  INFO     step 2.000000 - time: 3.871421, loss: 0.589942, perplexity: 1.803885, precision: 0.234375, batch_len: 92.000000
Train, loss=0.58994240:   2%|1         | 3/156 [00:18<17:20,  6.80s/it]2017-06-02 14:15:20,573 root  INFO     step 3.000000 - time: 4.388609, loss: 1.012197, perplexity: 2.751640, precision: 0.125000, batch_len: 108.000000
Train, loss=1.01219702:   3%|2         | 4/156 [00:22<15:25,  6.09s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1014 evicted_count=1000 eviction_rate=0.986193 and unsatisfied allocation rate=0
2017-06-02 14:15:21,864 root  INFO     step 4.000000 - time: 1.280080, loss: 0.432633, perplexity: 1.541310, precision: 0.531250, batch_len: 97.000000
Train, loss=0.43263286:   3%|3         | 5/156 [00:23<11:42,  4.65s/it]2017-06-02 14:15:23,370 root  INFO     step 5.000000 - time: 1.380143, loss: 0.453575, perplexity: 1.573929, precision: 0.531250, batch_len: 93.000000
Train, loss=0.45357525:   4%|3         | 6/156 [00:25<09:15,  3.71s/it]2017-06-02 14:15:27,494 root  INFO     step 6.000000 - time: 4.094442, loss: 0.227806, perplexity: 1.255842, precision: 0.656250, batch_len: 101.000000
Train, loss=0.22780609:   4%|4         | 7/156 [00:29<09:30,  3.83s/it]2017-06-02 14:15:31,439 root  INFO     step 7.000000 - time: 3.923035, loss: 0.209662, perplexity: 1.233261, precision: 0.687500, batch_len: 106.000000
Train, loss=0.20966171:   5%|5         | 8/156 [00:33<09:32,  3.87s/it]2017-06-02 14:15:37,274 root  INFO     step 8.000000 - time: 5.800693, loss: 0.251545, perplexity: 1.286010, precision: 0.578125, batch_len: 128.000000
Train, loss=0.25154465:   6%|5         | 9/156 [00:39<10:55,  4.46s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=3479 evicted_count=3000 eviction_rate=0.862317 and unsatisfied allocation rate=0.824723
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 233 to 256
2017-06-02 14:15:41,816 root  INFO     step 9.000000 - time: 4.394285, loss: 0.292489, perplexity: 1.339759, precision: 0.562500, batch_len: 112.000000
Train, loss=0.29248947:   6%|6         | 10/156 [00:43<10:54,  4.48s/it]2017-06-02 14:15:43,285 root  INFO     step 10.000000 - time: 1.387025, loss: 0.326556, perplexity: 1.386186, precision: 0.578125, batch_len: 113.000000
Train, loss=0.32655579:   7%|7         | 11/156 [00:45<08:38,  3.58s/it]2017-06-02 14:15:44,788 root  INFO     step 11.000000 - time: 1.489285, loss: 0.278330, perplexity: 1.320922, precision: 0.578125, batch_len: 102.000000
Train, loss=0.27833006:   8%|7         | 12/156 [00:46<07:05,  2.96s/it]2017-06-02 14:15:46,660 root  INFO     step 12.000000 - time: 1.849754, loss: 0.214154, perplexity: 1.238813, precision: 0.671875, batch_len: 110.000000
Train, loss=0.21415359:   8%|8         | 13/156 [00:48<06:16,  2.63s/it]2017-06-02 14:15:50,523 root  INFO     step 13.000000 - time: 3.791420, loss: 0.319310, perplexity: 1.376178, precision: 0.718750, batch_len: 86.000000
Train, loss=0.31931007:   9%|8         | 14/156 [00:52<07:06,  3.00s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=2630 evicted_count=2000 eviction_rate=0.760456 and unsatisfied allocation rate=0.771527
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 372 to 409
2017-06-02 14:15:52,091 root  INFO     step 14.000000 - time: 1.549395, loss: 0.272879, perplexity: 1.313741, precision: 0.640625, batch_len: 105.000000
Train, loss=0.27287862:  10%|9         | 15/156 [00:54<06:02,  2.57s/it]2017-06-02 14:15:53,728 root  INFO     step 15.000000 - time: 1.602872, loss: 0.166017, perplexity: 1.180593, precision: 0.671875, batch_len: 104.000000
Train, loss=0.16601667:  10%|#         | 16/156 [00:55<05:20,  2.29s/it]2017-06-02 14:15:57,492 root  INFO     step 16.000000 - time: 3.757687, loss: 0.342540, perplexity: 1.408520, precision: 0.593750, batch_len: 81.000000
Train, loss=0.34253985:  11%|#         | 17/156 [00:59<06:19,  2.73s/it]2017-06-02 14:15:58,811 root  INFO     step 17.000000 - time: 1.285465, loss: 0.380413, perplexity: 1.462888, precision: 0.625000, batch_len: 111.000000
Train, loss=0.38041282:  12%|#1        | 18/156 [01:00<05:18,  2.31s/it]2017-06-02 14:16:00,288 root  INFO     step 18.000000 - time: 1.441947, loss: 0.290307, perplexity: 1.336838, precision: 0.656250, batch_len: 100.000000
Train, loss=0.29030716:  12%|#2        | 19/156 [01:02<04:42,  2.06s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3133 get requests, put_count=2899 evicted_count=2000 eviction_rate=0.689893 and unsatisfied allocation rate=0.73029
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 596 to 655
2017-06-02 14:16:04,696 root  INFO     step 19.000000 - time: 4.291850, loss: 0.205986, perplexity: 1.228737, precision: 0.593750, batch_len: 124.000000
Train, loss=0.20598643:  13%|#2        | 20/156 [01:06<06:15,  2.76s/it]2017-06-02 14:16:06,172 root  INFO     step 20.000000 - time: 1.429009, loss: 0.239846, perplexity: 1.271053, precision: 0.687500, batch_len: 88.000000
Train, loss=0.23984598:  13%|#3        | 21/156 [01:08<05:20,  2.38s/it]2017-06-02 14:16:10,843 root  INFO     step 21.000000 - time: 4.621885, loss: 0.242740, perplexity: 1.274737, precision: 0.671875, batch_len: 121.000000
Train, loss=0.24273987:  14%|#4        | 22/156 [01:12<06:51,  3.07s/it]2017-06-02 14:16:12,449 root  INFO     step 22.000000 - time: 1.579488, loss: 0.204969, perplexity: 1.227487, precision: 0.671875, batch_len: 120.000000
Train, loss=0.20496872:  15%|#4        | 23/156 [01:14<05:49,  2.63s/it]2017-06-02 14:16:14,312 root  INFO     step 23.000000 - time: 1.841194, loss: 0.423346, perplexity: 1.527062, precision: 0.546875, batch_len: 114.000000
Train, loss=0.42334560:  15%|#5        | 24/156 [01:16<05:16,  2.40s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1095 evicted_count=1000 eviction_rate=0.913242 and unsatisfied allocation rate=0
2017-06-02 14:16:18,859 root  INFO     step 24.000000 - time: 4.386406, loss: 0.106706, perplexity: 1.112607, precision: 0.765625, batch_len: 117.000000
Train, loss=0.10670626:  16%|#6        | 25/156 [01:20<06:38,  3.04s/it]2017-06-02 14:16:19,999 root  INFO     step 25.000000 - time: 1.098353, loss: 0.345600, perplexity: 1.412837, precision: 0.750000, batch_len: 80.000000
Train, loss=0.34559959:  17%|#6        | 26/156 [01:21<05:21,  2.47s/it]2017-06-02 14:16:21,554 root  INFO     step 26.000000 - time: 1.526439, loss: 0.334247, perplexity: 1.396887, precision: 0.562500, batch_len: 109.000000
Train, loss=0.33424652:  17%|#7        | 27/156 [01:23<04:43,  2.20s/it]2017-06-02 14:16:23,016 root  INFO     step 27.000000 - time: 1.434870, loss: 0.172210, perplexity: 1.187928, precision: 0.687500, batch_len: 103.000000
Train, loss=0.17221035:  18%|#7        | 28/156 [01:24<04:12,  1.98s/it]2017-06-02 14:16:24,345 root  INFO     step 28.000000 - time: 1.210967, loss: 0.365533, perplexity: 1.441282, precision: 0.609375, batch_len: 91.000000
Train, loss=0.36553270:  19%|#8        | 29/156 [01:26<03:46,  1.78s/it]2017-06-02 14:16:25,563 root  INFO     step 29.000000 - time: 1.167399, loss: 0.261900, perplexity: 1.299396, precision: 0.750000, batch_len: 89.000000
Train, loss=0.26189959:  19%|#9        | 30/156 [01:27<03:23,  1.61s/it]2017-06-02 14:16:26,967 root  INFO     step 30.000000 - time: 1.371960, loss: 0.185533, perplexity: 1.203860, precision: 0.718750, batch_len: 115.000000
Train, loss=0.18553305:  20%|#9        | 31/156 [01:28<03:13,  1.55s/it]2017-06-02 14:16:28,741 root  INFO     step 31.000000 - time: 1.698084, loss: 0.245376, perplexity: 1.278102, precision: 0.515625, batch_len: 116.000000
Train, loss=0.24537586:  21%|##        | 32/156 [01:30<03:20,  1.62s/it]2017-06-02 14:16:30,240 root  INFO     step 32.000000 - time: 1.339154, loss: 0.209476, perplexity: 1.233032, precision: 0.625000, batch_len: 87.000000
Train, loss=0.20947617:  21%|##1       | 33/156 [01:32<03:14,  1.58s/it]2017-06-02 14:16:34,088 root  INFO     step 33.000000 - time: 3.838222, loss: 0.306152, perplexity: 1.358188, precision: 0.656250, batch_len: 79.000000
Train, loss=0.30615181:  22%|##1       | 34/156 [01:36<04:35,  2.26s/it]2017-06-02 14:16:38,658 root  INFO     step 34.000000 - time: 4.439148, loss: 0.238966, perplexity: 1.269935, precision: 0.609375, batch_len: 137.000000
Train, loss=0.23896579:  22%|##2       | 35/156 [01:40<05:57,  2.95s/it]2017-06-02 14:16:40,263 root  INFO     step 35.000000 - time: 1.505257, loss: 0.183354, perplexity: 1.201239, precision: 0.812500, batch_len: 94.000000
Train, loss=0.18335380:  23%|##3       | 36/156 [01:42<05:05,  2.55s/it]2017-06-02 14:16:41,823 root  INFO     step 36.000000 - time: 1.519454, loss: 0.267688, perplexity: 1.306939, precision: 0.718750, batch_len: 83.000000
Train, loss=0.26768810:  24%|##3       | 37/156 [01:43<04:28,  2.25s/it]2017-06-02 14:16:43,422 root  INFO     step 37.000000 - time: 1.564834, loss: 0.222680, perplexity: 1.249421, precision: 0.609375, batch_len: 129.000000
Train, loss=0.22268032:  24%|##4       | 38/156 [01:45<04:02,  2.06s/it]2017-06-02 14:16:45,292 root  INFO     step 38.000000 - time: 1.729122, loss: 0.201426, perplexity: 1.223146, precision: 0.750000, batch_len: 125.000000
Train, loss=0.20142606:  25%|##5       | 39/156 [01:47<03:54,  2.00s/it]2017-06-02 14:16:46,774 root  INFO     step 39.000000 - time: 1.463444, loss: 0.283896, perplexity: 1.328295, precision: 0.687500, batch_len: 85.000000
Train, loss=0.28389639:  26%|##5       | 40/156 [01:48<03:34,  1.85s/it]2017-06-02 14:16:48,647 root  INFO     step 40.000000 - time: 1.579009, loss: 0.229514, perplexity: 1.257989, precision: 0.734375, batch_len: 99.000000
Train, loss=0.22951445:  26%|##6       | 41/156 [01:50<03:33,  1.85s/it]2017-06-02 14:16:53,508 root  INFO     step 41.000000 - time: 4.732066, loss: 0.195360, perplexity: 1.215748, precision: 0.609375, batch_len: 133.000000
Train, loss=0.19535974:  27%|##6       | 42/156 [01:55<05:14,  2.76s/it]2017-06-02 14:16:54,945 root  INFO     step 42.000000 - time: 1.287934, loss: 0.186394, perplexity: 1.204897, precision: 0.718750, batch_len: 98.000000
Train, loss=0.18639369:  28%|##7       | 43/156 [01:56<04:26,  2.36s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 17238 get requests, put_count=17084 evicted_count=1000 eviction_rate=0.0585343 and unsatisfied allocation rate=0.0799977
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 2478 to 2725
2017-06-02 14:16:56,679 root  INFO     step 43.000000 - time: 1.718458, loss: 0.233752, perplexity: 1.263331, precision: 0.640625, batch_len: 135.000000
Train, loss=0.23375219:  28%|##8       | 44/156 [01:58<04:03,  2.17s/it]2017-06-02 14:16:58,945 root  INFO     step 44.000000 - time: 2.238833, loss: 0.221505, perplexity: 1.247954, precision: 0.687500, batch_len: 123.000000
Train, loss=0.22150524:  29%|##8       | 45/156 [02:00<04:04,  2.20s/it]2017-06-02 14:17:00,861 root  INFO     step 45.000000 - time: 1.889630, loss: 0.186207, perplexity: 1.204671, precision: 0.718750, batch_len: 119.000000
Train, loss=0.18620670:  29%|##9       | 46/156 [02:02<03:52,  2.11s/it]2017-06-02 14:17:02,034 root  INFO     step 46.000000 - time: 1.153494, loss: 0.204213, perplexity: 1.226559, precision: 0.765625, batch_len: 82.000000
Train, loss=0.20421281:  30%|###       | 47/156 [02:04<03:19,  1.83s/it]2017-06-02 14:17:04,022 root  INFO     step 47.000000 - time: 1.907206, loss: 0.241153, perplexity: 1.272715, precision: 0.562500, batch_len: 126.000000
Train, loss=0.24115270:  31%|###       | 48/156 [02:05<03:22,  1.88s/it]2017-06-02 14:17:05,562 root  INFO     step 48.000000 - time: 1.525573, loss: 0.469646, perplexity: 1.599427, precision: 0.390625, batch_len: 107.000000
Train, loss=0.46964574:  31%|###1      | 49/156 [02:07<03:10,  1.78s/it]2017-06-02 14:17:07,261 root  INFO     step 49.000000 - time: 1.642062, loss: 0.807639, perplexity: 2.242607, precision: 0.296875, batch_len: 122.000000
Train, loss=0.80763888:  32%|###2      | 50/156 [02:09<03:05,  1.75s/it]2017-06-02 14:17:08,268 root  INFO     step 50.000000 - time: 1.002810, loss: 0.448549, perplexity: 1.566038, precision: 0.437500, batch_len: 96.000000
Train, loss=0.44854867:  33%|###2      | 51/156 [02:10<02:40,  1.53s/it]2017-06-02 14:17:09,659 root  INFO     step 51.000000 - time: 1.358771, loss: 0.441755, perplexity: 1.555435, precision: 0.468750, batch_len: 136.000000
Train, loss=0.44175497:  33%|###3      | 52/156 [02:11<02:34,  1.49s/it]2017-06-02 14:17:11,411 root  INFO     step 52.000000 - time: 1.720640, loss: 0.366953, perplexity: 1.443330, precision: 0.437500, batch_len: 118.000000
Train, loss=0.36695305:  34%|###3      | 53/156 [02:13<02:41,  1.57s/it]2017-06-02 14:17:16,078 root  INFO     step 53.000000 - time: 4.467543, loss: 0.308824, perplexity: 1.361822, precision: 0.468750, batch_len: 144.000000
Train, loss=0.30882373:  35%|###4      | 54/156 [02:18<04:14,  2.50s/it]2017-06-02 14:17:17,285 root  INFO     step 54.000000 - time: 1.099639, loss: 0.232515, perplexity: 1.261769, precision: 0.671875, batch_len: 78.000000
Train, loss=0.23251465:  35%|###5      | 55/156 [02:19<03:33,  2.11s/it]2017-06-02 14:17:18,784 root  INFO     step 55.000000 - time: 1.466173, loss: 0.362029, perplexity: 1.436240, precision: 0.593750, batch_len: 84.000000
Train, loss=0.36202866:  36%|###5      | 56/156 [02:20<03:12,  1.93s/it]2017-06-02 14:17:20,613 root  INFO     step 56.000000 - time: 1.753720, loss: 0.397865, perplexity: 1.488643, precision: 0.265625, batch_len: 130.000000
Train, loss=0.39786524:  37%|###6      | 57/156 [02:22<03:07,  1.90s/it]2017-06-02 14:17:25,217 root  INFO     step 57.000000 - time: 4.228583, loss: 0.421698, perplexity: 1.524548, precision: 0.343750, batch_len: 141.000000
Train, loss=0.42169818:  37%|###7      | 58/156 [02:27<04:25,  2.71s/it]2017-06-02 14:17:27,295 root  INFO     step 58.000000 - time: 1.998442, loss: 0.232951, perplexity: 1.262320, precision: 0.484375, batch_len: 138.000000
Train, loss=0.23295119:  38%|###7      | 59/156 [02:29<04:04,  2.52s/it]2017-06-02 14:17:28,927 root  INFO     step 59.000000 - time: 1.544673, loss: 0.279100, perplexity: 1.321939, precision: 0.656250, batch_len: 76.000000
Train, loss=0.27909991:  38%|###8      | 60/156 [02:30<03:36,  2.25s/it]2017-06-02 14:17:32,585 root  INFO     step 60.000000 - time: 3.642160, loss: 0.096649, perplexity: 1.101474, precision: 0.859375, batch_len: 74.000000
Train, loss=0.09664910:  39%|###9      | 61/156 [02:34<04:14,  2.67s/it]2017-06-02 14:17:34,303 root  INFO     step 61.000000 - time: 1.712250, loss: 0.208819, perplexity: 1.232222, precision: 0.578125, batch_len: 134.000000
Train, loss=0.20881894:  40%|###9      | 62/156 [02:36<03:44,  2.39s/it]2017-06-02 14:17:36,037 root  INFO     step 62.000000 - time: 1.711028, loss: 0.206523, perplexity: 1.229395, precision: 0.703125, batch_len: 132.000000
Train, loss=0.20652255:  40%|####      | 63/156 [02:38<03:23,  2.19s/it]2017-06-02 14:17:43,552 root  INFO     step 63.000000 - time: 7.411062, loss: 0.260195, perplexity: 1.297183, precision: 0.500000, batch_len: 152.000000
Train, loss=0.26019502:  41%|####1     | 64/156 [02:45<05:48,  3.79s/it]2017-06-02 14:17:45,377 root  INFO     step 64.000000 - time: 1.706065, loss: 0.482715, perplexity: 1.620468, precision: 0.265625, batch_len: 142.000000
Train, loss=0.48271519:  42%|####1     | 65/156 [02:47<04:51,  3.20s/it]2017-06-02 14:17:49,411 root  INFO     step 65.000000 - time: 3.866722, loss: 0.184223, perplexity: 1.202284, precision: 0.781250, batch_len: 71.000000
Train, loss=0.18422340:  42%|####2     | 66/156 [02:51<05:10,  3.45s/it]2017-06-02 14:17:51,002 root  INFO     step 66.000000 - time: 1.576671, loss: 0.402602, perplexity: 1.495712, precision: 0.437500, batch_len: 139.000000
Train, loss=0.40260234:  43%|####2     | 67/156 [02:52<04:17,  2.89s/it]2017-06-02 14:17:52,650 root  INFO     step 67.000000 - time: 1.639602, loss: 0.171921, perplexity: 1.187585, precision: 0.812500, batch_len: 72.000000
Train, loss=0.17192143:  44%|####3     | 68/156 [02:54<03:41,  2.52s/it]2017-06-02 14:17:54,467 root  INFO     step 68.000000 - time: 1.770679, loss: 0.174181, perplexity: 1.190271, precision: 0.671875, batch_len: 131.000000
Train, loss=0.17418088:  44%|####4     | 69/156 [02:56<03:20,  2.31s/it]2017-06-02 14:17:59,320 root  INFO     step 69.000000 - time: 4.711067, loss: 0.251602, perplexity: 1.286084, precision: 0.593750, batch_len: 150.000000
Train, loss=0.25160220:  45%|####4     | 70/156 [03:01<04:24,  3.07s/it]2017-06-02 14:18:00,918 root  INFO     step 70.000000 - time: 1.473172, loss: 0.126830, perplexity: 1.135224, precision: 0.781250, batch_len: 77.000000
Train, loss=0.12683001:  46%|####5     | 71/156 [03:02<03:43,  2.63s/it]2017-06-02 14:18:01,105 root  INFO     Generating first batch)
2017-06-02 14:18:05,059 root  INFO     step 71.000000 - time: 1.141678, loss: 0.143763, perplexity: 1.154611, precision: 0.796875, batch_len: 96.000000
Train, loss=0.14376330:  46%|####6     | 72/156 [03:07<04:18,  3.08s/it]2017-06-02 14:18:06,460 root  INFO     step 72.000000 - time: 1.083862, loss: 0.260684, perplexity: 1.297818, precision: 0.656250, batch_len: 105.000000
Train, loss=0.26068401:  47%|####6     | 73/156 [03:08<03:34,  2.58s/it]2017-06-02 14:18:07,564 root  INFO     step 73.000000 - time: 1.015050, loss: 0.308834, perplexity: 1.361837, precision: 0.640625, batch_len: 93.000000
Train, loss=0.30883440:  47%|####7     | 74/156 [03:09<02:55,  2.14s/it]2017-06-02 14:18:08,877 root  INFO     step 74.000000 - time: 1.048311, loss: 0.359662, perplexity: 1.432844, precision: 0.406250, batch_len: 110.000000
Train, loss=0.35966158:  48%|####8     | 75/156 [03:10<02:33,  1.89s/it]2017-06-02 14:18:09,832 root  INFO     step 75.000000 - time: 0.877139, loss: 0.263048, perplexity: 1.300890, precision: 0.671875, batch_len: 89.000000
Train, loss=0.26304841:  49%|####8     | 76/156 [03:11<02:08,  1.61s/it]2017-06-02 14:18:11,139 root  INFO     step 76.000000 - time: 1.286310, loss: 0.308270, perplexity: 1.361068, precision: 0.656250, batch_len: 106.000000
Train, loss=0.30826977:  49%|####9     | 77/156 [03:13<01:59,  1.52s/it]2017-06-02 14:18:12,439 root  INFO     step 77.000000 - time: 1.231683, loss: 0.199696, perplexity: 1.221032, precision: 0.703125, batch_len: 100.000000
Train, loss=0.19969603:  50%|#####     | 78/156 [03:14<01:53,  1.45s/it]2017-06-02 14:18:13,583 root  INFO     step 78.000000 - time: 1.112217, loss: 0.177404, perplexity: 1.194114, precision: 0.750000, batch_len: 113.000000
Train, loss=0.17740424:  51%|#####     | 79/156 [03:15<01:44,  1.36s/it]2017-06-02 14:18:14,967 root  INFO     step 79.000000 - time: 1.312423, loss: 0.227241, perplexity: 1.255132, precision: 0.718750, batch_len: 112.000000
Train, loss=0.22724095:  51%|#####1    | 80/156 [03:16<01:43,  1.37s/it]2017-06-02 14:18:16,023 root  INFO     step 80.000000 - time: 0.981870, loss: 0.209040, perplexity: 1.232494, precision: 0.812500, batch_len: 101.000000
Train, loss=0.20903967:  52%|#####1    | 81/156 [03:17<01:35,  1.27s/it]2017-06-02 14:18:17,474 root  INFO     step 81.000000 - time: 1.422087, loss: 0.291958, perplexity: 1.339047, precision: 0.593750, batch_len: 120.000000
Train, loss=0.29195815:  53%|#####2    | 82/156 [03:19<01:38,  1.33s/it]2017-06-02 14:18:18,767 root  INFO     step 82.000000 - time: 1.261318, loss: 0.201151, perplexity: 1.222809, precision: 0.734375, batch_len: 92.000000
Train, loss=0.20115101:  53%|#####3    | 83/156 [03:20<01:36,  1.32s/it]2017-06-02 14:18:19,929 root  INFO     step 83.000000 - time: 1.115427, loss: 0.275796, perplexity: 1.317579, precision: 0.640625, batch_len: 108.000000
Train, loss=0.27579594:  54%|#####3    | 84/156 [03:21<01:31,  1.27s/it]2017-06-02 14:18:21,063 root  INFO     step 84.000000 - time: 0.986593, loss: 0.233737, perplexity: 1.263312, precision: 0.625000, batch_len: 102.000000
Train, loss=0.23373665:  54%|#####4    | 85/156 [03:23<01:27,  1.23s/it]2017-06-02 14:18:22,132 root  INFO     step 85.000000 - time: 1.021150, loss: 0.285129, perplexity: 1.329933, precision: 0.640625, batch_len: 104.000000
Train, loss=0.28512874:  55%|#####5    | 86/156 [03:24<01:22,  1.18s/it]2017-06-02 14:18:23,889 root  INFO     step 86.000000 - time: 1.538890, loss: 0.383372, perplexity: 1.467223, precision: 0.484375, batch_len: 90.000000
Train, loss=0.38337159:  56%|#####5    | 87/156 [03:25<01:33,  1.35s/it]2017-06-02 14:18:25,176 root  INFO     step 87.000000 - time: 1.228195, loss: 0.434214, perplexity: 1.543749, precision: 0.484375, batch_len: 88.000000
Train, loss=0.43421394:  56%|#####6    | 88/156 [03:27<01:30,  1.33s/it]2017-06-02 14:18:26,275 root  INFO     step 88.000000 - time: 1.071536, loss: 0.407906, perplexity: 1.503665, precision: 0.515625, batch_len: 97.000000
Train, loss=0.40790576:  57%|#####7    | 89/156 [03:28<01:24,  1.26s/it]2017-06-02 14:18:27,802 root  INFO     step 89.000000 - time: 1.495078, loss: 0.225751, perplexity: 1.253264, precision: 0.546875, batch_len: 128.000000
Train, loss=0.22575134:  58%|#####7    | 90/156 [03:29<01:28,  1.34s/it]2017-06-02 14:18:28,847 root  INFO     step 90.000000 - time: 1.036451, loss: 0.326911, perplexity: 1.386678, precision: 0.703125, batch_len: 91.000000
Train, loss=0.32691073:  58%|#####8    | 91/156 [03:30<01:21,  1.25s/it]2017-06-02 14:18:29,884 root  INFO     step 91.000000 - time: 1.021135, loss: 0.202760, perplexity: 1.224778, precision: 0.750000, batch_len: 85.000000
Train, loss=0.20275968:  59%|#####8    | 92/156 [03:31<01:16,  1.19s/it]2017-06-02 14:18:31,204 root  INFO     step 92.000000 - time: 1.254404, loss: 0.176454, perplexity: 1.192980, precision: 0.750000, batch_len: 114.000000
Train, loss=0.17645411:  60%|#####9    | 93/156 [03:33<01:17,  1.23s/it]2017-06-02 14:18:32,447 root  INFO     step 93.000000 - time: 1.132969, loss: 0.423076, perplexity: 1.526650, precision: 0.640625, batch_len: 111.000000
Train, loss=0.42307603:  60%|######    | 94/156 [03:34<01:16,  1.23s/it]2017-06-02 14:18:34,087 root  INFO     step 94.000000 - time: 1.590164, loss: 0.231205, perplexity: 1.260117, precision: 0.687500, batch_len: 124.000000
Train, loss=0.23120487:  61%|######    | 95/156 [03:36<01:22,  1.35s/it]2017-06-02 14:18:35,052 root  INFO     step 95.000000 - time: 0.960767, loss: 0.375236, perplexity: 1.455335, precision: 0.640625, batch_len: 86.000000
Train, loss=0.37523615:  62%|######1   | 96/156 [03:37<01:14,  1.24s/it]2017-06-02 14:18:36,373 root  INFO     step 96.000000 - time: 1.310057, loss: 0.230261, perplexity: 1.258928, precision: 0.671875, batch_len: 125.000000
Train, loss=0.23026063:  62%|######2   | 97/156 [03:38<01:14,  1.26s/it]2017-06-02 14:18:37,353 root  INFO     step 97.000000 - time: 0.905357, loss: 0.200995, perplexity: 1.222619, precision: 0.687500, batch_len: 81.000000
Train, loss=0.20099524:  63%|######2   | 98/156 [03:39<01:08,  1.18s/it]2017-06-02 14:18:38,399 root  INFO     step 98.000000 - time: 1.033898, loss: 0.298909, perplexity: 1.348387, precision: 0.625000, batch_len: 107.000000
Train, loss=0.29890919:  63%|######3   | 99/156 [03:40<01:04,  1.14s/it]2017-06-02 14:18:39,648 root  INFO     step 99.000000 - time: 1.216264, loss: 0.393140, perplexity: 1.481625, precision: 0.562500, batch_len: 109.000000
Train, loss=0.39313978:  64%|######4   | 100/156 [03:41<01:05,  1.17s/it]2017-06-02 14:18:41,414 root  INFO     step 100.000000 - time: 1.520252, loss: 0.627133, perplexity: 1.872235, precision: 0.421875, batch_len: 121.000000
Train, loss=0.62713301:  65%|######4   | 101/156 [03:43<01:14,  1.35s/it]2017-06-02 14:18:42,530 root  INFO     step 101.000000 - time: 1.111110, loss: 0.723145, perplexity: 2.060906, precision: 0.328125, batch_len: 115.000000
Train, loss=0.72314548:  65%|######5   | 102/156 [03:44<01:09,  1.28s/it]2017-06-02 14:18:43,945 root  INFO     step 102.000000 - time: 1.367508, loss: 0.419464, perplexity: 1.521146, precision: 0.453125, batch_len: 123.000000
Train, loss=0.41946387:  66%|######6   | 103/156 [03:45<01:09,  1.32s/it]2017-06-02 14:18:45,247 root  INFO     step 103.000000 - time: 1.272379, loss: 0.381184, perplexity: 1.464017, precision: 0.437500, batch_len: 117.000000
Train, loss=0.38118374:  67%|######6   | 104/156 [03:47<01:08,  1.31s/it]2017-06-02 14:18:46,447 root  INFO     step 104.000000 - time: 1.094536, loss: 0.352556, perplexity: 1.422699, precision: 0.609375, batch_len: 80.000000
Train, loss=0.35255611:  67%|######7   | 105/156 [03:48<01:05,  1.28s/it]2017-06-02 14:18:47,658 root  INFO     step 105.000000 - time: 1.204789, loss: 0.226369, perplexity: 1.254038, precision: 0.703125, batch_len: 103.000000
Train, loss=0.22636873:  68%|######7   | 106/156 [03:49<01:02,  1.26s/it]2017-06-02 14:18:48,705 root  INFO     step 106.000000 - time: 1.004984, loss: 0.335137, perplexity: 1.398132, precision: 0.609375, batch_len: 94.000000
Train, loss=0.33513707:  69%|######8   | 107/156 [03:50<00:58,  1.20s/it]2017-06-02 14:18:49,656 root  INFO     step 107.000000 - time: 0.948478, loss: 0.321204, perplexity: 1.378786, precision: 0.671875, batch_len: 79.000000
Train, loss=0.32120371:  69%|######9   | 108/156 [03:51<00:53,  1.12s/it]2017-06-02 14:18:51,074 root  INFO     step 108.000000 - time: 1.340920, loss: 0.228647, perplexity: 1.256899, precision: 0.531250, batch_len: 137.000000
Train, loss=0.22864726:  70%|######9   | 109/156 [03:53<00:56,  1.21s/it]2017-06-02 14:18:52,083 root  INFO     step 109.000000 - time: 0.969257, loss: 0.291250, perplexity: 1.338098, precision: 0.640625, batch_len: 84.000000
Train, loss=0.29124957:  71%|#######   | 110/156 [03:54<00:52,  1.15s/it]2017-06-02 14:18:53,662 root  INFO     step 110.000000 - time: 1.541828, loss: 0.148374, perplexity: 1.159947, precision: 0.781250, batch_len: 116.000000
Train, loss=0.14837405:  71%|#######1  | 111/156 [03:55<00:57,  1.28s/it]2017-06-02 14:18:54,921 root  INFO     step 111.000000 - time: 1.128497, loss: 0.140790, perplexity: 1.151182, precision: 0.765625, batch_len: 98.000000
Train, loss=0.14078963:  72%|#######1  | 112/156 [03:56<00:56,  1.27s/it]2017-06-02 14:18:56,053 root  INFO     step 112.000000 - time: 0.970571, loss: 0.173444, perplexity: 1.189394, precision: 0.750000, batch_len: 87.000000
Train, loss=0.17344373:  72%|#######2  | 113/156 [03:58<00:52,  1.23s/it]2017-06-02 14:18:57,139 root  INFO     step 113.000000 - time: 1.051982, loss: 0.127123, perplexity: 1.135557, precision: 0.828125, batch_len: 99.000000
Train, loss=0.12712312:  73%|#######3  | 114/156 [03:59<00:49,  1.19s/it]2017-06-02 14:18:58,481 root  INFO     step 114.000000 - time: 1.260429, loss: 0.165904, perplexity: 1.180459, precision: 0.765625, batch_len: 129.000000
Train, loss=0.16590369:  74%|#######3  | 115/156 [04:00<00:50,  1.23s/it]2017-06-02 14:18:59,619 root  INFO     step 115.000000 - time: 1.105789, loss: 0.281566, perplexity: 1.325203, precision: 0.656250, batch_len: 83.000000
Train, loss=0.28156602:  74%|#######4  | 116/156 [04:01<00:48,  1.20s/it]2017-06-02 14:19:01,167 root  INFO     step 116.000000 - time: 1.514936, loss: 0.196312, perplexity: 1.216906, precision: 0.671875, batch_len: 119.000000
Train, loss=0.19631194:  75%|#######5  | 117/156 [04:03<00:51,  1.31s/it]2017-06-02 14:19:02,667 root  INFO     step 117.000000 - time: 1.395769, loss: 0.168381, perplexity: 1.183388, precision: 0.687500, batch_len: 144.000000
Train, loss=0.16838136:  76%|#######5  | 118/156 [04:04<00:51,  1.37s/it]2017-06-02 14:19:04,275 root  INFO     step 118.000000 - time: 1.282257, loss: 0.208962, perplexity: 1.232399, precision: 0.703125, batch_len: 126.000000
Train, loss=0.20896234:  76%|#######6  | 119/156 [04:06<00:53,  1.44s/it]2017-06-02 14:19:05,642 root  INFO     step 119.000000 - time: 1.290437, loss: 0.216895, perplexity: 1.242214, precision: 0.625000, batch_len: 130.000000
Train, loss=0.21689537:  77%|#######6  | 120/156 [04:07<00:51,  1.42s/it]2017-06-02 14:19:07,319 root  INFO     step 120.000000 - time: 1.661344, loss: 0.145120, perplexity: 1.156178, precision: 0.734375, batch_len: 135.000000
Train, loss=0.14511985:  78%|#######7  | 121/156 [04:09<00:52,  1.49s/it]2017-06-02 14:19:08,692 root  INFO     step 121.000000 - time: 1.359454, loss: 0.147204, perplexity: 1.158590, precision: 0.703125, batch_len: 118.000000
Train, loss=0.14720374:  78%|#######8  | 122/156 [04:10<00:49,  1.46s/it]2017-06-02 14:19:10,207 root  INFO     step 122.000000 - time: 1.364404, loss: 0.141246, perplexity: 1.151708, precision: 0.718750, batch_len: 133.000000
Train, loss=0.14124608:  79%|#######8  | 123/156 [04:12<00:48,  1.48s/it]2017-06-02 14:19:11,226 root  INFO     step 123.000000 - time: 0.917609, loss: 0.127212, perplexity: 1.135658, precision: 0.812500, batch_len: 96.000000
Train, loss=0.12721200:  79%|#######9  | 124/156 [04:13<00:42,  1.34s/it]2017-06-02 14:19:12,977 root  INFO     step 124.000000 - time: 1.501878, loss: 0.149568, perplexity: 1.161332, precision: 0.765625, batch_len: 136.000000
Train, loss=0.14956766:  80%|########  | 125/156 [04:14<00:45,  1.46s/it]2017-06-02 14:19:14,221 root  INFO     step 125.000000 - time: 1.091994, loss: 0.143831, perplexity: 1.154689, precision: 0.843750, batch_len: 78.000000
Train, loss=0.14383063:  81%|########  | 126/156 [04:16<00:41,  1.40s/it]2017-06-02 14:19:15,425 root  INFO     step 126.000000 - time: 1.151822, loss: 0.113287, perplexity: 1.119953, precision: 0.921875, batch_len: 72.000000
Train, loss=0.11328685:  81%|########1 | 127/156 [04:17<00:38,  1.34s/it]2017-06-02 14:19:16,862 root  INFO     step 127.000000 - time: 1.364237, loss: 0.119585, perplexity: 1.127029, precision: 0.750000, batch_len: 122.000000
Train, loss=0.11958456:  82%|########2 | 128/156 [04:18<00:38,  1.37s/it]2017-06-02 14:19:18,170 root  INFO     step 128.000000 - time: 1.255513, loss: 0.162278, perplexity: 1.176187, precision: 0.671875, batch_len: 132.000000
Train, loss=0.16227767:  83%|########2 | 129/156 [04:20<00:36,  1.35s/it]2017-06-02 14:19:19,820 root  INFO     step 129.000000 - time: 1.602316, loss: 0.219228, perplexity: 1.245115, precision: 0.671875, batch_len: 139.000000
Train, loss=0.21922779:  83%|########3 | 130/156 [04:21<00:37,  1.44s/it]2017-06-02 14:19:21,084 root  INFO     step 130.000000 - time: 1.128558, loss: 0.281831, perplexity: 1.325555, precision: 0.687500, batch_len: 82.000000
Train, loss=0.28183115:  84%|########3 | 131/156 [04:23<00:34,  1.39s/it]2017-06-02 14:19:23,000 root  INFO     step 131.000000 - time: 1.863694, loss: 0.255196, perplexity: 1.290715, precision: 0.468750, batch_len: 152.000000
Train, loss=0.25519603:  85%|########4 | 132/156 [04:24<00:37,  1.55s/it]2017-06-02 14:19:24,436 root  INFO     step 132.000000 - time: 1.420071, loss: 0.193809, perplexity: 1.213864, precision: 0.609375, batch_len: 142.000000
Train, loss=0.19380870:  85%|########5 | 133/156 [04:26<00:34,  1.51s/it]2017-06-02 14:19:25,970 root  INFO     step 133.000000 - time: 1.481650, loss: 0.205298, perplexity: 1.227891, precision: 0.734375, batch_len: 141.000000
Train, loss=0.20529799:  86%|########5 | 134/156 [04:27<00:33,  1.52s/it]2017-06-02 14:19:27,479 root  INFO     step 134.000000 - time: 1.443004, loss: 0.114530, perplexity: 1.121346, precision: 0.812500, batch_len: 134.000000
Train, loss=0.11453003:  87%|########6 | 135/156 [04:29<00:31,  1.52s/it]2017-06-02 14:19:29,544 root  INFO     step 135.000000 - time: 1.999735, loss: 0.157801, perplexity: 1.170933, precision: 0.734375, batch_len: 150.000000
Train, loss=0.15780054:  87%|########7 | 136/156 [04:31<00:33,  1.68s/it]2017-06-02 14:19:31,175 root  INFO     step 136.000000 - time: 1.567790, loss: 0.125544, perplexity: 1.133765, precision: 0.765625, batch_len: 138.000000
Train, loss=0.12554379:  88%|########7 | 137/156 [04:33<00:31,  1.67s/it]2017-06-02 14:19:32,210 root  INFO     step 137.000000 - time: 0.991985, loss: 0.147850, perplexity: 1.159340, precision: 0.734375, batch_len: 74.000000
Train, loss=0.14785050:  88%|########8 | 138/156 [04:34<00:26,  1.48s/it]2017-06-02 14:19:33,379 root  INFO     step 138.000000 - time: 0.997079, loss: 0.149758, perplexity: 1.161553, precision: 0.734375, batch_len: 77.000000
Train, loss=0.14975788:  89%|########9 | 139/156 [04:35<00:23,  1.38s/it]2017-06-02 14:19:34,556 root  INFO     step 139.000000 - time: 1.167096, loss: 0.204775, perplexity: 1.227249, precision: 0.750000, batch_len: 76.000000
Train, loss=0.20477478:  90%|########9 | 140/156 [04:36<00:21,  1.32s/it]2017-06-02 14:19:35,875 root  INFO     step 140.000000 - time: 1.091888, loss: 0.094046, perplexity: 1.098611, precision: 0.890625, batch_len: 71.000000
Train, loss=0.09404637:  90%|######### | 141/156 [04:37<00:19,  1.32s/it]2017-06-02 14:19:37,303 root  INFO     step 141.000000 - time: 1.362800, loss: 0.116519, perplexity: 1.123579, precision: 0.765625, batch_len: 131.000000
Train, loss=0.11651906:  91%|#########1| 142/156 [04:39<00:18,  1.35s/it]2017-06-02 14:19:37,371 root  INFO     Generating first batch)
2017-06-02 14:19:41,449 root  INFO     step 142.000000 - time: 1.142288, loss: 0.212505, perplexity: 1.236773, precision: 0.625000, batch_len: 110.000000
Train, loss=0.21250525:  92%|#########1| 143/156 [04:43<00:28,  2.19s/it]2017-06-02 14:19:42,685 root  INFO     step 143.000000 - time: 0.997142, loss: 0.130902, perplexity: 1.139856, precision: 0.890625, batch_len: 96.000000
Train, loss=0.13090184:  92%|#########2| 144/156 [04:44<00:22,  1.90s/it]2017-06-02 14:19:43,845 root  INFO     step 144.000000 - time: 1.065037, loss: 0.223706, perplexity: 1.250703, precision: 0.718750, batch_len: 108.000000
Train, loss=0.22370559:  93%|#########2| 145/156 [04:45<00:18,  1.68s/it]2017-06-02 14:19:45,018 root  INFO     step 145.000000 - time: 0.993278, loss: 0.247863, perplexity: 1.281284, precision: 0.718750, batch_len: 113.000000
Train, loss=0.24786294:  94%|#########3| 146/156 [04:46<00:15,  1.53s/it]2017-06-02 14:19:46,581 root  INFO     step 146.000000 - time: 1.354057, loss: 0.285777, perplexity: 1.330795, precision: 0.734375, batch_len: 111.000000
Train, loss=0.28577685:  94%|#########4| 147/156 [04:48<00:13,  1.54s/it]2017-06-02 14:19:48,039 root  INFO     step 147.000000 - time: 1.445025, loss: 0.234691, perplexity: 1.264518, precision: 0.656250, batch_len: 120.000000
Train, loss=0.23469114:  95%|#########4| 148/156 [04:50<00:12,  1.51s/it]2017-06-02 14:19:49,090 root  INFO     step 148.000000 - time: 0.992755, loss: 0.221044, perplexity: 1.247378, precision: 0.671875, batch_len: 103.000000
Train, loss=0.22104391:  96%|#########5| 149/156 [04:51<00:09,  1.38s/it]2017-06-02 14:19:50,160 root  INFO     step 149.000000 - time: 1.039104, loss: 0.267396, perplexity: 1.306558, precision: 0.656250, batch_len: 93.000000
Train, loss=0.26739594:  96%|#########6| 150/156 [04:52<00:07,  1.28s/it]2017-06-02 14:19:51,198 root  INFO     step 150.000000 - time: 1.024316, loss: 0.235530, perplexity: 1.265580, precision: 0.718750, batch_len: 114.000000
Train, loss=0.23553029:  97%|#########6| 151/156 [04:53<00:06,  1.21s/it]2017-06-02 14:19:52,334 root  INFO     step 151.000000 - time: 1.116086, loss: 0.253543, perplexity: 1.288583, precision: 0.765625, batch_len: 92.000000
Train, loss=0.25354302:  97%|#########7| 152/156 [04:54<00:04,  1.19s/it]2017-06-02 14:19:53,695 root  INFO     step 152.000000 - time: 1.314152, loss: 0.271821, perplexity: 1.312352, precision: 0.687500, batch_len: 105.000000
Train, loss=0.27182120:  98%|#########8| 153/156 [04:55<00:03,  1.24s/it]2017-06-02 14:19:55,038 root  INFO     step 153.000000 - time: 1.113444, loss: 0.228122, perplexity: 1.256238, precision: 0.687500, batch_len: 104.000000
Train, loss=0.22812155:  99%|#########8| 154/156 [04:57<00:02,  1.27s/it]2017-06-02 14:19:56,013 root  INFO     step 154.000000 - time: 0.957265, loss: 0.207546, perplexity: 1.230655, precision: 0.718750, batch_len: 81.000000
Train, loss=0.20754634:  99%|#########9| 155/156 [04:57<00:01,  1.18s/it]2017-06-02 14:19:57,348 root  INFO     step 155.000000 - time: 1.291617, loss: 0.135609, perplexity: 1.145234, precision: 0.843750, batch_len: 117.000000
Train, loss=0.13560906: 100%|##########| 156/156 [04:59<00:00,  1.23s/it]2017-06-02 14:19:58,244 root  INFO     step 156.000000 - time: 0.889006, loss: 0.259555, perplexity: 1.296354, precision: 0.671875, batch_len: 88.000000
Train, loss=0.25955540: 157it [05:00,  1.13s/it]                         2017-06-02 14:19:59,782 root  INFO     step 157.000000 - time: 1.486164, loss: 0.237520, perplexity: 1.268100, precision: 0.718750, batch_len: 112.000000
Train, loss=0.23751974: 158it [05:01,  1.25s/it]2017-06-02 14:20:00,929 root  INFO     step 158.000000 - time: 1.139957, loss: 0.153316, perplexity: 1.165694, precision: 0.812500, batch_len: 89.000000
Train, loss=0.15331635: 159it [05:02,  1.22s/it]2017-06-02 14:20:01,905 root  INFO     step 159.000000 - time: 0.971366, loss: 0.223537, perplexity: 1.250492, precision: 0.812500, batch_len: 86.000000
Train, loss=0.22353737: 160it [05:03,  1.15s/it]2017-06-02 14:20:03,007 root  INFO     step 160.000000 - time: 1.093331, loss: 0.127042, perplexity: 1.135464, precision: 0.828125, batch_len: 101.000000
Train, loss=0.12704176: 161it [05:04,  1.13s/it]2017-06-02 14:20:04,033 root  INFO     step 161.000000 - time: 0.964348, loss: 0.185433, perplexity: 1.203740, precision: 0.687500, batch_len: 102.000000
Train, loss=0.18543339: 162it [05:06,  1.10s/it]2017-06-02 14:20:05,042 root  INFO     step 162.000000 - time: 0.957611, loss: 0.209505, perplexity: 1.233068, precision: 0.656250, batch_len: 97.000000
Train, loss=0.20950504: 163it [05:07,  1.07s/it]2017-06-02 14:20:06,630 root  INFO     step 163.000000 - time: 1.580186, loss: 0.222846, perplexity: 1.249628, precision: 0.687500, batch_len: 128.000000
Train, loss=0.22284563: 164it [05:08,  1.23s/it]2017-06-02 14:20:08,007 root  INFO     step 164.000000 - time: 1.353374, loss: 0.345615, perplexity: 1.412858, precision: 0.687500, batch_len: 91.000000
Train, loss=0.34561494: 165it [05:09,  1.27s/it]2017-06-02 14:20:09,203 root  INFO     step 165.000000 - time: 1.107314, loss: 0.258633, perplexity: 1.295158, precision: 0.546875, batch_len: 100.000000
Train, loss=0.25863290: 166it [05:11,  1.25s/it]2017-06-02 14:20:10,427 root  INFO     step 166.000000 - time: 1.040514, loss: 0.103260, perplexity: 1.108779, precision: 0.812500, batch_len: 99.000000
Train, loss=0.10325975: 167it [05:12,  1.24s/it]2017-06-02 14:20:11,559 root  INFO     step 167.000000 - time: 1.031656, loss: 0.246363, perplexity: 1.279363, precision: 0.703125, batch_len: 107.000000
Train, loss=0.24636257: 168it [05:13,  1.21s/it]2017-06-02 14:20:13,055 root  INFO     step 168.000000 - time: 1.424339, loss: 0.174656, perplexity: 1.190836, precision: 0.734375, batch_len: 125.000000
Train, loss=0.17465560: 169it [05:15,  1.29s/it]2017-06-02 14:20:14,829 root  INFO     step 169.000000 - time: 1.723748, loss: 0.297055, perplexity: 1.345890, precision: 0.703125, batch_len: 90.000000
Train, loss=0.29705530: 170it [05:16,  1.44s/it]2017-06-02 14:20:15,946 root  INFO     step 170.000000 - time: 1.081748, loss: 0.223591, perplexity: 1.250560, precision: 0.734375, batch_len: 106.000000
Train, loss=0.22359146: 171it [05:17,  1.34s/it]2017-06-02 14:20:17,297 root  INFO     step 171.000000 - time: 1.305377, loss: 0.118029, perplexity: 1.125277, precision: 0.796875, batch_len: 124.000000
Train, loss=0.11802950: 172it [05:19,  1.34s/it]2017-06-02 14:20:18,629 root  INFO     step 172.000000 - time: 1.305541, loss: 0.165865, perplexity: 1.180413, precision: 0.750000, batch_len: 123.000000
Train, loss=0.16586465: 173it [05:20,  1.34s/it]2017-06-02 14:20:19,865 root  INFO     step 173.000000 - time: 1.197090, loss: 0.104255, perplexity: 1.109884, precision: 0.843750, batch_len: 98.000000
Train, loss=0.10425547: 174it [05:21,  1.31s/it]2017-06-02 14:20:21,024 root  INFO     step 174.000000 - time: 1.089325, loss: 0.258086, perplexity: 1.294451, precision: 0.703125, batch_len: 79.000000
Train, loss=0.25808632: 175it [05:22,  1.26s/it]2017-06-02 14:20:22,426 root  INFO     step 175.000000 - time: 1.358731, loss: 0.127907, perplexity: 1.136447, precision: 0.734375, batch_len: 116.000000
Train, loss=0.12790698: 176it [05:24,  1.31s/it]2017-06-02 14:20:23,577 root  INFO     step 176.000000 - time: 1.116075, loss: 0.254844, perplexity: 1.290260, precision: 0.671875, batch_len: 109.000000
Train, loss=0.25484362: 177it [05:25,  1.26s/it]2017-06-02 14:20:24,878 root  INFO     step 177.000000 - time: 1.222733, loss: 0.339983, perplexity: 1.404923, precision: 0.593750, batch_len: 121.000000
Train, loss=0.33998263: 178it [05:26,  1.27s/it]2017-06-02 14:20:26,550 root  INFO     step 178.000000 - time: 1.567342, loss: 0.651337, perplexity: 1.918103, precision: 0.312500, batch_len: 137.000000
Train, loss=0.65133679: 179it [05:28,  1.39s/it]2017-06-02 14:20:27,962 root  INFO     step 179.000000 - time: 1.356745, loss: 1.190444, perplexity: 3.288543, precision: 0.062500, batch_len: 115.000000
Train, loss=1.19044447: 180it [05:29,  1.40s/it]2017-06-02 14:20:29,620 root  INFO     step 180.000000 - time: 1.645683, loss: 0.594928, perplexity: 1.812901, precision: 0.171875, batch_len: 129.000000
Train, loss=0.59492815: 181it [05:31,  1.48s/it]2017-06-02 14:20:30,649 root  INFO     step 181.000000 - time: 0.947258, loss: 0.340894, perplexity: 1.406204, precision: 0.515625, batch_len: 80.000000
Train, loss=0.34089416: 182it [05:32,  1.34s/it]2017-06-02 14:20:31,701 root  INFO     step 182.000000 - time: 1.021798, loss: 0.290144, perplexity: 1.336620, precision: 0.640625, batch_len: 83.000000
Train, loss=0.29014379: 183it [05:33,  1.25s/it]2017-06-02 14:20:32,920 root  INFO     step 183.000000 - time: 1.058642, loss: 0.341217, perplexity: 1.406658, precision: 0.578125, batch_len: 85.000000
Train, loss=0.34121662: 184it [05:34,  1.24s/it]2017-06-02 14:20:33,918 root  INFO     step 184.000000 - time: 0.937555, loss: 0.261302, perplexity: 1.298620, precision: 0.656250, batch_len: 87.000000
Train, loss=0.26130193: 185it [05:35,  1.17s/it]2017-06-02 14:20:35,688 root  INFO     step 185.000000 - time: 1.732220, loss: 0.380406, perplexity: 1.462879, precision: 0.640625, batch_len: 135.000000
Train, loss=0.38040635: 186it [05:37,  1.35s/it]2017-06-02 14:20:37,065 root  INFO     step 186.000000 - time: 1.344650, loss: 0.252943, perplexity: 1.287810, precision: 0.593750, batch_len: 118.000000
Train, loss=0.25294334: 187it [05:39,  1.36s/it]2017-06-02 14:20:38,439 root  INFO     step 187.000000 - time: 1.351700, loss: 0.206522, perplexity: 1.229395, precision: 0.718750, batch_len: 119.000000
Train, loss=0.20652196: 188it [05:40,  1.36s/it]2017-06-02 14:20:39,462 root  INFO     step 188.000000 - time: 0.968763, loss: 0.254400, perplexity: 1.289687, precision: 0.640625, batch_len: 94.000000
Train, loss=0.25439972: 189it [05:41,  1.26s/it]2017-06-02 14:20:40,851 root  INFO     step 189.000000 - time: 1.092912, loss: 0.243293, perplexity: 1.275442, precision: 0.765625, batch_len: 84.000000
Train, loss=0.24329317: 190it [05:42,  1.30s/it]2017-06-02 14:20:42,485 root  INFO     step 190.000000 - time: 1.531283, loss: 0.204140, perplexity: 1.226470, precision: 0.734375, batch_len: 126.000000
Train, loss=0.20413986: 191it [05:44,  1.40s/it]2017-06-02 14:20:44,010 root  INFO     step 191.000000 - time: 1.431144, loss: 0.164283, perplexity: 1.178548, precision: 0.765625, batch_len: 144.000000
Train, loss=0.16428313: 192it [05:45,  1.44s/it]2017-06-02 14:20:45,458 root  INFO     step 192.000000 - time: 1.373528, loss: 0.152315, perplexity: 1.164527, precision: 0.656250, batch_len: 130.000000
Train, loss=0.15231472: 193it [05:47,  1.44s/it]2017-06-02 14:20:47,633 root  INFO     step 193.000000 - time: 1.900374, loss: 0.156093, perplexity: 1.168935, precision: 0.671875, batch_len: 152.000000
Train, loss=0.15609269: 194it [05:49,  1.66s/it]2017-06-02 14:20:48,857 root  INFO     step 194.000000 - time: 1.149921, loss: 0.193415, perplexity: 1.213386, precision: 0.765625, batch_len: 82.000000
Train, loss=0.19341494: 195it [05:50,  1.53s/it]2017-06-02 14:20:50,402 root  INFO     step 195.000000 - time: 1.393789, loss: 0.106315, perplexity: 1.112172, precision: 0.781250, batch_len: 133.000000
Train, loss=0.10631451: 196it [05:52,  1.53s/it]2017-06-02 14:20:51,340 root  INFO     step 196.000000 - time: 0.922582, loss: 0.169962, perplexity: 1.185260, precision: 0.750000, batch_len: 78.000000
Train, loss=0.16996244: 197it [05:53,  1.36s/it]2017-06-02 14:20:52,745 root  INFO     step 197.000000 - time: 1.279707, loss: 0.116807, perplexity: 1.123903, precision: 0.796875, batch_len: 136.000000
Train, loss=0.11680719: 198it [05:54,  1.37s/it]2017-06-02 14:20:54,327 root  INFO     step 198.000000 - time: 1.524275, loss: 0.132880, perplexity: 1.142113, precision: 0.750000, batch_len: 134.000000
Train, loss=0.13288018: 199it [05:56,  1.43s/it]2017-06-02 14:20:55,604 root  INFO     step 199.000000 - time: 1.238006, loss: 0.085565, perplexity: 1.089332, precision: 0.843750, batch_len: 96.000000
Train, loss=0.08556486: 200it [05:57,  1.39s/it]2017-06-02 14:20:56,671 root  INFO     step 200.000000 - time: 0.993433, loss: 0.208219, perplexity: 1.231482, precision: 0.765625, batch_len: 76.000000
Train, loss=0.20821866: 201it [05:58,  1.29s/it]2017-06-02 14:20:58,066 root  INFO     step 201.000000 - time: 1.377954, loss: 0.190980, perplexity: 1.210435, precision: 0.750000, batch_len: 141.000000
Train, loss=0.19098008: 202it [06:00,  1.32s/it]2017-06-02 14:20:59,034 root  INFO     step 202.000000 - time: 0.898157, loss: 0.068086, perplexity: 1.070458, precision: 0.906250, batch_len: 74.000000
Train, loss=0.06808636: 203it [06:01,  1.22s/it]2017-06-02 14:21:00,447 root  INFO     step 203.000000 - time: 1.363108, loss: 0.164900, perplexity: 1.179275, precision: 0.812500, batch_len: 138.000000
Train, loss=0.16489975: 204it [06:02,  1.27s/it]2017-06-02 14:21:01,809 root  INFO     step 204.000000 - time: 1.324959, loss: 0.122687, perplexity: 1.130530, precision: 0.890625, batch_len: 72.000000
Train, loss=0.12268677: 205it [06:03,  1.30s/it]2017-06-02 14:21:03,356 root  INFO     step 205.000000 - time: 1.468046, loss: 0.110700, perplexity: 1.117059, precision: 0.875000, batch_len: 122.000000
Train, loss=0.11069964: 206it [06:05,  1.37s/it]2017-06-02 14:21:04,870 root  INFO     step 206.000000 - time: 1.409364, loss: 0.143856, perplexity: 1.154717, precision: 0.843750, batch_len: 139.000000
Train, loss=0.14385563: 207it [06:06,  1.42s/it]2017-06-02 14:21:06,221 root  INFO     step 207.000000 - time: 1.318156, loss: 0.138803, perplexity: 1.148897, precision: 0.828125, batch_len: 132.000000
Train, loss=0.13880265: 208it [06:08,  1.40s/it]2017-06-02 14:21:07,552 root  INFO     step 208.000000 - time: 1.113463, loss: 0.101529, perplexity: 1.106862, precision: 0.875000, batch_len: 77.000000
Train, loss=0.10152873: 209it [06:09,  1.38s/it]2017-06-02 14:21:09,287 root  INFO     step 209.000000 - time: 1.586182, loss: 0.376356, perplexity: 1.456966, precision: 0.515625, batch_len: 142.000000
Train, loss=0.37635612: 210it [06:11,  1.48s/it]2017-06-02 14:21:11,231 root  INFO     step 210.000000 - time: 1.921304, loss: 2.080499, perplexity: 8.008466, precision: 0.171875, batch_len: 150.000000
Train, loss=2.08049917: 211it [06:13,  1.62s/it]2017-06-02 14:21:12,244 root  INFO     step 211.000000 - time: 0.888311, loss: 0.134233, perplexity: 1.143660, precision: 0.828125, batch_len: 71.000000
Train, loss=0.13423333: 212it [06:14,  1.44s/it]2017-06-02 14:21:13,610 root  INFO     step 212.000000 - time: 1.290902, loss: 0.158166, perplexity: 1.171361, precision: 0.640625, batch_len: 131.000000
Train, loss=0.15816641: 213it [06:15,  1.42s/it]2017-06-02 14:21:13,810 root  INFO     Generating first batch)
2017-06-02 14:21:17,428 root  INFO     step 213.000000 - time: 0.968004, loss: 0.190439, perplexity: 1.209780, precision: 0.750000, batch_len: 96.000000
Train, loss=0.19043860: 214it [06:19,  2.14s/it]2017-06-02 14:21:19,130 root  INFO     step 214.000000 - time: 1.225870, loss: 0.139756, perplexity: 1.149994, precision: 0.734375, batch_len: 101.000000
Train, loss=0.13975646: 215it [06:21,  2.01s/it]2017-06-02 14:21:20,704 root  INFO     step 215.000000 - time: 1.245947, loss: 0.193410, perplexity: 1.213380, precision: 0.734375, batch_len: 105.000000
Train, loss=0.19340998: 216it [06:22,  1.88s/it]2017-06-02 14:21:21,762 root  INFO     step 216.000000 - time: 1.019987, loss: 0.177025, perplexity: 1.193661, precision: 0.750000, batch_len: 102.000000
Train, loss=0.17702496: 217it [06:23,  1.63s/it]2017-06-02 14:21:22,915 root  INFO     step 217.000000 - time: 1.104356, loss: 0.193680, perplexity: 1.213708, precision: 0.765625, batch_len: 106.000000
Train, loss=0.19367985: 218it [06:24,  1.49s/it]2017-06-02 14:21:23,907 root  INFO     step 218.000000 - time: 0.986394, loss: 0.122873, perplexity: 1.130741, precision: 0.765625, batch_len: 92.000000
Train, loss=0.12287281: 219it [06:25,  1.34s/it]2017-06-02 14:21:25,160 root  INFO     step 219.000000 - time: 1.100312, loss: 0.169287, perplexity: 1.184460, precision: 0.765625, batch_len: 112.000000
Train, loss=0.16928673: 220it [06:27,  1.31s/it]2017-06-02 14:21:26,635 root  INFO     step 220.000000 - time: 1.389379, loss: 0.143316, perplexity: 1.154095, precision: 0.671875, batch_len: 128.000000
Train, loss=0.14331634: 221it [06:28,  1.36s/it]2017-06-02 14:21:27,830 root  INFO     step 221.000000 - time: 1.169785, loss: 0.217068, perplexity: 1.242428, precision: 0.703125, batch_len: 90.000000
Train, loss=0.21706775: 222it [06:29,  1.31s/it]2017-06-02 14:21:29,142 root  INFO     step 222.000000 - time: 1.295701, loss: 0.253240, perplexity: 1.288192, precision: 0.640625, batch_len: 100.000000
Train, loss=0.25324005: 223it [06:31,  1.31s/it]2017-06-02 14:21:30,398 root  INFO     step 223.000000 - time: 1.226807, loss: 0.270982, perplexity: 1.311252, precision: 0.625000, batch_len: 114.000000
Train, loss=0.27098227: 224it [06:32,  1.29s/it]2017-06-02 14:21:31,846 root  INFO     step 224.000000 - time: 1.440550, loss: 0.094791, perplexity: 1.099429, precision: 0.765625, batch_len: 117.000000
Train, loss=0.09479050: 225it [06:33,  1.34s/it]2017-06-02 14:21:32,834 root  INFO     step 225.000000 - time: 0.949360, loss: 0.274207, perplexity: 1.315487, precision: 0.625000, batch_len: 93.000000
Train, loss=0.27420658: 226it [06:34,  1.23s/it]2017-06-02 14:21:34,222 root  INFO     step 226.000000 - time: 1.336722, loss: 0.242064, perplexity: 1.273876, precision: 0.640625, batch_len: 110.000000
Train, loss=0.24206409: 227it [06:36,  1.28s/it]2017-06-02 14:21:35,425 root  INFO     step 227.000000 - time: 1.193160, loss: 0.310077, perplexity: 1.363530, precision: 0.656250, batch_len: 88.000000
Train, loss=0.31007677: 228it [06:37,  1.26s/it]2017-06-02 14:21:36,614 root  INFO     step 228.000000 - time: 1.140907, loss: 0.179071, perplexity: 1.196106, precision: 0.750000, batch_len: 113.000000
Train, loss=0.17907089: 229it [06:38,  1.24s/it]2017-06-02 14:21:37,872 root  INFO     step 229.000000 - time: 1.117004, loss: 0.312736, perplexity: 1.367161, precision: 0.625000, batch_len: 111.000000
Train, loss=0.31273609: 230it [06:39,  1.24s/it]2017-06-02 14:21:38,996 root  INFO     step 230.000000 - time: 1.107514, loss: 0.206849, perplexity: 1.229796, precision: 0.640625, batch_len: 108.000000
Train, loss=0.20684865: 231it [06:40,  1.21s/it]2017-06-02 14:21:40,035 root  INFO     step 231.000000 - time: 0.973762, loss: 0.234856, perplexity: 1.264727, precision: 0.609375, batch_len: 104.000000
Train, loss=0.23485617: 232it [06:42,  1.16s/it]2017-06-02 14:21:41,792 root  INFO     step 232.000000 - time: 1.416985, loss: 0.260470, perplexity: 1.297539, precision: 0.671875, batch_len: 109.000000
Train, loss=0.26046976: 233it [06:43,  1.34s/it]2017-06-02 14:21:43,203 root  INFO     step 233.000000 - time: 1.357572, loss: 0.244301, perplexity: 1.276728, precision: 0.625000, batch_len: 120.000000
Train, loss=0.24430090: 234it [06:45,  1.36s/it]2017-06-02 14:21:44,183 root  INFO     step 234.000000 - time: 0.935817, loss: 0.268355, perplexity: 1.307811, precision: 0.609375, batch_len: 83.000000
Train, loss=0.26835507: 235it [06:46,  1.25s/it]2017-06-02 14:21:45,204 root  INFO     step 235.000000 - time: 0.996530, loss: 0.209377, perplexity: 1.232910, precision: 0.656250, batch_len: 97.000000
Train, loss=0.20937724: 236it [06:47,  1.18s/it]2017-06-02 14:21:46,501 root  INFO     step 236.000000 - time: 1.286559, loss: 0.180415, perplexity: 1.197714, precision: 0.781250, batch_len: 121.000000
Train, loss=0.18041490: 237it [06:48,  1.21s/it]2017-06-02 14:21:47,631 root  INFO     step 237.000000 - time: 1.089997, loss: 0.170089, perplexity: 1.185411, precision: 0.796875, batch_len: 81.000000
Train, loss=0.17008929: 238it [06:49,  1.19s/it]2017-06-02 14:21:48,832 root  INFO     step 238.000000 - time: 1.179608, loss: 0.133870, perplexity: 1.143244, precision: 0.781250, batch_len: 103.000000
Train, loss=0.13386995: 239it [06:50,  1.19s/it]2017-06-02 14:21:50,065 root  INFO     step 239.000000 - time: 1.195732, loss: 0.190024, perplexity: 1.209279, precision: 0.781250, batch_len: 91.000000
Train, loss=0.19002442: 240it [06:52,  1.20s/it]2017-06-02 14:21:51,411 root  INFO     step 240.000000 - time: 1.333933, loss: 0.164341, perplexity: 1.178616, precision: 0.765625, batch_len: 119.000000
Train, loss=0.16434103: 241it [06:53,  1.25s/it]2017-06-02 14:21:52,674 root  INFO     step 241.000000 - time: 1.252548, loss: 0.085143, perplexity: 1.088872, precision: 0.796875, batch_len: 124.000000
Train, loss=0.08514264: 242it [06:54,  1.25s/it]2017-06-02 14:21:53,776 root  INFO     step 242.000000 - time: 0.987482, loss: 0.221650, perplexity: 1.248135, precision: 0.781250, batch_len: 79.000000
Train, loss=0.22165006: 243it [06:55,  1.21s/it]2017-06-02 14:21:54,909 root  INFO     step 243.000000 - time: 1.115053, loss: 0.152956, perplexity: 1.165274, precision: 0.765625, batch_len: 89.000000
Train, loss=0.15295644: 244it [06:56,  1.18s/it]2017-06-02 14:21:56,031 root  INFO     step 244.000000 - time: 1.076220, loss: 0.170623, perplexity: 1.186044, precision: 0.796875, batch_len: 80.000000
Train, loss=0.17062329: 245it [06:58,  1.17s/it]2017-06-02 14:21:57,395 root  INFO     step 245.000000 - time: 1.308548, loss: 0.124533, perplexity: 1.132620, precision: 0.796875, batch_len: 123.000000
Train, loss=0.12453337: 246it [06:59,  1.23s/it]2017-06-02 14:21:58,533 root  INFO     step 246.000000 - time: 1.034172, loss: 0.171898, perplexity: 1.187557, precision: 0.750000, batch_len: 85.000000
Train, loss=0.17189807: 247it [07:00,  1.20s/it]2017-06-02 14:22:00,024 root  INFO     step 247.000000 - time: 1.338235, loss: 0.163120, perplexity: 1.177178, precision: 0.718750, batch_len: 137.000000
Train, loss=0.16311979: 248it [07:01,  1.29s/it]2017-06-02 14:22:01,219 root  INFO     step 248.000000 - time: 1.172780, loss: 0.159640, perplexity: 1.173089, precision: 0.781250, batch_len: 87.000000
Train, loss=0.15964006: 249it [07:03,  1.26s/it]2017-06-02 14:22:02,706 root  INFO     step 249.000000 - time: 1.463041, loss: 0.125387, perplexity: 1.133587, precision: 0.796875, batch_len: 125.000000
Train, loss=0.12538701: 250it [07:04,  1.33s/it]2017-06-02 14:22:03,845 root  INFO     step 250.000000 - time: 1.132791, loss: 0.215132, perplexity: 1.240026, precision: 0.781250, batch_len: 115.000000
Train, loss=0.21513224: 251it [07:05,  1.27s/it]2017-06-02 14:22:04,802 root  INFO     step 251.000000 - time: 0.915349, loss: 0.168933, perplexity: 1.184041, precision: 0.828125, batch_len: 86.000000
Train, loss=0.16893277: 252it [07:06,  1.18s/it]2017-06-02 14:22:06,004 root  INFO     step 252.000000 - time: 1.015646, loss: 0.168517, perplexity: 1.183549, precision: 0.781250, batch_len: 107.000000
Train, loss=0.16851726: 253it [07:07,  1.18s/it]2017-06-02 14:22:07,248 root  INFO     step 253.000000 - time: 1.162170, loss: 0.224021, perplexity: 1.251097, precision: 0.718750, batch_len: 84.000000
Train, loss=0.22402067: 254it [07:09,  1.20s/it]2017-06-02 14:22:08,652 root  INFO     step 254.000000 - time: 1.294168, loss: 0.238717, perplexity: 1.269619, precision: 0.812500, batch_len: 94.000000
Train, loss=0.23871660: 255it [07:10,  1.26s/it]2017-06-02 14:22:09,714 root  INFO     step 255.000000 - time: 1.052891, loss: 0.141672, perplexity: 1.152199, precision: 0.828125, batch_len: 99.000000
Train, loss=0.14167233: 256it [07:11,  1.20s/it]2017-06-02 14:22:11,075 root  INFO     step 256.000000 - time: 1.299000, loss: 0.165029, perplexity: 1.179427, precision: 0.796875, batch_len: 126.000000
Train, loss=0.16502906: 257it [07:13,  1.25s/it]2017-06-02 14:22:12,537 root  INFO     step 257.000000 - time: 1.420981, loss: 0.172511, perplexity: 1.188284, precision: 0.718750, batch_len: 130.000000
Train, loss=0.17251056: 258it [07:14,  1.31s/it]2017-06-02 14:22:13,989 root  INFO     step 258.000000 - time: 1.441673, loss: 0.130869, perplexity: 1.139819, precision: 0.796875, batch_len: 116.000000
Train, loss=0.13086906: 259it [07:15,  1.36s/it]2017-06-02 14:22:15,672 root  INFO     step 259.000000 - time: 1.619613, loss: 0.211730, perplexity: 1.235814, precision: 0.734375, batch_len: 135.000000
Train, loss=0.21173018: 260it [07:17,  1.45s/it]2017-06-02 14:22:17,125 root  INFO     step 260.000000 - time: 1.406229, loss: 0.150383, perplexity: 1.162280, precision: 0.703125, batch_len: 129.000000
Train, loss=0.15038347: 261it [07:19,  1.45s/it]2017-06-02 14:22:18,513 root  INFO     step 261.000000 - time: 1.367013, loss: 0.114855, perplexity: 1.121711, precision: 0.796875, batch_len: 118.000000
Train, loss=0.11485476: 262it [07:20,  1.43s/it]2017-06-02 14:22:19,865 root  INFO     step 262.000000 - time: 1.310837, loss: 0.108233, perplexity: 1.114307, precision: 0.796875, batch_len: 144.000000
Train, loss=0.10823251: 263it [07:21,  1.41s/it]2017-06-02 14:22:21,088 root  INFO     step 263.000000 - time: 1.191847, loss: 0.106749, perplexity: 1.112655, precision: 0.781250, batch_len: 98.000000
Train, loss=0.10674863: 264it [07:23,  1.35s/it]2017-06-02 14:22:22,748 root  INFO     step 264.000000 - time: 1.496396, loss: 0.105686, perplexity: 1.111473, precision: 0.812500, batch_len: 136.000000
Train, loss=0.10568636: 265it [07:24,  1.45s/it]2017-06-02 14:22:23,794 root  INFO     step 265.000000 - time: 0.908356, loss: 0.126527, perplexity: 1.134880, precision: 0.781250, batch_len: 82.000000
Train, loss=0.12652683: 266it [07:25,  1.33s/it]2017-06-02 14:22:24,851 root  INFO     step 266.000000 - time: 0.963817, loss: 0.087075, perplexity: 1.090979, precision: 0.875000, batch_len: 78.000000
Train, loss=0.08707507: 267it [07:26,  1.25s/it]2017-06-02 14:22:26,204 root  INFO     step 267.000000 - time: 1.264663, loss: 0.114973, perplexity: 1.121843, precision: 0.796875, batch_len: 133.000000
Train, loss=0.11497276: 268it [07:28,  1.28s/it]2017-06-02 14:22:27,619 root  INFO     step 268.000000 - time: 1.370118, loss: 0.095967, perplexity: 1.100722, precision: 0.875000, batch_len: 72.000000
Train, loss=0.09596661: 269it [07:29,  1.32s/it]2017-06-02 14:22:28,610 root  INFO     step 269.000000 - time: 0.906575, loss: 0.089343, perplexity: 1.093455, precision: 0.859375, batch_len: 96.000000
Train, loss=0.08934253: 270it [07:30,  1.22s/it]2017-06-02 14:22:29,790 root  INFO     step 270.000000 - time: 1.125201, loss: 0.180419, perplexity: 1.197719, precision: 0.781250, batch_len: 76.000000
Train, loss=0.18041897: 271it [07:31,  1.21s/it]2017-06-02 14:22:31,604 root  INFO     step 271.000000 - time: 1.432237, loss: 0.132064, perplexity: 1.141182, precision: 0.812500, batch_len: 138.000000
Train, loss=0.13206436: 272it [07:33,  1.39s/it]2017-06-02 14:22:33,067 root  INFO     step 272.000000 - time: 1.355156, loss: 0.103317, perplexity: 1.108843, precision: 0.812500, batch_len: 122.000000
Train, loss=0.10331683: 273it [07:35,  1.41s/it]2017-06-02 14:22:34,878 root  INFO     step 273.000000 - time: 1.718139, loss: 0.170571, perplexity: 1.185982, precision: 0.671875, batch_len: 152.000000
Train, loss=0.17057110: 274it [07:36,  1.53s/it]2017-06-02 14:22:36,539 root  INFO     step 274.000000 - time: 1.589245, loss: 0.918171, perplexity: 2.504705, precision: 0.187500, batch_len: 139.000000
Train, loss=0.91817093: 275it [07:38,  1.57s/it]2017-06-02 14:22:37,678 root  INFO     step 275.000000 - time: 1.083820, loss: 0.122200, perplexity: 1.129980, precision: 0.843750, batch_len: 74.000000
Train, loss=0.12219974: 276it [07:39,  1.44s/it]2017-06-02 14:22:39,053 root  INFO     step 276.000000 - time: 1.328812, loss: 0.450906, perplexity: 1.569734, precision: 0.453125, batch_len: 141.000000
Train, loss=0.45090613: 277it [07:41,  1.42s/it]2017-06-02 14:22:40,457 root  INFO     step 277.000000 - time: 1.357073, loss: 0.166322, perplexity: 1.180953, precision: 0.671875, batch_len: 131.000000
Train, loss=0.16632207: 278it [07:42,  1.42s/it]2017-06-02 14:22:42,245 root  INFO     step 278.000000 - time: 1.780867, loss: 0.224508, perplexity: 1.251707, precision: 0.640625, batch_len: 150.000000
Train, loss=0.22450835: 279it [07:44,  1.53s/it]2017-06-02 14:22:43,967 root  INFO     step 279.000000 - time: 1.670171, loss: 0.240213, perplexity: 1.271520, precision: 0.609375, batch_len: 134.000000
Train, loss=0.24021281: 280it [07:45,  1.59s/it]2017-06-02 14:22:45,484 root  INFO     step 280.000000 - time: 1.405609, loss: 0.479530, perplexity: 1.615315, precision: 0.515625, batch_len: 132.000000
Train, loss=0.47952974: 281it [07:47,  1.57s/it]2017-06-02 14:22:46,634 root  INFO     step 281.000000 - time: 0.999425, loss: 0.534638, perplexity: 1.706831, precision: 0.406250, batch_len: 77.000000
Train, loss=0.53463840: 282it [07:48,  1.44s/it]2017-06-02 14:22:48,041 root  INFO     step 282.000000 - time: 1.256197, loss: 0.544286, perplexity: 1.723378, precision: 0.218750, batch_len: 142.000000
Train, loss=0.54428649: 283it [07:50,  1.43s/it]2017-06-02 14:22:49,255 root  INFO     step 283.000000 - time: 1.033224, loss: 0.120644, perplexity: 1.128223, precision: 0.812500, batch_len: 71.000000
Train, loss=0.12064363: 284it [07:51,  1.37s/it]2017-06-02 14:22:49,441 root  INFO     Generating first batch)
2017-06-02 14:22:53,349 root  INFO     step 284.000000 - time: 0.979205, loss: 0.142221, perplexity: 1.152831, precision: 0.750000, batch_len: 96.000000
Train, loss=0.14222074: 285it [07:55,  2.18s/it]2017-06-02 14:22:55,066 root  INFO     step 285.000000 - time: 1.314597, loss: 0.183744, perplexity: 1.201708, precision: 0.703125, batch_len: 108.000000
Train, loss=0.18374395: 286it [07:57,  2.04s/it]2017-06-02 14:22:56,257 root  INFO     step 286.000000 - time: 1.118462, loss: 0.319048, perplexity: 1.375818, precision: 0.750000, batch_len: 113.000000
Train, loss=0.31904817: 287it [07:58,  1.79s/it]2017-06-02 14:22:57,609 root  INFO     step 287.000000 - time: 1.308309, loss: 0.201494, perplexity: 1.223229, precision: 0.718750, batch_len: 120.000000
Train, loss=0.20149425: 288it [07:59,  1.66s/it]2017-06-02 14:22:58,754 root  INFO     step 288.000000 - time: 1.064559, loss: 0.198734, perplexity: 1.219858, precision: 0.718750, batch_len: 105.000000
Train, loss=0.19873449: 289it [08:00,  1.50s/it]2017-06-02 14:22:59,846 root  INFO     step 289.000000 - time: 0.969536, loss: 0.216078, perplexity: 1.241199, precision: 0.625000, batch_len: 93.000000
Train, loss=0.21607772: 290it [08:01,  1.38s/it]2017-06-02 14:23:01,070 root  INFO     step 290.000000 - time: 1.193333, loss: 0.190769, perplexity: 1.210180, precision: 0.625000, batch_len: 102.000000
Train, loss=0.19076870: 291it [08:03,  1.33s/it]2017-06-02 14:23:02,310 root  INFO     step 291.000000 - time: 1.152724, loss: 0.194699, perplexity: 1.214945, precision: 0.843750, batch_len: 92.000000
Train, loss=0.19469854: 292it [08:04,  1.31s/it]2017-06-02 14:23:03,501 root  INFO     step 292.000000 - time: 1.063234, loss: 0.158937, perplexity: 1.172264, precision: 0.859375, batch_len: 101.000000
Train, loss=0.15893675: 293it [08:05,  1.27s/it]2017-06-02 14:23:04,592 root  INFO     step 293.000000 - time: 1.078520, loss: 0.235567, perplexity: 1.265626, precision: 0.750000, batch_len: 100.000000
Train, loss=0.23556720: 294it [08:06,  1.22s/it]2017-06-02 14:23:05,649 root  INFO     step 294.000000 - time: 0.996488, loss: 0.419356, perplexity: 1.520982, precision: 0.609375, batch_len: 111.000000
Train, loss=0.41935638: 295it [08:07,  1.17s/it]2017-06-02 14:23:06,671 root  INFO     step 295.000000 - time: 1.001013, loss: 0.143556, perplexity: 1.154371, precision: 0.765625, batch_len: 103.000000
Train, loss=0.14355575: 296it [08:08,  1.12s/it]2017-06-02 14:23:07,892 root  INFO     step 296.000000 - time: 1.150511, loss: 0.235612, perplexity: 1.265683, precision: 0.703125, batch_len: 97.000000
Train, loss=0.23561200: 297it [08:09,  1.15s/it]2017-06-02 14:23:09,702 root  INFO     step 297.000000 - time: 1.767649, loss: 0.246190, perplexity: 1.279142, precision: 0.718750, batch_len: 90.000000
Train, loss=0.24618983: 298it [08:11,  1.35s/it]2017-06-02 14:23:10,699 root  INFO     step 298.000000 - time: 0.965535, loss: 0.326659, perplexity: 1.386329, precision: 0.734375, batch_len: 88.000000
Train, loss=0.32665947: 299it [08:12,  1.24s/it]2017-06-02 14:23:11,820 root  INFO     step 299.000000 - time: 1.063944, loss: 0.178110, perplexity: 1.194957, precision: 0.687500, batch_len: 110.000000
Train, loss=0.17811005: 300it [08:13,  1.21s/it]2017-06-02 14:23:13,163 root  INFO     step 300.000000 - time: 1.317624, loss: 0.111440, perplexity: 1.117887, precision: 0.781250, batch_len: 128.000000
Train, loss=0.11143997: 301it [08:15,  1.25s/it]2017-06-02 14:23:14,542 root  INFO     step 301.000000 - time: 1.327614, loss: 0.151116, perplexity: 1.163132, precision: 0.765625, batch_len: 104.000000
Train, loss=0.15111627: 302it [08:16,  1.29s/it]2017-06-02 14:23:15,877 root  INFO     step 302.000000 - time: 1.294547, loss: 0.187417, perplexity: 1.206130, precision: 0.671875, batch_len: 109.000000
Train, loss=0.18741688: 303it [08:17,  1.30s/it]2017-06-02 14:23:17,284 root  INFO     step 303.000000 - time: 1.341848, loss: 0.102652, perplexity: 1.108105, precision: 0.859375, batch_len: 117.000000
Train, loss=0.10265177: 304it [08:19,  1.33s/it]2017-06-02 14:23:18,379 root  INFO     step 304.000000 - time: 0.979034, loss: 0.225174, perplexity: 1.252541, precision: 0.718750, batch_len: 91.000000
Train, loss=0.22517428: 305it [08:20,  1.26s/it]2017-06-02 14:23:19,536 root  INFO     step 305.000000 - time: 1.001997, loss: 0.202074, perplexity: 1.223939, precision: 0.718750, batch_len: 106.000000
Train, loss=0.20207439: 306it [08:21,  1.23s/it]2017-06-02 14:23:20,807 root  INFO     step 306.000000 - time: 1.229339, loss: 0.167812, perplexity: 1.182714, precision: 0.765625, batch_len: 112.000000
Train, loss=0.16781151: 307it [08:22,  1.24s/it]2017-06-02 14:23:22,148 root  INFO     step 307.000000 - time: 1.304153, loss: 0.163026, perplexity: 1.177068, precision: 0.687500, batch_len: 114.000000
Train, loss=0.16302644: 308it [08:24,  1.27s/it]2017-06-02 14:23:23,357 root  INFO     step 308.000000 - time: 0.963095, loss: 0.158758, perplexity: 1.172054, precision: 0.765625, batch_len: 89.000000
Train, loss=0.15875798: 309it [08:25,  1.25s/it]2017-06-02 14:23:24,765 root  INFO     step 309.000000 - time: 1.332057, loss: 0.235408, perplexity: 1.265425, precision: 0.796875, batch_len: 126.000000
Train, loss=0.23540790: 310it [08:26,  1.30s/it]2017-06-02 14:23:26,122 root  INFO     step 310.000000 - time: 1.273400, loss: 0.226237, perplexity: 1.253872, precision: 0.796875, batch_len: 124.000000
Train, loss=0.22623669: 311it [08:28,  1.32s/it]2017-06-02 14:23:27,303 root  INFO     step 311.000000 - time: 1.132356, loss: 0.206043, perplexity: 1.228806, precision: 0.750000, batch_len: 80.000000
Train, loss=0.20604277: 312it [08:29,  1.28s/it]2017-06-02 14:23:28,323 root  INFO     step 312.000000 - time: 0.943931, loss: 0.122331, perplexity: 1.130128, precision: 0.843750, batch_len: 86.000000
Train, loss=0.12233122: 313it [08:30,  1.20s/it]2017-06-02 14:23:29,496 root  INFO     step 313.000000 - time: 1.109460, loss: 0.187550, perplexity: 1.206290, precision: 0.750000, batch_len: 79.000000
Train, loss=0.18754983: 314it [08:31,  1.19s/it]2017-06-02 14:23:30,630 root  INFO     step 314.000000 - time: 1.122366, loss: 0.167627, perplexity: 1.182495, precision: 0.765625, batch_len: 81.000000
Train, loss=0.16762669: 315it [08:32,  1.17s/it]2017-06-02 14:23:31,951 root  INFO     step 315.000000 - time: 1.317764, loss: 0.139605, perplexity: 1.149820, precision: 0.796875, batch_len: 125.000000
Train, loss=0.13960543: 316it [08:33,  1.22s/it]2017-06-02 14:23:33,383 root  INFO     step 316.000000 - time: 1.400806, loss: 0.167173, perplexity: 1.181958, precision: 0.703125, batch_len: 129.000000
Train, loss=0.16717279: 317it [08:35,  1.28s/it]2017-06-02 14:23:34,334 root  INFO     step 317.000000 - time: 0.930526, loss: 0.331237, perplexity: 1.392690, precision: 0.625000, batch_len: 94.000000
Train, loss=0.33123744: 318it [08:36,  1.18s/it]2017-06-02 14:23:35,495 root  INFO     step 318.000000 - time: 1.154039, loss: 0.165356, perplexity: 1.179813, precision: 0.765625, batch_len: 85.000000
Train, loss=0.16535570: 319it [08:37,  1.18s/it]2017-06-02 14:23:36,870 root  INFO     step 319.000000 - time: 1.355205, loss: 0.165085, perplexity: 1.179493, precision: 0.750000, batch_len: 107.000000
Train, loss=0.16508496: 320it [08:38,  1.24s/it]2017-06-02 14:23:38,003 root  INFO     step 320.000000 - time: 0.952393, loss: 0.212908, perplexity: 1.237271, precision: 0.687500, batch_len: 83.000000
Train, loss=0.21290848: 321it [08:39,  1.21s/it]2017-06-02 14:23:39,442 root  INFO     step 321.000000 - time: 1.371363, loss: 0.137264, perplexity: 1.147130, precision: 0.703125, batch_len: 123.000000
Train, loss=0.13726351: 322it [08:41,  1.28s/it]2017-06-02 14:23:40,477 root  INFO     step 322.000000 - time: 0.912012, loss: 0.112606, perplexity: 1.119191, precision: 0.812500, batch_len: 87.000000
Train, loss=0.11260632: 323it [08:42,  1.20s/it]2017-06-02 14:23:41,786 root  INFO     step 323.000000 - time: 1.296582, loss: 0.159684, perplexity: 1.173140, precision: 0.750000, batch_len: 121.000000
Train, loss=0.15968378: 324it [08:43,  1.23s/it]2017-06-02 14:23:43,586 root  INFO     step 324.000000 - time: 1.694381, loss: 0.095653, perplexity: 1.100377, precision: 0.750000, batch_len: 116.000000
Train, loss=0.09565251: 325it [08:45,  1.40s/it]2017-06-02 14:23:45,085 root  INFO     step 325.000000 - time: 1.439342, loss: 0.160938, perplexity: 1.174612, precision: 0.781250, batch_len: 144.000000
Train, loss=0.16093779: 326it [08:47,  1.43s/it]2017-06-02 14:23:46,146 root  INFO     step 326.000000 - time: 1.037866, loss: 0.249190, perplexity: 1.282986, precision: 0.750000, batch_len: 84.000000
Train, loss=0.24918979: 327it [08:48,  1.32s/it]2017-06-02 14:23:47,352 root  INFO     step 327.000000 - time: 1.113255, loss: 0.178033, perplexity: 1.194864, precision: 0.718750, batch_len: 115.000000
Train, loss=0.17803271: 328it [08:49,  1.29s/it]2017-06-02 14:23:48,867 root  INFO     step 328.000000 - time: 1.479467, loss: 0.142678, perplexity: 1.153358, precision: 0.703125, batch_len: 130.000000
Train, loss=0.14267752: 329it [08:50,  1.36s/it]2017-06-02 14:23:50,469 root  INFO     step 329.000000 - time: 1.591522, loss: 0.164635, perplexity: 1.178963, precision: 0.859375, batch_len: 137.000000
Train, loss=0.16463487: 330it [08:52,  1.43s/it]2017-06-02 14:23:51,575 root  INFO     step 330.000000 - time: 1.050853, loss: 0.087309, perplexity: 1.091234, precision: 0.843750, batch_len: 98.000000
Train, loss=0.08730943: 331it [08:53,  1.33s/it]2017-06-02 14:23:53,153 root  INFO     step 331.000000 - time: 1.345147, loss: 0.076624, perplexity: 1.079636, precision: 0.812500, batch_len: 136.000000
Train, loss=0.07662378: 332it [08:55,  1.41s/it]2017-06-02 14:23:54,381 root  INFO     step 332.000000 - time: 1.221196, loss: 0.095387, perplexity: 1.100084, precision: 0.812500, batch_len: 118.000000
Train, loss=0.09538695: 333it [08:56,  1.35s/it]2017-06-02 14:23:55,575 root  INFO     step 333.000000 - time: 1.104814, loss: 0.206605, perplexity: 1.229497, precision: 0.750000, batch_len: 82.000000
Train, loss=0.20660476: 334it [08:57,  1.30s/it]2017-06-02 14:23:56,929 root  INFO     step 334.000000 - time: 1.336334, loss: 0.149888, perplexity: 1.161705, precision: 0.734375, batch_len: 99.000000
Train, loss=0.14988846: 335it [08:58,  1.32s/it]2017-06-02 14:23:58,380 root  INFO     step 335.000000 - time: 1.397814, loss: 0.154257, perplexity: 1.166790, precision: 0.812500, batch_len: 135.000000
Train, loss=0.15425655: 336it [09:00,  1.36s/it]2017-06-02 14:23:59,638 root  INFO     step 336.000000 - time: 0.939006, loss: 0.104994, perplexity: 1.110704, precision: 0.859375, batch_len: 78.000000
Train, loss=0.10499372: 337it [09:01,  1.33s/it]2017-06-02 14:24:00,696 root  INFO     step 337.000000 - time: 0.931784, loss: 0.114527, perplexity: 1.121343, precision: 0.828125, batch_len: 96.000000
Train, loss=0.11452684: 338it [09:02,  1.25s/it]2017-06-02 14:24:02,307 root  INFO     step 338.000000 - time: 1.561471, loss: 0.224295, perplexity: 1.251441, precision: 0.671875, batch_len: 141.000000
Train, loss=0.22429541: 339it [09:04,  1.36s/it]2017-06-02 14:24:03,951 root  INFO     step 339.000000 - time: 1.480602, loss: 0.341512, perplexity: 1.407074, precision: 0.593750, batch_len: 119.000000
Train, loss=0.34151244: 340it [09:05,  1.44s/it]2017-06-02 14:24:05,416 root  INFO     step 340.000000 - time: 1.434241, loss: 0.207253, perplexity: 1.230294, precision: 0.562500, batch_len: 138.000000
Train, loss=0.20725322: 341it [09:07,  1.45s/it]2017-06-02 14:24:06,852 root  INFO     step 341.000000 - time: 1.393159, loss: 0.074018, perplexity: 1.076826, precision: 0.843750, batch_len: 133.000000
Train, loss=0.07401755: 342it [09:08,  1.45s/it]2017-06-02 14:24:08,185 root  INFO     step 342.000000 - time: 1.269021, loss: 0.141808, perplexity: 1.152355, precision: 0.828125, batch_len: 122.000000
Train, loss=0.14180753: 343it [09:10,  1.41s/it]2017-06-02 14:24:09,449 root  INFO     step 343.000000 - time: 1.095049, loss: 0.083684, perplexity: 1.087286, precision: 0.859375, batch_len: 74.000000
Train, loss=0.08368429: 344it [09:11,  1.37s/it]2017-06-02 14:24:10,784 root  INFO     step 344.000000 - time: 1.230508, loss: 0.090325, perplexity: 1.094529, precision: 0.906250, batch_len: 72.000000
Train, loss=0.09032454: 345it [09:12,  1.36s/it]2017-06-02 14:24:12,238 root  INFO     step 345.000000 - time: 1.377421, loss: 0.086163, perplexity: 1.089984, precision: 0.859375, batch_len: 132.000000
Train, loss=0.08616262: 346it [09:14,  1.39s/it]2017-06-02 14:24:14,056 root  INFO     step 346.000000 - time: 1.797578, loss: 0.061734, perplexity: 1.063679, precision: 0.859375, batch_len: 152.000000
Train, loss=0.06173368: 347it [09:16,  1.52s/it]2017-06-02 14:24:15,720 root  INFO     step 347.000000 - time: 1.636484, loss: 0.114599, perplexity: 1.121423, precision: 0.890625, batch_len: 139.000000
Train, loss=0.11459869: 348it [09:17,  1.56s/it]2017-06-02 14:24:17,823 root  INFO     step 348.000000 - time: 2.027612, loss: 0.116231, perplexity: 1.123256, precision: 0.843750, batch_len: 150.000000
Train, loss=0.11623125: 349it [09:19,  1.72s/it]2017-06-02 14:24:19,030 root  INFO     step 349.000000 - time: 0.995612, loss: 0.099208, perplexity: 1.104296, precision: 0.843750, batch_len: 77.000000
Train, loss=0.09920815: 350it [09:21,  1.57s/it]2017-06-02 14:24:20,482 root  INFO     step 350.000000 - time: 1.394667, loss: 0.138075, perplexity: 1.148061, precision: 0.765625, batch_len: 134.000000
Train, loss=0.13807477: 351it [09:22,  1.53s/it]2017-06-02 14:24:21,779 root  INFO     step 351.000000 - time: 1.097642, loss: 0.188407, perplexity: 1.207325, precision: 0.718750, batch_len: 76.000000
Train, loss=0.18840736: 352it [09:23,  1.46s/it]2017-06-02 14:24:23,476 root  INFO     step 352.000000 - time: 1.609404, loss: 0.071554, perplexity: 1.074176, precision: 0.828125, batch_len: 131.000000
Train, loss=0.07155369: 353it [09:25,  1.53s/it]2017-06-02 14:24:24,495 root  INFO     step 353.000000 - time: 0.927538, loss: 0.108267, perplexity: 1.114345, precision: 0.890625, batch_len: 71.000000
Train, loss=0.10826720: 354it [09:26,  1.38s/it]2017-06-02 14:24:25,997 root  INFO     step 354.000000 - time: 1.474405, loss: 0.119082, perplexity: 1.126463, precision: 0.765625, batch_len: 142.000000
Train, loss=0.11908229: 355it [09:27,  1.42s/it]2017-06-02 14:24:26,120 root  INFO     Generating first batch)
2017-06-02 14:24:29,469 root  INFO     step 355.000000 - time: 1.137504, loss: 0.093499, perplexity: 1.098009, precision: 0.875000, batch_len: 96.000000
Train, loss=0.09349898: 356it [09:31,  2.03s/it]2017-06-02 14:24:31,129 root  INFO     step 356.000000 - time: 1.033185, loss: 0.295773, perplexity: 1.344165, precision: 0.671875, batch_len: 93.000000
Train, loss=0.29577267: 357it [09:33,  1.92s/it]2017-06-02 14:24:32,499 root  INFO     step 357.000000 - time: 1.058237, loss: 0.398237, perplexity: 1.489197, precision: 0.609375, batch_len: 105.000000
Train, loss=0.39823711: 358it [09:34,  1.76s/it]2017-06-02 14:24:33,916 root  INFO     step 358.000000 - time: 1.264432, loss: 0.164772, perplexity: 1.179124, precision: 0.718750, batch_len: 128.000000
Train, loss=0.16477174: 359it [09:35,  1.65s/it]2017-06-02 14:24:35,092 root  INFO     step 359.000000 - time: 1.163325, loss: 0.327347, perplexity: 1.387283, precision: 0.593750, batch_len: 97.000000
Train, loss=0.32734701: 360it [09:37,  1.51s/it]2017-06-02 14:24:36,577 root  INFO     step 360.000000 - time: 1.323606, loss: 0.306113, perplexity: 1.358136, precision: 0.671875, batch_len: 113.000000
Train, loss=0.30611306: 361it [09:38,  1.50s/it]2017-06-02 14:24:37,702 root  INFO     step 361.000000 - time: 1.063403, loss: 0.222220, perplexity: 1.248847, precision: 0.765625, batch_len: 100.000000
Train, loss=0.22222036: 362it [09:39,  1.39s/it]2017-06-02 14:24:38,728 root  INFO     step 362.000000 - time: 1.010425, loss: 0.258575, perplexity: 1.295084, precision: 0.718750, batch_len: 89.000000
Train, loss=0.25857538: 363it [09:40,  1.28s/it]2017-06-02 14:24:39,942 root  INFO     step 363.000000 - time: 1.072348, loss: 0.146727, perplexity: 1.158038, precision: 0.703125, batch_len: 110.000000
Train, loss=0.14672728: 364it [09:41,  1.26s/it]2017-06-02 14:24:40,904 root  INFO     step 364.000000 - time: 0.947091, loss: 0.166947, perplexity: 1.181692, precision: 0.718750, batch_len: 101.000000
Train, loss=0.16694739: 365it [09:42,  1.17s/it]2017-06-02 14:24:42,394 root  INFO     step 365.000000 - time: 1.485518, loss: 0.170796, perplexity: 1.186249, precision: 0.703125, batch_len: 120.000000
Train, loss=0.17079601: 366it [09:44,  1.27s/it]2017-06-02 14:24:43,675 root  INFO     step 366.000000 - time: 1.259207, loss: 0.160902, perplexity: 1.174570, precision: 0.750000, batch_len: 112.000000
Train, loss=0.16090211: 367it [09:45,  1.27s/it]2017-06-02 14:24:44,664 root  INFO     step 367.000000 - time: 0.962020, loss: 0.200131, perplexity: 1.221563, precision: 0.687500, batch_len: 90.000000
Train, loss=0.20013149: 368it [09:46,  1.19s/it]2017-06-02 14:24:46,086 root  INFO     step 368.000000 - time: 1.353080, loss: 0.101037, perplexity: 1.106317, precision: 0.750000, batch_len: 117.000000
Train, loss=0.10103686: 369it [09:48,  1.26s/it]2017-06-02 14:24:47,168 root  INFO     step 369.000000 - time: 1.062339, loss: 0.142855, perplexity: 1.153563, precision: 0.734375, batch_len: 106.000000
Train, loss=0.14285500: 370it [09:49,  1.20s/it]2017-06-02 14:24:48,082 root  INFO     step 370.000000 - time: 0.892366, loss: 0.165466, perplexity: 1.179943, precision: 0.734375, batch_len: 81.000000
Train, loss=0.16546629: 371it [09:50,  1.12s/it]2017-06-02 14:24:49,412 root  INFO     step 371.000000 - time: 1.258558, loss: 0.208870, perplexity: 1.232285, precision: 0.718750, batch_len: 111.000000
Train, loss=0.20887008: 372it [09:51,  1.18s/it]2017-06-02 14:24:50,592 root  INFO     step 372.000000 - time: 1.164102, loss: 0.191831, perplexity: 1.211466, precision: 0.781250, batch_len: 92.000000
Train, loss=0.19183144: 373it [09:52,  1.18s/it]2017-06-02 14:24:51,815 root  INFO     step 373.000000 - time: 1.110838, loss: 0.139018, perplexity: 1.149145, precision: 0.734375, batch_len: 108.000000
Train, loss=0.13901839: 374it [09:53,  1.19s/it]2017-06-02 14:24:52,968 root  INFO     step 374.000000 - time: 1.114133, loss: 0.166950, perplexity: 1.181695, precision: 0.765625, batch_len: 109.000000
Train, loss=0.16694976: 375it [09:54,  1.18s/it]2017-06-02 14:24:53,989 root  INFO     step 375.000000 - time: 1.009555, loss: 0.134428, perplexity: 1.143882, precision: 0.718750, batch_len: 114.000000
Train, loss=0.13442782: 376it [09:55,  1.13s/it]2017-06-02 14:24:55,088 root  INFO     step 376.000000 - time: 1.083460, loss: 0.112844, perplexity: 1.119457, precision: 0.796875, batch_len: 107.000000
Train, loss=0.11284417: 377it [09:57,  1.12s/it]2017-06-02 14:24:56,462 root  INFO     step 377.000000 - time: 1.294694, loss: 0.144681, perplexity: 1.155671, precision: 0.750000, batch_len: 104.000000
Train, loss=0.14468074: 378it [09:58,  1.20s/it]2017-06-02 14:24:57,527 root  INFO     step 378.000000 - time: 0.992644, loss: 0.186866, perplexity: 1.205466, precision: 0.812500, batch_len: 94.000000
Train, loss=0.18686590: 379it [09:59,  1.16s/it]2017-06-02 14:24:58,625 root  INFO     step 379.000000 - time: 1.027620, loss: 0.218511, perplexity: 1.244223, precision: 0.750000, batch_len: 102.000000
Train, loss=0.21851096: 380it [10:00,  1.14s/it]2017-06-02 14:24:59,653 root  INFO     step 380.000000 - time: 0.969350, loss: 0.238129, perplexity: 1.268873, precision: 0.718750, batch_len: 88.000000
Train, loss=0.23812920: 381it [10:01,  1.11s/it]2017-06-02 14:25:00,645 root  INFO     step 381.000000 - time: 0.984771, loss: 0.101880, perplexity: 1.107250, precision: 0.812500, batch_len: 103.000000
Train, loss=0.10187982: 382it [10:02,  1.07s/it]2017-06-02 14:25:02,116 root  INFO     step 382.000000 - time: 1.354267, loss: 0.188797, perplexity: 1.207796, precision: 0.750000, batch_len: 121.000000
Train, loss=0.18879735: 383it [10:04,  1.19s/it]2017-06-02 14:25:03,387 root  INFO     step 383.000000 - time: 1.215158, loss: 0.278931, perplexity: 1.321716, precision: 0.703125, batch_len: 86.000000
Train, loss=0.27893099: 384it [10:05,  1.22s/it]2017-06-02 14:25:04,440 root  INFO     step 384.000000 - time: 0.966612, loss: 0.154957, perplexity: 1.167608, precision: 0.750000, batch_len: 87.000000
Train, loss=0.15495682: 385it [10:06,  1.17s/it]2017-06-02 14:25:05,519 root  INFO     step 385.000000 - time: 0.922053, loss: 0.213875, perplexity: 1.238468, precision: 0.703125, batch_len: 80.000000
Train, loss=0.21387476: 386it [10:07,  1.14s/it]2017-06-02 14:25:06,947 root  INFO     step 386.000000 - time: 1.291414, loss: 0.100303, perplexity: 1.105506, precision: 0.765625, batch_len: 124.000000
Train, loss=0.10030331: 387it [10:08,  1.23s/it]2017-06-02 14:25:07,880 root  INFO     step 387.000000 - time: 0.866522, loss: 0.175983, perplexity: 1.192418, precision: 0.781250, batch_len: 83.000000
Train, loss=0.17598300: 388it [10:09,  1.14s/it]2017-06-02 14:25:09,412 root  INFO     step 388.000000 - time: 1.456674, loss: 0.140559, perplexity: 1.150917, precision: 0.781250, batch_len: 123.000000
Train, loss=0.14055943: 389it [10:11,  1.26s/it]2017-06-02 14:25:10,787 root  INFO     step 389.000000 - time: 1.295080, loss: 0.131145, perplexity: 1.140134, precision: 0.812500, batch_len: 115.000000
Train, loss=0.13114539: 390it [10:12,  1.29s/it]2017-06-02 14:25:11,923 root  INFO     step 390.000000 - time: 1.057225, loss: 0.099053, perplexity: 1.104125, precision: 0.796875, batch_len: 99.000000
Train, loss=0.09905323: 391it [10:13,  1.25s/it]2017-06-02 14:25:13,321 root  INFO     step 391.000000 - time: 1.389394, loss: 0.114164, perplexity: 1.120936, precision: 0.828125, batch_len: 129.000000
Train, loss=0.11416371: 392it [10:15,  1.29s/it]2017-06-02 14:25:14,695 root  INFO     step 392.000000 - time: 1.249374, loss: 0.171423, perplexity: 1.186993, precision: 0.718750, batch_len: 125.000000
Train, loss=0.17142306: 393it [10:16,  1.32s/it]2017-06-02 14:25:16,234 root  INFO     step 393.000000 - time: 1.498940, loss: 0.210576, perplexity: 1.234388, precision: 0.656250, batch_len: 137.000000
Train, loss=0.21057558: 394it [10:18,  1.38s/it]2017-06-02 14:25:17,585 root  INFO     step 394.000000 - time: 1.279744, loss: 0.201686, perplexity: 1.223463, precision: 0.796875, batch_len: 91.000000
Train, loss=0.20168556: 395it [10:19,  1.37s/it]2017-06-02 14:25:18,978 root  INFO     step 395.000000 - time: 1.388340, loss: 0.105432, perplexity: 1.111190, precision: 0.796875, batch_len: 135.000000
Train, loss=0.10543166: 396it [10:20,  1.38s/it]2017-06-02 14:25:20,003 root  INFO     step 396.000000 - time: 0.959257, loss: 0.150144, perplexity: 1.162002, precision: 0.781250, batch_len: 82.000000
Train, loss=0.15014414: 397it [10:21,  1.27s/it]2017-06-02 14:25:21,364 root  INFO     step 397.000000 - time: 1.250640, loss: 0.072759, perplexity: 1.075471, precision: 0.890625, batch_len: 116.000000
Train, loss=0.07275917: 398it [10:23,  1.30s/it]2017-06-02 14:25:22,934 root  INFO     step 398.000000 - time: 1.534460, loss: 0.100828, perplexity: 1.106086, precision: 0.781250, batch_len: 133.000000
Train, loss=0.10082756: 399it [10:24,  1.38s/it]2017-06-02 14:25:24,557 root  INFO     step 399.000000 - time: 1.567635, loss: 0.140892, perplexity: 1.151300, precision: 0.765625, batch_len: 144.000000
Train, loss=0.14089169: 400it [10:26,  1.45s/it]2017-06-02 14:25:25,933 root  INFO     step 400.000000 - time: 1.351908, loss: 0.078942, perplexity: 1.082141, precision: 0.828125, batch_len: 118.000000
Train, loss=0.07894150: 401it [10:27,  1.43s/it]2017-06-02 14:25:27,523 root  INFO     step 401.000000 - time: 1.566424, loss: 0.114508, perplexity: 1.121322, precision: 0.765625, batch_len: 130.000000
Train, loss=0.11450849: 402it [10:29,  1.48s/it]2017-06-02 14:25:29,151 root  INFO     step 402.000000 - time: 1.508018, loss: 0.152972, perplexity: 1.165293, precision: 0.812500, batch_len: 126.000000
Train, loss=0.15297216: 403it [10:31,  1.52s/it]2017-06-02 14:25:30,510 root  INFO     step 403.000000 - time: 1.294754, loss: 0.169260, perplexity: 1.184428, precision: 0.765625, batch_len: 119.000000
Train, loss=0.16925961: 404it [10:32,  1.47s/it]2017-06-02 14:25:31,742 root  INFO     step 404.000000 - time: 1.223194, loss: 0.562076, perplexity: 1.754311, precision: 0.531250, batch_len: 85.000000
Train, loss=0.56207621: 405it [10:33,  1.40s/it]2017-06-02 14:25:32,826 root  INFO     step 405.000000 - time: 1.035671, loss: 1.103070, perplexity: 3.013403, precision: 0.328125, batch_len: 79.000000
Train, loss=1.10307002: 406it [10:34,  1.31s/it]2017-06-02 14:25:34,038 root  INFO     step 406.000000 - time: 1.026569, loss: 0.188491, perplexity: 1.207427, precision: 0.718750, batch_len: 98.000000
Train, loss=0.18849130: 407it [10:36,  1.28s/it]2017-06-02 14:25:35,049 root  INFO     step 407.000000 - time: 0.928332, loss: 0.164188, perplexity: 1.178436, precision: 0.765625, batch_len: 78.000000
Train, loss=0.16418836: 408it [10:37,  1.20s/it]2017-06-02 14:25:36,031 root  INFO     step 408.000000 - time: 0.935578, loss: 0.089347, perplexity: 1.093460, precision: 0.890625, batch_len: 96.000000
Train, loss=0.08934680: 409it [10:38,  1.13s/it]2017-06-02 14:25:37,509 root  INFO     step 409.000000 - time: 1.402544, loss: 0.156823, perplexity: 1.169789, precision: 0.687500, batch_len: 136.000000
Train, loss=0.15682304: 410it [10:39,  1.24s/it]2017-06-02 14:25:38,851 root  INFO     step 410.000000 - time: 1.245178, loss: 0.171724, perplexity: 1.187350, precision: 0.781250, batch_len: 84.000000
Train, loss=0.17172426: 411it [10:40,  1.27s/it]2017-06-02 14:25:40,236 root  INFO     step 411.000000 - time: 1.379052, loss: 0.117438, perplexity: 1.124612, precision: 0.796875, batch_len: 122.000000
Train, loss=0.11743798: 412it [10:42,  1.30s/it]2017-06-02 14:25:42,450 root  INFO     step 412.000000 - time: 1.863275, loss: 0.098119, perplexity: 1.103094, precision: 0.765625, batch_len: 152.000000
Train, loss=0.09811907: 413it [10:44,  1.58s/it]2017-06-02 14:25:43,921 root  INFO     step 413.000000 - time: 1.285564, loss: 0.080044, perplexity: 1.083334, precision: 0.921875, batch_len: 72.000000
Train, loss=0.08004373: 414it [10:45,  1.54s/it]2017-06-02 14:25:45,201 root  INFO     step 414.000000 - time: 1.240929, loss: 0.061760, perplexity: 1.063707, precision: 0.906250, batch_len: 74.000000
Train, loss=0.06176024: 415it [10:47,  1.47s/it]2017-06-02 14:25:46,789 root  INFO     step 415.000000 - time: 1.525619, loss: 0.094178, perplexity: 1.098755, precision: 0.843750, batch_len: 138.000000
Train, loss=0.09417754: 416it [10:48,  1.50s/it]2017-06-02 14:25:47,845 root  INFO     step 416.000000 - time: 1.011113, loss: 0.132098, perplexity: 1.141221, precision: 0.812500, batch_len: 76.000000
Train, loss=0.13209841: 417it [10:49,  1.37s/it]2017-06-02 14:25:49,251 root  INFO     step 417.000000 - time: 1.388922, loss: 0.131607, perplexity: 1.140660, precision: 0.828125, batch_len: 139.000000
Train, loss=0.13160707: 418it [10:51,  1.38s/it]2017-06-02 14:25:50,324 root  INFO     step 418.000000 - time: 1.045184, loss: 0.065359, perplexity: 1.067542, precision: 0.906250, batch_len: 77.000000
Train, loss=0.06535925: 419it [10:52,  1.29s/it]2017-06-02 14:25:52,099 root  INFO     step 419.000000 - time: 1.771346, loss: 0.104559, perplexity: 1.110221, precision: 0.765625, batch_len: 132.000000
Train, loss=0.10455948: 420it [10:54,  1.43s/it]2017-06-02 14:25:54,091 root  INFO     step 420.000000 - time: 1.876898, loss: 0.114837, perplexity: 1.121691, precision: 0.781250, batch_len: 150.000000
Train, loss=0.11483727: 421it [10:56,  1.60s/it]2017-06-02 14:25:55,566 root  INFO     step 421.000000 - time: 1.361075, loss: 0.072312, perplexity: 1.074991, precision: 0.906250, batch_len: 131.000000
Train, loss=0.07231188: 422it [10:57,  1.56s/it]2017-06-02 14:25:56,857 root  INFO     step 422.000000 - time: 1.220114, loss: 0.181153, perplexity: 1.198599, precision: 0.828125, batch_len: 141.000000
Train, loss=0.18115300: 423it [10:58,  1.48s/it]2017-06-02 14:25:58,774 root  INFO     step 423.000000 - time: 1.687123, loss: 0.120628, perplexity: 1.128206, precision: 0.765625, batch_len: 134.000000
Train, loss=0.12062834: 424it [11:00,  1.61s/it]2017-06-02 14:26:00,339 root  INFO     step 424.000000 - time: 1.474647, loss: 0.087964, perplexity: 1.091949, precision: 0.828125, batch_len: 142.000000
Train, loss=0.08796404: 425it [11:02,  1.60s/it]2017-06-02 14:26:01,244 root  INFO     step 425.000000 - time: 0.880876, loss: 0.095405, perplexity: 1.100104, precision: 0.859375, batch_len: 71.000000
Train, loss=0.09540486: 426it [11:03,  1.39s/it]2017-06-02 14:26:01,437 root  INFO     Generating first batch)
2017-06-02 14:26:05,890 root  INFO     step 426.000000 - time: 1.341218, loss: 0.065207, perplexity: 1.067380, precision: 0.812500, batch_len: 128.000000
Train, loss=0.06520708: 427it [11:07,  2.37s/it]2017-06-02 14:26:06,903 root  INFO     step 427.000000 - time: 0.994249, loss: 0.105476, perplexity: 1.111240, precision: 0.828125, batch_len: 92.000000
Train, loss=0.10547639: 428it [11:08,  1.96s/it]2017-06-02 14:26:07,841 root  INFO     step 428.000000 - time: 0.923434, loss: 0.056346, perplexity: 1.057963, precision: 0.921875, batch_len: 96.000000
Train, loss=0.05634566: 429it [11:09,  1.65s/it]2017-06-02 14:26:09,163 root  INFO     step 429.000000 - time: 1.275662, loss: 0.246780, perplexity: 1.279898, precision: 0.781250, batch_len: 113.000000
Train, loss=0.24678013: 430it [11:11,  1.55s/it]2017-06-02 14:26:10,532 root  INFO     step 430.000000 - time: 1.292945, loss: 0.129227, perplexity: 1.137948, precision: 0.796875, batch_len: 110.000000
Train, loss=0.12922707: 431it [11:12,  1.50s/it]2017-06-02 14:26:11,486 root  INFO     step 431.000000 - time: 0.921514, loss: 0.130609, perplexity: 1.139522, precision: 0.875000, batch_len: 88.000000
Train, loss=0.13060877: 432it [11:13,  1.34s/it]2017-06-02 14:26:12,968 root  INFO     step 432.000000 - time: 1.338767, loss: 0.163108, perplexity: 1.177164, precision: 0.812500, batch_len: 120.000000
Train, loss=0.16310783: 433it [11:14,  1.38s/it]2017-06-02 14:26:14,095 root  INFO     step 433.000000 - time: 1.002433, loss: 0.194247, perplexity: 1.214396, precision: 0.750000, batch_len: 115.000000
Train, loss=0.19424689: 434it [11:16,  1.30s/it]2017-06-02 14:26:15,219 root  INFO     step 434.000000 - time: 1.112971, loss: 0.197069, perplexity: 1.217828, precision: 0.718750, batch_len: 111.000000
Train, loss=0.19706860: 435it [11:17,  1.25s/it]2017-06-02 14:26:16,574 root  INFO     step 435.000000 - time: 1.310781, loss: 0.158755, perplexity: 1.172050, precision: 0.781250, batch_len: 104.000000
Train, loss=0.15875474: 436it [11:18,  1.28s/it]2017-06-02 14:26:17,632 root  INFO     step 436.000000 - time: 1.008689, loss: 0.172965, perplexity: 1.188824, precision: 0.796875, batch_len: 93.000000
Train, loss=0.17296453: 437it [11:19,  1.21s/it]2017-06-02 14:26:18,795 root  INFO     step 437.000000 - time: 1.079540, loss: 0.110387, perplexity: 1.116710, precision: 0.781250, batch_len: 101.000000
Train, loss=0.11038689: 438it [11:20,  1.20s/it]2017-06-02 14:26:20,077 root  INFO     step 438.000000 - time: 1.077301, loss: 0.100906, perplexity: 1.106173, precision: 0.781250, batch_len: 109.000000
Train, loss=0.10090630: 439it [11:22,  1.22s/it]2017-06-02 14:26:21,036 root  INFO     step 439.000000 - time: 0.903808, loss: 0.185155, perplexity: 1.203405, precision: 0.734375, batch_len: 89.000000
Train, loss=0.18515474: 440it [11:23,  1.14s/it]2017-06-02 14:26:22,329 root  INFO     step 440.000000 - time: 1.203541, loss: 0.176910, perplexity: 1.193524, precision: 0.656250, batch_len: 100.000000
Train, loss=0.17690992: 441it [11:24,  1.19s/it]2017-06-02 14:26:23,632 root  INFO     step 441.000000 - time: 1.264403, loss: 0.235066, perplexity: 1.264992, precision: 0.625000, batch_len: 105.000000
Train, loss=0.23506586: 442it [11:25,  1.22s/it]2017-06-02 14:26:25,228 root  INFO     step 442.000000 - time: 1.577486, loss: 0.239985, perplexity: 1.271230, precision: 0.750000, batch_len: 90.000000
Train, loss=0.23998505: 443it [11:27,  1.34s/it]2017-06-02 14:26:26,275 root  INFO     step 443.000000 - time: 1.030738, loss: 0.180988, perplexity: 1.198400, precision: 0.750000, batch_len: 80.000000
Train, loss=0.18098773: 444it [11:28,  1.25s/it]2017-06-02 14:26:27,498 root  INFO     step 444.000000 - time: 1.150367, loss: 0.145634, perplexity: 1.156772, precision: 0.828125, batch_len: 91.000000
Train, loss=0.14563362: 445it [11:29,  1.24s/it]2017-06-02 14:26:29,071 root  INFO     step 445.000000 - time: 1.530192, loss: 0.084547, perplexity: 1.088224, precision: 0.796875, batch_len: 124.000000
Train, loss=0.08454689: 446it [11:31,  1.34s/it]2017-06-02 14:26:30,128 root  INFO     step 446.000000 - time: 1.029979, loss: 0.207591, perplexity: 1.230709, precision: 0.734375, batch_len: 108.000000
Train, loss=0.20759061: 447it [11:32,  1.26s/it]2017-06-02 14:26:31,353 root  INFO     step 447.000000 - time: 1.214937, loss: 0.201142, perplexity: 1.222798, precision: 0.703125, batch_len: 97.000000
Train, loss=0.20114204: 448it [11:33,  1.25s/it]2017-06-02 14:26:32,913 root  INFO     step 448.000000 - time: 1.472819, loss: 0.119745, perplexity: 1.127209, precision: 0.781250, batch_len: 118.000000
Train, loss=0.11974479: 449it [11:34,  1.34s/it]2017-06-02 14:26:34,016 root  INFO     step 449.000000 - time: 1.090093, loss: 0.137196, perplexity: 1.147053, precision: 0.781250, batch_len: 107.000000
Train, loss=0.13719606: 450it [11:35,  1.27s/it]2017-06-02 14:26:35,322 root  INFO     step 450.000000 - time: 1.291851, loss: 0.111461, perplexity: 1.117910, precision: 0.796875, batch_len: 121.000000
Train, loss=0.11146089: 451it [11:37,  1.28s/it]2017-06-02 14:26:36,248 root  INFO     step 451.000000 - time: 0.917073, loss: 0.201166, perplexity: 1.222827, precision: 0.781250, batch_len: 85.000000
Train, loss=0.20116565: 452it [11:38,  1.17s/it]2017-06-02 14:26:37,815 root  INFO     step 452.000000 - time: 1.539269, loss: 0.076558, perplexity: 1.079564, precision: 0.921875, batch_len: 117.000000
Train, loss=0.07655755: 453it [11:39,  1.29s/it]2017-06-02 14:26:38,973 root  INFO     step 453.000000 - time: 1.113766, loss: 0.112607, perplexity: 1.119192, precision: 0.843750, batch_len: 81.000000
Train, loss=0.11260693: 454it [11:40,  1.25s/it]2017-06-02 14:26:40,016 root  INFO     step 454.000000 - time: 1.031529, loss: 0.115332, perplexity: 1.122246, precision: 0.765625, batch_len: 102.000000
Train, loss=0.11533179: 455it [11:41,  1.19s/it]2017-06-02 14:26:41,126 root  INFO     step 455.000000 - time: 0.994066, loss: 0.091886, perplexity: 1.096240, precision: 0.828125, batch_len: 103.000000
Train, loss=0.09188604: 456it [11:43,  1.17s/it]2017-06-02 14:26:42,471 root  INFO     step 456.000000 - time: 1.337706, loss: 0.226615, perplexity: 1.254347, precision: 0.625000, batch_len: 112.000000
Train, loss=0.22661546: 457it [11:44,  1.22s/it]2017-06-02 14:26:43,590 root  INFO     step 457.000000 - time: 1.015674, loss: 0.164591, perplexity: 1.178911, precision: 0.750000, batch_len: 106.000000
Train, loss=0.16459134: 458it [11:45,  1.19s/it]2017-06-02 14:26:44,830 root  INFO     step 458.000000 - time: 1.185860, loss: 0.163453, perplexity: 1.177570, precision: 0.812500, batch_len: 94.000000
Train, loss=0.16345268: 459it [11:46,  1.20s/it]2017-06-02 14:26:46,545 root  INFO     step 459.000000 - time: 1.418716, loss: 0.096222, perplexity: 1.101003, precision: 0.828125, batch_len: 129.000000
Train, loss=0.09622160: 460it [11:48,  1.36s/it]2017-06-02 14:26:47,658 root  INFO     step 460.000000 - time: 1.036180, loss: 0.132677, perplexity: 1.141881, precision: 0.843750, batch_len: 99.000000
Train, loss=0.13267694: 461it [11:49,  1.28s/it]2017-06-02 14:26:48,842 root  INFO     step 461.000000 - time: 1.077155, loss: 0.173620, perplexity: 1.189603, precision: 0.750000, batch_len: 114.000000
Train, loss=0.17361985: 462it [11:50,  1.25s/it]2017-06-02 14:26:50,128 root  INFO     step 462.000000 - time: 1.247382, loss: 0.124917, perplexity: 1.133054, precision: 0.828125, batch_len: 125.000000
Train, loss=0.12491695: 463it [11:52,  1.26s/it]2017-06-02 14:26:51,236 root  INFO     step 463.000000 - time: 1.088747, loss: 0.206928, perplexity: 1.229894, precision: 0.750000, batch_len: 79.000000
Train, loss=0.20692779: 464it [11:53,  1.22s/it]2017-06-02 14:26:52,391 root  INFO     step 464.000000 - time: 1.122452, loss: 0.198257, perplexity: 1.219275, precision: 0.843750, batch_len: 86.000000
Train, loss=0.19825658: 465it [11:54,  1.20s/it]2017-06-02 14:26:53,422 root  INFO     step 465.000000 - time: 0.988139, loss: 0.122092, perplexity: 1.129858, precision: 0.812500, batch_len: 87.000000
Train, loss=0.12209222: 466it [11:55,  1.15s/it]2017-06-02 14:26:54,438 root  INFO     step 466.000000 - time: 0.953813, loss: 0.202149, perplexity: 1.224031, precision: 0.796875, batch_len: 83.000000
Train, loss=0.20214936: 467it [11:56,  1.11s/it]2017-06-02 14:26:55,739 root  INFO     step 467.000000 - time: 1.280983, loss: 0.149784, perplexity: 1.161583, precision: 0.765625, batch_len: 126.000000
Train, loss=0.14978372: 468it [11:57,  1.17s/it]2017-06-02 14:26:57,017 root  INFO     step 468.000000 - time: 1.253904, loss: 0.110753, perplexity: 1.117119, precision: 0.765625, batch_len: 123.000000
Train, loss=0.11075302: 469it [11:58,  1.20s/it]2017-06-02 14:26:58,686 root  INFO     step 469.000000 - time: 1.622552, loss: 0.038476, perplexity: 1.039226, precision: 0.937500, batch_len: 116.000000
Train, loss=0.03847580: 470it [12:00,  1.34s/it]2017-06-02 14:27:00,265 root  INFO     step 470.000000 - time: 1.392957, loss: 0.086041, perplexity: 1.089851, precision: 0.859375, batch_len: 144.000000
Train, loss=0.08604141: 471it [12:02,  1.41s/it]2017-06-02 14:27:01,735 root  INFO     step 471.000000 - time: 1.336434, loss: 0.131441, perplexity: 1.140471, precision: 0.812500, batch_len: 137.000000
Train, loss=0.13144121: 472it [12:03,  1.43s/it]2017-06-02 14:27:02,760 root  INFO     step 472.000000 - time: 1.003712, loss: 0.065451, perplexity: 1.067640, precision: 0.906250, batch_len: 98.000000
Train, loss=0.06545056: 473it [12:04,  1.31s/it]2017-06-02 14:27:03,849 root  INFO     step 473.000000 - time: 0.998635, loss: 0.100835, perplexity: 1.106095, precision: 0.812500, batch_len: 82.000000
Train, loss=0.10083544: 474it [12:05,  1.24s/it]2017-06-02 14:27:05,586 root  INFO     step 474.000000 - time: 1.660575, loss: 0.097036, perplexity: 1.101900, precision: 0.875000, batch_len: 119.000000
Train, loss=0.09703616: 475it [12:07,  1.39s/it]2017-06-02 14:27:06,773 root  INFO     step 475.000000 - time: 1.049112, loss: 0.166125, perplexity: 1.180720, precision: 0.796875, batch_len: 84.000000
Train, loss=0.16612454: 476it [12:08,  1.33s/it]2017-06-02 14:27:08,244 root  INFO     step 476.000000 - time: 1.330402, loss: 0.148651, perplexity: 1.160269, precision: 0.781250, batch_len: 135.000000
Train, loss=0.14865148: 477it [12:10,  1.37s/it]2017-06-02 14:27:09,614 root  INFO     step 477.000000 - time: 1.290433, loss: 0.166106, perplexity: 1.180698, precision: 0.687500, batch_len: 133.000000
Train, loss=0.16610618: 478it [12:11,  1.37s/it]2017-06-02 14:27:11,288 root  INFO     step 478.000000 - time: 1.586601, loss: 0.304634, perplexity: 1.356129, precision: 0.531250, batch_len: 130.000000
Train, loss=0.30463445: 479it [12:13,  1.46s/it]2017-06-02 14:27:12,629 root  INFO     step 479.000000 - time: 1.323311, loss: 0.242047, perplexity: 1.273855, precision: 0.687500, batch_len: 72.000000
Train, loss=0.24204746: 480it [12:14,  1.43s/it]2017-06-02 14:27:14,231 root  INFO     step 480.000000 - time: 1.369699, loss: 0.184866, perplexity: 1.203057, precision: 0.718750, batch_len: 122.000000
Train, loss=0.18486595: 481it [12:16,  1.48s/it]2017-06-02 14:27:15,181 root  INFO     step 481.000000 - time: 0.909132, loss: 0.175462, perplexity: 1.191797, precision: 0.781250, batch_len: 78.000000
Train, loss=0.17546242: 482it [12:17,  1.32s/it]2017-06-02 14:27:16,684 root  INFO     step 482.000000 - time: 1.316779, loss: 0.267633, perplexity: 1.306867, precision: 0.609375, batch_len: 134.000000
Train, loss=0.26763254: 483it [12:18,  1.37s/it]2017-06-02 14:27:18,441 root  INFO     step 483.000000 - time: 1.747900, loss: 0.280381, perplexity: 1.323634, precision: 0.406250, batch_len: 136.000000
Train, loss=0.28038114: 484it [12:20,  1.49s/it]2017-06-02 14:27:19,591 root  INFO     step 484.000000 - time: 0.996518, loss: 0.470055, perplexity: 1.600082, precision: 0.546875, batch_len: 96.000000
Train, loss=0.47005516: 485it [12:21,  1.39s/it]2017-06-02 14:27:21,060 root  INFO     step 485.000000 - time: 1.397532, loss: 0.502577, perplexity: 1.652975, precision: 0.343750, batch_len: 138.000000
Train, loss=0.50257671: 486it [12:23,  1.41s/it]2017-06-02 14:27:22,509 root  INFO     step 486.000000 - time: 1.406761, loss: 0.619283, perplexity: 1.857596, precision: 0.140625, batch_len: 139.000000
Train, loss=0.61928332: 487it [12:24,  1.42s/it]2017-06-02 14:27:24,383 root  INFO     step 487.000000 - time: 1.633178, loss: 0.641757, perplexity: 1.899817, precision: 0.234375, batch_len: 142.000000
Train, loss=0.64175737: 488it [12:26,  1.56s/it]2017-06-02 14:27:25,516 root  INFO     step 488.000000 - time: 1.056164, loss: 0.336451, perplexity: 1.399970, precision: 0.515625, batch_len: 74.000000
Train, loss=0.33645082: 489it [12:27,  1.43s/it]2017-06-02 14:27:27,018 root  INFO     step 489.000000 - time: 1.442543, loss: 0.607004, perplexity: 1.834927, precision: 0.390625, batch_len: 141.000000
Train, loss=0.60700446: 490it [12:28,  1.45s/it]2017-06-02 14:27:29,071 root  INFO     step 490.000000 - time: 2.033401, loss: 0.674365, perplexity: 1.962786, precision: 0.265625, batch_len: 152.000000
Train, loss=0.67436469: 491it [12:31,  1.63s/it]2017-06-02 14:27:29,951 root  INFO     step 491.000000 - time: 0.860349, loss: 0.177968, perplexity: 1.194788, precision: 0.703125, batch_len: 71.000000
Train, loss=0.17796837: 492it [12:31,  1.41s/it]2017-06-02 14:27:31,060 root  INFO     step 492.000000 - time: 1.061977, loss: 0.219984, perplexity: 1.246056, precision: 0.718750, batch_len: 77.000000
Train, loss=0.21998370: 493it [12:33,  1.32s/it]2017-06-02 14:27:32,044 root  INFO     step 493.000000 - time: 0.948365, loss: 0.239521, perplexity: 1.270640, precision: 0.640625, batch_len: 76.000000
Train, loss=0.23952106: 494it [12:34,  1.22s/it]2017-06-02 14:27:33,794 root  INFO     step 494.000000 - time: 1.660336, loss: 0.146197, perplexity: 1.157425, precision: 0.734375, batch_len: 132.000000
Train, loss=0.14619729: 495it [12:35,  1.38s/it]2017-06-02 14:27:35,869 root  INFO     step 495.000000 - time: 1.864086, loss: 0.226008, perplexity: 1.253586, precision: 0.593750, batch_len: 150.000000
Train, loss=0.22600797: 496it [12:37,  1.59s/it]2017-06-02 14:27:37,292 root  INFO     step 496.000000 - time: 1.354888, loss: 0.143178, perplexity: 1.153935, precision: 0.687500, batch_len: 131.000000
Train, loss=0.14317806: 497it [12:39,  1.54s/it]2017-06-02 14:27:37,477 root  INFO     Generating first batch)
2017-06-02 14:27:41,258 root  INFO     step 497.000000 - time: 1.010244, loss: 0.103625, perplexity: 1.109184, precision: 0.859375, batch_len: 92.000000
Train, loss=0.10362467: 498it [12:43,  2.27s/it]2017-06-02 14:27:42,559 root  INFO     step 498.000000 - time: 0.995301, loss: 0.266461, perplexity: 1.305337, precision: 0.625000, batch_len: 113.000000
Train, loss=0.26646098: 499it [12:44,  1.98s/it]2017-06-02 14:27:43,814 root  INFO     step 499.000000 - time: 1.240519, loss: 0.121201, perplexity: 1.128852, precision: 0.765625, batch_len: 110.000000
Train, loss=0.12120135: 500it [12:45,  1.76s/it]2017-06-02 14:27:45,461 root  INFO     step 500.000000 - time: 1.623277, loss: 0.132021, perplexity: 1.141132, precision: 0.812500, batch_len: 120.000000
Train, loss=0.13202086: 501it [12:47,  1.73s/it]2017-06-02 14:27:46,549 root  INFO     step 501.000000 - time: 1.067817, loss: 0.160313, perplexity: 1.173878, precision: 0.781250, batch_len: 105.000000
Train, loss=0.16031276: 502it [12:48,  1.53s/it]2017-06-02 14:27:47,779 root  INFO     step 502.000000 - time: 1.086832, loss: 0.133099, perplexity: 1.142363, precision: 0.828125, batch_len: 101.000000
Train, loss=0.13309911: 503it [12:49,  1.44s/it]2017-06-02 14:27:48,871 root  INFO     step 503.000000 - time: 1.086398, loss: 0.213549, perplexity: 1.238064, precision: 0.703125, batch_len: 111.000000
Train, loss=0.21354875: 504it [12:50,  1.34s/it]2017-06-02 14:27:49,787 root  INFO     step 504.000000 - time: 0.860229, loss: 0.113333, perplexity: 1.120005, precision: 0.859375, batch_len: 96.000000
Train, loss=0.11333338: 505it [12:51,  1.21s/it]2017-06-02 14:27:51,180 root  INFO     step 505.000000 - time: 1.171794, loss: 0.282069, perplexity: 1.325870, precision: 0.703125, batch_len: 93.000000
Train, loss=0.28206852: 506it [12:53,  1.27s/it]2017-06-02 14:27:52,370 root  INFO     step 506.000000 - time: 1.180597, loss: 0.139917, perplexity: 1.150178, precision: 0.750000, batch_len: 97.000000
Train, loss=0.13991678: 507it [12:54,  1.24s/it]2017-06-02 14:27:53,942 root  INFO     step 507.000000 - time: 1.417131, loss: 0.121944, perplexity: 1.129691, precision: 0.812500, batch_len: 128.000000
Train, loss=0.12194439: 508it [12:55,  1.34s/it]2017-06-02 14:27:55,050 root  INFO     step 508.000000 - time: 1.061928, loss: 0.146675, perplexity: 1.157977, precision: 0.796875, batch_len: 108.000000
Train, loss=0.14667456: 509it [12:57,  1.27s/it]2017-06-02 14:27:55,991 root  INFO     step 509.000000 - time: 0.915028, loss: 0.262481, perplexity: 1.300152, precision: 0.687500, batch_len: 88.000000
Train, loss=0.26248145: 510it [12:57,  1.17s/it]2017-06-02 14:27:57,410 root  INFO     step 510.000000 - time: 1.278274, loss: 0.100092, perplexity: 1.105273, precision: 0.843750, batch_len: 100.000000
Train, loss=0.10009193: 511it [12:59,  1.25s/it]2017-06-02 14:27:58,620 root  INFO     step 511.000000 - time: 1.192964, loss: 0.160064, perplexity: 1.173586, precision: 0.812500, batch_len: 85.000000
Train, loss=0.16006443: 512it [13:00,  1.24s/it]2017-06-02 14:27:59,605 root  INFO     step 512.000000 - time: 0.967006, loss: 0.079782, perplexity: 1.083051, precision: 0.843750, batch_len: 90.000000
Train, loss=0.07978193: 513it [13:01,  1.16s/it]2017-06-02 14:28:00,624 root  INFO     step 513.000000 - time: 0.954737, loss: 0.163654, perplexity: 1.177807, precision: 0.781250, batch_len: 89.000000
Train, loss=0.16365440: 514it [13:02,  1.12s/it]2017-06-02 14:28:01,606 root  INFO     step 514.000000 - time: 0.976724, loss: 0.180873, perplexity: 1.198263, precision: 0.781250, batch_len: 81.000000
Train, loss=0.18087327: 515it [13:03,  1.08s/it]2017-06-02 14:28:02,736 root  INFO     step 515.000000 - time: 1.041171, loss: 0.174759, perplexity: 1.190959, precision: 0.812500, batch_len: 112.000000
Train, loss=0.17475876: 516it [13:04,  1.09s/it]2017-06-02 14:28:03,992 root  INFO     step 516.000000 - time: 1.203703, loss: 0.088763, perplexity: 1.092822, precision: 0.812500, batch_len: 104.000000
Train, loss=0.08876342: 517it [13:05,  1.14s/it]2017-06-02 14:28:05,521 root  INFO     step 517.000000 - time: 1.475564, loss: 0.062421, perplexity: 1.064410, precision: 0.921875, batch_len: 117.000000
Train, loss=0.06242051: 518it [13:07,  1.26s/it]2017-06-02 14:28:06,529 root  INFO     step 518.000000 - time: 0.967292, loss: 0.153624, perplexity: 1.166053, precision: 0.781250, batch_len: 102.000000
Train, loss=0.15362437: 519it [13:08,  1.18s/it]2017-06-02 14:28:07,587 root  INFO     step 519.000000 - time: 0.975777, loss: 0.171253, perplexity: 1.186791, precision: 0.718750, batch_len: 91.000000
Train, loss=0.17125322: 520it [13:09,  1.15s/it]2017-06-02 14:28:08,567 root  INFO     step 520.000000 - time: 0.951113, loss: 0.148523, perplexity: 1.160119, precision: 0.750000, batch_len: 84.000000
Train, loss=0.14852299: 521it [13:10,  1.10s/it]2017-06-02 14:28:09,496 root  INFO     step 521.000000 - time: 0.912898, loss: 0.122705, perplexity: 1.130551, precision: 0.828125, batch_len: 86.000000
Train, loss=0.12270523: 522it [13:11,  1.05s/it]2017-06-02 14:28:10,800 root  INFO     step 522.000000 - time: 1.234771, loss: 0.162077, perplexity: 1.175951, precision: 0.750000, batch_len: 114.000000
Train, loss=0.16207695: 523it [13:12,  1.12s/it]2017-06-02 14:28:12,325 root  INFO     step 523.000000 - time: 1.467359, loss: 0.087703, perplexity: 1.091664, precision: 0.859375, batch_len: 124.000000
Train, loss=0.08770312: 524it [13:14,  1.24s/it]2017-06-02 14:28:13,508 root  INFO     step 524.000000 - time: 1.011251, loss: 0.090266, perplexity: 1.094466, precision: 0.859375, batch_len: 103.000000
Train, loss=0.09026626: 525it [13:15,  1.23s/it]2017-06-02 14:28:14,622 root  INFO     step 525.000000 - time: 1.094190, loss: 0.134701, perplexity: 1.144195, precision: 0.859375, batch_len: 107.000000
Train, loss=0.13470095: 526it [13:16,  1.19s/it]2017-06-02 14:28:15,739 root  INFO     step 526.000000 - time: 1.074372, loss: 0.100604, perplexity: 1.105838, precision: 0.796875, batch_len: 106.000000
Train, loss=0.10060358: 527it [13:17,  1.17s/it]2017-06-02 14:28:16,711 root  INFO     step 527.000000 - time: 0.942695, loss: 0.185197, perplexity: 1.203455, precision: 0.812500, batch_len: 94.000000
Train, loss=0.18519661: 528it [13:18,  1.11s/it]2017-06-02 14:28:18,484 root  INFO     step 528.000000 - time: 1.664039, loss: 0.121611, perplexity: 1.129315, precision: 0.843750, batch_len: 125.000000
Train, loss=0.12161110: 529it [13:20,  1.31s/it]2017-06-02 14:28:19,874 root  INFO     step 529.000000 - time: 1.336736, loss: 0.136390, perplexity: 1.146129, precision: 0.718750, batch_len: 137.000000
Train, loss=0.13639040: 530it [13:21,  1.33s/it]2017-06-02 14:28:21,284 root  INFO     step 530.000000 - time: 1.372155, loss: 0.059654, perplexity: 1.061469, precision: 0.875000, batch_len: 118.000000
Train, loss=0.05965365: 531it [13:23,  1.36s/it]2017-06-02 14:28:22,384 root  INFO     step 531.000000 - time: 1.049475, loss: 0.059644, perplexity: 1.061458, precision: 0.890625, batch_len: 98.000000
Train, loss=0.05964383: 532it [13:24,  1.28s/it]2017-06-02 14:28:23,649 root  INFO     step 532.000000 - time: 1.253684, loss: 0.057598, perplexity: 1.059290, precision: 0.906250, batch_len: 121.000000
Train, loss=0.05759846: 533it [13:25,  1.28s/it]2017-06-02 14:28:25,107 root  INFO     step 533.000000 - time: 1.369452, loss: 0.113768, perplexity: 1.120492, precision: 0.796875, batch_len: 109.000000
Train, loss=0.11376777: 534it [13:27,  1.33s/it]2017-06-02 14:28:26,317 root  INFO     step 534.000000 - time: 1.197773, loss: 0.111643, perplexity: 1.118114, precision: 0.890625, batch_len: 115.000000
Train, loss=0.11164300: 535it [13:28,  1.29s/it]2017-06-02 14:28:27,357 root  INFO     step 535.000000 - time: 1.019275, loss: 0.127988, perplexity: 1.136539, precision: 0.859375, batch_len: 80.000000
Train, loss=0.12798776: 536it [13:29,  1.22s/it]2017-06-02 14:28:28,954 root  INFO     step 536.000000 - time: 1.449706, loss: 0.122226, perplexity: 1.130009, precision: 0.828125, batch_len: 123.000000
Train, loss=0.12222569: 537it [13:30,  1.33s/it]2017-06-02 14:28:30,148 root  INFO     step 537.000000 - time: 1.187055, loss: 0.094500, perplexity: 1.099110, precision: 0.875000, batch_len: 116.000000
Train, loss=0.09450044: 538it [13:32,  1.29s/it]2017-06-02 14:28:31,251 root  INFO     step 538.000000 - time: 1.036581, loss: 0.084541, perplexity: 1.088218, precision: 0.875000, batch_len: 79.000000
Train, loss=0.08454114: 539it [13:33,  1.23s/it]2017-06-02 14:28:32,323 root  INFO     step 539.000000 - time: 1.062421, loss: 0.082557, perplexity: 1.086061, precision: 0.890625, batch_len: 83.000000
Train, loss=0.08255702: 540it [13:34,  1.19s/it]2017-06-02 14:28:34,070 root  INFO     step 540.000000 - time: 1.704007, loss: 0.103040, perplexity: 1.108536, precision: 0.812500, batch_len: 130.000000
Train, loss=0.10304040: 541it [13:36,  1.35s/it]2017-06-02 14:28:35,545 root  INFO     step 541.000000 - time: 1.324029, loss: 0.085159, perplexity: 1.088890, precision: 0.859375, batch_len: 126.000000
Train, loss=0.08515853: 542it [13:37,  1.39s/it]2017-06-02 14:28:37,002 root  INFO     step 542.000000 - time: 1.416657, loss: 0.088826, perplexity: 1.092890, precision: 0.828125, batch_len: 129.000000
Train, loss=0.08882597: 543it [13:38,  1.41s/it]2017-06-02 14:28:38,349 root  INFO     step 543.000000 - time: 1.288951, loss: 0.103324, perplexity: 1.108851, precision: 0.843750, batch_len: 135.000000
Train, loss=0.10332406: 544it [13:40,  1.39s/it]2017-06-02 14:28:39,459 root  INFO     step 544.000000 - time: 1.075016, loss: 0.085327, perplexity: 1.089073, precision: 0.781250, batch_len: 87.000000
Train, loss=0.08532678: 545it [13:41,  1.31s/it]2017-06-02 14:28:40,816 root  INFO     step 545.000000 - time: 1.240928, loss: 0.065490, perplexity: 1.067682, precision: 0.875000, batch_len: 99.000000
Train, loss=0.06549040: 546it [13:42,  1.32s/it]2017-06-02 14:28:42,258 root  INFO     step 546.000000 - time: 1.367739, loss: 0.074795, perplexity: 1.077663, precision: 0.921875, batch_len: 122.000000
Train, loss=0.07479480: 547it [13:44,  1.36s/it]2017-06-02 14:28:43,892 root  INFO     step 547.000000 - time: 1.414088, loss: 0.078142, perplexity: 1.081276, precision: 0.843750, batch_len: 144.000000
Train, loss=0.07814156: 548it [13:45,  1.44s/it]2017-06-02 14:28:45,215 root  INFO     step 548.000000 - time: 1.313896, loss: 0.089713, perplexity: 1.093860, precision: 0.890625, batch_len: 119.000000
Train, loss=0.08971289: 549it [13:47,  1.41s/it]2017-06-02 14:28:47,021 root  INFO     step 549.000000 - time: 1.768710, loss: 0.111718, perplexity: 1.118198, precision: 0.718750, batch_len: 133.000000
Train, loss=0.11171825: 550it [13:48,  1.53s/it]2017-06-02 14:28:48,482 root  INFO     step 550.000000 - time: 1.350784, loss: 0.157899, perplexity: 1.171048, precision: 0.703125, batch_len: 136.000000
Train, loss=0.15789890: 551it [13:50,  1.51s/it]2017-06-02 14:28:49,450 root  INFO     step 551.000000 - time: 0.946048, loss: 0.084146, perplexity: 1.087788, precision: 0.859375, batch_len: 78.000000
Train, loss=0.08414643: 552it [13:51,  1.34s/it]2017-06-02 14:28:50,551 root  INFO     step 552.000000 - time: 0.951364, loss: 0.124583, perplexity: 1.132676, precision: 0.828125, batch_len: 82.000000
Train, loss=0.12458337: 553it [13:52,  1.27s/it]2017-06-02 14:28:51,988 root  INFO     step 553.000000 - time: 1.059268, loss: 0.053376, perplexity: 1.054826, precision: 0.921875, batch_len: 96.000000
Train, loss=0.05337560: 554it [13:53,  1.32s/it]2017-06-02 14:28:53,704 root  INFO     step 554.000000 - time: 1.670435, loss: 0.137214, perplexity: 1.147074, precision: 0.859375, batch_len: 138.000000
Train, loss=0.13721430: 555it [13:55,  1.44s/it]2017-06-02 14:28:55,417 root  INFO     step 555.000000 - time: 1.395483, loss: 0.140932, perplexity: 1.151346, precision: 0.796875, batch_len: 139.000000
Train, loss=0.14093187: 556it [13:57,  1.52s/it]2017-06-02 14:28:57,369 root  INFO     step 556.000000 - time: 1.812331, loss: 0.104800, perplexity: 1.110488, precision: 0.765625, batch_len: 152.000000
Train, loss=0.10479981: 557it [13:59,  1.65s/it]2017-06-02 14:28:58,459 root  INFO     step 557.000000 - time: 1.067496, loss: 0.121734, perplexity: 1.129453, precision: 0.843750, batch_len: 76.000000
Train, loss=0.12173378: 558it [14:00,  1.48s/it]2017-06-02 14:29:00,244 root  INFO     step 558.000000 - time: 1.747843, loss: 0.104746, perplexity: 1.110429, precision: 0.750000, batch_len: 134.000000
Train, loss=0.10474626: 559it [14:02,  1.57s/it]2017-06-02 14:29:01,686 root  INFO     step 559.000000 - time: 1.433602, loss: 0.069950, perplexity: 1.072454, precision: 0.875000, batch_len: 132.000000
Train, loss=0.06994981: 560it [14:03,  1.53s/it]2017-06-02 14:29:03,559 root  INFO     step 560.000000 - time: 1.821838, loss: 0.078207, perplexity: 1.081346, precision: 0.875000, batch_len: 150.000000
Train, loss=0.07820650: 561it [14:05,  1.64s/it]2017-06-02 14:29:04,849 root  INFO     step 561.000000 - time: 1.277902, loss: 0.120324, perplexity: 1.127863, precision: 0.843750, batch_len: 141.000000
Train, loss=0.12032443: 562it [14:06,  1.53s/it]2017-06-02 14:29:05,901 root  INFO     step 562.000000 - time: 1.035242, loss: 0.056303, perplexity: 1.057918, precision: 0.890625, batch_len: 74.000000
Train, loss=0.05630308: 563it [14:07,  1.39s/it]2017-06-02 14:29:07,329 root  INFO     step 563.000000 - time: 1.309553, loss: 0.071529, perplexity: 1.074150, precision: 0.906250, batch_len: 72.000000
Train, loss=0.07152946: 564it [14:09,  1.40s/it]2017-06-02 14:29:08,467 root  INFO     step 564.000000 - time: 0.986128, loss: 0.163983, perplexity: 1.178194, precision: 0.734375, batch_len: 77.000000
Train, loss=0.16398314: 565it [14:10,  1.32s/it]2017-06-02 14:29:10,010 root  INFO     step 565.000000 - time: 1.408274, loss: 0.183289, perplexity: 1.201162, precision: 0.687500, batch_len: 142.000000
Train, loss=0.18328911: 566it [14:11,  1.39s/it]2017-06-02 14:29:11,395 root  INFO     step 566.000000 - time: 1.353343, loss: 0.169873, perplexity: 1.185154, precision: 0.765625, batch_len: 131.000000
Train, loss=0.16987295: 567it [14:13,  1.39s/it]2017-06-02 14:29:12,527 root  INFO     step 567.000000 - time: 1.051427, loss: 0.093146, perplexity: 1.097622, precision: 0.828125, batch_len: 71.000000
Train, loss=0.09314577: 568it [14:14,  1.31s/it]2017-06-02 14:29:12,671 root  INFO     Generating first batch)
2017-06-02 14:29:16,609 root  INFO     step 568.000000 - time: 1.163541, loss: 0.080731, perplexity: 1.084080, precision: 0.781250, batch_len: 110.000000
Train, loss=0.08073125: 569it [14:18,  2.14s/it]2017-06-02 14:29:18,127 root  INFO     step 569.000000 - time: 1.326691, loss: 0.067637, perplexity: 1.069977, precision: 0.906250, batch_len: 96.000000
Train, loss=0.06763747: 570it [14:20,  1.95s/it]2017-06-02 14:29:19,488 root  INFO     step 570.000000 - time: 1.328864, loss: 0.148482, perplexity: 1.160072, precision: 0.796875, batch_len: 120.000000
Train, loss=0.14848232: 571it [14:21,  1.78s/it]2017-06-02 14:29:20,996 root  INFO     step 571.000000 - time: 1.398637, loss: 0.148778, perplexity: 1.160415, precision: 0.750000, batch_len: 128.000000
Train, loss=0.14877775: 572it [14:22,  1.70s/it]2017-06-02 14:29:21,981 root  INFO     step 572.000000 - time: 0.942737, loss: 0.229627, perplexity: 1.258131, precision: 0.718750, batch_len: 88.000000
Train, loss=0.22962743: 573it [14:23,  1.48s/it]2017-06-02 14:29:23,120 root  INFO     step 573.000000 - time: 1.053960, loss: 0.160426, perplexity: 1.174011, precision: 0.750000, batch_len: 113.000000
Train, loss=0.16042572: 574it [14:25,  1.38s/it]2017-06-02 14:29:24,348 root  INFO     step 574.000000 - time: 1.207129, loss: 0.135337, perplexity: 1.144923, precision: 0.843750, batch_len: 101.000000
Train, loss=0.13533695: 575it [14:26,  1.33s/it]2017-06-02 14:29:25,743 root  INFO     step 575.000000 - time: 1.192792, loss: 0.106197, perplexity: 1.112041, precision: 0.750000, batch_len: 106.000000
Train, loss=0.10619707: 576it [14:27,  1.35s/it]2017-06-02 14:29:26,942 root  INFO     step 576.000000 - time: 1.172554, loss: 0.110620, perplexity: 1.116971, precision: 0.796875, batch_len: 108.000000
Train, loss=0.11062014: 577it [14:28,  1.31s/it]2017-06-02 14:29:28,025 root  INFO     step 577.000000 - time: 1.060078, loss: 0.162355, perplexity: 1.176277, precision: 0.796875, batch_len: 93.000000
Train, loss=0.16235462: 578it [14:29,  1.24s/it]2017-06-02 14:29:29,243 root  INFO     step 578.000000 - time: 1.040117, loss: 0.099415, perplexity: 1.104524, precision: 0.812500, batch_len: 103.000000
Train, loss=0.09941462: 579it [14:31,  1.23s/it]2017-06-02 14:29:30,388 root  INFO     step 579.000000 - time: 1.114027, loss: 0.127677, perplexity: 1.136186, precision: 0.765625, batch_len: 97.000000
Train, loss=0.12767699: 580it [14:32,  1.21s/it]2017-06-02 14:29:31,350 root  INFO     step 580.000000 - time: 0.879555, loss: 0.186901, perplexity: 1.205508, precision: 0.781250, batch_len: 89.000000
Train, loss=0.18690102: 581it [14:33,  1.13s/it]2017-06-02 14:29:32,484 root  INFO     step 581.000000 - time: 1.126830, loss: 0.195377, perplexity: 1.215770, precision: 0.718750, batch_len: 92.000000
Train, loss=0.19537735: 582it [14:34,  1.13s/it]2017-06-02 14:29:33,756 root  INFO     step 582.000000 - time: 1.182686, loss: 0.111661, perplexity: 1.118133, precision: 0.796875, batch_len: 102.000000
Train, loss=0.11166052: 583it [14:35,  1.17s/it]2017-06-02 14:29:35,100 root  INFO     step 583.000000 - time: 1.339395, loss: 0.102577, perplexity: 1.108022, precision: 0.812500, batch_len: 112.000000
Train, loss=0.10257678: 584it [14:37,  1.23s/it]2017-06-02 14:29:36,328 root  INFO     step 584.000000 - time: 1.098066, loss: 0.119235, perplexity: 1.126635, precision: 0.812500, batch_len: 105.000000
Train, loss=0.11923548: 585it [14:38,  1.23s/it]2017-06-02 14:29:37,389 root  INFO     step 585.000000 - time: 1.049717, loss: 0.083820, perplexity: 1.087433, precision: 0.859375, batch_len: 104.000000
Train, loss=0.08381976: 586it [14:39,  1.18s/it]2017-06-02 14:29:38,488 root  INFO     step 586.000000 - time: 1.090339, loss: 0.167384, perplexity: 1.182209, precision: 0.781250, batch_len: 111.000000
Train, loss=0.16738442: 587it [14:40,  1.15s/it]2017-06-02 14:29:39,988 root  INFO     step 587.000000 - time: 1.429799, loss: 0.114585, perplexity: 1.121408, precision: 0.796875, batch_len: 109.000000
Train, loss=0.11458515: 588it [14:41,  1.26s/it]2017-06-02 14:29:41,107 root  INFO     step 588.000000 - time: 1.112944, loss: 0.147328, perplexity: 1.158735, precision: 0.828125, batch_len: 107.000000
Train, loss=0.14732847: 589it [14:43,  1.22s/it]2017-06-02 14:29:42,716 root  INFO     step 589.000000 - time: 1.584910, loss: 0.145981, perplexity: 1.157174, precision: 0.828125, batch_len: 90.000000
Train, loss=0.14598098: 590it [14:44,  1.33s/it]2017-06-02 14:29:43,906 root  INFO     step 590.000000 - time: 1.111355, loss: 0.120250, perplexity: 1.127779, precision: 0.750000, batch_len: 114.000000
Train, loss=0.12025009: 591it [14:45,  1.29s/it]2017-06-02 14:29:44,768 root  INFO     step 591.000000 - time: 0.846459, loss: 0.080790, perplexity: 1.084143, precision: 0.843750, batch_len: 80.000000
Train, loss=0.08079019: 592it [14:46,  1.16s/it]2017-06-02 14:29:46,333 root  INFO     step 592.000000 - time: 1.504558, loss: 0.052438, perplexity: 1.053837, precision: 0.921875, batch_len: 117.000000
Train, loss=0.05243766: 593it [14:48,  1.28s/it]2017-06-02 14:29:47,607 root  INFO     step 593.000000 - time: 1.250169, loss: 0.127213, perplexity: 1.135659, precision: 0.828125, batch_len: 100.000000
Train, loss=0.12721328: 594it [14:49,  1.28s/it]2017-06-02 14:29:48,954 root  INFO     step 594.000000 - time: 1.186327, loss: 0.120956, perplexity: 1.128575, precision: 0.796875, batch_len: 91.000000
Train, loss=0.12095620: 595it [14:50,  1.30s/it]2017-06-02 14:29:50,381 root  INFO     step 595.000000 - time: 1.337719, loss: 0.046093, perplexity: 1.047172, precision: 0.937500, batch_len: 116.000000
Train, loss=0.04609349: 596it [14:52,  1.34s/it]2017-06-02 14:29:51,738 root  INFO     step 596.000000 - time: 1.223587, loss: 0.069685, perplexity: 1.072170, precision: 0.875000, batch_len: 121.000000
Train, loss=0.06968503: 597it [14:53,  1.34s/it]2017-06-02 14:29:52,835 root  INFO     step 597.000000 - time: 1.089243, loss: 0.074914, perplexity: 1.077792, precision: 0.921875, batch_len: 81.000000
Train, loss=0.07491414: 598it [14:54,  1.27s/it]2017-06-02 14:29:54,088 root  INFO     step 598.000000 - time: 1.107050, loss: 0.180017, perplexity: 1.197237, precision: 0.828125, batch_len: 83.000000
Train, loss=0.18001670: 599it [14:56,  1.26s/it]2017-06-02 14:29:55,167 root  INFO     step 599.000000 - time: 0.965310, loss: 0.090952, perplexity: 1.095216, precision: 0.859375, batch_len: 86.000000
Train, loss=0.09095153: 600it [14:57,  1.21s/it]2017-06-02 14:29:56,273 root  INFO     step 600.000000 - time: 0.982873, loss: 0.224022, perplexity: 1.251099, precision: 0.765625, batch_len: 84.000000
Train, loss=0.22402242: 601it [14:58,  1.18s/it]2017-06-02 14:29:57,562 root  INFO     step 601.000000 - time: 1.177069, loss: 0.124315, perplexity: 1.132373, precision: 0.718750, batch_len: 125.000000
Train, loss=0.12431544: 602it [14:59,  1.21s/it]2017-06-02 14:29:58,814 root  INFO     step 602.000000 - time: 1.219497, loss: 0.188213, perplexity: 1.207091, precision: 0.781250, batch_len: 85.000000
Train, loss=0.18821299: 603it [15:00,  1.22s/it]2017-06-02 14:30:00,055 root  INFO     step 603.000000 - time: 1.207703, loss: 0.223081, perplexity: 1.249921, precision: 0.781250, batch_len: 79.000000
Train, loss=0.22308055: 604it [15:02,  1.23s/it]2017-06-02 14:30:01,515 root  INFO     step 604.000000 - time: 1.407641, loss: 0.196701, perplexity: 1.217380, precision: 0.703125, batch_len: 129.000000
Train, loss=0.19670109: 605it [15:03,  1.30s/it]2017-06-02 14:30:02,941 root  INFO     step 605.000000 - time: 1.369922, loss: 0.076548, perplexity: 1.079554, precision: 0.875000, batch_len: 123.000000
Train, loss=0.07654774: 606it [15:04,  1.34s/it]2017-06-02 14:30:04,168 root  INFO     step 606.000000 - time: 1.213860, loss: 0.063409, perplexity: 1.065462, precision: 0.859375, batch_len: 124.000000
Train, loss=0.06340897: 607it [15:06,  1.30s/it]2017-06-02 14:30:05,769 root  INFO     step 607.000000 - time: 1.535659, loss: 0.142978, perplexity: 1.153704, precision: 0.843750, batch_len: 135.000000
Train, loss=0.14297751: 608it [15:07,  1.39s/it]2017-06-02 14:30:07,077 root  INFO     step 608.000000 - time: 1.285233, loss: 0.108955, perplexity: 1.115112, precision: 0.859375, batch_len: 115.000000
Train, loss=0.10895511: 609it [15:09,  1.37s/it]2017-06-02 14:30:08,126 root  INFO     step 609.000000 - time: 0.969530, loss: 0.079120, perplexity: 1.082334, precision: 0.875000, batch_len: 87.000000
Train, loss=0.07911979: 610it [15:10,  1.27s/it]2017-06-02 14:30:09,512 root  INFO     step 610.000000 - time: 1.285519, loss: 0.103811, perplexity: 1.109391, precision: 0.843750, batch_len: 126.000000
Train, loss=0.10381125: 611it [15:11,  1.31s/it]2017-06-02 14:30:10,541 root  INFO     step 611.000000 - time: 0.935470, loss: 0.110633, perplexity: 1.116985, precision: 0.859375, batch_len: 94.000000
Train, loss=0.11063331: 612it [15:12,  1.22s/it]2017-06-02 14:30:11,920 root  INFO     step 612.000000 - time: 1.310836, loss: 0.131490, perplexity: 1.140526, precision: 0.828125, batch_len: 137.000000
Train, loss=0.13148984: 613it [15:13,  1.27s/it]2017-06-02 14:30:13,615 root  INFO     step 613.000000 - time: 1.623494, loss: 0.118216, perplexity: 1.125487, precision: 0.843750, batch_len: 136.000000
Train, loss=0.11821553: 614it [15:15,  1.40s/it]2017-06-02 14:30:14,625 root  INFO     step 614.000000 - time: 0.965688, loss: 0.103997, perplexity: 1.109597, precision: 0.828125, batch_len: 82.000000
Train, loss=0.10399674: 615it [15:16,  1.28s/it]2017-06-02 14:30:16,041 root  INFO     step 615.000000 - time: 1.357933, loss: 0.097713, perplexity: 1.102646, precision: 0.843750, batch_len: 118.000000
Train, loss=0.09771308: 616it [15:18,  1.32s/it]2017-06-02 14:30:17,197 root  INFO     step 616.000000 - time: 1.033929, loss: 0.066848, perplexity: 1.069133, precision: 0.906250, batch_len: 98.000000
Train, loss=0.06684774: 617it [15:19,  1.27s/it]2017-06-02 14:30:18,682 root  INFO     step 617.000000 - time: 1.463659, loss: 0.088511, perplexity: 1.092546, precision: 0.718750, batch_len: 130.000000
Train, loss=0.08851077: 618it [15:20,  1.34s/it]2017-06-02 14:30:20,020 root  INFO     step 618.000000 - time: 1.180456, loss: 0.100540, perplexity: 1.105768, precision: 0.812500, batch_len: 78.000000
Train, loss=0.10054045: 619it [15:21,  1.34s/it]2017-06-02 14:30:21,105 root  INFO     step 619.000000 - time: 1.066679, loss: 0.069966, perplexity: 1.072472, precision: 0.828125, batch_len: 99.000000
Train, loss=0.06996591: 620it [15:23,  1.26s/it]2017-06-02 14:30:22,489 root  INFO     step 620.000000 - time: 1.350651, loss: 0.069625, perplexity: 1.072106, precision: 0.875000, batch_len: 119.000000
Train, loss=0.06962463: 621it [15:24,  1.30s/it]2017-06-02 14:30:23,952 root  INFO     step 621.000000 - time: 1.401928, loss: 0.086627, perplexity: 1.090490, precision: 0.859375, batch_len: 133.000000
Train, loss=0.08662723: 622it [15:25,  1.35s/it]2017-06-02 14:30:25,760 root  INFO     step 622.000000 - time: 1.625886, loss: 0.099602, perplexity: 1.104731, precision: 0.796875, batch_len: 144.000000
Train, loss=0.09960198: 623it [15:27,  1.49s/it]2017-06-02 14:30:27,603 root  INFO     step 623.000000 - time: 1.586360, loss: 0.108518, perplexity: 1.114624, precision: 0.859375, batch_len: 122.000000
Train, loss=0.10851754: 624it [15:29,  1.59s/it]2017-06-02 14:30:29,233 root  INFO     step 624.000000 - time: 1.600393, loss: 0.091262, perplexity: 1.095556, precision: 0.875000, batch_len: 141.000000
Train, loss=0.09126177: 625it [15:31,  1.60s/it]2017-06-02 14:30:30,492 root  INFO     step 625.000000 - time: 1.166907, loss: 0.065537, perplexity: 1.067732, precision: 0.921875, batch_len: 72.000000
Train, loss=0.06553672: 626it [15:32,  1.50s/it]2017-06-02 14:30:32,097 root  INFO     step 626.000000 - time: 1.480392, loss: 0.113525, perplexity: 1.120220, precision: 0.796875, batch_len: 138.000000
Train, loss=0.11352516: 627it [15:34,  1.53s/it]2017-06-02 14:30:33,001 root  INFO     step 627.000000 - time: 0.880492, loss: 0.115166, perplexity: 1.122059, precision: 0.843750, batch_len: 96.000000
Train, loss=0.11516552: 628it [15:34,  1.34s/it]2017-06-02 14:30:34,631 root  INFO     step 628.000000 - time: 1.528450, loss: 0.134486, perplexity: 1.143949, precision: 0.906250, batch_len: 139.000000
Train, loss=0.13448609: 629it [15:36,  1.43s/it]2017-06-02 14:30:35,822 root  INFO     step 629.000000 - time: 1.158093, loss: 0.193720, perplexity: 1.213757, precision: 0.875000, batch_len: 77.000000
Train, loss=0.19372025: 630it [15:37,  1.36s/it]2017-06-02 14:30:36,775 root  INFO     step 630.000000 - time: 0.873282, loss: 0.049047, perplexity: 1.050270, precision: 0.953125, batch_len: 71.000000
Train, loss=0.04904697: 631it [15:38,  1.24s/it]2017-06-02 14:30:37,798 root  INFO     step 631.000000 - time: 0.957454, loss: 0.037790, perplexity: 1.038513, precision: 0.937500, batch_len: 74.000000
Train, loss=0.03779020: 632it [15:39,  1.17s/it]2017-06-02 14:30:39,505 root  INFO     step 632.000000 - time: 1.683644, loss: 0.075272, perplexity: 1.078177, precision: 0.781250, batch_len: 152.000000
Train, loss=0.07527155: 633it [15:41,  1.33s/it]2017-06-02 14:30:41,117 root  INFO     step 633.000000 - time: 1.506546, loss: 0.075702, perplexity: 1.078641, precision: 0.812500, batch_len: 132.000000
Train, loss=0.07570213: 634it [15:43,  1.42s/it]2017-06-02 14:30:43,304 root  INFO     step 634.000000 - time: 2.045557, loss: 0.152340, perplexity: 1.164556, precision: 0.656250, batch_len: 150.000000
Train, loss=0.15234002: 635it [15:45,  1.65s/it]2017-06-02 14:30:44,872 root  INFO     step 635.000000 - time: 1.407565, loss: 0.393142, perplexity: 1.481629, precision: 0.515625, batch_len: 142.000000
Train, loss=0.39314246: 636it [15:46,  1.62s/it]2017-06-02 14:30:45,828 root  INFO     step 636.000000 - time: 0.922313, loss: 0.448675, perplexity: 1.566236, precision: 0.546875, batch_len: 76.000000
Train, loss=0.44867539: 637it [15:47,  1.42s/it]2017-06-02 14:30:47,374 root  INFO     step 637.000000 - time: 1.494940, loss: 0.262819, perplexity: 1.300592, precision: 0.562500, batch_len: 134.000000
Train, loss=0.26281923: 638it [15:49,  1.46s/it]2017-06-02 14:30:49,182 root  INFO     step 638.000000 - time: 1.582838, loss: 0.084613, perplexity: 1.088296, precision: 0.781250, batch_len: 131.000000
Train, loss=0.08461317: 639it [15:51,  1.56s/it]2017-06-02 14:30:49,228 root  INFO     Generating first batch)
2017-06-02 14:30:53,231 root  INFO     step 639.000000 - time: 1.253473, loss: 0.070999, perplexity: 1.073581, precision: 0.906250, batch_len: 96.000000
Train, loss=0.07099943: 640it [15:55,  2.31s/it]2017-06-02 14:30:54,720 root  INFO     step 640.000000 - time: 1.076972, loss: 0.140400, perplexity: 1.150734, precision: 0.781250, batch_len: 108.000000
Train, loss=0.14040042: 641it [15:56,  2.06s/it]2017-06-02 14:30:55,974 root  INFO     step 641.000000 - time: 1.080482, loss: 0.107090, perplexity: 1.113035, precision: 0.828125, batch_len: 105.000000
Train, loss=0.10709044: 642it [15:57,  1.82s/it]2017-06-02 14:30:56,991 root  INFO     step 642.000000 - time: 0.982777, loss: 0.108452, perplexity: 1.114551, precision: 0.812500, batch_len: 101.000000
Train, loss=0.10845172: 643it [15:58,  1.58s/it]2017-06-02 14:30:58,322 root  INFO     step 643.000000 - time: 1.103046, loss: 0.057594, perplexity: 1.059285, precision: 0.828125, batch_len: 102.000000
Train, loss=0.05759369: 644it [16:00,  1.50s/it]2017-06-02 14:30:59,659 root  INFO     step 644.000000 - time: 1.295220, loss: 0.107463, perplexity: 1.113449, precision: 0.859375, batch_len: 110.000000
Train, loss=0.10746253: 645it [16:01,  1.45s/it]2017-06-02 14:31:00,765 root  INFO     step 645.000000 - time: 1.088384, loss: 0.093433, perplexity: 1.097938, precision: 0.890625, batch_len: 104.000000
Train, loss=0.09343348: 646it [16:02,  1.35s/it]2017-06-02 14:31:02,134 root  INFO     step 646.000000 - time: 1.343766, loss: 0.118903, perplexity: 1.126261, precision: 0.859375, batch_len: 120.000000
Train, loss=0.11890344: 647it [16:04,  1.36s/it]2017-06-02 14:31:03,316 root  INFO     step 647.000000 - time: 1.108266, loss: 0.096586, perplexity: 1.101404, precision: 0.812500, batch_len: 113.000000
Train, loss=0.09658553: 648it [16:05,  1.30s/it]2017-06-02 14:31:04,237 root  INFO     step 648.000000 - time: 0.908568, loss: 0.088459, perplexity: 1.092490, precision: 0.828125, batch_len: 92.000000
Train, loss=0.08845919: 649it [16:06,  1.19s/it]2017-06-02 14:31:05,353 root  INFO     step 649.000000 - time: 1.069472, loss: 0.142465, perplexity: 1.153113, precision: 0.859375, batch_len: 88.000000
Train, loss=0.14246510: 650it [16:07,  1.17s/it]2017-06-02 14:31:06,761 root  INFO     step 650.000000 - time: 1.349288, loss: 0.156067, perplexity: 1.168905, precision: 0.812500, batch_len: 91.000000
Train, loss=0.15606707: 651it [16:08,  1.24s/it]2017-06-02 14:31:08,443 root  INFO     step 651.000000 - time: 1.598694, loss: 0.211732, perplexity: 1.235816, precision: 0.765625, batch_len: 90.000000
Train, loss=0.21173188: 652it [16:10,  1.37s/it]2017-06-02 14:31:09,625 root  INFO     step 652.000000 - time: 1.023893, loss: 0.138478, perplexity: 1.148524, precision: 0.890625, batch_len: 100.000000
Train, loss=0.13847791: 653it [16:11,  1.32s/it]2017-06-02 14:31:10,683 root  INFO     step 653.000000 - time: 0.864789, loss: 0.149575, perplexity: 1.161341, precision: 0.843750, batch_len: 80.000000
Train, loss=0.14957508: 654it [16:12,  1.24s/it]2017-06-02 14:31:12,224 root  INFO     step 654.000000 - time: 1.522375, loss: 0.098228, perplexity: 1.103214, precision: 0.812500, batch_len: 128.000000
Train, loss=0.09822750: 655it [16:14,  1.33s/it]2017-06-02 14:31:13,514 root  INFO     step 655.000000 - time: 1.254729, loss: 0.134683, perplexity: 1.144174, precision: 0.828125, batch_len: 107.000000
Train, loss=0.13468325: 656it [16:15,  1.32s/it]2017-06-02 14:31:14,571 root  INFO     step 656.000000 - time: 0.970493, loss: 0.106699, perplexity: 1.112600, precision: 0.843750, batch_len: 93.000000
Train, loss=0.10669944: 657it [16:16,  1.24s/it]2017-06-02 14:31:16,016 root  INFO     step 657.000000 - time: 1.360987, loss: 0.082501, perplexity: 1.086000, precision: 0.843750, batch_len: 121.000000
Train, loss=0.08250126: 658it [16:17,  1.30s/it]2017-06-02 14:31:17,248 root  INFO     step 658.000000 - time: 1.221921, loss: 0.074760, perplexity: 1.077625, precision: 0.859375, batch_len: 117.000000
Train, loss=0.07475974: 659it [16:19,  1.28s/it]2017-06-02 14:31:18,470 root  INFO     step 659.000000 - time: 1.176142, loss: 0.118442, perplexity: 1.125741, precision: 0.750000, batch_len: 89.000000
Train, loss=0.11844152: 660it [16:20,  1.26s/it]2017-06-02 14:31:19,616 root  INFO     step 660.000000 - time: 1.136922, loss: 0.139685, perplexity: 1.149912, precision: 0.843750, batch_len: 79.000000
Train, loss=0.13968524: 661it [16:21,  1.23s/it]2017-06-02 14:31:20,973 root  INFO     step 661.000000 - time: 1.328666, loss: 0.114714, perplexity: 1.121552, precision: 0.843750, batch_len: 112.000000
Train, loss=0.11471380: 662it [16:22,  1.27s/it]2017-06-02 14:31:21,977 root  INFO     step 662.000000 - time: 0.970140, loss: 0.074815, perplexity: 1.077685, precision: 0.859375, batch_len: 81.000000
Train, loss=0.07481527: 663it [16:23,  1.19s/it]2017-06-02 14:31:23,019 root  INFO     step 663.000000 - time: 0.980007, loss: 0.109738, perplexity: 1.115986, precision: 0.796875, batch_len: 97.000000
Train, loss=0.10973850: 664it [16:24,  1.14s/it]2017-06-02 14:31:24,053 root  INFO     step 664.000000 - time: 1.009170, loss: 0.123211, perplexity: 1.131123, precision: 0.859375, batch_len: 111.000000
Train, loss=0.12321095: 665it [16:26,  1.11s/it]2017-06-02 14:31:25,219 root  INFO     step 665.000000 - time: 1.132895, loss: 0.078662, perplexity: 1.081838, precision: 0.890625, batch_len: 103.000000
Train, loss=0.07866163: 666it [16:27,  1.13s/it]2017-06-02 14:31:26,283 root  INFO     step 666.000000 - time: 1.049362, loss: 0.094778, perplexity: 1.099415, precision: 0.796875, batch_len: 114.000000
Train, loss=0.09477778: 667it [16:28,  1.11s/it]2017-06-02 14:31:27,488 root  INFO     step 667.000000 - time: 1.139982, loss: 0.114871, perplexity: 1.121729, precision: 0.843750, batch_len: 86.000000
Train, loss=0.11487091: 668it [16:29,  1.14s/it]2017-06-02 14:31:28,779 root  INFO     step 668.000000 - time: 1.255695, loss: 0.081938, perplexity: 1.085388, precision: 0.828125, batch_len: 109.000000
Train, loss=0.08193770: 669it [16:30,  1.18s/it]2017-06-02 14:31:30,263 root  INFO     step 669.000000 - time: 1.418892, loss: 0.104799, perplexity: 1.110488, precision: 0.875000, batch_len: 137.000000
Train, loss=0.10479937: 670it [16:32,  1.27s/it]2017-06-02 14:31:31,456 root  INFO     step 670.000000 - time: 1.135702, loss: 0.099485, perplexity: 1.104602, precision: 0.828125, batch_len: 106.000000
Train, loss=0.09948510: 671it [16:33,  1.25s/it]2017-06-02 14:31:32,535 root  INFO     step 671.000000 - time: 0.952221, loss: 0.133899, perplexity: 1.143278, precision: 0.843750, batch_len: 94.000000
Train, loss=0.13389918: 672it [16:34,  1.20s/it]2017-06-02 14:31:34,045 root  INFO     step 672.000000 - time: 1.436734, loss: 0.040150, perplexity: 1.040967, precision: 0.906250, batch_len: 124.000000
Train, loss=0.04015040: 673it [16:36,  1.29s/it]2017-06-02 14:31:35,593 root  INFO     step 673.000000 - time: 1.482486, loss: 0.046870, perplexity: 1.047986, precision: 0.921875, batch_len: 119.000000
Train, loss=0.04687044: 674it [16:37,  1.37s/it]2017-06-02 14:31:37,082 root  INFO     step 674.000000 - time: 1.362590, loss: 0.046765, perplexity: 1.047875, precision: 0.937500, batch_len: 118.000000
Train, loss=0.04676453: 675it [16:39,  1.40s/it]2017-06-02 14:31:38,222 root  INFO     step 675.000000 - time: 1.102767, loss: 0.090481, perplexity: 1.094701, precision: 0.890625, batch_len: 115.000000
Train, loss=0.09048133: 676it [16:40,  1.33s/it]2017-06-02 14:31:39,474 root  INFO     step 676.000000 - time: 1.232000, loss: 0.082617, perplexity: 1.086125, precision: 0.875000, batch_len: 125.000000
Train, loss=0.08261658: 677it [16:41,  1.30s/it]2017-06-02 14:31:40,638 root  INFO     step 677.000000 - time: 1.126081, loss: 0.080511, perplexity: 1.083840, precision: 0.875000, batch_len: 85.000000
Train, loss=0.08051051: 678it [16:42,  1.26s/it]2017-06-02 14:31:42,342 root  INFO     step 678.000000 - time: 1.547673, loss: 0.057899, perplexity: 1.059608, precision: 0.875000, batch_len: 129.000000
Train, loss=0.05789896: 679it [16:44,  1.39s/it]2017-06-02 14:31:43,339 root  INFO     step 679.000000 - time: 0.936483, loss: 0.108762, perplexity: 1.114896, precision: 0.859375, batch_len: 83.000000
Train, loss=0.10876151: 680it [16:45,  1.28s/it]2017-06-02 14:31:44,777 root  INFO     step 680.000000 - time: 1.347369, loss: 0.064097, perplexity: 1.066195, precision: 0.906250, batch_len: 123.000000
Train, loss=0.06409650: 681it [16:46,  1.32s/it]2017-06-02 14:31:46,036 root  INFO     step 681.000000 - time: 1.223364, loss: 0.074477, perplexity: 1.077321, precision: 0.906250, batch_len: 116.000000
Train, loss=0.07447720: 682it [16:48,  1.30s/it]2017-06-02 14:31:47,272 root  INFO     step 682.000000 - time: 1.186582, loss: 0.054957, perplexity: 1.056495, precision: 0.859375, batch_len: 98.000000
Train, loss=0.05495680: 683it [16:49,  1.28s/it]2017-06-02 14:31:48,784 root  INFO     step 683.000000 - time: 1.479067, loss: 0.054816, perplexity: 1.056346, precision: 0.875000, batch_len: 126.000000
Train, loss=0.05481583: 684it [16:50,  1.35s/it]2017-06-02 14:31:49,728 root  INFO     step 684.000000 - time: 0.938051, loss: 0.110528, perplexity: 1.116868, precision: 0.843750, batch_len: 82.000000
Train, loss=0.11052841: 685it [16:51,  1.23s/it]2017-06-02 14:31:51,232 root  INFO     step 685.000000 - time: 1.430567, loss: 0.110716, perplexity: 1.117078, precision: 0.859375, batch_len: 144.000000
Train, loss=0.11071609: 686it [16:53,  1.31s/it]2017-06-02 14:31:52,233 root  INFO     step 686.000000 - time: 0.890097, loss: 0.061898, perplexity: 1.063854, precision: 0.859375, batch_len: 87.000000
Train, loss=0.06189775: 687it [16:54,  1.22s/it]2017-06-02 14:31:53,380 root  INFO     step 687.000000 - time: 1.033273, loss: 0.176663, perplexity: 1.193229, precision: 0.796875, batch_len: 84.000000
Train, loss=0.17666347: 688it [16:55,  1.20s/it]2017-06-02 14:31:55,116 root  INFO     step 688.000000 - time: 1.638883, loss: 0.045485, perplexity: 1.046536, precision: 0.875000, batch_len: 133.000000
Train, loss=0.04548524: 689it [16:57,  1.36s/it]2017-06-02 14:31:56,651 root  INFO     step 689.000000 - time: 1.434186, loss: 0.058698, perplexity: 1.060455, precision: 0.890625, batch_len: 135.000000
Train, loss=0.05869761: 690it [16:58,  1.41s/it]2017-06-02 14:31:58,039 root  INFO     step 690.000000 - time: 1.334366, loss: 0.049162, perplexity: 1.050390, precision: 0.890625, batch_len: 136.000000
Train, loss=0.04916190: 691it [17:00,  1.40s/it]2017-06-02 14:31:59,077 root  INFO     step 691.000000 - time: 0.949935, loss: 0.111654, perplexity: 1.118126, precision: 0.843750, batch_len: 99.000000
Train, loss=0.11165368: 692it [17:01,  1.29s/it]2017-06-02 14:32:00,769 root  INFO     step 692.000000 - time: 1.607539, loss: 0.207907, perplexity: 1.231098, precision: 0.625000, batch_len: 122.000000
Train, loss=0.20790672: 693it [17:02,  1.41s/it]2017-06-02 14:32:01,954 root  INFO     step 693.000000 - time: 1.119119, loss: 0.340155, perplexity: 1.405165, precision: 0.640625, batch_len: 78.000000
Train, loss=0.34015453: 694it [17:03,  1.35s/it]2017-06-02 14:32:03,417 root  INFO     step 694.000000 - time: 1.410808, loss: 0.222727, perplexity: 1.249480, precision: 0.718750, batch_len: 130.000000
Train, loss=0.22272721: 695it [17:05,  1.38s/it]2017-06-02 14:32:04,494 root  INFO     step 695.000000 - time: 0.999207, loss: 0.129568, perplexity: 1.138337, precision: 0.890625, batch_len: 96.000000
Train, loss=0.12956834: 696it [17:06,  1.29s/it]2017-06-02 14:32:05,985 root  INFO     step 696.000000 - time: 1.310220, loss: 0.090382, perplexity: 1.094592, precision: 0.859375, batch_len: 139.000000
Train, loss=0.09038174: 697it [17:07,  1.35s/it]2017-06-02 14:32:07,640 root  INFO     step 697.000000 - time: 1.544300, loss: 0.064769, perplexity: 1.066913, precision: 0.859375, batch_len: 138.000000
Train, loss=0.06476927: 698it [17:09,  1.44s/it]2017-06-02 14:32:09,712 root  INFO     step 698.000000 - time: 2.046516, loss: 0.065534, perplexity: 1.067729, precision: 0.812500, batch_len: 152.000000
Train, loss=0.06553355: 699it [17:11,  1.63s/it]2017-06-02 14:32:11,197 root  INFO     step 699.000000 - time: 1.400244, loss: 0.056294, perplexity: 1.057908, precision: 0.890625, batch_len: 132.000000
Train, loss=0.05629358: 700it [17:13,  1.59s/it]2017-06-02 14:32:12,342 root  INFO     step 700.000000 - time: 1.049216, loss: 0.105828, perplexity: 1.111631, precision: 0.828125, batch_len: 72.000000
Train, loss=0.10582849: 701it [17:14,  1.45s/it]2017-06-02 14:32:14,060 root  INFO     step 701.000000 - time: 1.543807, loss: 0.113235, perplexity: 1.119895, precision: 0.687500, batch_len: 134.000000
Train, loss=0.11323500: 702it [17:16,  1.53s/it]2017-06-02 14:32:15,715 root  INFO     step 702.000000 - time: 1.486731, loss: 0.127294, perplexity: 1.135751, precision: 0.781250, batch_len: 141.000000
Train, loss=0.12729387: 703it [17:17,  1.57s/it]2017-06-02 14:32:16,827 root  INFO     step 703.000000 - time: 1.005420, loss: 0.121454, perplexity: 1.129137, precision: 0.828125, batch_len: 76.000000
Train, loss=0.12145363: 704it [17:18,  1.43s/it]2017-06-02 14:32:17,865 root  INFO     step 704.000000 - time: 1.008847, loss: 0.088261, perplexity: 1.092273, precision: 0.843750, batch_len: 77.000000
Train, loss=0.08826076: 705it [17:19,  1.31s/it]2017-06-02 14:32:19,129 root  INFO     step 705.000000 - time: 1.254253, loss: 0.065862, perplexity: 1.068079, precision: 0.843750, batch_len: 131.000000
Train, loss=0.06586197: 706it [17:21,  1.30s/it]2017-06-02 14:32:20,392 root  INFO     step 706.000000 - time: 1.122515, loss: 0.069230, perplexity: 1.071683, precision: 0.875000, batch_len: 74.000000
Train, loss=0.06923024: 707it [17:22,  1.29s/it]2017-06-02 14:32:22,104 root  INFO     step 707.000000 - time: 1.587217, loss: 0.118458, perplexity: 1.125760, precision: 0.781250, batch_len: 142.000000
Train, loss=0.11845808: 708it [17:24,  1.42s/it]2017-06-02 14:32:24,056 root  INFO     step 708.000000 - time: 1.876547, loss: 0.246018, perplexity: 1.278923, precision: 0.656250, batch_len: 150.000000
Train, loss=0.24601823: 709it [17:26,  1.58s/it]2017-06-02 14:32:25,094 root  INFO     step 709.000000 - time: 0.893262, loss: 0.112182, perplexity: 1.118717, precision: 0.796875, batch_len: 71.000000
Train, loss=0.11218245: 710it [17:27,  1.41s/it]2017-06-02 14:32:25,192 root  INFO     Generating first batch)
2017-06-02 14:32:29,508 root  INFO     step 710.000000 - time: 1.162094, loss: 0.237569, perplexity: 1.268162, precision: 0.671875, batch_len: 110.000000
Train, loss=0.23756899: 711it [17:31,  2.31s/it]2017-06-02 14:32:30,700 root  INFO     step 711.000000 - time: 1.049935, loss: 0.065747, perplexity: 1.067957, precision: 0.875000, batch_len: 96.000000
Train, loss=0.06574749: 712it [17:32,  1.98s/it]2017-06-02 14:32:32,144 root  INFO     step 712.000000 - time: 1.068642, loss: 0.095964, perplexity: 1.100719, precision: 0.828125, batch_len: 105.000000
Train, loss=0.09596352: 713it [17:34,  1.82s/it]2017-06-02 14:32:33,415 root  INFO     step 713.000000 - time: 1.242237, loss: 0.084414, perplexity: 1.088079, precision: 0.859375, batch_len: 115.000000
Train, loss=0.08441366: 714it [17:35,  1.65s/it]2017-06-02 14:32:34,556 root  INFO     step 714.000000 - time: 1.130609, loss: 0.136084, perplexity: 1.145778, precision: 0.781250, batch_len: 89.000000
Train, loss=0.13608405: 715it [17:36,  1.50s/it]2017-06-02 14:32:35,787 root  INFO     step 715.000000 - time: 1.093535, loss: 0.083633, perplexity: 1.087230, precision: 0.796875, batch_len: 108.000000
Train, loss=0.08363315: 716it [17:37,  1.42s/it]2017-06-02 14:32:36,949 root  INFO     step 716.000000 - time: 1.072666, loss: 0.052533, perplexity: 1.053938, precision: 0.906250, batch_len: 100.000000
Train, loss=0.05253331: 717it [17:38,  1.34s/it]2017-06-02 14:32:38,000 root  INFO     step 717.000000 - time: 1.023405, loss: 0.181960, perplexity: 1.199566, precision: 0.796875, batch_len: 93.000000
Train, loss=0.18196002: 718it [17:39,  1.25s/it]2017-06-02 14:32:38,915 root  INFO     step 718.000000 - time: 0.899935, loss: 0.146942, perplexity: 1.158287, precision: 0.765625, batch_len: 88.000000
Train, loss=0.14694202: 719it [17:40,  1.15s/it]2017-06-02 14:32:40,163 root  INFO     step 719.000000 - time: 1.241219, loss: 0.126545, perplexity: 1.134900, precision: 0.796875, batch_len: 113.000000
Train, loss=0.12654461: 720it [17:42,  1.18s/it]2017-06-02 14:32:41,337 root  INFO     step 720.000000 - time: 1.145583, loss: 0.086451, perplexity: 1.090298, precision: 0.890625, batch_len: 86.000000
Train, loss=0.08645103: 721it [17:43,  1.18s/it]2017-06-02 14:32:42,445 root  INFO     step 721.000000 - time: 1.020045, loss: 0.116185, perplexity: 1.123204, precision: 0.828125, batch_len: 102.000000
Train, loss=0.11618511: 722it [17:44,  1.16s/it]2017-06-02 14:32:43,744 root  INFO     step 722.000000 - time: 1.143582, loss: 0.113486, perplexity: 1.120176, precision: 0.828125, batch_len: 109.000000
Train, loss=0.11348569: 723it [17:45,  1.20s/it]2017-06-02 14:32:44,784 root  INFO     step 723.000000 - time: 0.951538, loss: 0.180217, perplexity: 1.197477, precision: 0.750000, batch_len: 90.000000
Train, loss=0.18021715: 724it [17:46,  1.15s/it]2017-06-02 14:32:45,976 root  INFO     step 724.000000 - time: 1.176469, loss: 0.073682, perplexity: 1.076465, precision: 0.890625, batch_len: 128.000000
Train, loss=0.07368248: 725it [17:47,  1.16s/it]2017-06-02 14:32:47,218 root  INFO     step 725.000000 - time: 1.209495, loss: 0.052958, perplexity: 1.054385, precision: 0.890625, batch_len: 101.000000
Train, loss=0.05295754: 726it [17:49,  1.19s/it]2017-06-02 14:32:48,488 root  INFO     step 726.000000 - time: 1.260742, loss: 0.100337, perplexity: 1.105544, precision: 0.843750, batch_len: 104.000000
Train, loss=0.10033713: 727it [17:50,  1.21s/it]2017-06-02 14:32:49,543 root  INFO     step 727.000000 - time: 1.029106, loss: 0.106466, perplexity: 1.112340, precision: 0.843750, batch_len: 94.000000
Train, loss=0.10646573: 728it [17:51,  1.17s/it]2017-06-02 14:32:50,899 root  INFO     step 728.000000 - time: 1.330467, loss: 0.082153, perplexity: 1.085622, precision: 0.812500, batch_len: 120.000000
Train, loss=0.08215274: 729it [17:52,  1.22s/it]2017-06-02 14:32:51,947 root  INFO     step 729.000000 - time: 1.009717, loss: 0.122011, perplexity: 1.129767, precision: 0.812500, batch_len: 111.000000
Train, loss=0.12201136: 730it [17:53,  1.17s/it]2017-06-02 14:32:53,353 root  INFO     step 730.000000 - time: 1.304657, loss: 0.104336, perplexity: 1.109973, precision: 0.875000, batch_len: 114.000000
Train, loss=0.10433567: 731it [17:55,  1.24s/it]2017-06-02 14:32:54,700 root  INFO     step 731.000000 - time: 1.180988, loss: 0.068362, perplexity: 1.070753, precision: 0.843750, batch_len: 103.000000
Train, loss=0.06836228: 732it [17:56,  1.27s/it]2017-06-02 14:32:56,034 root  INFO     step 732.000000 - time: 1.326359, loss: 0.079682, perplexity: 1.082943, precision: 0.875000, batch_len: 112.000000
Train, loss=0.07968190: 733it [17:58,  1.29s/it]2017-06-02 14:32:57,185 root  INFO     step 733.000000 - time: 1.117784, loss: 0.047730, perplexity: 1.048888, precision: 0.843750, batch_len: 106.000000
Train, loss=0.04773030: 734it [17:59,  1.25s/it]2017-06-02 14:32:58,505 root  INFO     step 734.000000 - time: 1.302837, loss: 0.060382, perplexity: 1.062242, precision: 0.937500, batch_len: 117.000000
Train, loss=0.06038170: 735it [18:00,  1.27s/it]2017-06-02 14:32:59,572 root  INFO     step 735.000000 - time: 0.976509, loss: 0.103357, perplexity: 1.108887, precision: 0.812500, batch_len: 97.000000
Train, loss=0.10335685: 736it [18:01,  1.21s/it]2017-06-02 14:33:00,775 root  INFO     step 736.000000 - time: 1.160899, loss: 0.062597, perplexity: 1.064597, precision: 0.953125, batch_len: 92.000000
Train, loss=0.06259654: 737it [18:02,  1.21s/it]2017-06-02 14:33:01,940 root  INFO     step 737.000000 - time: 1.122655, loss: 0.094635, perplexity: 1.099258, precision: 0.828125, batch_len: 81.000000
Train, loss=0.09463502: 738it [18:03,  1.19s/it]2017-06-02 14:33:03,346 root  INFO     step 738.000000 - time: 1.315805, loss: 0.036995, perplexity: 1.037688, precision: 0.890625, batch_len: 124.000000
Train, loss=0.03699481: 739it [18:05,  1.26s/it]2017-06-02 14:33:04,712 root  INFO     step 739.000000 - time: 1.184933, loss: 0.095842, perplexity: 1.100585, precision: 0.906250, batch_len: 91.000000
Train, loss=0.09584195: 740it [18:06,  1.29s/it]2017-06-02 14:33:05,690 root  INFO     step 740.000000 - time: 0.927310, loss: 0.088805, perplexity: 1.092867, precision: 0.875000, batch_len: 85.000000
Train, loss=0.08880467: 741it [18:07,  1.20s/it]2017-06-02 14:33:06,929 root  INFO     step 741.000000 - time: 1.214549, loss: 0.088543, perplexity: 1.092581, precision: 0.843750, batch_len: 107.000000
Train, loss=0.08854277: 742it [18:08,  1.21s/it]2017-06-02 14:33:08,187 root  INFO     step 742.000000 - time: 1.158339, loss: 0.078463, perplexity: 1.081624, precision: 0.890625, batch_len: 83.000000
Train, loss=0.07846327: 743it [18:10,  1.22s/it]2017-06-02 14:33:09,155 root  INFO     step 743.000000 - time: 0.924010, loss: 0.067870, perplexity: 1.070226, precision: 0.859375, batch_len: 80.000000
Train, loss=0.06786957: 744it [18:11,  1.15s/it]2017-06-02 14:33:10,320 root  INFO     step 744.000000 - time: 1.049903, loss: 0.092188, perplexity: 1.096571, precision: 0.875000, batch_len: 84.000000
Train, loss=0.09218816: 745it [18:12,  1.15s/it]2017-06-02 14:33:11,718 root  INFO     step 745.000000 - time: 1.337437, loss: 0.035328, perplexity: 1.035959, precision: 0.921875, batch_len: 116.000000
Train, loss=0.03532752: 746it [18:13,  1.23s/it]2017-06-02 14:33:13,266 root  INFO     step 746.000000 - time: 1.467076, loss: 0.050110, perplexity: 1.051386, precision: 0.890625, batch_len: 129.000000
Train, loss=0.05010962: 747it [18:15,  1.32s/it]2017-06-02 14:33:14,850 root  INFO     step 747.000000 - time: 1.504433, loss: 0.059714, perplexity: 1.061533, precision: 0.875000, batch_len: 121.000000
Train, loss=0.05971393: 748it [18:16,  1.40s/it]2017-06-02 14:33:16,260 root  INFO     step 748.000000 - time: 1.351860, loss: 0.039186, perplexity: 1.039964, precision: 0.921875, batch_len: 123.000000
Train, loss=0.03918592: 749it [18:18,  1.40s/it]2017-06-02 14:33:17,244 root  INFO     step 749.000000 - time: 0.930887, loss: 0.208444, perplexity: 1.231760, precision: 0.875000, batch_len: 79.000000
Train, loss=0.20844385: 750it [18:19,  1.28s/it]2017-06-02 14:33:18,505 root  INFO     step 750.000000 - time: 1.219724, loss: 0.049919, perplexity: 1.051186, precision: 0.937500, batch_len: 118.000000
Train, loss=0.04991940: 751it [18:20,  1.27s/it]2017-06-02 14:33:20,001 root  INFO     step 751.000000 - time: 1.445006, loss: 0.206555, perplexity: 1.229435, precision: 0.812500, batch_len: 126.000000
Train, loss=0.20655465: 752it [18:21,  1.34s/it]2017-06-02 14:33:21,272 root  INFO     step 752.000000 - time: 1.153144, loss: 0.103069, perplexity: 1.108568, precision: 0.781250, batch_len: 87.000000
Train, loss=0.10306901: 753it [18:23,  1.32s/it]2017-06-02 14:33:22,367 root  INFO     step 753.000000 - time: 1.079874, loss: 0.038243, perplexity: 1.038983, precision: 0.968750, batch_len: 98.000000
Train, loss=0.03824257: 754it [18:24,  1.25s/it]2017-06-02 14:33:23,833 root  INFO     step 754.000000 - time: 1.325523, loss: 0.053889, perplexity: 1.055367, precision: 0.906250, batch_len: 125.000000
Train, loss=0.05388902: 755it [18:25,  1.32s/it]2017-06-02 14:33:25,245 root  INFO     step 755.000000 - time: 1.307962, loss: 0.114217, perplexity: 1.120996, precision: 0.843750, batch_len: 135.000000
Train, loss=0.11421729: 756it [18:27,  1.35s/it]2017-06-02 14:33:26,884 root  INFO     step 756.000000 - time: 1.530445, loss: 0.073888, perplexity: 1.076686, precision: 0.828125, batch_len: 137.000000
Train, loss=0.07388771: 757it [18:28,  1.43s/it]2017-06-02 14:33:28,418 root  INFO     step 757.000000 - time: 1.486243, loss: 0.058997, perplexity: 1.060772, precision: 0.906250, batch_len: 144.000000
Train, loss=0.05899731: 758it [18:30,  1.46s/it]2017-06-02 14:33:29,799 root  INFO     step 758.000000 - time: 1.230100, loss: 0.050750, perplexity: 1.052060, precision: 0.921875, batch_len: 99.000000
Train, loss=0.05075039: 759it [18:31,  1.44s/it]2017-06-02 14:33:31,148 root  INFO     step 759.000000 - time: 1.107624, loss: 0.070652, perplexity: 1.073208, precision: 0.906250, batch_len: 82.000000
Train, loss=0.07065237: 760it [18:33,  1.41s/it]2017-06-02 14:33:32,694 root  INFO     step 760.000000 - time: 1.520781, loss: 0.035481, perplexity: 1.036118, precision: 0.906250, batch_len: 130.000000
Train, loss=0.03548134: 761it [18:34,  1.45s/it]2017-06-02 14:33:33,961 root  INFO     step 761.000000 - time: 1.235436, loss: 0.069178, perplexity: 1.071627, precision: 0.937500, batch_len: 119.000000
Train, loss=0.06917785: 762it [18:35,  1.40s/it]2017-06-02 14:33:35,118 root  INFO     step 762.000000 - time: 1.076526, loss: 0.077080, perplexity: 1.080129, precision: 0.906250, batch_len: 78.000000
Train, loss=0.07708043: 763it [18:37,  1.32s/it]2017-06-02 14:33:36,683 root  INFO     step 763.000000 - time: 1.526032, loss: 0.145997, perplexity: 1.157193, precision: 0.828125, batch_len: 122.000000
Train, loss=0.14599708: 764it [18:38,  1.40s/it]2017-06-02 14:33:38,194 root  INFO     step 764.000000 - time: 1.409884, loss: 0.043098, perplexity: 1.044040, precision: 0.906250, batch_len: 133.000000
Train, loss=0.04309771: 765it [18:40,  1.43s/it]2017-06-02 14:33:39,680 root  INFO     step 765.000000 - time: 1.381928, loss: 0.038725, perplexity: 1.039484, precision: 0.921875, batch_len: 138.000000
Train, loss=0.03872453: 766it [18:41,  1.45s/it]2017-06-02 14:33:40,627 root  INFO     step 766.000000 - time: 0.939482, loss: 0.053757, perplexity: 1.055229, precision: 0.890625, batch_len: 96.000000
Train, loss=0.05375741: 767it [18:42,  1.30s/it]2017-06-02 14:33:42,226 root  INFO     step 767.000000 - time: 1.498782, loss: 0.051174, perplexity: 1.052506, precision: 0.953125, batch_len: 136.000000
Train, loss=0.05117405: 768it [18:44,  1.39s/it]2017-06-02 14:33:44,397 root  INFO     step 768.000000 - time: 2.051217, loss: 0.034000, perplexity: 1.034585, precision: 0.921875, batch_len: 152.000000
Train, loss=0.03400037: 769it [18:46,  1.62s/it]2017-06-02 14:33:45,620 root  INFO     step 769.000000 - time: 1.142242, loss: 0.062699, perplexity: 1.064706, precision: 0.906250, batch_len: 72.000000
Train, loss=0.06269891: 770it [18:47,  1.50s/it]2017-06-02 14:33:47,128 root  INFO     step 770.000000 - time: 1.356769, loss: 0.029066, perplexity: 1.029493, precision: 0.906250, batch_len: 132.000000
Train, loss=0.02906608: 771it [18:49,  1.50s/it]2017-06-02 14:33:48,302 root  INFO     step 771.000000 - time: 1.158268, loss: 0.058780, perplexity: 1.060542, precision: 0.921875, batch_len: 76.000000
Train, loss=0.05877984: 772it [18:50,  1.41s/it]2017-06-02 14:33:50,013 root  INFO     step 772.000000 - time: 1.636734, loss: 0.058405, perplexity: 1.060144, precision: 0.937500, batch_len: 139.000000
Train, loss=0.05840452: 773it [18:51,  1.50s/it]2017-06-02 14:33:50,977 root  INFO     step 773.000000 - time: 0.954845, loss: 0.020827, perplexity: 1.021045, precision: 0.953125, batch_len: 74.000000
Train, loss=0.02082689: 774it [18:52,  1.34s/it]2017-06-02 14:33:52,927 root  INFO     step 774.000000 - time: 1.884600, loss: 0.080141, perplexity: 1.083440, precision: 0.859375, batch_len: 150.000000
Train, loss=0.08014072: 775it [18:54,  1.52s/it]2017-06-02 14:33:54,351 root  INFO     step 775.000000 - time: 1.272060, loss: 0.035078, perplexity: 1.035701, precision: 0.937500, batch_len: 134.000000
Train, loss=0.03507810: 776it [18:56,  1.49s/it]2017-06-02 14:33:55,975 root  INFO     step 776.000000 - time: 1.618818, loss: 0.044851, perplexity: 1.045872, precision: 0.859375, batch_len: 142.000000
Train, loss=0.04485141: 777it [18:57,  1.53s/it]2017-06-02 14:33:57,448 root  INFO     step 777.000000 - time: 1.441230, loss: 0.115529, perplexity: 1.122467, precision: 0.875000, batch_len: 141.000000
Train, loss=0.11552864: 778it [18:59,  1.51s/it]2017-06-02 14:33:58,611 root  INFO     step 778.000000 - time: 1.026389, loss: 0.060196, perplexity: 1.062045, precision: 0.859375, batch_len: 77.000000
Train, loss=0.06019632: 779it [19:00,  1.41s/it]2017-06-02 14:34:00,153 root  INFO     step 779.000000 - time: 1.391030, loss: 0.028135, perplexity: 1.028534, precision: 0.921875, batch_len: 131.000000
Train, loss=0.02813497: 780it [19:02,  1.45s/it]2017-06-02 14:34:01,050 root  INFO     step 780.000000 - time: 0.881732, loss: 0.022834, perplexity: 1.023097, precision: 0.984375, batch_len: 71.000000
Train, loss=0.02283406: 781it [19:03,  1.28s/it]2017-06-02 14:34:01,271 root  INFO     Generating first batch)
2017-06-02 14:34:05,052 root  INFO     step 781.000000 - time: 0.978663, loss: 0.071372, perplexity: 1.073981, precision: 0.921875, batch_len: 96.000000
Train, loss=0.07137223: 782it [19:07,  2.10s/it]2017-06-02 14:34:06,230 root  INFO     step 782.000000 - time: 1.067421, loss: 0.035877, perplexity: 1.036528, precision: 0.937500, batch_len: 102.000000
Train, loss=0.03587705: 783it [19:08,  1.82s/it]2017-06-02 14:34:07,890 root  INFO     step 783.000000 - time: 1.294544, loss: 0.061418, perplexity: 1.063344, precision: 0.859375, batch_len: 113.000000
Train, loss=0.06141835: 784it [19:09,  1.77s/it]2017-06-02 14:34:09,149 root  INFO     step 784.000000 - time: 1.108954, loss: 0.078715, perplexity: 1.081896, precision: 0.875000, batch_len: 105.000000
Train, loss=0.07871478: 785it [19:11,  1.62s/it]2017-06-02 14:34:10,421 root  INFO     step 785.000000 - time: 1.091990, loss: 0.108417, perplexity: 1.114512, precision: 0.843750, batch_len: 110.000000
Train, loss=0.10841683: 786it [19:12,  1.52s/it]2017-06-02 14:34:11,404 root  INFO     step 786.000000 - time: 0.947903, loss: 0.093419, perplexity: 1.097922, precision: 0.875000, batch_len: 93.000000
Train, loss=0.09341919: 787it [19:13,  1.36s/it]2017-06-02 14:34:12,415 root  INFO     step 787.000000 - time: 0.999887, loss: 0.060546, perplexity: 1.062416, precision: 0.906250, batch_len: 104.000000
Train, loss=0.06054567: 788it [19:14,  1.25s/it]2017-06-02 14:34:14,181 root  INFO     step 788.000000 - time: 1.718183, loss: 0.064500, perplexity: 1.066626, precision: 0.875000, batch_len: 120.000000
Train, loss=0.06450044: 789it [19:16,  1.41s/it]2017-06-02 14:34:15,260 root  INFO     step 789.000000 - time: 1.072653, loss: 0.101876, perplexity: 1.107246, precision: 0.906250, batch_len: 100.000000
Train, loss=0.10187589: 790it [19:17,  1.31s/it]2017-06-02 14:34:16,396 root  INFO     step 790.000000 - time: 1.129723, loss: 0.091893, perplexity: 1.096248, precision: 0.875000, batch_len: 111.000000
Train, loss=0.09189311: 791it [19:18,  1.26s/it]2017-06-02 14:34:17,456 root  INFO     step 791.000000 - time: 1.041840, loss: 0.076802, perplexity: 1.079829, precision: 0.890625, batch_len: 103.000000
Train, loss=0.07680240: 792it [19:19,  1.20s/it]2017-06-02 14:34:18,396 root  INFO     step 792.000000 - time: 0.893997, loss: 0.113945, perplexity: 1.120690, precision: 0.781250, batch_len: 89.000000
Train, loss=0.11394475: 793it [19:20,  1.12s/it]2017-06-02 14:34:19,335 root  INFO     step 793.000000 - time: 0.922958, loss: 0.170948, perplexity: 1.186429, precision: 0.703125, batch_len: 88.000000
Train, loss=0.17094803: 794it [19:21,  1.07s/it]2017-06-02 14:34:20,769 root  INFO     step 794.000000 - time: 1.312354, loss: 0.258891, perplexity: 1.295492, precision: 0.671875, batch_len: 108.000000
Train, loss=0.25889063: 795it [19:22,  1.18s/it]2017-06-02 14:34:21,938 root  INFO     step 795.000000 - time: 1.155391, loss: 0.284743, perplexity: 1.329420, precision: 0.687500, batch_len: 85.000000
Train, loss=0.28474295: 796it [19:23,  1.17s/it]2017-06-02 14:34:22,966 root  INFO     step 796.000000 - time: 1.004550, loss: 0.104186, perplexity: 1.109807, precision: 0.781250, batch_len: 92.000000
Train, loss=0.10418601: 797it [19:24,  1.13s/it]2017-06-02 14:34:24,300 root  INFO     step 797.000000 - time: 1.288468, loss: 0.205354, perplexity: 1.227960, precision: 0.718750, batch_len: 124.000000
Train, loss=0.20535445: 798it [19:26,  1.19s/it]2017-06-02 14:34:25,321 root  INFO     step 798.000000 - time: 1.002960, loss: 0.088999, perplexity: 1.093080, precision: 0.812500, batch_len: 101.000000
Train, loss=0.08899945: 799it [19:27,  1.14s/it]2017-06-02 14:34:26,970 root  INFO     step 799.000000 - time: 1.608545, loss: 0.113241, perplexity: 1.119902, precision: 0.781250, batch_len: 128.000000
Train, loss=0.11324121: 800it [19:28,  1.29s/it]2017-06-02 14:34:28,208 root  INFO     step 800.000000 - time: 1.157441, loss: 0.211913, perplexity: 1.236041, precision: 0.734375, batch_len: 114.000000
Train, loss=0.21191345: 801it [19:30,  1.28s/it]2017-06-02 14:34:29,529 root  INFO     step 801.000000 - time: 1.290797, loss: 0.164456, perplexity: 1.178751, precision: 0.671875, batch_len: 97.000000
Train, loss=0.16445553: 802it [19:31,  1.29s/it]2017-06-02 14:34:30,651 root  INFO     step 802.000000 - time: 1.116066, loss: 0.130133, perplexity: 1.138980, precision: 0.843750, batch_len: 80.000000
Train, loss=0.13013326: 803it [19:32,  1.24s/it]2017-06-02 14:34:31,849 root  INFO     step 803.000000 - time: 1.149592, loss: 0.131657, perplexity: 1.140717, precision: 0.765625, batch_len: 106.000000
Train, loss=0.13165674: 804it [19:33,  1.23s/it]2017-06-02 14:34:33,256 root  INFO     step 804.000000 - time: 1.397473, loss: 0.094561, perplexity: 1.099176, precision: 0.750000, batch_len: 117.000000
Train, loss=0.09456055: 805it [19:35,  1.28s/it]2017-06-02 14:34:34,658 root  INFO     step 805.000000 - time: 1.239353, loss: 0.171274, perplexity: 1.186816, precision: 0.765625, batch_len: 121.000000
Train, loss=0.17127432: 806it [19:36,  1.32s/it]2017-06-02 14:34:35,918 root  INFO     step 806.000000 - time: 1.245367, loss: 0.192282, perplexity: 1.212013, precision: 0.718750, batch_len: 107.000000
Train, loss=0.19228236: 807it [19:37,  1.30s/it]2017-06-02 14:34:37,518 root  INFO     step 807.000000 - time: 1.521661, loss: 0.084702, perplexity: 1.088393, precision: 0.843750, batch_len: 123.000000
Train, loss=0.08470190: 808it [19:39,  1.39s/it]2017-06-02 14:34:39,144 root  INFO     step 808.000000 - time: 1.617227, loss: 0.156470, perplexity: 1.169375, precision: 0.796875, batch_len: 90.000000
Train, loss=0.15646984: 809it [19:41,  1.46s/it]2017-06-02 14:34:40,108 root  INFO     step 809.000000 - time: 0.928936, loss: 0.118252, perplexity: 1.125527, precision: 0.796875, batch_len: 91.000000
Train, loss=0.11825172: 810it [19:42,  1.31s/it]2017-06-02 14:34:41,137 root  INFO     step 810.000000 - time: 1.017876, loss: 0.124112, perplexity: 1.132143, precision: 0.796875, batch_len: 109.000000
Train, loss=0.12411222: 811it [19:43,  1.23s/it]2017-06-02 14:34:42,358 root  INFO     step 811.000000 - time: 1.106240, loss: 0.135225, perplexity: 1.144794, precision: 0.828125, batch_len: 81.000000
Train, loss=0.13522507: 812it [19:44,  1.23s/it]2017-06-02 14:34:43,565 root  INFO     step 812.000000 - time: 1.177426, loss: 0.089159, perplexity: 1.093255, precision: 0.937500, batch_len: 86.000000
Train, loss=0.08915928: 813it [19:45,  1.22s/it]2017-06-02 14:34:44,936 root  INFO     step 813.000000 - time: 1.340793, loss: 0.104132, perplexity: 1.109747, precision: 0.812500, batch_len: 112.000000
Train, loss=0.10413227: 814it [19:46,  1.27s/it]2017-06-02 14:34:46,080 root  INFO     step 814.000000 - time: 0.954807, loss: 0.127084, perplexity: 1.135512, precision: 0.796875, batch_len: 83.000000
Train, loss=0.12708387: 815it [19:48,  1.23s/it]2017-06-02 14:34:47,115 root  INFO     step 815.000000 - time: 0.994713, loss: 0.086200, perplexity: 1.090024, precision: 0.875000, batch_len: 99.000000
Train, loss=0.08620008: 816it [19:49,  1.17s/it]2017-06-02 14:34:48,715 root  INFO     step 816.000000 - time: 1.493270, loss: 0.101070, perplexity: 1.106354, precision: 0.843750, batch_len: 125.000000
Train, loss=0.10106950: 817it [19:50,  1.30s/it]2017-06-02 14:34:49,876 root  INFO     step 817.000000 - time: 1.151379, loss: 0.137958, perplexity: 1.147927, precision: 0.828125, batch_len: 79.000000
Train, loss=0.13795802: 818it [19:51,  1.26s/it]2017-06-02 14:34:51,337 root  INFO     step 818.000000 - time: 1.352276, loss: 0.042678, perplexity: 1.043602, precision: 0.921875, batch_len: 116.000000
Train, loss=0.04267820: 819it [19:53,  1.32s/it]2017-06-02 14:34:52,478 root  INFO     step 819.000000 - time: 1.065736, loss: 0.066065, perplexity: 1.068296, precision: 0.843750, batch_len: 98.000000
Train, loss=0.06606458: 820it [19:54,  1.27s/it]2017-06-02 14:34:53,778 root  INFO     step 820.000000 - time: 1.271785, loss: 0.083683, perplexity: 1.087285, precision: 0.890625, batch_len: 119.000000
Train, loss=0.08368348: 821it [19:55,  1.28s/it]2017-06-02 14:34:55,048 root  INFO     step 821.000000 - time: 1.161795, loss: 0.153786, perplexity: 1.166242, precision: 0.843750, batch_len: 94.000000
Train, loss=0.15378647: 822it [19:57,  1.27s/it]2017-06-02 14:34:56,353 root  INFO     step 822.000000 - time: 1.283868, loss: 0.129396, perplexity: 1.138140, precision: 0.828125, batch_len: 84.000000
Train, loss=0.12939578: 823it [19:58,  1.28s/it]2017-06-02 14:34:57,807 root  INFO     step 823.000000 - time: 1.346801, loss: 0.132353, perplexity: 1.141511, precision: 0.765625, batch_len: 137.000000
Train, loss=0.13235280: 824it [19:59,  1.33s/it]2017-06-02 14:34:58,989 root  INFO     step 824.000000 - time: 1.123186, loss: 0.121007, perplexity: 1.128633, precision: 0.859375, batch_len: 115.000000
Train, loss=0.12100712: 825it [20:00,  1.29s/it]2017-06-02 14:35:00,019 root  INFO     step 825.000000 - time: 0.897285, loss: 0.159287, perplexity: 1.172674, precision: 0.796875, batch_len: 82.000000
Train, loss=0.15928675: 826it [20:01,  1.21s/it]2017-06-02 14:35:01,419 root  INFO     step 826.000000 - time: 1.342064, loss: 0.163234, perplexity: 1.177313, precision: 0.812500, batch_len: 129.000000
Train, loss=0.16323447: 827it [20:03,  1.27s/it]2017-06-02 14:35:03,247 root  INFO     step 827.000000 - time: 1.776012, loss: 0.204265, perplexity: 1.226624, precision: 0.656250, batch_len: 144.000000
Train, loss=0.20426534: 828it [20:05,  1.44s/it]2017-06-02 14:35:04,723 root  INFO     step 828.000000 - time: 1.400182, loss: 0.258354, perplexity: 1.294797, precision: 0.656250, batch_len: 135.000000
Train, loss=0.25835416: 829it [20:06,  1.45s/it]2017-06-02 14:35:06,210 root  INFO     step 829.000000 - time: 1.435881, loss: 0.466968, perplexity: 1.595150, precision: 0.468750, batch_len: 133.000000
Train, loss=0.46696797: 830it [20:08,  1.46s/it]2017-06-02 14:35:07,197 root  INFO     step 830.000000 - time: 0.900026, loss: 0.262850, perplexity: 1.300631, precision: 0.562500, batch_len: 87.000000
Train, loss=0.26284990: 831it [20:09,  1.32s/it]2017-06-02 14:35:08,748 root  INFO     step 831.000000 - time: 1.478105, loss: 0.448453, perplexity: 1.565888, precision: 0.421875, batch_len: 118.000000
Train, loss=0.44845325: 832it [20:10,  1.39s/it]2017-06-02 14:35:10,525 root  INFO     step 832.000000 - time: 1.531912, loss: 0.772995, perplexity: 2.166244, precision: 0.078125, batch_len: 130.000000
Train, loss=0.77299488: 833it [20:12,  1.50s/it]2017-06-02 14:35:11,925 root  INFO     step 833.000000 - time: 1.326836, loss: 0.677824, perplexity: 1.969587, precision: 0.281250, batch_len: 126.000000
Train, loss=0.67782396: 834it [20:13,  1.47s/it]2017-06-02 14:35:13,317 root  INFO     step 834.000000 - time: 1.312260, loss: 0.271521, perplexity: 1.311959, precision: 0.484375, batch_len: 136.000000
Train, loss=0.27152115: 835it [20:15,  1.45s/it]2017-06-02 14:35:14,829 root  INFO     step 835.000000 - time: 1.090381, loss: 0.225108, perplexity: 1.252458, precision: 0.703125, batch_len: 78.000000
Train, loss=0.22510791: 836it [20:16,  1.47s/it]2017-06-02 14:35:16,520 root  INFO     step 836.000000 - time: 1.593092, loss: 0.112577, perplexity: 1.119159, precision: 0.765625, batch_len: 132.000000
Train, loss=0.11257707: 837it [20:18,  1.53s/it]2017-06-02 14:35:17,998 root  INFO     step 837.000000 - time: 1.436237, loss: 0.130442, perplexity: 1.139331, precision: 0.765625, batch_len: 138.000000
Train, loss=0.13044165: 838it [20:19,  1.52s/it]2017-06-02 14:35:19,026 root  INFO     step 838.000000 - time: 1.021565, loss: 0.068686, perplexity: 1.071100, precision: 0.843750, batch_len: 96.000000
Train, loss=0.06868649: 839it [20:20,  1.37s/it]2017-06-02 14:35:20,393 root  INFO     step 839.000000 - time: 1.255170, loss: 0.136507, perplexity: 1.146263, precision: 0.828125, batch_len: 141.000000
Train, loss=0.13650666: 840it [20:22,  1.37s/it]2017-06-02 14:35:22,107 root  INFO     step 840.000000 - time: 1.608302, loss: 0.112781, perplexity: 1.119387, precision: 0.796875, batch_len: 122.000000
Train, loss=0.11278084: 841it [20:24,  1.47s/it]2017-06-02 14:35:23,361 root  INFO     step 841.000000 - time: 1.173272, loss: 0.120280, perplexity: 1.127813, precision: 0.843750, batch_len: 76.000000
Train, loss=0.12027996: 842it [20:25,  1.41s/it]2017-06-02 14:35:25,327 root  INFO     step 842.000000 - time: 1.909072, loss: 0.076066, perplexity: 1.079034, precision: 0.828125, batch_len: 152.000000
Train, loss=0.07606583: 843it [20:27,  1.57s/it]2017-06-02 14:35:27,220 root  INFO     step 843.000000 - time: 1.838727, loss: 0.101352, perplexity: 1.106666, precision: 0.796875, batch_len: 150.000000
Train, loss=0.10135158: 844it [20:29,  1.67s/it]2017-06-02 14:35:28,423 root  INFO     step 844.000000 - time: 1.154532, loss: 0.042289, perplexity: 1.043196, precision: 0.937500, batch_len: 74.000000
Train, loss=0.04228862: 845it [20:30,  1.53s/it]2017-06-02 14:35:29,388 root  INFO     step 845.000000 - time: 0.955934, loss: 0.053928, perplexity: 1.055409, precision: 0.937500, batch_len: 72.000000
Train, loss=0.05392817: 846it [20:31,  1.36s/it]2017-06-02 14:35:31,176 root  INFO     step 846.000000 - time: 1.705817, loss: 0.043494, perplexity: 1.044454, precision: 0.906250, batch_len: 134.000000
Train, loss=0.04349440: 847it [20:33,  1.49s/it]2017-06-02 14:35:32,797 root  INFO     step 847.000000 - time: 1.409489, loss: 0.027524, perplexity: 1.027906, precision: 0.968750, batch_len: 131.000000
Train, loss=0.02752403: 848it [20:34,  1.53s/it]2017-06-02 14:35:34,256 root  INFO     step 848.000000 - time: 1.437315, loss: 0.051568, perplexity: 1.052920, precision: 0.906250, batch_len: 142.000000
Train, loss=0.05156760: 849it [20:36,  1.51s/it]2017-06-02 14:35:35,539 root  INFO     step 849.000000 - time: 1.266486, loss: 0.076895, perplexity: 1.079928, precision: 0.890625, batch_len: 139.000000
Train, loss=0.07689457: 850it [20:37,  1.44s/it]2017-06-02 14:35:36,680 root  INFO     step 850.000000 - time: 1.061946, loss: 0.059741, perplexity: 1.061562, precision: 0.875000, batch_len: 71.000000
Train, loss=0.05974133: 851it [20:38,  1.35s/it]2017-06-02 14:35:38,017 root  INFO     step 851.000000 - time: 1.167045, loss: 0.073988, perplexity: 1.076794, precision: 0.859375, batch_len: 77.000000
Train, loss=0.07398841: 852it [20:39,  1.35s/it]2017-06-02 14:35:38,144 root  INFO     Generating first batch)
2017-06-02 14:35:42,382 root  INFO     step 852.000000 - time: 1.257612, loss: 0.050987, perplexity: 1.052310, precision: 0.921875, batch_len: 96.000000
Train, loss=0.05098731: 853it [20:44,  2.25s/it]2017-06-02 14:35:43,935 root  INFO     step 853.000000 - time: 1.138383, loss: 0.099468, perplexity: 1.104583, precision: 0.843750, batch_len: 110.000000
Train, loss=0.09946777: 854it [20:45,  2.04s/it]2017-06-02 14:35:45,093 root  INFO     step 854.000000 - time: 1.095394, loss: 0.114078, perplexity: 1.120840, precision: 0.812500, batch_len: 113.000000
Train, loss=0.11407816: 855it [20:47,  1.78s/it]2017-06-02 14:35:46,159 root  INFO     step 855.000000 - time: 0.892441, loss: 0.141149, perplexity: 1.151596, precision: 0.875000, batch_len: 88.000000
Train, loss=0.14114887: 856it [20:48,  1.56s/it]2017-06-02 14:35:47,286 root  INFO     step 856.000000 - time: 1.027819, loss: 0.212112, perplexity: 1.236287, precision: 0.781250, batch_len: 101.000000
Train, loss=0.21211229: 857it [20:49,  1.43s/it]2017-06-02 14:35:48,707 root  INFO     step 857.000000 - time: 1.356719, loss: 0.357517, perplexity: 1.429775, precision: 0.703125, batch_len: 108.000000
Train, loss=0.35751718: 858it [20:50,  1.43s/it]2017-06-02 14:35:50,188 root  INFO     step 858.000000 - time: 1.422003, loss: 0.136060, perplexity: 1.145750, precision: 0.687500, batch_len: 128.000000
Train, loss=0.13605972: 859it [20:52,  1.44s/it]2017-06-02 14:35:51,223 root  INFO     step 859.000000 - time: 1.017910, loss: 0.177344, perplexity: 1.194042, precision: 0.781250, batch_len: 97.000000
Train, loss=0.17734426: 860it [20:53,  1.32s/it]2017-06-02 14:35:52,820 root  INFO     step 860.000000 - time: 1.552443, loss: 0.178430, perplexity: 1.195340, precision: 0.750000, batch_len: 90.000000
Train, loss=0.17843050: 861it [20:54,  1.40s/it]2017-06-02 14:35:53,718 root  INFO     step 861.000000 - time: 0.863989, loss: 0.080105, perplexity: 1.083401, precision: 0.906250, batch_len: 92.000000
Train, loss=0.08010504: 862it [20:55,  1.25s/it]2017-06-02 14:35:55,088 root  INFO     step 862.000000 - time: 1.251406, loss: 0.108399, perplexity: 1.114493, precision: 0.796875, batch_len: 100.000000
Train, loss=0.10839921: 863it [20:57,  1.29s/it]2017-06-02 14:35:56,444 root  INFO     step 863.000000 - time: 1.238491, loss: 0.122169, perplexity: 1.129945, precision: 0.812500, batch_len: 111.000000
Train, loss=0.12216870: 864it [20:58,  1.31s/it]2017-06-02 14:35:57,574 root  INFO     step 864.000000 - time: 1.100341, loss: 0.090864, perplexity: 1.095120, precision: 0.859375, batch_len: 104.000000
Train, loss=0.09086375: 865it [20:59,  1.25s/it]2017-06-02 14:35:58,752 root  INFO     step 865.000000 - time: 1.081013, loss: 0.065297, perplexity: 1.067477, precision: 0.859375, batch_len: 107.000000
Train, loss=0.06529745: 866it [21:00,  1.23s/it]2017-06-02 14:35:59,976 root  INFO     step 866.000000 - time: 1.183710, loss: 0.060537, perplexity: 1.062407, precision: 0.906250, batch_len: 117.000000
Train, loss=0.06053716: 867it [21:01,  1.23s/it]2017-06-02 14:36:01,151 root  INFO     step 867.000000 - time: 1.171090, loss: 0.106631, perplexity: 1.112523, precision: 0.812500, batch_len: 102.000000
Train, loss=0.10663055: 868it [21:03,  1.21s/it]2017-06-02 14:36:02,456 root  INFO     step 868.000000 - time: 1.297734, loss: 0.086566, perplexity: 1.090423, precision: 0.812500, batch_len: 89.000000
Train, loss=0.08656568: 869it [21:04,  1.24s/it]2017-06-02 14:36:03,836 root  INFO     step 869.000000 - time: 1.334873, loss: 0.090628, perplexity: 1.094862, precision: 0.812500, batch_len: 120.000000
Train, loss=0.09062836: 870it [21:05,  1.28s/it]2017-06-02 14:36:05,211 root  INFO     step 870.000000 - time: 1.336618, loss: 0.045927, perplexity: 1.046998, precision: 0.921875, batch_len: 116.000000
Train, loss=0.04592732: 871it [21:07,  1.31s/it]2017-06-02 14:36:06,294 root  INFO     step 871.000000 - time: 0.982027, loss: 0.109311, perplexity: 1.115510, precision: 0.828125, batch_len: 93.000000
Train, loss=0.10931148: 872it [21:08,  1.24s/it]2017-06-02 14:36:07,291 root  INFO     step 872.000000 - time: 0.928805, loss: 0.121702, perplexity: 1.129417, precision: 0.812500, batch_len: 81.000000
Train, loss=0.12170158: 873it [21:09,  1.17s/it]2017-06-02 14:36:08,624 root  INFO     step 873.000000 - time: 1.325560, loss: 0.133847, perplexity: 1.143217, precision: 0.796875, batch_len: 114.000000
Train, loss=0.13384664: 874it [21:10,  1.22s/it]2017-06-02 14:36:09,805 root  INFO     step 874.000000 - time: 1.124446, loss: 0.212312, perplexity: 1.236534, precision: 0.750000, batch_len: 86.000000
Train, loss=0.21231206: 875it [21:11,  1.21s/it]2017-06-02 14:36:11,137 root  INFO     step 875.000000 - time: 1.318650, loss: 0.135909, perplexity: 1.145578, precision: 0.859375, batch_len: 112.000000
Train, loss=0.13590911: 876it [21:13,  1.24s/it]2017-06-02 14:36:12,272 root  INFO     step 876.000000 - time: 1.078698, loss: 0.101292, perplexity: 1.106600, precision: 0.828125, batch_len: 109.000000
Train, loss=0.10129216: 877it [21:14,  1.21s/it]2017-06-02 14:36:13,511 root  INFO     step 877.000000 - time: 1.230563, loss: 0.089968, perplexity: 1.094139, precision: 0.875000, batch_len: 137.000000
Train, loss=0.08996780: 878it [21:15,  1.22s/it]2017-06-02 14:36:15,114 root  INFO     step 878.000000 - time: 1.536857, loss: 0.053648, perplexity: 1.055113, precision: 0.859375, batch_len: 121.000000
Train, loss=0.05364812: 879it [21:17,  1.33s/it]2017-06-02 14:36:16,429 root  INFO     step 879.000000 - time: 1.275567, loss: 0.087516, perplexity: 1.091460, precision: 0.921875, batch_len: 115.000000
Train, loss=0.08751634: 880it [21:18,  1.33s/it]2017-06-02 14:36:17,496 root  INFO     step 880.000000 - time: 0.984715, loss: 0.087299, perplexity: 1.091223, precision: 0.875000, batch_len: 91.000000
Train, loss=0.08729945: 881it [21:19,  1.25s/it]2017-06-02 14:36:18,545 root  INFO     step 881.000000 - time: 1.029321, loss: 0.055598, perplexity: 1.057173, precision: 0.875000, batch_len: 103.000000
Train, loss=0.05559794: 882it [21:20,  1.19s/it]2017-06-02 14:36:19,858 root  INFO     step 882.000000 - time: 1.283743, loss: 0.105515, perplexity: 1.111283, precision: 0.859375, batch_len: 126.000000
Train, loss=0.10551502: 883it [21:21,  1.23s/it]2017-06-02 14:36:21,065 root  INFO     step 883.000000 - time: 1.201978, loss: 0.077146, perplexity: 1.080200, precision: 0.890625, batch_len: 125.000000
Train, loss=0.07714599: 884it [21:23,  1.22s/it]2017-06-02 14:36:22,432 root  INFO     step 884.000000 - time: 1.270225, loss: 0.103222, perplexity: 1.108738, precision: 0.843750, batch_len: 105.000000
Train, loss=0.10322203: 885it [21:24,  1.26s/it]2017-06-02 14:36:23,949 root  INFO     step 885.000000 - time: 1.463056, loss: 0.050817, perplexity: 1.052130, precision: 0.875000, batch_len: 129.000000
Train, loss=0.05081678: 886it [21:25,  1.34s/it]2017-06-02 14:36:24,932 root  INFO     step 886.000000 - time: 0.962872, loss: 0.081060, perplexity: 1.084436, precision: 0.921875, batch_len: 80.000000
Train, loss=0.08105960: 887it [21:26,  1.23s/it]2017-06-02 14:36:26,122 root  INFO     step 887.000000 - time: 1.033865, loss: 0.137022, perplexity: 1.146853, precision: 0.812500, batch_len: 85.000000
Train, loss=0.13702205: 888it [21:28,  1.22s/it]2017-06-02 14:36:27,504 root  INFO     step 888.000000 - time: 1.358642, loss: 0.052836, perplexity: 1.054257, precision: 0.828125, batch_len: 124.000000
Train, loss=0.05283589: 889it [21:29,  1.27s/it]2017-06-02 14:36:28,742 root  INFO     step 889.000000 - time: 1.083797, loss: 0.086630, perplexity: 1.090493, precision: 0.859375, batch_len: 79.000000
Train, loss=0.08662970: 890it [21:30,  1.26s/it]2017-06-02 14:36:30,053 root  INFO     step 890.000000 - time: 1.275852, loss: 0.069825, perplexity: 1.072321, precision: 0.875000, batch_len: 106.000000
Train, loss=0.06982522: 891it [21:32,  1.27s/it]2017-06-02 14:36:31,643 root  INFO     step 891.000000 - time: 1.583086, loss: 0.091340, perplexity: 1.095641, precision: 0.875000, batch_len: 118.000000
Train, loss=0.09133959: 892it [21:33,  1.37s/it]2017-06-02 14:36:32,700 root  INFO     step 892.000000 - time: 0.986791, loss: 0.211424, perplexity: 1.235437, precision: 0.750000, batch_len: 83.000000
Train, loss=0.21142447: 893it [21:34,  1.28s/it]2017-06-02 14:36:33,808 root  INFO     step 893.000000 - time: 1.051881, loss: 0.196948, perplexity: 1.217680, precision: 0.781250, batch_len: 84.000000
Train, loss=0.19694752: 894it [21:35,  1.23s/it]2017-06-02 14:36:35,163 root  INFO     step 894.000000 - time: 1.265514, loss: 0.060330, perplexity: 1.062187, precision: 0.906250, batch_len: 123.000000
Train, loss=0.06032966: 895it [21:37,  1.26s/it]2017-06-02 14:36:36,455 root  INFO     step 895.000000 - time: 1.099277, loss: 0.111087, perplexity: 1.117492, precision: 0.875000, batch_len: 94.000000
Train, loss=0.11108722: 896it [21:38,  1.27s/it]2017-06-02 14:36:38,150 root  INFO     step 896.000000 - time: 1.587043, loss: 0.087883, perplexity: 1.091860, precision: 0.859375, batch_len: 135.000000
Train, loss=0.08788306: 897it [21:40,  1.40s/it]2017-06-02 14:36:39,164 root  INFO     step 897.000000 - time: 0.992114, loss: 0.049909, perplexity: 1.051175, precision: 0.968750, batch_len: 87.000000
Train, loss=0.04990897: 898it [21:41,  1.28s/it]2017-06-02 14:36:40,424 root  INFO     step 898.000000 - time: 1.074020, loss: 0.028407, perplexity: 1.028814, precision: 0.968750, batch_len: 98.000000
Train, loss=0.02840708: 899it [21:42,  1.28s/it]2017-06-02 14:36:41,355 root  INFO     step 899.000000 - time: 0.926750, loss: 0.088221, perplexity: 1.092230, precision: 0.921875, batch_len: 82.000000
Train, loss=0.08822122: 900it [21:43,  1.17s/it]2017-06-02 14:36:42,717 root  INFO     step 900.000000 - time: 1.331922, loss: 0.040784, perplexity: 1.041628, precision: 0.906250, batch_len: 133.000000
Train, loss=0.04078449: 901it [21:44,  1.23s/it]2017-06-02 14:36:44,483 root  INFO     step 901.000000 - time: 1.647408, loss: 0.072826, perplexity: 1.075543, precision: 0.890625, batch_len: 122.000000
Train, loss=0.07282599: 902it [21:46,  1.39s/it]2017-06-02 14:36:46,020 root  INFO     step 902.000000 - time: 1.351969, loss: 0.049921, perplexity: 1.051188, precision: 0.906250, batch_len: 136.000000
Train, loss=0.04992093: 903it [21:47,  1.43s/it]2017-06-02 14:36:47,179 root  INFO     step 903.000000 - time: 1.008024, loss: 0.050354, perplexity: 1.051644, precision: 0.906250, batch_len: 96.000000
Train, loss=0.05035422: 904it [21:49,  1.35s/it]2017-06-02 14:36:48,224 root  INFO     step 904.000000 - time: 1.037047, loss: 0.038970, perplexity: 1.039739, precision: 0.937500, batch_len: 99.000000
Train, loss=0.03897011: 905it [21:50,  1.26s/it]2017-06-02 14:36:49,684 root  INFO     step 905.000000 - time: 1.423312, loss: 0.070054, perplexity: 1.072566, precision: 0.890625, batch_len: 119.000000
Train, loss=0.07005362: 906it [21:51,  1.32s/it]2017-06-02 14:36:51,356 root  INFO     step 906.000000 - time: 1.641885, loss: 0.040445, perplexity: 1.041274, precision: 0.890625, batch_len: 130.000000
Train, loss=0.04044536: 907it [21:53,  1.43s/it]2017-06-02 14:36:53,364 root  INFO     step 907.000000 - time: 1.862782, loss: 0.038916, perplexity: 1.039683, precision: 0.921875, batch_len: 152.000000
Train, loss=0.03891593: 908it [21:55,  1.60s/it]2017-06-02 14:36:54,805 root  INFO     step 908.000000 - time: 1.398298, loss: 0.043663, perplexity: 1.044630, precision: 0.906250, batch_len: 134.000000
Train, loss=0.04366304: 909it [21:56,  1.55s/it]2017-06-02 14:36:56,386 root  INFO     step 909.000000 - time: 1.569569, loss: 0.057090, perplexity: 1.058752, precision: 0.921875, batch_len: 144.000000
Train, loss=0.05709039: 910it [21:58,  1.56s/it]2017-06-02 14:36:58,074 root  INFO     step 910.000000 - time: 1.555507, loss: 0.022437, perplexity: 1.022691, precision: 0.937500, batch_len: 132.000000
Train, loss=0.02243700: 911it [22:00,  1.60s/it]2017-06-02 14:37:00,010 root  INFO     step 911.000000 - time: 1.864171, loss: 0.045466, perplexity: 1.046515, precision: 0.937500, batch_len: 150.000000
Train, loss=0.04546574: 912it [22:01,  1.70s/it]2017-06-02 14:37:01,478 root  INFO     step 912.000000 - time: 1.346387, loss: 0.059374, perplexity: 1.061172, precision: 0.921875, batch_len: 141.000000
Train, loss=0.05937421: 913it [22:03,  1.63s/it]2017-06-02 14:37:02,424 root  INFO     step 913.000000 - time: 0.865381, loss: 0.061290, perplexity: 1.063207, precision: 0.875000, batch_len: 78.000000
Train, loss=0.06129026: 914it [22:04,  1.43s/it]2017-06-02 14:37:04,166 root  INFO     step 914.000000 - time: 1.685575, loss: 0.028012, perplexity: 1.028408, precision: 0.953125, batch_len: 138.000000
Train, loss=0.02801228: 915it [22:06,  1.52s/it]2017-06-02 14:37:05,659 root  INFO     step 915.000000 - time: 1.464927, loss: 0.038185, perplexity: 1.038923, precision: 0.937500, batch_len: 139.000000
Train, loss=0.03818487: 916it [22:07,  1.51s/it]2017-06-02 14:37:06,692 root  INFO     step 916.000000 - time: 0.976816, loss: 0.018510, perplexity: 1.018683, precision: 0.968750, batch_len: 77.000000
Train, loss=0.01851039: 917it [22:08,  1.37s/it]2017-06-02 14:37:07,654 root  INFO     step 917.000000 - time: 0.939179, loss: 0.017496, perplexity: 1.017649, precision: 1.000000, batch_len: 74.000000
Train, loss=0.01749554: 918it [22:09,  1.25s/it]2017-06-02 14:37:08,683 root  INFO     step 918.000000 - time: 0.887862, loss: 0.053906, perplexity: 1.055385, precision: 0.953125, batch_len: 76.000000
Train, loss=0.05390555: 919it [22:10,  1.18s/it]2017-06-02 14:37:10,152 root  INFO     step 919.000000 - time: 1.283446, loss: 0.040898, perplexity: 1.041746, precision: 0.953125, batch_len: 72.000000
Train, loss=0.04089801: 920it [22:12,  1.27s/it]2017-06-02 14:37:11,729 root  INFO     step 920.000000 - time: 1.542290, loss: 0.026031, perplexity: 1.026373, precision: 0.937500, batch_len: 131.000000
Train, loss=0.02603120: 921it [22:13,  1.36s/it]2017-06-02 14:37:13,316 root  INFO     step 921.000000 - time: 1.439639, loss: 0.042358, perplexity: 1.043268, precision: 0.906250, batch_len: 142.000000
Train, loss=0.04235845: 922it [22:15,  1.43s/it]2017-06-02 14:37:14,395 root  INFO     step 922.000000 - time: 0.917518, loss: 0.037561, perplexity: 1.038276, precision: 0.968750, batch_len: 71.000000
Train, loss=0.03756126: 923it [22:16,  1.32s/it]2017-06-02 14:37:14,523 root  INFO     Generating first batch)
2017-06-02 14:37:18,265 root  INFO     step 923.000000 - time: 1.031407, loss: 0.043430, perplexity: 1.044387, precision: 0.937500, batch_len: 96.000000
Train, loss=0.04342992: 924it [22:20,  2.09s/it]2017-06-02 14:37:19,825 root  INFO     step 924.000000 - time: 1.238673, loss: 0.101576, perplexity: 1.106914, precision: 0.843750, batch_len: 120.000000
Train, loss=0.10157618: 925it [22:21,  1.93s/it]2017-06-02 14:37:21,345 root  INFO     step 925.000000 - time: 1.221255, loss: 0.124191, perplexity: 1.132232, precision: 0.812500, batch_len: 93.000000
Train, loss=0.12419073: 926it [22:23,  1.81s/it]2017-06-02 14:37:22,636 root  INFO     step 926.000000 - time: 1.244682, loss: 0.143231, perplexity: 1.153997, precision: 0.734375, batch_len: 105.000000
Train, loss=0.14323124: 927it [22:24,  1.65s/it]2017-06-02 14:37:23,812 root  INFO     step 927.000000 - time: 1.149482, loss: 0.167930, perplexity: 1.182854, precision: 0.765625, batch_len: 114.000000
Train, loss=0.16793007: 928it [22:25,  1.51s/it]2017-06-02 14:37:24,960 root  INFO     step 928.000000 - time: 1.117189, loss: 0.136928, perplexity: 1.146745, precision: 0.828125, batch_len: 113.000000
Train, loss=0.13692769: 929it [22:26,  1.40s/it]2017-06-02 14:37:26,102 root  INFO     step 929.000000 - time: 1.012341, loss: 0.098926, perplexity: 1.103985, precision: 0.921875, batch_len: 92.000000
Train, loss=0.09892616: 930it [22:28,  1.32s/it]2017-06-02 14:37:27,064 root  INFO     step 930.000000 - time: 0.943747, loss: 0.110209, perplexity: 1.116511, precision: 0.890625, batch_len: 88.000000
Train, loss=0.11020894: 931it [22:29,  1.21s/it]2017-06-02 14:37:28,224 root  INFO     step 931.000000 - time: 1.119249, loss: 0.044411, perplexity: 1.045412, precision: 0.906250, batch_len: 101.000000
Train, loss=0.04441066: 932it [22:30,  1.20s/it]2017-06-02 14:37:29,532 root  INFO     step 932.000000 - time: 1.220201, loss: 0.068356, perplexity: 1.070747, precision: 0.843750, batch_len: 110.000000
Train, loss=0.06835645: 933it [22:31,  1.23s/it]2017-06-02 14:37:30,869 root  INFO     step 933.000000 - time: 1.324185, loss: 0.077041, perplexity: 1.080086, precision: 0.890625, batch_len: 108.000000
Train, loss=0.07704095: 934it [22:32,  1.26s/it]2017-06-02 14:37:31,975 root  INFO     step 934.000000 - time: 1.065749, loss: 0.084199, perplexity: 1.087846, precision: 0.843750, batch_len: 100.000000
Train, loss=0.08419923: 935it [22:33,  1.22s/it]2017-06-02 14:37:33,372 root  INFO     step 935.000000 - time: 1.385671, loss: 0.081508, perplexity: 1.084922, precision: 0.843750, batch_len: 128.000000
Train, loss=0.08150806: 936it [22:35,  1.27s/it]2017-06-02 14:37:34,599 root  INFO     step 936.000000 - time: 1.070417, loss: 0.125576, perplexity: 1.133801, precision: 0.859375, batch_len: 111.000000
Train, loss=0.12557569: 937it [22:36,  1.26s/it]2017-06-02 14:37:35,877 root  INFO     step 937.000000 - time: 1.075021, loss: 0.202881, perplexity: 1.224927, precision: 0.796875, batch_len: 86.000000
Train, loss=0.20288113: 938it [22:37,  1.26s/it]2017-06-02 14:37:37,196 root  INFO     step 938.000000 - time: 1.303523, loss: 0.075012, perplexity: 1.077897, precision: 0.875000, batch_len: 103.000000
Train, loss=0.07501233: 939it [22:39,  1.28s/it]2017-06-02 14:37:38,431 root  INFO     step 939.000000 - time: 1.127680, loss: 0.078412, perplexity: 1.081569, precision: 0.843750, batch_len: 109.000000
Train, loss=0.07841232: 940it [22:40,  1.27s/it]2017-06-02 14:37:39,892 root  INFO     step 940.000000 - time: 1.351053, loss: 0.035965, perplexity: 1.036619, precision: 0.906250, batch_len: 123.000000
Train, loss=0.03596482: 941it [22:41,  1.32s/it]2017-06-02 14:37:40,929 root  INFO     step 941.000000 - time: 0.988502, loss: 0.066057, perplexity: 1.068288, precision: 0.859375, batch_len: 102.000000
Train, loss=0.06605718: 942it [22:42,  1.24s/it]2017-06-02 14:37:42,009 root  INFO     step 942.000000 - time: 1.019899, loss: 0.071852, perplexity: 1.074496, precision: 0.875000, batch_len: 106.000000
Train, loss=0.07185197: 943it [22:43,  1.19s/it]2017-06-02 14:37:43,167 root  INFO     step 943.000000 - time: 1.129841, loss: 0.094764, perplexity: 1.099399, precision: 0.859375, batch_len: 80.000000
Train, loss=0.09476388: 944it [22:45,  1.18s/it]2017-06-02 14:37:44,558 root  INFO     step 944.000000 - time: 1.287907, loss: 0.078340, perplexity: 1.081491, precision: 0.875000, batch_len: 112.000000
Train, loss=0.07834046: 945it [22:46,  1.24s/it]2017-06-02 14:37:45,963 root  INFO     step 945.000000 - time: 1.349050, loss: 0.036332, perplexity: 1.037000, precision: 0.921875, batch_len: 117.000000
Train, loss=0.03633158: 946it [22:47,  1.29s/it]2017-06-02 14:37:47,340 root  INFO     step 946.000000 - time: 1.365026, loss: 0.057715, perplexity: 1.059413, precision: 0.875000, batch_len: 121.000000
Train, loss=0.05771456: 947it [22:49,  1.32s/it]2017-06-02 14:37:48,283 root  INFO     step 947.000000 - time: 0.938352, loss: 0.056933, perplexity: 1.058585, precision: 0.890625, batch_len: 97.000000
Train, loss=0.05693265: 948it [22:50,  1.21s/it]2017-06-02 14:37:49,763 root  INFO     step 948.000000 - time: 1.329980, loss: 0.067930, perplexity: 1.070291, precision: 0.906250, batch_len: 115.000000
Train, loss=0.06793031: 949it [22:51,  1.29s/it]2017-06-02 14:37:51,270 root  INFO     step 949.000000 - time: 1.479279, loss: 0.040099, perplexity: 1.040913, precision: 0.921875, batch_len: 124.000000
Train, loss=0.04009853: 950it [22:53,  1.35s/it]2017-06-02 14:37:52,374 root  INFO     step 950.000000 - time: 1.003937, loss: 0.103151, perplexity: 1.108659, precision: 0.812500, batch_len: 85.000000
Train, loss=0.10315123: 951it [22:54,  1.28s/it]2017-06-02 14:37:53,446 root  INFO     step 951.000000 - time: 0.944483, loss: 0.120783, perplexity: 1.128380, precision: 0.750000, batch_len: 81.000000
Train, loss=0.12078279: 952it [22:55,  1.22s/it]2017-06-02 14:37:54,590 root  INFO     step 952.000000 - time: 1.063477, loss: 0.046214, perplexity: 1.047299, precision: 0.890625, batch_len: 98.000000
Train, loss=0.04621420: 953it [22:56,  1.19s/it]2017-06-02 14:37:55,635 root  INFO     step 953.000000 - time: 1.028132, loss: 0.047018, perplexity: 1.048141, precision: 0.921875, batch_len: 104.000000
Train, loss=0.04701792: 954it [22:57,  1.15s/it]2017-06-02 14:37:56,824 root  INFO     step 954.000000 - time: 1.164418, loss: 0.135947, perplexity: 1.145621, precision: 0.859375, batch_len: 94.000000
Train, loss=0.13594703: 955it [22:58,  1.16s/it]2017-06-02 14:37:57,993 root  INFO     step 955.000000 - time: 1.121834, loss: 0.104100, perplexity: 1.109712, precision: 0.875000, batch_len: 79.000000
Train, loss=0.10410031: 956it [22:59,  1.16s/it]2017-06-02 14:37:59,380 root  INFO     step 956.000000 - time: 1.310869, loss: 0.064496, perplexity: 1.066621, precision: 0.890625, batch_len: 125.000000
Train, loss=0.06449560: 957it [23:01,  1.23s/it]2017-06-02 14:38:00,371 root  INFO     step 957.000000 - time: 0.959174, loss: 0.109098, perplexity: 1.115272, precision: 0.875000, batch_len: 91.000000
Train, loss=0.10909796: 958it [23:02,  1.16s/it]2017-06-02 14:38:01,614 root  INFO     step 958.000000 - time: 1.203386, loss: 0.038342, perplexity: 1.039087, precision: 0.906250, batch_len: 118.000000
Train, loss=0.03834249: 959it [23:03,  1.18s/it]2017-06-02 14:38:02,674 root  INFO     step 959.000000 - time: 1.031683, loss: 0.040296, perplexity: 1.041119, precision: 0.921875, batch_len: 89.000000
Train, loss=0.04029649: 960it [23:04,  1.15s/it]2017-06-02 14:38:03,819 root  INFO     step 960.000000 - time: 1.107154, loss: 0.058509, perplexity: 1.060255, precision: 0.937500, batch_len: 90.000000
Train, loss=0.05850901: 961it [23:05,  1.15s/it]2017-06-02 14:38:04,892 root  INFO     step 961.000000 - time: 1.046889, loss: 0.073552, perplexity: 1.076324, precision: 0.843750, batch_len: 83.000000
Train, loss=0.07355176: 962it [23:06,  1.12s/it]2017-06-02 14:38:06,434 root  INFO     step 962.000000 - time: 1.373094, loss: 0.061025, perplexity: 1.062926, precision: 0.906250, batch_len: 119.000000
Train, loss=0.06102528: 963it [23:08,  1.25s/it]2017-06-02 14:38:07,440 root  INFO     step 963.000000 - time: 0.991005, loss: 0.105231, perplexity: 1.110967, precision: 0.828125, batch_len: 84.000000
Train, loss=0.10523067: 964it [23:09,  1.18s/it]2017-06-02 14:38:08,790 root  INFO     step 964.000000 - time: 1.242782, loss: 0.073377, perplexity: 1.076137, precision: 0.875000, batch_len: 116.000000
Train, loss=0.07337739: 965it [23:10,  1.23s/it]2017-06-02 14:38:10,175 root  INFO     step 965.000000 - time: 1.253252, loss: 0.102798, perplexity: 1.108267, precision: 0.828125, batch_len: 107.000000
Train, loss=0.10279771: 966it [23:12,  1.28s/it]2017-06-02 14:38:11,333 root  INFO     step 966.000000 - time: 1.110664, loss: 0.023133, perplexity: 1.023403, precision: 0.968750, batch_len: 87.000000
Train, loss=0.02313346: 967it [23:13,  1.24s/it]2017-06-02 14:38:12,886 root  INFO     step 967.000000 - time: 1.410336, loss: 0.082361, perplexity: 1.085848, precision: 0.875000, batch_len: 135.000000
Train, loss=0.08236128: 968it [23:14,  1.33s/it]2017-06-02 14:38:14,383 root  INFO     step 968.000000 - time: 1.403649, loss: 0.098409, perplexity: 1.103414, precision: 0.953125, batch_len: 130.000000
Train, loss=0.09840880: 969it [23:16,  1.38s/it]2017-06-02 14:38:15,672 root  INFO     step 969.000000 - time: 1.037285, loss: 0.050050, perplexity: 1.051323, precision: 0.921875, batch_len: 82.000000
Train, loss=0.05004976: 970it [23:17,  1.35s/it]2017-06-02 14:38:17,345 root  INFO     step 970.000000 - time: 1.570984, loss: 0.104234, perplexity: 1.109861, precision: 0.875000, batch_len: 126.000000
Train, loss=0.10423449: 971it [23:19,  1.45s/it]2017-06-02 14:38:18,465 root  INFO     step 971.000000 - time: 1.052647, loss: 0.060141, perplexity: 1.061986, precision: 0.890625, batch_len: 99.000000
Train, loss=0.06014065: 972it [23:20,  1.35s/it]2017-06-02 14:38:19,500 root  INFO     step 972.000000 - time: 1.000312, loss: 0.034513, perplexity: 1.035116, precision: 0.906250, batch_len: 96.000000
Train, loss=0.03451345: 973it [23:21,  1.26s/it]2017-06-02 14:38:21,085 root  INFO     step 973.000000 - time: 1.419985, loss: 0.039382, perplexity: 1.040167, precision: 0.937500, batch_len: 138.000000
Train, loss=0.03938168: 974it [23:23,  1.35s/it]2017-06-02 14:38:22,686 root  INFO     step 974.000000 - time: 1.547465, loss: 0.026588, perplexity: 1.026944, precision: 0.906250, batch_len: 133.000000
Train, loss=0.02658775: 975it [23:24,  1.43s/it]2017-06-02 14:38:24,244 root  INFO     step 975.000000 - time: 1.553356, loss: 0.031711, perplexity: 1.032219, precision: 0.921875, batch_len: 137.000000
Train, loss=0.03171098: 976it [23:26,  1.47s/it]2017-06-02 14:38:25,630 root  INFO     step 976.000000 - time: 1.373289, loss: 0.039525, perplexity: 1.040317, precision: 0.906250, batch_len: 122.000000
Train, loss=0.03952509: 977it [23:27,  1.44s/it]2017-06-02 14:38:27,047 root  INFO     step 977.000000 - time: 1.392340, loss: 0.041662, perplexity: 1.042542, precision: 0.890625, batch_len: 129.000000
Train, loss=0.04166199: 978it [23:29,  1.44s/it]2017-06-02 14:38:28,033 root  INFO     step 978.000000 - time: 0.928367, loss: 0.034822, perplexity: 1.035436, precision: 0.921875, batch_len: 78.000000
Train, loss=0.03482228: 979it [23:30,  1.30s/it]2017-06-02 14:38:29,551 root  INFO     step 979.000000 - time: 1.483583, loss: 0.074944, perplexity: 1.077824, precision: 0.875000, batch_len: 141.000000
Train, loss=0.07494396: 980it [23:31,  1.37s/it]2017-06-02 14:38:30,895 root  INFO     step 980.000000 - time: 1.318400, loss: 0.036239, perplexity: 1.036903, precision: 0.953125, batch_len: 144.000000
Train, loss=0.03623865: 981it [23:32,  1.36s/it]2017-06-02 14:38:33,141 root  INFO     step 981.000000 - time: 2.218961, loss: 0.035734, perplexity: 1.036380, precision: 0.906250, batch_len: 152.000000
Train, loss=0.03573351: 982it [23:35,  1.63s/it]2017-06-02 14:38:34,632 root  INFO     step 982.000000 - time: 1.443162, loss: 0.050261, perplexity: 1.051545, precision: 0.921875, batch_len: 139.000000
Train, loss=0.05026061: 983it [23:36,  1.58s/it]2017-06-02 14:38:36,044 root  INFO     step 983.000000 - time: 1.373393, loss: 0.012591, perplexity: 1.012670, precision: 0.984375, batch_len: 136.000000
Train, loss=0.01259063: 984it [23:38,  1.53s/it]2017-06-02 14:38:37,082 root  INFO     step 984.000000 - time: 0.769152, loss: 0.018388, perplexity: 1.018558, precision: 0.984375, batch_len: 74.000000
Train, loss=0.01838812: 985it [23:39,  1.38s/it]2017-06-02 14:38:38,710 root  INFO     step 985.000000 - time: 1.613799, loss: 0.014327, perplexity: 1.014430, precision: 0.984375, batch_len: 132.000000
Train, loss=0.01432691: 986it [23:40,  1.46s/it]2017-06-02 14:38:40,118 root  INFO     step 986.000000 - time: 1.227000, loss: 0.045011, perplexity: 1.046039, precision: 0.937500, batch_len: 72.000000
Train, loss=0.04501110: 987it [23:42,  1.44s/it]2017-06-02 14:38:42,206 root  INFO     step 987.000000 - time: 1.841071, loss: 0.026851, perplexity: 1.027215, precision: 0.953125, batch_len: 150.000000
Train, loss=0.02685107: 988it [23:44,  1.64s/it]2017-06-02 14:38:43,119 root  INFO     step 988.000000 - time: 0.905221, loss: 0.060486, perplexity: 1.062353, precision: 0.937500, batch_len: 76.000000
Train, loss=0.06048606: 989it [23:45,  1.42s/it]2017-06-02 14:38:44,572 root  INFO     step 989.000000 - time: 1.431822, loss: 0.018845, perplexity: 1.019023, precision: 0.937500, batch_len: 134.000000
Train, loss=0.01884469: 990it [23:46,  1.43s/it]2017-06-02 14:38:46,307 root  INFO     step 990.000000 - time: 1.642778, loss: 0.032298, perplexity: 1.032825, precision: 0.906250, batch_len: 142.000000
Train, loss=0.03229794: 991it [23:48,  1.52s/it]2017-06-02 14:38:47,780 root  INFO     step 991.000000 - time: 1.394614, loss: 0.018449, perplexity: 1.018620, precision: 0.953125, batch_len: 131.000000
Train, loss=0.01844924: 992it [23:49,  1.51s/it]2017-06-02 14:38:48,920 root  INFO     step 992.000000 - time: 0.947601, loss: 0.023172, perplexity: 1.023442, precision: 0.953125, batch_len: 71.000000
Train, loss=0.02317164: 993it [23:50,  1.40s/it]2017-06-02 14:38:49,786 root  INFO     step 993.000000 - time: 0.847705, loss: 0.017709, perplexity: 1.017867, precision: 0.937500, batch_len: 77.000000
Train, loss=0.01770882: 994it [23:51,  1.24s/it]2017-06-02 14:38:49,887 root  INFO     Generating first batch)
2017-06-02 14:38:54,195 root  INFO     step 994.000000 - time: 1.084670, loss: 0.086247, perplexity: 1.090076, precision: 0.906250, batch_len: 108.000000
Train, loss=0.08624700: 995it [23:56,  2.19s/it]2017-06-02 14:38:55,224 root  INFO     step 995.000000 - time: 0.977549, loss: 0.019455, perplexity: 1.019645, precision: 0.968750, batch_len: 96.000000
Train, loss=0.01945477: 996it [23:57,  1.84s/it]2017-06-02 14:38:56,758 root  INFO     step 996.000000 - time: 1.378675, loss: 0.062804, perplexity: 1.064818, precision: 0.906250, batch_len: 113.000000
Train, loss=0.06280415: 997it [23:58,  1.75s/it]2017-06-02 14:38:58,042 root  INFO     step 997.000000 - time: 1.075513, loss: 0.092063, perplexity: 1.096434, precision: 0.859375, batch_len: 104.000000
Train, loss=0.09206299: 998it [24:00,  1.61s/it]2017-06-02 14:38:59,248 root  INFO     step 998.000000 - time: 1.109070, loss: 0.224307, perplexity: 1.251455, precision: 0.750000, batch_len: 106.000000
Train, loss=0.22430676: 999it [24:01,  1.49s/it]2017-06-02 14:39:00,277 root  INFO     step 999.000000 - time: 1.008370, loss: 0.083625, perplexity: 1.087221, precision: 0.875000, batch_len: 92.000000
Train, loss=0.08362482: 1000it [24:02,  1.35s/it]2017-06-02 14:39:01,182 root  INFO     step 1000.000000 - time: 0.893692, loss: 0.217590, perplexity: 1.243077, precision: 0.781250, batch_len: 88.000000
Train, loss=0.21758991: 1001it [24:03,  1.22s/it]2017-06-02 14:39:02,324 root  INFO     step 1001.000000 - time: 1.102798, loss: 0.166054, perplexity: 1.180637, precision: 0.812500, batch_len: 93.000000
Train, loss=0.16605422: 1002it [24:04,  1.19s/it]2017-06-02 14:39:03,685 root  INFO     step 1002.000000 - time: 1.351946, loss: 0.082824, perplexity: 1.086351, precision: 0.828125, batch_len: 105.000000
Train, loss=0.08282438: 1003it [24:05,  1.24s/it]2017-06-02 14:39:05,157 root  INFO     step 1003.000000 - time: 1.390782, loss: 0.054267, perplexity: 1.055767, precision: 0.828125, batch_len: 128.000000
Train, loss=0.05426716: 1004it [24:07,  1.31s/it]2017-06-02 14:39:06,178 root  INFO     step 1004.000000 - time: 0.994989, loss: 0.053494, perplexity: 1.054950, precision: 0.890625, batch_len: 85.000000
Train, loss=0.05349370: 1005it [24:08,  1.23s/it]2017-06-02 14:39:07,238 root  INFO     step 1005.000000 - time: 0.981961, loss: 0.057253, perplexity: 1.058924, precision: 0.875000, batch_len: 97.000000
Train, loss=0.05725285: 1006it [24:09,  1.18s/it]2017-06-02 14:39:08,247 root  INFO     step 1006.000000 - time: 0.991810, loss: 0.028551, perplexity: 1.028962, precision: 0.953125, batch_len: 112.000000
Train, loss=0.02855081: 1007it [24:10,  1.13s/it]2017-06-02 14:39:09,507 root  INFO     step 1007.000000 - time: 1.228837, loss: 0.099758, perplexity: 1.104904, precision: 0.875000, batch_len: 107.000000
Train, loss=0.09975849: 1008it [24:11,  1.17s/it]2017-06-02 14:39:10,987 root  INFO     step 1008.000000 - time: 1.469911, loss: 0.101681, perplexity: 1.107030, precision: 0.921875, batch_len: 117.000000
Train, loss=0.10168074: 1009it [24:12,  1.26s/it]2017-06-02 14:39:12,144 root  INFO     step 1009.000000 - time: 1.085146, loss: 0.112171, perplexity: 1.118704, precision: 0.843750, batch_len: 101.000000
Train, loss=0.11217116: 1010it [24:14,  1.23s/it]2017-06-02 14:39:13,195 root  INFO     step 1010.000000 - time: 1.017236, loss: 0.044835, perplexity: 1.045855, precision: 0.890625, batch_len: 103.000000
Train, loss=0.04483508: 1011it [24:15,  1.18s/it]2017-06-02 14:39:14,341 root  INFO     step 1011.000000 - time: 1.109920, loss: 0.048613, perplexity: 1.049814, precision: 0.875000, batch_len: 110.000000
Train, loss=0.04861301: 1012it [24:16,  1.17s/it]2017-06-02 14:39:15,501 root  INFO     step 1012.000000 - time: 1.095796, loss: 0.026514, perplexity: 1.026868, precision: 0.953125, batch_len: 114.000000
Train, loss=0.02651353: 1013it [24:17,  1.16s/it]2017-06-02 14:39:17,259 root  INFO     step 1013.000000 - time: 1.708806, loss: 0.066794, perplexity: 1.069075, precision: 0.890625, batch_len: 120.000000
Train, loss=0.06679365: 1014it [24:19,  1.34s/it]2017-06-02 14:39:18,423 root  INFO     step 1014.000000 - time: 1.092009, loss: 0.131792, perplexity: 1.140871, precision: 0.859375, batch_len: 111.000000
Train, loss=0.13179186: 1015it [24:20,  1.29s/it]2017-06-02 14:39:19,429 root  INFO     step 1015.000000 - time: 0.961589, loss: 0.071714, perplexity: 1.074349, precision: 0.859375, batch_len: 90.000000
Train, loss=0.07171445: 1016it [24:21,  1.20s/it]2017-06-02 14:39:20,809 root  INFO     step 1016.000000 - time: 1.317855, loss: 0.074846, perplexity: 1.077718, precision: 0.859375, batch_len: 121.000000
Train, loss=0.07484576: 1017it [24:22,  1.26s/it]2017-06-02 14:39:21,791 root  INFO     step 1017.000000 - time: 0.974683, loss: 0.040320, perplexity: 1.041144, precision: 0.937500, batch_len: 100.000000
Train, loss=0.04032048: 1018it [24:23,  1.17s/it]2017-06-02 14:39:22,963 root  INFO     step 1018.000000 - time: 1.144525, loss: 0.063296, perplexity: 1.065342, precision: 0.875000, batch_len: 89.000000
Train, loss=0.06329568: 1019it [24:24,  1.17s/it]2017-06-02 14:39:24,501 root  INFO     step 1019.000000 - time: 1.452714, loss: 0.068004, perplexity: 1.070370, precision: 0.906250, batch_len: 125.000000
Train, loss=0.06800414: 1020it [24:26,  1.28s/it]2017-06-02 14:39:25,829 root  INFO     step 1020.000000 - time: 1.299415, loss: 0.032068, perplexity: 1.032588, precision: 0.906250, batch_len: 124.000000
Train, loss=0.03206798: 1021it [24:27,  1.30s/it]2017-06-02 14:39:26,866 root  INFO     step 1021.000000 - time: 1.025292, loss: 0.080438, perplexity: 1.083761, precision: 0.859375, batch_len: 81.000000
Train, loss=0.08043773: 1022it [24:28,  1.22s/it]2017-06-02 14:39:28,038 root  INFO     step 1022.000000 - time: 1.143575, loss: 0.059219, perplexity: 1.061008, precision: 0.859375, batch_len: 109.000000
Train, loss=0.05921911: 1023it [24:30,  1.20s/it]2017-06-02 14:39:29,437 root  INFO     step 1023.000000 - time: 1.164341, loss: 0.047932, perplexity: 1.049099, precision: 0.953125, batch_len: 94.000000
Train, loss=0.04793189: 1024it [24:31,  1.26s/it]2017-06-02 14:39:30,534 root  INFO     step 1024.000000 - time: 1.037528, loss: 0.034742, perplexity: 1.035352, precision: 0.890625, batch_len: 98.000000
Train, loss=0.03474193: 1025it [24:32,  1.21s/it]2017-06-02 14:39:31,828 root  INFO     step 1025.000000 - time: 1.204799, loss: 0.047202, perplexity: 1.048334, precision: 0.906250, batch_len: 102.000000
Train, loss=0.04720245: 1026it [24:33,  1.24s/it]2017-06-02 14:39:33,042 root  INFO     step 1026.000000 - time: 1.136987, loss: 0.058230, perplexity: 1.059959, precision: 0.875000, batch_len: 86.000000
Train, loss=0.05823033: 1027it [24:35,  1.23s/it]2017-06-02 14:39:34,114 root  INFO     step 1027.000000 - time: 0.963711, loss: 0.040914, perplexity: 1.041762, precision: 0.937500, batch_len: 80.000000
Train, loss=0.04091366: 1028it [24:36,  1.18s/it]2017-06-02 14:39:35,133 root  INFO     step 1028.000000 - time: 0.981046, loss: 0.044426, perplexity: 1.045428, precision: 0.875000, batch_len: 87.000000
Train, loss=0.04442602: 1029it [24:37,  1.13s/it]2017-06-02 14:39:36,499 root  INFO     step 1029.000000 - time: 1.321381, loss: 0.020957, perplexity: 1.021178, precision: 0.953125, batch_len: 129.000000
Train, loss=0.02095669: 1030it [24:38,  1.20s/it]2017-06-02 14:39:37,602 root  INFO     step 1030.000000 - time: 1.091690, loss: 0.055841, perplexity: 1.057430, precision: 0.906250, batch_len: 115.000000
Train, loss=0.05584130: 1031it [24:39,  1.17s/it]2017-06-02 14:39:39,292 root  INFO     step 1031.000000 - time: 1.621396, loss: 0.019244, perplexity: 1.019430, precision: 0.968750, batch_len: 116.000000
Train, loss=0.01924350: 1032it [24:41,  1.33s/it]2017-06-02 14:39:40,261 root  INFO     step 1032.000000 - time: 0.961472, loss: 0.090265, perplexity: 1.094465, precision: 0.843750, batch_len: 83.000000
Train, loss=0.09026533: 1033it [24:42,  1.22s/it]2017-06-02 14:39:41,426 root  INFO     step 1033.000000 - time: 0.940284, loss: 0.112866, perplexity: 1.119482, precision: 0.812500, batch_len: 79.000000
Train, loss=0.11286633: 1034it [24:43,  1.20s/it]2017-06-02 14:39:42,603 root  INFO     step 1034.000000 - time: 1.169508, loss: 0.162881, perplexity: 1.176896, precision: 0.781250, batch_len: 91.000000
Train, loss=0.16288081: 1035it [24:44,  1.20s/it]2017-06-02 14:39:43,984 root  INFO     step 1035.000000 - time: 1.298860, loss: 0.076124, perplexity: 1.079096, precision: 0.859375, batch_len: 123.000000
Train, loss=0.07612387: 1036it [24:45,  1.25s/it]2017-06-02 14:39:45,765 root  INFO     step 1036.000000 - time: 1.728737, loss: 0.051533, perplexity: 1.052884, precision: 0.859375, batch_len: 136.000000
Train, loss=0.05153289: 1037it [24:47,  1.41s/it]2017-06-02 14:39:46,913 root  INFO     step 1037.000000 - time: 1.015674, loss: 0.183505, perplexity: 1.201421, precision: 0.718750, batch_len: 84.000000
Train, loss=0.18350489: 1038it [24:48,  1.33s/it]2017-06-02 14:39:48,496 root  INFO     step 1038.000000 - time: 1.354541, loss: 0.043419, perplexity: 1.044375, precision: 0.921875, batch_len: 118.000000
Train, loss=0.04341898: 1039it [24:50,  1.41s/it]2017-06-02 14:39:49,889 root  INFO     step 1039.000000 - time: 1.214529, loss: 0.078191, perplexity: 1.081329, precision: 0.859375, batch_len: 135.000000
Train, loss=0.07819109: 1040it [24:51,  1.40s/it]2017-06-02 14:39:50,966 root  INFO     step 1040.000000 - time: 1.034693, loss: 0.072428, perplexity: 1.075116, precision: 0.890625, batch_len: 82.000000
Train, loss=0.07242814: 1041it [24:52,  1.31s/it]2017-06-02 14:39:52,566 root  INFO     step 1041.000000 - time: 1.571272, loss: 0.036393, perplexity: 1.037063, precision: 0.921875, batch_len: 119.000000
Train, loss=0.03639304: 1042it [24:54,  1.39s/it]2017-06-02 14:39:53,694 root  INFO     step 1042.000000 - time: 1.090364, loss: 0.044135, perplexity: 1.045124, precision: 0.921875, batch_len: 99.000000
Train, loss=0.04413527: 1043it [24:55,  1.31s/it]2017-06-02 14:39:55,140 root  INFO     step 1043.000000 - time: 1.409777, loss: 0.038431, perplexity: 1.039179, precision: 0.875000, batch_len: 130.000000
Train, loss=0.03843094: 1044it [24:57,  1.35s/it]2017-06-02 14:39:56,396 root  INFO     step 1044.000000 - time: 1.250832, loss: 0.058181, perplexity: 1.059907, precision: 0.937500, batch_len: 137.000000
Train, loss=0.05818094: 1045it [24:58,  1.32s/it]2017-06-02 14:39:58,182 root  INFO     step 1045.000000 - time: 1.643184, loss: 0.033758, perplexity: 1.034334, precision: 0.937500, batch_len: 144.000000
Train, loss=0.03375791: 1046it [25:00,  1.46s/it]2017-06-02 14:39:59,467 root  INFO     step 1046.000000 - time: 1.115278, loss: 0.035977, perplexity: 1.036632, precision: 0.937500, batch_len: 78.000000
Train, loss=0.03597726: 1047it [25:01,  1.41s/it]2017-06-02 14:40:00,953 root  INFO     step 1047.000000 - time: 1.273937, loss: 0.102920, perplexity: 1.108403, precision: 0.921875, batch_len: 126.000000
Train, loss=0.10291980: 1048it [25:02,  1.43s/it]2017-06-02 14:40:02,332 root  INFO     step 1048.000000 - time: 1.340775, loss: 0.035659, perplexity: 1.036302, precision: 0.937500, batch_len: 133.000000
Train, loss=0.03565856: 1049it [25:04,  1.42s/it]2017-06-02 14:40:03,746 root  INFO     step 1049.000000 - time: 1.322006, loss: 0.085709, perplexity: 1.089489, precision: 0.890625, batch_len: 139.000000
Train, loss=0.08570872: 1050it [25:05,  1.42s/it]2017-06-02 14:40:05,281 root  INFO     step 1050.000000 - time: 1.450228, loss: 0.067120, perplexity: 1.069423, precision: 0.875000, batch_len: 72.000000
Train, loss=0.06711950: 1051it [25:07,  1.45s/it]2017-06-02 14:40:06,367 root  INFO     step 1051.000000 - time: 1.028965, loss: 0.052282, perplexity: 1.053672, precision: 0.921875, batch_len: 96.000000
Train, loss=0.05228152: 1052it [25:08,  1.34s/it]2017-06-02 14:40:07,741 root  INFO     step 1052.000000 - time: 1.346778, loss: 0.063102, perplexity: 1.065135, precision: 0.921875, batch_len: 141.000000
Train, loss=0.06310183: 1053it [25:09,  1.35s/it]2017-06-02 14:40:09,150 root  INFO     step 1053.000000 - time: 1.391478, loss: 0.026064, perplexity: 1.026407, precision: 0.937500, batch_len: 138.000000
Train, loss=0.02606427: 1054it [25:11,  1.37s/it]2017-06-02 14:40:10,048 root  INFO     step 1054.000000 - time: 0.893425, loss: 0.056054, perplexity: 1.057655, precision: 0.937500, batch_len: 76.000000
Train, loss=0.05605450: 1055it [25:12,  1.23s/it]2017-06-02 14:40:11,278 root  INFO     step 1055.000000 - time: 1.105260, loss: 0.019612, perplexity: 1.019805, precision: 0.953125, batch_len: 77.000000
Train, loss=0.01961180: 1056it [25:13,  1.23s/it]2017-06-02 14:40:12,962 root  INFO     step 1056.000000 - time: 1.533895, loss: 0.042105, perplexity: 1.043004, precision: 0.890625, batch_len: 122.000000
Train, loss=0.04210547: 1057it [25:14,  1.37s/it]2017-06-02 14:40:14,849 root  INFO     step 1057.000000 - time: 1.851809, loss: 0.044085, perplexity: 1.045071, precision: 0.843750, batch_len: 152.000000
Train, loss=0.04408497: 1058it [25:16,  1.52s/it]2017-06-02 14:40:15,801 root  INFO     step 1058.000000 - time: 0.915225, loss: 0.013110, perplexity: 1.013197, precision: 0.984375, batch_len: 74.000000
Train, loss=0.01311028: 1059it [25:17,  1.35s/it]2017-06-02 14:40:17,845 root  INFO     step 1059.000000 - time: 1.897511, loss: 0.045052, perplexity: 1.046082, precision: 0.937500, batch_len: 150.000000
Train, loss=0.04505183: 1060it [25:19,  1.56s/it]2017-06-02 14:40:19,441 root  INFO     step 1060.000000 - time: 1.533509, loss: 0.017537, perplexity: 1.017691, precision: 0.968750, batch_len: 134.000000
Train, loss=0.01753661: 1061it [25:21,  1.57s/it]2017-06-02 14:40:20,972 root  INFO     step 1061.000000 - time: 1.411739, loss: 0.019213, perplexity: 1.019398, precision: 0.921875, batch_len: 132.000000
Train, loss=0.01921263: 1062it [25:22,  1.56s/it]2017-06-02 14:40:22,410 root  INFO     step 1062.000000 - time: 1.391321, loss: 0.027209, perplexity: 1.027582, precision: 0.921875, batch_len: 142.000000
Train, loss=0.02720862: 1063it [25:24,  1.52s/it]2017-06-02 14:40:23,833 root  INFO     step 1063.000000 - time: 1.333638, loss: 0.016133, perplexity: 1.016264, precision: 0.968750, batch_len: 131.000000
Train, loss=0.01613304: 1064it [25:25,  1.49s/it]2017-06-02 14:40:25,099 root  INFO     step 1064.000000 - time: 1.075810, loss: 0.017538, perplexity: 1.017693, precision: 0.968750, batch_len: 71.000000
Train, loss=0.01753782: 1065it [25:27,  1.42s/it]2017-06-02 14:40:25,152 root  INFO     Generating first batch)
2017-06-02 14:40:29,087 root  INFO     step 1065.000000 - time: 1.157073, loss: 0.023187, perplexity: 1.023458, precision: 0.968750, batch_len: 96.000000
Train, loss=0.02318725: 1066it [25:31,  2.19s/it]2017-06-02 14:40:30,409 root  INFO     step 1066.000000 - time: 0.966510, loss: 0.098277, perplexity: 1.103268, precision: 0.875000, batch_len: 88.000000
Train, loss=0.09827701: 1067it [25:32,  1.93s/it]2017-06-02 14:40:31,881 root  INFO     step 1067.000000 - time: 1.248806, loss: 0.055449, perplexity: 1.057016, precision: 0.921875, batch_len: 97.000000
Train, loss=0.05544945: 1068it [25:33,  1.79s/it]2017-06-02 14:40:33,246 root  INFO     step 1068.000000 - time: 1.107781, loss: 0.045592, perplexity: 1.046648, precision: 0.937500, batch_len: 101.000000
Train, loss=0.04559228: 1069it [25:35,  1.67s/it]2017-06-02 14:40:34,405 root  INFO     step 1069.000000 - time: 1.079694, loss: 0.060050, perplexity: 1.061889, precision: 0.859375, batch_len: 104.000000
Train, loss=0.06004982: 1070it [25:36,  1.51s/it]2017-06-02 14:40:35,578 root  INFO     step 1070.000000 - time: 1.058708, loss: 0.051943, perplexity: 1.053316, precision: 0.843750, batch_len: 110.000000
Train, loss=0.05194300: 1071it [25:37,  1.41s/it]2017-06-02 14:40:36,519 root  INFO     step 1071.000000 - time: 0.905534, loss: 0.142270, perplexity: 1.152887, precision: 0.875000, batch_len: 92.000000
Train, loss=0.14226951: 1072it [25:38,  1.27s/it]2017-06-02 14:40:37,822 root  INFO     step 1072.000000 - time: 1.299347, loss: 0.171147, perplexity: 1.186665, precision: 0.812500, batch_len: 108.000000
Train, loss=0.17114720: 1073it [25:39,  1.28s/it]2017-06-02 14:40:39,060 root  INFO     step 1073.000000 - time: 1.141361, loss: 0.151148, perplexity: 1.163169, precision: 0.781250, batch_len: 86.000000
Train, loss=0.15114820: 1074it [25:41,  1.27s/it]2017-06-02 14:40:40,223 root  INFO     step 1074.000000 - time: 1.088150, loss: 0.135631, perplexity: 1.145259, precision: 0.765625, batch_len: 106.000000
Train, loss=0.13563120: 1075it [25:42,  1.24s/it]2017-06-02 14:40:41,290 root  INFO     step 1075.000000 - time: 1.032918, loss: 0.528633, perplexity: 1.696611, precision: 0.531250, batch_len: 93.000000
Train, loss=0.52863300: 1076it [25:43,  1.19s/it]2017-06-02 14:40:42,649 root  INFO     step 1076.000000 - time: 1.346422, loss: 0.096284, perplexity: 1.101072, precision: 0.734375, batch_len: 120.000000
Train, loss=0.09628433: 1077it [25:44,  1.24s/it]2017-06-02 14:40:43,918 root  INFO     step 1077.000000 - time: 1.196370, loss: 0.312591, perplexity: 1.366962, precision: 0.671875, batch_len: 113.000000
Train, loss=0.31259102: 1078it [25:45,  1.25s/it]2017-06-02 14:40:45,258 root  INFO     step 1078.000000 - time: 1.285285, loss: 0.653101, perplexity: 1.921490, precision: 0.375000, batch_len: 102.000000
Train, loss=0.65310103: 1079it [25:47,  1.27s/it]2017-06-02 14:40:46,454 root  INFO     step 1079.000000 - time: 1.114554, loss: 0.578521, perplexity: 1.783398, precision: 0.515625, batch_len: 111.000000
Train, loss=0.57852054: 1080it [25:48,  1.25s/it]2017-06-02 14:40:47,556 root  INFO     step 1080.000000 - time: 1.068692, loss: 0.250663, perplexity: 1.284877, precision: 0.640625, batch_len: 107.000000
Train, loss=0.25066286: 1081it [25:49,  1.21s/it]2017-06-02 14:40:48,654 root  INFO     step 1081.000000 - time: 1.077761, loss: 0.223880, perplexity: 1.250921, precision: 0.625000, batch_len: 105.000000
Train, loss=0.22388029: 1082it [25:50,  1.17s/it]2017-06-02 14:40:49,909 root  INFO     step 1082.000000 - time: 1.245420, loss: 0.088992, perplexity: 1.093072, precision: 0.843750, batch_len: 128.000000
Train, loss=0.08899221: 1083it [25:51,  1.20s/it]2017-06-02 14:40:51,490 root  INFO     step 1083.000000 - time: 1.575055, loss: 0.060453, perplexity: 1.062318, precision: 0.906250, batch_len: 121.000000
Train, loss=0.06045319: 1084it [25:53,  1.31s/it]2017-06-02 14:40:52,807 root  INFO     step 1084.000000 - time: 1.256349, loss: 0.108310, perplexity: 1.114393, precision: 0.875000, batch_len: 100.000000
Train, loss=0.10830991: 1085it [25:54,  1.31s/it]2017-06-02 14:40:53,887 root  INFO     step 1085.000000 - time: 0.951696, loss: 0.107217, perplexity: 1.113176, precision: 0.812500, batch_len: 90.000000
Train, loss=0.10721689: 1086it [25:55,  1.24s/it]2017-06-02 14:40:54,906 root  INFO     step 1086.000000 - time: 1.000958, loss: 0.140540, perplexity: 1.150895, precision: 0.781250, batch_len: 89.000000
Train, loss=0.14053990: 1087it [25:56,  1.18s/it]2017-06-02 14:40:56,125 root  INFO     step 1087.000000 - time: 1.135659, loss: 0.186405, perplexity: 1.204910, precision: 0.796875, batch_len: 91.000000
Train, loss=0.18640520: 1088it [25:58,  1.19s/it]2017-06-02 14:40:56,981 root  INFO     step 1088.000000 - time: 0.845824, loss: 0.085412, perplexity: 1.089166, precision: 0.890625, batch_len: 80.000000
Train, loss=0.08541190: 1089it [25:58,  1.09s/it]2017-06-02 14:40:58,165 root  INFO     step 1089.000000 - time: 1.081501, loss: 0.072988, perplexity: 1.075717, precision: 0.875000, batch_len: 81.000000
Train, loss=0.07298761: 1090it [26:00,  1.12s/it]2017-06-02 14:40:59,744 root  INFO     step 1090.000000 - time: 1.513058, loss: 0.057386, perplexity: 1.059064, precision: 0.937500, batch_len: 117.000000
Train, loss=0.05738550: 1091it [26:01,  1.26s/it]2017-06-02 14:41:00,923 root  INFO     step 1091.000000 - time: 1.134850, loss: 0.047318, perplexity: 1.048456, precision: 0.890625, batch_len: 112.000000
Train, loss=0.04731824: 1092it [26:02,  1.23s/it]2017-06-02 14:41:01,966 root  INFO     step 1092.000000 - time: 1.018234, loss: 0.072723, perplexity: 1.075432, precision: 0.875000, batch_len: 85.000000
Train, loss=0.07272283: 1093it [26:03,  1.18s/it]2017-06-02 14:41:03,078 root  INFO     step 1093.000000 - time: 1.008837, loss: 0.046036, perplexity: 1.047112, precision: 0.906250, batch_len: 109.000000
Train, loss=0.04603627: 1094it [26:05,  1.16s/it]2017-06-02 14:41:04,665 root  INFO     step 1094.000000 - time: 1.518060, loss: 0.036428, perplexity: 1.037100, precision: 0.921875, batch_len: 124.000000
Train, loss=0.03642830: 1095it [26:06,  1.29s/it]2017-06-02 14:41:06,166 root  INFO     step 1095.000000 - time: 1.468797, loss: 0.032376, perplexity: 1.032905, precision: 0.906250, batch_len: 126.000000
Train, loss=0.03237556: 1096it [26:08,  1.35s/it]2017-06-02 14:41:07,278 root  INFO     step 1096.000000 - time: 1.092366, loss: 0.074764, perplexity: 1.077630, precision: 0.875000, batch_len: 114.000000
Train, loss=0.07476421: 1097it [26:09,  1.28s/it]2017-06-02 14:41:08,827 root  INFO     step 1097.000000 - time: 1.359802, loss: 0.081865, perplexity: 1.085309, precision: 0.859375, batch_len: 137.000000
Train, loss=0.08186471: 1098it [26:10,  1.36s/it]2017-06-02 14:41:10,308 root  INFO     step 1098.000000 - time: 1.330519, loss: 0.035753, perplexity: 1.036400, precision: 0.937500, batch_len: 116.000000
Train, loss=0.03575312: 1099it [26:12,  1.40s/it]2017-06-02 14:41:11,599 root  INFO     step 1099.000000 - time: 1.224500, loss: 0.110587, perplexity: 1.116934, precision: 0.921875, batch_len: 94.000000
Train, loss=0.11058749: 1100it [26:13,  1.36s/it]2017-06-02 14:41:12,752 root  INFO     step 1100.000000 - time: 1.132492, loss: 0.049822, perplexity: 1.051084, precision: 0.921875, batch_len: 87.000000
Train, loss=0.04982195: 1101it [26:14,  1.30s/it]2017-06-02 14:41:13,864 root  INFO     step 1101.000000 - time: 1.070529, loss: 0.038866, perplexity: 1.039631, precision: 0.921875, batch_len: 98.000000
Train, loss=0.03886622: 1102it [26:15,  1.24s/it]2017-06-02 14:41:14,910 root  INFO     step 1102.000000 - time: 1.003850, loss: 0.034002, perplexity: 1.034587, precision: 0.921875, batch_len: 103.000000
Train, loss=0.03400248: 1103it [26:16,  1.18s/it]2017-06-02 14:41:16,071 root  INFO     step 1103.000000 - time: 1.103857, loss: 0.058223, perplexity: 1.059952, precision: 0.890625, batch_len: 115.000000
Train, loss=0.05822345: 1104it [26:18,  1.18s/it]2017-06-02 14:41:17,734 root  INFO     step 1104.000000 - time: 1.558001, loss: 0.055577, perplexity: 1.057150, precision: 0.921875, batch_len: 144.000000
Train, loss=0.05557684: 1105it [26:19,  1.32s/it]2017-06-02 14:41:19,299 root  INFO     step 1105.000000 - time: 1.516987, loss: 0.032364, perplexity: 1.032894, precision: 0.890625, batch_len: 125.000000
Train, loss=0.03236416: 1106it [26:21,  1.40s/it]2017-06-02 14:41:20,288 root  INFO     step 1106.000000 - time: 0.948494, loss: 0.092384, perplexity: 1.096786, precision: 0.859375, batch_len: 83.000000
Train, loss=0.09238368: 1107it [26:22,  1.27s/it]2017-06-02 14:41:21,667 root  INFO     step 1107.000000 - time: 1.334139, loss: 0.026515, perplexity: 1.026869, precision: 0.921875, batch_len: 119.000000
Train, loss=0.02651479: 1108it [26:23,  1.31s/it]2017-06-02 14:41:23,006 root  INFO     step 1108.000000 - time: 1.331881, loss: 0.044795, perplexity: 1.045813, precision: 0.875000, batch_len: 123.000000
Train, loss=0.04479458: 1109it [26:24,  1.32s/it]2017-06-02 14:41:24,257 root  INFO     step 1109.000000 - time: 1.099862, loss: 0.074861, perplexity: 1.077735, precision: 0.828125, batch_len: 79.000000
Train, loss=0.07486130: 1110it [26:26,  1.30s/it]2017-06-02 14:41:25,597 root  INFO     step 1110.000000 - time: 1.300847, loss: 0.094922, perplexity: 1.099573, precision: 0.859375, batch_len: 84.000000
Train, loss=0.09492159: 1111it [26:27,  1.31s/it]2017-06-02 14:41:26,594 root  INFO     step 1111.000000 - time: 0.990609, loss: 0.053134, perplexity: 1.054571, precision: 0.875000, batch_len: 78.000000
Train, loss=0.05313372: 1112it [26:28,  1.22s/it]2017-06-02 14:41:28,168 root  INFO     step 1112.000000 - time: 1.462789, loss: 0.027630, perplexity: 1.028015, precision: 0.937500, batch_len: 129.000000
Train, loss=0.02763006: 1113it [26:30,  1.32s/it]2017-06-02 14:41:29,398 root  INFO     step 1113.000000 - time: 1.107872, loss: 0.022898, perplexity: 1.023162, precision: 0.968750, batch_len: 99.000000
Train, loss=0.02289793: 1114it [26:31,  1.30s/it]2017-06-02 14:41:31,036 root  INFO     step 1114.000000 - time: 1.485723, loss: 0.032602, perplexity: 1.033140, precision: 0.968750, batch_len: 130.000000
Train, loss=0.03260230: 1115it [26:33,  1.40s/it]2017-06-02 14:41:32,329 root  INFO     step 1115.000000 - time: 1.235805, loss: 0.023829, perplexity: 1.024115, precision: 0.968750, batch_len: 118.000000
Train, loss=0.02382854: 1116it [26:34,  1.37s/it]2017-06-02 14:41:34,141 root  INFO     step 1116.000000 - time: 1.681066, loss: 0.033769, perplexity: 1.034345, precision: 0.953125, batch_len: 136.000000
Train, loss=0.03376880: 1117it [26:36,  1.50s/it]2017-06-02 14:41:35,552 root  INFO     step 1117.000000 - time: 1.386386, loss: 0.031683, perplexity: 1.032190, precision: 0.953125, batch_len: 133.000000
Train, loss=0.03168306: 1118it [26:37,  1.47s/it]2017-06-02 14:41:37,220 root  INFO     step 1118.000000 - time: 1.395290, loss: 0.064519, perplexity: 1.066646, precision: 0.937500, batch_len: 135.000000
Train, loss=0.06451947: 1119it [26:39,  1.53s/it]2017-06-02 14:41:38,179 root  INFO     step 1119.000000 - time: 0.856329, loss: 0.050949, perplexity: 1.052269, precision: 0.906250, batch_len: 82.000000
Train, loss=0.05094876: 1120it [26:40,  1.36s/it]2017-06-02 14:41:39,692 root  INFO     step 1120.000000 - time: 1.148631, loss: 0.030733, perplexity: 1.031210, precision: 0.906250, batch_len: 96.000000
Train, loss=0.03073294: 1121it [26:41,  1.41s/it]2017-06-02 14:41:41,266 root  INFO     step 1121.000000 - time: 1.537783, loss: 0.044130, perplexity: 1.045118, precision: 0.921875, batch_len: 122.000000
Train, loss=0.04412985: 1122it [26:43,  1.46s/it]2017-06-02 14:41:42,760 root  INFO     step 1122.000000 - time: 1.429485, loss: 0.040671, perplexity: 1.041509, precision: 0.921875, batch_len: 138.000000
Train, loss=0.04067058: 1123it [26:44,  1.47s/it]2017-06-02 14:41:44,188 root  INFO     step 1123.000000 - time: 1.410095, loss: 0.032479, perplexity: 1.033012, precision: 0.937500, batch_len: 132.000000
Train, loss=0.03247888: 1124it [26:46,  1.46s/it]2017-06-02 14:41:46,359 root  INFO     step 1124.000000 - time: 1.980687, loss: 0.029445, perplexity: 1.029883, precision: 0.921875, batch_len: 152.000000
Train, loss=0.02944488: 1125it [26:48,  1.67s/it]2017-06-02 14:41:47,804 root  INFO     step 1125.000000 - time: 1.313516, loss: 0.032876, perplexity: 1.033422, precision: 0.968750, batch_len: 72.000000
Train, loss=0.03287563: 1126it [26:49,  1.60s/it]2017-06-02 14:41:49,221 root  INFO     step 1126.000000 - time: 1.338812, loss: 0.057291, perplexity: 1.058963, precision: 0.906250, batch_len: 141.000000
Train, loss=0.05729057: 1127it [26:51,  1.55s/it]2017-06-02 14:41:51,087 root  INFO     step 1127.000000 - time: 1.822768, loss: 0.103735, perplexity: 1.109307, precision: 0.859375, batch_len: 150.000000
Train, loss=0.10373540: 1128it [26:53,  1.64s/it]2017-06-02 14:41:52,187 root  INFO     step 1128.000000 - time: 0.957888, loss: 0.118693, perplexity: 1.126025, precision: 0.828125, batch_len: 74.000000
Train, loss=0.11869336: 1129it [26:54,  1.48s/it]2017-06-02 14:41:53,955 root  INFO     step 1129.000000 - time: 1.691516, loss: 0.156850, perplexity: 1.169820, precision: 0.812500, batch_len: 139.000000
Train, loss=0.15684968: 1130it [26:55,  1.57s/it]2017-06-02 14:41:55,365 root  INFO     step 1130.000000 - time: 1.403165, loss: 0.081749, perplexity: 1.085183, precision: 0.812500, batch_len: 131.000000
Train, loss=0.08174906: 1131it [26:57,  1.52s/it]2017-06-02 14:41:56,390 root  INFO     step 1131.000000 - time: 1.008444, loss: 0.083686, perplexity: 1.087287, precision: 0.859375, batch_len: 77.000000
Train, loss=0.08368584: 1132it [26:58,  1.37s/it]2017-06-02 14:41:57,384 root  INFO     step 1132.000000 - time: 0.990149, loss: 0.121295, perplexity: 1.128958, precision: 0.828125, batch_len: 76.000000
Train, loss=0.12129533: 1133it [26:59,  1.26s/it]2017-06-02 14:41:58,747 root  INFO     step 1133.000000 - time: 1.337869, loss: 0.044243, perplexity: 1.045236, precision: 0.890625, batch_len: 142.000000
Train, loss=0.04424296: 1134it [27:00,  1.29s/it]2017-06-02 14:42:00,442 root  INFO     step 1134.000000 - time: 1.682112, loss: 0.040640, perplexity: 1.041477, precision: 0.921875, batch_len: 134.000000
Train, loss=0.04064026: 1135it [27:02,  1.41s/it]2017-06-02 14:42:01,601 root  INFO     step 1135.000000 - time: 0.952508, loss: 0.029331, perplexity: 1.029766, precision: 0.984375, batch_len: 71.000000
Train, loss=0.02933147: 1136it [27:03,  1.34s/it]2017-06-02 14:42:01,711 root  INFO     Generating first batch)
2017-06-02 14:42:05,890 root  INFO     step 1136.000000 - time: 1.090846, loss: 0.014652, perplexity: 1.014760, precision: 0.984375, batch_len: 96.000000
Train, loss=0.01465214: 1137it [27:07,  2.22s/it]2017-06-02 14:42:07,007 root  INFO     step 1137.000000 - time: 0.942131, loss: 0.086509, perplexity: 1.090362, precision: 0.859375, batch_len: 86.000000
Train, loss=0.08650935: 1138it [27:08,  1.89s/it]2017-06-02 14:42:08,157 root  INFO     step 1138.000000 - time: 1.092677, loss: 0.050665, perplexity: 1.051970, precision: 0.906250, batch_len: 104.000000
Train, loss=0.05066453: 1139it [27:10,  1.67s/it]2017-06-02 14:42:09,245 root  INFO     step 1139.000000 - time: 1.000003, loss: 0.106302, perplexity: 1.112157, precision: 0.843750, batch_len: 113.000000
Train, loss=0.10630175: 1140it [27:11,  1.49s/it]2017-06-02 14:42:10,535 root  INFO     step 1140.000000 - time: 1.235099, loss: 0.111014, perplexity: 1.117410, precision: 0.828125, batch_len: 110.000000
Train, loss=0.11101388: 1141it [27:12,  1.43s/it]2017-06-02 14:42:11,865 root  INFO     step 1141.000000 - time: 1.243252, loss: 0.093046, perplexity: 1.097512, precision: 0.843750, batch_len: 93.000000
Train, loss=0.09304600: 1142it [27:13,  1.40s/it]2017-06-02 14:42:12,865 root  INFO     step 1142.000000 - time: 0.976673, loss: 0.037927, perplexity: 1.038655, precision: 0.906250, batch_len: 97.000000
Train, loss=0.03792675: 1143it [27:14,  1.28s/it]2017-06-02 14:42:13,817 root  INFO     step 1143.000000 - time: 0.941934, loss: 0.100646, perplexity: 1.105885, precision: 0.906250, batch_len: 88.000000
Train, loss=0.10064589: 1144it [27:15,  1.18s/it]2017-06-02 14:42:15,237 root  INFO     step 1144.000000 - time: 1.343338, loss: 0.050574, perplexity: 1.051874, precision: 0.921875, batch_len: 128.000000
Train, loss=0.05057364: 1145it [27:17,  1.25s/it]2017-06-02 14:42:16,260 root  INFO     step 1145.000000 - time: 0.975678, loss: 0.088078, perplexity: 1.092073, precision: 0.812500, batch_len: 101.000000
Train, loss=0.08807807: 1146it [27:18,  1.18s/it]2017-06-02 14:42:17,969 root  INFO     step 1146.000000 - time: 1.586069, loss: 0.045938, perplexity: 1.047009, precision: 0.921875, batch_len: 120.000000
Train, loss=0.04593797: 1147it [27:19,  1.34s/it]2017-06-02 14:42:19,310 root  INFO     step 1147.000000 - time: 1.282823, loss: 0.072823, perplexity: 1.075540, precision: 0.828125, batch_len: 111.000000
Train, loss=0.07282260: 1148it [27:21,  1.34s/it]2017-06-02 14:42:20,339 root  INFO     step 1148.000000 - time: 1.018044, loss: 0.048913, perplexity: 1.050129, precision: 0.921875, batch_len: 92.000000
Train, loss=0.04891342: 1149it [27:22,  1.25s/it]2017-06-02 14:42:21,464 root  INFO     step 1149.000000 - time: 1.013505, loss: 0.132317, perplexity: 1.141470, precision: 0.875000, batch_len: 102.000000
Train, loss=0.13231725: 1150it [27:23,  1.21s/it]2017-06-02 14:42:22,593 root  INFO     step 1150.000000 - time: 1.024411, loss: 0.050897, perplexity: 1.052215, precision: 0.859375, batch_len: 114.000000
Train, loss=0.05089732: 1151it [27:24,  1.19s/it]2017-06-02 14:42:23,666 root  INFO     step 1151.000000 - time: 1.055269, loss: 0.117947, perplexity: 1.125185, precision: 0.828125, batch_len: 89.000000
Train, loss=0.11794749: 1152it [27:25,  1.15s/it]2017-06-02 14:42:25,159 root  INFO     step 1152.000000 - time: 1.386941, loss: 0.114480, perplexity: 1.121290, precision: 0.812500, batch_len: 105.000000
Train, loss=0.11447956: 1153it [27:27,  1.25s/it]2017-06-02 14:42:26,290 root  INFO     step 1153.000000 - time: 1.050405, loss: 0.074846, perplexity: 1.077718, precision: 0.781250, batch_len: 90.000000
Train, loss=0.07484614: 1154it [27:28,  1.22s/it]2017-06-02 14:42:27,469 root  INFO     step 1154.000000 - time: 1.148329, loss: 0.065674, perplexity: 1.067878, precision: 0.937500, batch_len: 106.000000
Train, loss=0.06567384: 1155it [27:29,  1.21s/it]2017-06-02 14:42:28,686 root  INFO     step 1155.000000 - time: 1.188905, loss: 0.063997, perplexity: 1.066089, precision: 0.828125, batch_len: 109.000000
Train, loss=0.06399700: 1156it [27:30,  1.21s/it]2017-06-02 14:42:29,744 root  INFO     step 1156.000000 - time: 1.050274, loss: 0.058884, perplexity: 1.060652, precision: 0.812500, batch_len: 108.000000
Train, loss=0.05888412: 1157it [27:31,  1.16s/it]2017-06-02 14:42:31,245 root  INFO     step 1157.000000 - time: 1.406639, loss: 0.030453, perplexity: 1.030921, precision: 0.937500, batch_len: 124.000000
Train, loss=0.03045304: 1158it [27:33,  1.26s/it]2017-06-02 14:42:32,808 root  INFO     step 1158.000000 - time: 1.548862, loss: 0.018208, perplexity: 1.018375, precision: 0.953125, batch_len: 117.000000
Train, loss=0.01820804: 1159it [27:34,  1.35s/it]2017-06-02 14:42:33,932 root  INFO     step 1159.000000 - time: 1.120288, loss: 0.044010, perplexity: 1.044993, precision: 0.890625, batch_len: 81.000000
Train, loss=0.04400999: 1160it [27:35,  1.29s/it]2017-06-02 14:42:35,365 root  INFO     step 1160.000000 - time: 1.311708, loss: 0.045747, perplexity: 1.046809, precision: 0.890625, batch_len: 116.000000
Train, loss=0.04574659: 1161it [27:37,  1.33s/it]2017-06-02 14:42:36,445 root  INFO     step 1161.000000 - time: 1.017699, loss: 0.059977, perplexity: 1.061812, precision: 0.843750, batch_len: 103.000000
Train, loss=0.05997718: 1162it [27:38,  1.25s/it]2017-06-02 14:42:37,537 root  INFO     step 1162.000000 - time: 1.079367, loss: 0.044673, perplexity: 1.045686, precision: 0.875000, batch_len: 107.000000
Train, loss=0.04467272: 1163it [27:39,  1.21s/it]2017-06-02 14:42:38,589 root  INFO     step 1163.000000 - time: 0.989929, loss: 0.059041, perplexity: 1.060819, precision: 0.906250, batch_len: 100.000000
Train, loss=0.05904147: 1164it [27:40,  1.16s/it]2017-06-02 14:42:40,010 root  INFO     step 1164.000000 - time: 1.408545, loss: 0.082947, perplexity: 1.086484, precision: 0.875000, batch_len: 91.000000
Train, loss=0.08294718: 1165it [27:41,  1.24s/it]2017-06-02 14:42:41,187 root  INFO     step 1165.000000 - time: 1.035613, loss: 0.061384, perplexity: 1.063307, precision: 0.906250, batch_len: 80.000000
Train, loss=0.06138397: 1166it [27:43,  1.22s/it]2017-06-02 14:42:42,744 root  INFO     step 1166.000000 - time: 1.340334, loss: 0.048897, perplexity: 1.050112, precision: 0.906250, batch_len: 112.000000
Train, loss=0.04889655: 1167it [27:44,  1.32s/it]2017-06-02 14:42:44,095 root  INFO     step 1167.000000 - time: 1.302439, loss: 0.027677, perplexity: 1.028064, precision: 0.953125, batch_len: 125.000000
Train, loss=0.02767737: 1168it [27:46,  1.33s/it]2017-06-02 14:42:45,347 root  INFO     step 1168.000000 - time: 1.201848, loss: 0.027599, perplexity: 1.027983, precision: 0.921875, batch_len: 123.000000
Train, loss=0.02759901: 1169it [27:47,  1.31s/it]2017-06-02 14:42:47,081 root  INFO     step 1169.000000 - time: 1.639583, loss: 0.048119, perplexity: 1.049295, precision: 0.906250, batch_len: 121.000000
Train, loss=0.04811897: 1170it [27:49,  1.43s/it]2017-06-02 14:42:48,129 root  INFO     step 1170.000000 - time: 0.933610, loss: 0.146785, perplexity: 1.158105, precision: 0.859375, batch_len: 83.000000
Train, loss=0.14678490: 1171it [27:50,  1.32s/it]2017-06-02 14:42:49,219 root  INFO     step 1171.000000 - time: 1.023751, loss: 0.297552, perplexity: 1.346558, precision: 0.734375, batch_len: 84.000000
Train, loss=0.29755187: 1172it [27:51,  1.25s/it]2017-06-02 14:42:50,300 root  INFO     step 1172.000000 - time: 1.005686, loss: 0.150154, perplexity: 1.162014, precision: 0.781250, batch_len: 85.000000
Train, loss=0.15015438: 1173it [27:52,  1.20s/it]2017-06-02 14:42:51,633 root  INFO     step 1173.000000 - time: 1.251384, loss: 0.031345, perplexity: 1.031841, precision: 0.921875, batch_len: 118.000000
Train, loss=0.03134478: 1174it [27:53,  1.24s/it]2017-06-02 14:42:53,271 root  INFO     step 1174.000000 - time: 1.375509, loss: 0.068779, perplexity: 1.071200, precision: 0.890625, batch_len: 115.000000
Train, loss=0.06877947: 1175it [27:55,  1.36s/it]2017-06-02 14:42:54,704 root  INFO     step 1175.000000 - time: 1.382224, loss: 0.018258, perplexity: 1.018426, precision: 0.984375, batch_len: 119.000000
Train, loss=0.01825811: 1176it [27:56,  1.38s/it]2017-06-02 14:42:56,143 root  INFO     step 1176.000000 - time: 1.404961, loss: 0.044549, perplexity: 1.045556, precision: 0.906250, batch_len: 135.000000
Train, loss=0.04454909: 1177it [27:58,  1.40s/it]2017-06-02 14:42:57,153 root  INFO     step 1177.000000 - time: 1.001170, loss: 0.114911, perplexity: 1.121773, precision: 0.843750, batch_len: 94.000000
Train, loss=0.11491055: 1178it [27:59,  1.28s/it]2017-06-02 14:42:58,453 root  INFO     step 1178.000000 - time: 1.261079, loss: 0.039810, perplexity: 1.040613, precision: 0.921875, batch_len: 136.000000
Train, loss=0.03980973: 1179it [28:00,  1.29s/it]2017-06-02 14:43:00,083 root  INFO     step 1179.000000 - time: 1.613819, loss: 0.048426, perplexity: 1.049618, precision: 0.906250, batch_len: 137.000000
Train, loss=0.04842624: 1180it [28:02,  1.39s/it]2017-06-02 14:43:01,160 root  INFO     step 1180.000000 - time: 1.013535, loss: 0.086915, perplexity: 1.090804, precision: 0.875000, batch_len: 79.000000
Train, loss=0.08691484: 1181it [28:03,  1.30s/it]2017-06-02 14:43:02,188 root  INFO     step 1181.000000 - time: 0.934297, loss: 0.061804, perplexity: 1.063754, precision: 0.953125, batch_len: 82.000000
Train, loss=0.06180388: 1182it [28:04,  1.22s/it]2017-06-02 14:43:03,541 root  INFO     step 1182.000000 - time: 1.298243, loss: 0.039292, perplexity: 1.040075, precision: 0.937500, batch_len: 126.000000
Train, loss=0.03929241: 1183it [28:05,  1.26s/it]2017-06-02 14:43:04,928 root  INFO     step 1183.000000 - time: 1.299971, loss: 0.031075, perplexity: 1.031563, precision: 0.937500, batch_len: 129.000000
Train, loss=0.03107502: 1184it [28:06,  1.30s/it]2017-06-02 14:43:06,035 root  INFO     step 1184.000000 - time: 1.075778, loss: 0.042523, perplexity: 1.043440, precision: 0.906250, batch_len: 87.000000
Train, loss=0.04252292: 1185it [28:08,  1.24s/it]2017-06-02 14:43:07,315 root  INFO     step 1185.000000 - time: 1.231507, loss: 0.024291, perplexity: 1.024588, precision: 0.953125, batch_len: 98.000000
Train, loss=0.02429075: 1186it [28:09,  1.25s/it]2017-06-02 14:43:08,596 root  INFO     step 1186.000000 - time: 1.084258, loss: 0.013674, perplexity: 1.013768, precision: 0.968750, batch_len: 99.000000
Train, loss=0.01367412: 1187it [28:10,  1.26s/it]2017-06-02 14:43:10,145 root  INFO     step 1187.000000 - time: 1.397410, loss: 0.055412, perplexity: 1.056976, precision: 0.906250, batch_len: 144.000000
Train, loss=0.05541201: 1188it [28:12,  1.35s/it]2017-06-02 14:43:11,614 root  INFO     step 1188.000000 - time: 1.330834, loss: 0.038401, perplexity: 1.039148, precision: 0.906250, batch_len: 133.000000
Train, loss=0.03840087: 1189it [28:13,  1.38s/it]2017-06-02 14:43:13,139 root  INFO     step 1189.000000 - time: 1.510495, loss: 0.073813, perplexity: 1.076606, precision: 0.906250, batch_len: 130.000000
Train, loss=0.07381341: 1190it [28:15,  1.43s/it]2017-06-02 14:43:14,324 root  INFO     step 1190.000000 - time: 1.142854, loss: 0.044090, perplexity: 1.045076, precision: 0.937500, batch_len: 96.000000
Train, loss=0.04408981: 1191it [28:16,  1.35s/it]2017-06-02 14:43:16,215 root  INFO     step 1191.000000 - time: 1.853199, loss: 0.017338, perplexity: 1.017489, precision: 0.953125, batch_len: 152.000000
Train, loss=0.01733778: 1192it [28:18,  1.51s/it]2017-06-02 14:43:17,629 root  INFO     step 1192.000000 - time: 1.259668, loss: 0.032192, perplexity: 1.032716, precision: 0.953125, batch_len: 122.000000
Train, loss=0.03219191: 1193it [28:19,  1.48s/it]2017-06-02 14:43:19,369 root  INFO     step 1193.000000 - time: 1.623439, loss: 0.041194, perplexity: 1.042055, precision: 0.921875, batch_len: 138.000000
Train, loss=0.04119430: 1194it [28:21,  1.56s/it]2017-06-02 14:43:20,999 root  INFO     step 1194.000000 - time: 1.604145, loss: 0.038419, perplexity: 1.039166, precision: 0.968750, batch_len: 139.000000
Train, loss=0.03841878: 1195it [28:22,  1.58s/it]2017-06-02 14:43:22,068 root  INFO     step 1195.000000 - time: 0.942357, loss: 0.036963, perplexity: 1.037654, precision: 0.953125, batch_len: 78.000000
Train, loss=0.03696280: 1196it [28:24,  1.43s/it]2017-06-02 14:43:23,535 root  INFO     step 1196.000000 - time: 1.404842, loss: 0.042766, perplexity: 1.043694, precision: 0.890625, batch_len: 134.000000
Train, loss=0.04276611: 1197it [28:25,  1.44s/it]2017-06-02 14:43:24,842 root  INFO     step 1197.000000 - time: 1.011000, loss: 0.031393, perplexity: 1.031891, precision: 0.968750, batch_len: 72.000000
Train, loss=0.03139330: 1198it [28:26,  1.40s/it]2017-06-02 14:43:26,288 root  INFO     step 1198.000000 - time: 1.419132, loss: 0.047902, perplexity: 1.049068, precision: 0.906250, batch_len: 141.000000
Train, loss=0.04790242: 1199it [28:28,  1.41s/it]2017-06-02 14:43:27,924 root  INFO     step 1199.000000 - time: 1.598799, loss: 0.096585, perplexity: 1.101403, precision: 0.843750, batch_len: 132.000000
Train, loss=0.09658471: 1200it [28:29,  1.48s/it]2017-06-02 14:43:29,098 root  INFO     step 1200.000000 - time: 1.132157, loss: 0.038526, perplexity: 1.039278, precision: 0.953125, batch_len: 71.000000
Train, loss=0.03852601: 1201it [28:31,  1.39s/it]2017-06-02 14:43:30,225 root  INFO     step 1201.000000 - time: 1.072918, loss: 0.074048, perplexity: 1.076858, precision: 0.843750, batch_len: 76.000000
Train, loss=0.07404762: 1202it [28:32,  1.31s/it]2017-06-02 14:43:31,731 root  INFO     step 1202.000000 - time: 1.495644, loss: 0.241953, perplexity: 1.273735, precision: 0.671875, batch_len: 142.000000
Train, loss=0.24195337: 1203it [28:33,  1.37s/it]2017-06-02 14:43:32,729 root  INFO     step 1203.000000 - time: 0.962059, loss: 0.236365, perplexity: 1.266637, precision: 0.640625, batch_len: 74.000000
Train, loss=0.23636536: 1204it [28:34,  1.26s/it]2017-06-02 14:43:33,891 root  INFO     step 1204.000000 - time: 1.093214, loss: 0.427607, perplexity: 1.533584, precision: 0.531250, batch_len: 77.000000
Train, loss=0.42760724: 1205it [28:35,  1.23s/it]2017-06-02 14:43:36,141 root  INFO     step 1205.000000 - time: 2.136567, loss: 0.588835, perplexity: 1.801888, precision: 0.437500, batch_len: 150.000000
Train, loss=0.58883518: 1206it [28:38,  1.54s/it]2017-06-02 14:43:37,603 root  INFO     step 1206.000000 - time: 1.411568, loss: 0.312855, perplexity: 1.367323, precision: 0.593750, batch_len: 131.000000
Train, loss=0.31285492: 1207it [28:39,  1.51s/it]2017-06-02 14:43:37,803 root  INFO     Generating first batch)
2017-06-02 14:43:41,810 root  INFO     step 1207.000000 - time: 1.006931, loss: 0.174991, perplexity: 1.191235, precision: 0.671875, batch_len: 96.000000
Train, loss=0.17499086: 1208it [28:43,  2.32s/it]2017-06-02 14:43:43,119 root  INFO     step 1208.000000 - time: 1.001021, loss: 0.100588, perplexity: 1.105821, precision: 0.812500, batch_len: 92.000000
Train, loss=0.10058831: 1209it [28:45,  2.02s/it]2017-06-02 14:43:44,254 root  INFO     step 1209.000000 - time: 0.989681, loss: 0.080346, perplexity: 1.083662, precision: 0.812500, batch_len: 112.000000
Train, loss=0.08034572: 1210it [28:46,  1.75s/it]2017-06-02 14:43:45,556 root  INFO     step 1210.000000 - time: 1.152299, loss: 0.052356, perplexity: 1.053751, precision: 0.890625, batch_len: 81.000000
Train, loss=0.05235628: 1211it [28:47,  1.62s/it]2017-06-02 14:43:47,137 root  INFO     step 1211.000000 - time: 1.553527, loss: 0.087863, perplexity: 1.091839, precision: 0.796875, batch_len: 128.000000
Train, loss=0.08786349: 1212it [28:49,  1.61s/it]2017-06-02 14:43:48,301 root  INFO     step 1212.000000 - time: 1.090982, loss: 0.049659, perplexity: 1.050913, precision: 0.937500, batch_len: 110.000000
Train, loss=0.04965933: 1213it [28:50,  1.47s/it]2017-06-02 14:43:49,451 root  INFO     step 1213.000000 - time: 1.032386, loss: 0.185195, perplexity: 1.203453, precision: 0.734375, batch_len: 93.000000
Train, loss=0.18519491: 1214it [28:51,  1.38s/it]2017-06-02 14:43:50,406 root  INFO     step 1214.000000 - time: 0.923116, loss: 0.119734, perplexity: 1.127197, precision: 0.812500, batch_len: 90.000000
Train, loss=0.11973391: 1215it [28:52,  1.25s/it]2017-06-02 14:43:51,479 root  INFO     step 1215.000000 - time: 1.000547, loss: 0.066500, perplexity: 1.068761, precision: 0.859375, batch_len: 101.000000
Train, loss=0.06649993: 1216it [28:53,  1.20s/it]2017-06-02 14:43:52,752 root  INFO     step 1216.000000 - time: 1.261163, loss: 0.138415, perplexity: 1.148452, precision: 0.890625, batch_len: 114.000000
Train, loss=0.13841516: 1217it [28:54,  1.22s/it]2017-06-02 14:43:54,104 root  INFO     step 1217.000000 - time: 1.212456, loss: 0.088369, perplexity: 1.092391, precision: 0.843750, batch_len: 105.000000
Train, loss=0.08836889: 1218it [28:56,  1.26s/it]2017-06-02 14:43:55,481 root  INFO     step 1218.000000 - time: 1.319099, loss: 0.059109, perplexity: 1.060891, precision: 0.859375, batch_len: 120.000000
Train, loss=0.05910933: 1219it [28:57,  1.29s/it]2017-06-02 14:43:56,614 root  INFO     step 1219.000000 - time: 1.116076, loss: 0.047235, perplexity: 1.048368, precision: 0.906250, batch_len: 113.000000
Train, loss=0.04723505: 1220it [28:58,  1.25s/it]2017-06-02 14:43:57,553 root  INFO     step 1220.000000 - time: 0.883116, loss: 0.066759, perplexity: 1.069038, precision: 0.875000, batch_len: 102.000000
Train, loss=0.06675912: 1221it [28:59,  1.15s/it]2017-06-02 14:43:58,836 root  INFO     step 1221.000000 - time: 1.234035, loss: 0.080908, perplexity: 1.084271, precision: 0.875000, batch_len: 108.000000
Train, loss=0.08090758: 1222it [29:00,  1.19s/it]2017-06-02 14:44:00,132 root  INFO     step 1222.000000 - time: 1.147096, loss: 0.094966, perplexity: 1.099622, precision: 0.890625, batch_len: 86.000000
Train, loss=0.09496616: 1223it [29:02,  1.22s/it]2017-06-02 14:44:01,500 root  INFO     step 1223.000000 - time: 1.363212, loss: 0.045991, perplexity: 1.047065, precision: 0.968750, batch_len: 121.000000
Train, loss=0.04599147: 1224it [29:03,  1.27s/it]2017-06-02 14:44:02,462 root  INFO     step 1224.000000 - time: 0.939147, loss: 0.050258, perplexity: 1.051543, precision: 0.906250, batch_len: 91.000000
Train, loss=0.05025849: 1225it [29:04,  1.18s/it]2017-06-02 14:44:03,506 root  INFO     step 1225.000000 - time: 1.003608, loss: 0.055506, perplexity: 1.057075, precision: 0.906250, batch_len: 100.000000
Train, loss=0.05550578: 1226it [29:05,  1.14s/it]2017-06-02 14:44:04,554 root  INFO     step 1226.000000 - time: 1.029298, loss: 0.040628, perplexity: 1.041464, precision: 0.906250, batch_len: 104.000000
Train, loss=0.04062758: 1227it [29:06,  1.11s/it]2017-06-02 14:44:05,759 root  INFO     step 1227.000000 - time: 1.170213, loss: 0.049735, perplexity: 1.050993, precision: 0.937500, batch_len: 80.000000
Train, loss=0.04973546: 1228it [29:07,  1.14s/it]2017-06-02 14:44:07,036 root  INFO     step 1228.000000 - time: 1.267797, loss: 0.103104, perplexity: 1.108606, precision: 0.796875, batch_len: 111.000000
Train, loss=0.10310367: 1229it [29:09,  1.18s/it]2017-06-02 14:44:08,469 root  INFO     step 1229.000000 - time: 1.314423, loss: 0.029614, perplexity: 1.030057, precision: 0.953125, batch_len: 124.000000
Train, loss=0.02961429: 1230it [29:10,  1.26s/it]2017-06-02 14:44:09,487 root  INFO     step 1230.000000 - time: 0.983051, loss: 0.090303, perplexity: 1.094506, precision: 0.843750, batch_len: 88.000000
Train, loss=0.09030302: 1231it [29:11,  1.18s/it]2017-06-02 14:44:10,836 root  INFO     step 1231.000000 - time: 1.319076, loss: 0.038557, perplexity: 1.039310, precision: 0.875000, batch_len: 117.000000
Train, loss=0.03855661: 1232it [29:12,  1.23s/it]2017-06-02 14:44:11,848 root  INFO     step 1232.000000 - time: 0.965619, loss: 0.029633, perplexity: 1.030077, precision: 0.921875, batch_len: 103.000000
Train, loss=0.02963341: 1233it [29:13,  1.17s/it]2017-06-02 14:44:13,480 root  INFO     step 1233.000000 - time: 1.619471, loss: 0.030820, perplexity: 1.031300, precision: 0.984375, batch_len: 123.000000
Train, loss=0.03082017: 1234it [29:15,  1.31s/it]2017-06-02 14:44:14,566 root  INFO     step 1234.000000 - time: 1.069172, loss: 0.031870, perplexity: 1.032383, precision: 0.921875, batch_len: 99.000000
Train, loss=0.03186972: 1235it [29:16,  1.24s/it]2017-06-02 14:44:15,674 root  INFO     step 1235.000000 - time: 0.976088, loss: 0.036393, perplexity: 1.037063, precision: 0.890625, batch_len: 97.000000
Train, loss=0.03639253: 1236it [29:17,  1.20s/it]2017-06-02 14:44:17,059 root  INFO     step 1236.000000 - time: 1.317650, loss: 0.023576, perplexity: 1.023856, precision: 0.953125, batch_len: 125.000000
Train, loss=0.02357595: 1237it [29:19,  1.26s/it]2017-06-02 14:44:18,048 root  INFO     step 1237.000000 - time: 0.944244, loss: 0.042830, perplexity: 1.043761, precision: 0.921875, batch_len: 85.000000
Train, loss=0.04283017: 1238it [29:20,  1.18s/it]2017-06-02 14:44:19,254 root  INFO     step 1238.000000 - time: 1.164728, loss: 0.039802, perplexity: 1.040605, precision: 0.890625, batch_len: 89.000000
Train, loss=0.03980214: 1239it [29:21,  1.18s/it]2017-06-02 14:44:20,552 root  INFO     step 1239.000000 - time: 1.277343, loss: 0.023715, perplexity: 1.023999, precision: 0.953125, batch_len: 109.000000
Train, loss=0.02371548: 1240it [29:22,  1.22s/it]2017-06-02 14:44:21,636 root  INFO     step 1240.000000 - time: 1.026209, loss: 0.051809, perplexity: 1.053175, precision: 0.906250, batch_len: 94.000000
Train, loss=0.05180940: 1241it [29:23,  1.18s/it]2017-06-02 14:44:22,796 root  INFO     step 1241.000000 - time: 1.083001, loss: 0.043610, perplexity: 1.044575, precision: 0.953125, batch_len: 106.000000
Train, loss=0.04361036: 1242it [29:24,  1.17s/it]2017-06-02 14:44:24,002 root  INFO     step 1242.000000 - time: 1.094162, loss: 0.058129, perplexity: 1.059852, precision: 0.890625, batch_len: 107.000000
Train, loss=0.05812895: 1243it [29:25,  1.18s/it]2017-06-02 14:44:25,546 root  INFO     step 1243.000000 - time: 1.412138, loss: 0.045690, perplexity: 1.046750, precision: 0.968750, batch_len: 119.000000
Train, loss=0.04569025: 1244it [29:27,  1.29s/it]2017-06-02 14:44:26,928 root  INFO     step 1244.000000 - time: 1.377811, loss: 0.022531, perplexity: 1.022786, precision: 0.953125, batch_len: 116.000000
Train, loss=0.02253067: 1245it [29:28,  1.32s/it]2017-06-02 14:44:28,050 root  INFO     step 1245.000000 - time: 1.065164, loss: 0.052207, perplexity: 1.053594, precision: 0.921875, batch_len: 79.000000
Train, loss=0.05220730: 1246it [29:30,  1.26s/it]2017-06-02 14:44:29,573 root  INFO     step 1246.000000 - time: 1.374973, loss: 0.038302, perplexity: 1.039045, precision: 0.921875, batch_len: 115.000000
Train, loss=0.03830178: 1247it [29:31,  1.34s/it]2017-06-02 14:44:30,672 root  INFO     step 1247.000000 - time: 1.042270, loss: 0.061834, perplexity: 1.063786, precision: 0.906250, batch_len: 83.000000
Train, loss=0.06183416: 1248it [29:32,  1.27s/it]2017-06-02 14:44:32,200 root  INFO     step 1248.000000 - time: 1.473016, loss: 0.055085, perplexity: 1.056630, precision: 0.968750, batch_len: 135.000000
Train, loss=0.05508460: 1249it [29:34,  1.35s/it]2017-06-02 14:44:33,626 root  INFO     step 1249.000000 - time: 1.265764, loss: 0.021326, perplexity: 1.021555, precision: 0.937500, batch_len: 118.000000
Train, loss=0.02132631: 1250it [29:35,  1.37s/it]2017-06-02 14:44:35,424 root  INFO     step 1250.000000 - time: 1.705666, loss: 0.031270, perplexity: 1.031764, precision: 0.937500, batch_len: 126.000000
Train, loss=0.03126976: 1251it [29:37,  1.50s/it]2017-06-02 14:44:36,855 root  INFO     step 1251.000000 - time: 1.358893, loss: 0.026793, perplexity: 1.027155, precision: 0.953125, batch_len: 122.000000
Train, loss=0.02679281: 1252it [29:38,  1.48s/it]2017-06-02 14:44:37,901 root  INFO     step 1252.000000 - time: 1.029401, loss: 0.109477, perplexity: 1.115695, precision: 0.859375, batch_len: 84.000000
Train, loss=0.10947730: 1253it [29:39,  1.35s/it]2017-06-02 14:44:39,314 root  INFO     step 1253.000000 - time: 1.369743, loss: 0.022205, perplexity: 1.022453, precision: 0.953125, batch_len: 129.000000
Train, loss=0.02220513: 1254it [29:41,  1.37s/it]2017-06-02 14:44:40,711 root  INFO     step 1254.000000 - time: 1.383758, loss: 0.059036, perplexity: 1.060814, precision: 0.906250, batch_len: 137.000000
Train, loss=0.05903624: 1255it [29:42,  1.38s/it]2017-06-02 14:44:41,950 root  INFO     step 1255.000000 - time: 1.160132, loss: 0.034813, perplexity: 1.035426, precision: 0.921875, batch_len: 78.000000
Train, loss=0.03481301: 1256it [29:43,  1.34s/it]2017-06-02 14:44:43,529 root  INFO     step 1256.000000 - time: 1.442841, loss: 0.054531, perplexity: 1.056045, precision: 0.906250, batch_len: 144.000000
Train, loss=0.05453071: 1257it [29:45,  1.41s/it]2017-06-02 14:44:44,959 root  INFO     step 1257.000000 - time: 1.384243, loss: 0.018990, perplexity: 1.019171, precision: 0.968750, batch_len: 133.000000
Train, loss=0.01898953: 1258it [29:46,  1.42s/it]2017-06-02 14:44:46,022 root  INFO     step 1258.000000 - time: 0.858747, loss: 0.033799, perplexity: 1.034376, precision: 0.921875, batch_len: 87.000000
Train, loss=0.03379865: 1259it [29:47,  1.31s/it]2017-06-02 14:44:47,152 root  INFO     step 1259.000000 - time: 1.042807, loss: 0.071553, perplexity: 1.074175, precision: 0.953125, batch_len: 98.000000
Train, loss=0.07155290: 1260it [29:49,  1.26s/it]2017-06-02 14:44:48,954 root  INFO     step 1260.000000 - time: 1.598250, loss: 0.019377, perplexity: 1.019566, precision: 0.984375, batch_len: 130.000000
Train, loss=0.01937670: 1261it [29:50,  1.42s/it]2017-06-02 14:44:50,066 root  INFO     step 1261.000000 - time: 0.982981, loss: 0.062710, perplexity: 1.064718, precision: 0.906250, batch_len: 77.000000
Train, loss=0.06270991: 1262it [29:52,  1.33s/it]2017-06-02 14:44:51,077 root  INFO     step 1262.000000 - time: 0.944382, loss: 0.070017, perplexity: 1.072527, precision: 0.890625, batch_len: 82.000000
Train, loss=0.07001711: 1263it [29:53,  1.23s/it]2017-06-02 14:44:52,455 root  INFO     step 1263.000000 - time: 1.315191, loss: 0.037985, perplexity: 1.038716, precision: 0.953125, batch_len: 136.000000
Train, loss=0.03798529: 1264it [29:54,  1.28s/it]2017-06-02 14:44:54,496 root  INFO     step 1264.000000 - time: 1.947690, loss: 0.021726, perplexity: 1.021964, precision: 0.937500, batch_len: 152.000000
Train, loss=0.02172630: 1265it [29:56,  1.51s/it]2017-06-02 14:44:55,754 root  INFO     step 1265.000000 - time: 1.164140, loss: 0.040631, perplexity: 1.041468, precision: 0.875000, batch_len: 96.000000
Train, loss=0.04063132: 1266it [29:57,  1.43s/it]2017-06-02 14:44:57,344 root  INFO     step 1266.000000 - time: 1.417118, loss: 0.026456, perplexity: 1.026809, precision: 0.953125, batch_len: 139.000000
Train, loss=0.02645553: 1267it [29:59,  1.48s/it]2017-06-02 14:44:58,524 root  INFO     step 1267.000000 - time: 1.155604, loss: 0.023052, perplexity: 1.023320, precision: 0.984375, batch_len: 72.000000
Train, loss=0.02305222: 1268it [30:00,  1.39s/it]2017-06-02 14:44:59,942 root  INFO     step 1268.000000 - time: 1.317088, loss: 0.019426, perplexity: 1.019616, precision: 0.953125, batch_len: 138.000000
Train, loss=0.01942587: 1269it [30:01,  1.40s/it]2017-06-02 14:45:01,687 root  INFO     step 1269.000000 - time: 1.648324, loss: 0.041188, perplexity: 1.042048, precision: 0.968750, batch_len: 132.000000
Train, loss=0.04118780: 1270it [30:03,  1.50s/it]2017-06-02 14:45:02,748 root  INFO     step 1270.000000 - time: 0.997072, loss: 0.062533, perplexity: 1.064530, precision: 0.921875, batch_len: 76.000000
Train, loss=0.06253330: 1271it [30:04,  1.37s/it]2017-06-02 14:45:04,194 root  INFO     step 1271.000000 - time: 1.411763, loss: 0.053218, perplexity: 1.054660, precision: 0.937500, batch_len: 134.000000
Train, loss=0.05321836: 1272it [30:06,  1.39s/it]2017-06-02 14:45:05,207 root  INFO     step 1272.000000 - time: 0.920078, loss: 0.254089, perplexity: 1.289286, precision: 0.812500, batch_len: 74.000000
Train, loss=0.25408888: 1273it [30:07,  1.28s/it]2017-06-02 14:45:06,619 root  INFO     step 1273.000000 - time: 1.284966, loss: 0.099459, perplexity: 1.104573, precision: 0.812500, batch_len: 141.000000
Train, loss=0.09945880: 1274it [30:08,  1.32s/it]2017-06-02 14:45:08,556 root  INFO     step 1274.000000 - time: 1.813520, loss: 0.049594, perplexity: 1.050845, precision: 0.906250, batch_len: 142.000000
Train, loss=0.04959425: 1275it [30:10,  1.50s/it]2017-06-02 14:45:10,441 root  INFO     step 1275.000000 - time: 1.858129, loss: 0.173896, perplexity: 1.189932, precision: 0.796875, batch_len: 150.000000
Train, loss=0.17389616: 1276it [30:12,  1.62s/it]2017-06-02 14:45:11,986 root  INFO     step 1276.000000 - time: 1.356052, loss: 0.418272, perplexity: 1.519333, precision: 0.656250, batch_len: 131.000000
Train, loss=0.41827166: 1277it [30:13,  1.60s/it]2017-06-02 14:45:12,946 root  INFO     step 1277.000000 - time: 0.790103, loss: 0.079776, perplexity: 1.083045, precision: 0.812500, batch_len: 71.000000
Train, loss=0.07977647: 1278it [30:14,  1.41s/it]2017-06-02 14:45:12,998 root  INFO     Generating first batch)
2017-06-02 14:45:16,683 root  INFO     step 1278.000000 - time: 1.013355, loss: 0.094689, perplexity: 1.099317, precision: 0.875000, batch_len: 96.000000
Train, loss=0.09468937: 1279it [30:18,  2.10s/it]2017-06-02 14:45:18,168 root  INFO     step 1279.000000 - time: 1.072951, loss: 0.037543, perplexity: 1.038257, precision: 0.937500, batch_len: 101.000000
Train, loss=0.03754310: 1280it [30:20,  1.92s/it]2017-06-02 14:45:19,521 root  INFO     step 1280.000000 - time: 1.255873, loss: 0.053274, perplexity: 1.054719, precision: 0.906250, batch_len: 92.000000
Train, loss=0.05327424: 1281it [30:21,  1.75s/it]2017-06-02 14:45:20,658 root  INFO     step 1281.000000 - time: 1.105102, loss: 0.134563, perplexity: 1.144037, precision: 0.843750, batch_len: 93.000000
Train, loss=0.13456337: 1282it [30:22,  1.57s/it]2017-06-02 14:45:22,064 root  INFO     step 1282.000000 - time: 1.125370, loss: 0.072840, perplexity: 1.075558, precision: 0.828125, batch_len: 113.000000
Train, loss=0.07283957: 1283it [30:24,  1.52s/it]2017-06-02 14:45:23,116 root  INFO     step 1283.000000 - time: 1.047414, loss: 0.057012, perplexity: 1.058668, precision: 0.875000, batch_len: 105.000000
Train, loss=0.05701159: 1284it [30:25,  1.38s/it]2017-06-02 14:45:24,482 root  INFO     step 1284.000000 - time: 1.334584, loss: 0.087203, perplexity: 1.091118, precision: 0.828125, batch_len: 128.000000
Train, loss=0.08720253: 1285it [30:26,  1.37s/it]2017-06-02 14:45:25,933 root  INFO     step 1285.000000 - time: 1.396999, loss: 0.067049, perplexity: 1.069348, precision: 0.859375, batch_len: 120.000000
Train, loss=0.06704950: 1286it [30:27,  1.40s/it]2017-06-02 14:45:27,305 root  INFO     step 1286.000000 - time: 1.297866, loss: 0.063686, perplexity: 1.065757, precision: 0.859375, batch_len: 110.000000
Train, loss=0.06368554: 1287it [30:29,  1.39s/it]2017-06-02 14:45:29,281 root  INFO     step 1287.000000 - time: 1.867619, loss: 0.134020, perplexity: 1.143415, precision: 0.796875, batch_len: 90.000000
Train, loss=0.13401972: 1288it [30:31,  1.57s/it]2017-06-02 14:45:30,414 root  INFO     step 1288.000000 - time: 1.110628, loss: 0.053399, perplexity: 1.054850, precision: 0.875000, batch_len: 108.000000
Train, loss=0.05339861: 1289it [30:32,  1.44s/it]2017-06-02 14:45:31,517 root  INFO     step 1289.000000 - time: 1.098642, loss: 0.061014, perplexity: 1.062913, precision: 0.906250, batch_len: 104.000000
Train, loss=0.06101372: 1290it [30:33,  1.34s/it]2017-06-02 14:45:32,473 root  INFO     step 1290.000000 - time: 0.934248, loss: 0.077354, perplexity: 1.080424, precision: 0.890625, batch_len: 89.000000
Train, loss=0.07735368: 1291it [30:34,  1.22s/it]2017-06-02 14:45:33,598 root  INFO     step 1291.000000 - time: 1.102262, loss: 0.128306, perplexity: 1.136900, precision: 0.843750, batch_len: 88.000000
Train, loss=0.12830564: 1292it [30:35,  1.19s/it]2017-06-02 14:45:34,954 root  INFO     step 1292.000000 - time: 1.313023, loss: 0.048895, perplexity: 1.050110, precision: 0.937500, batch_len: 103.000000
Train, loss=0.04889497: 1293it [30:36,  1.24s/it]2017-06-02 14:45:36,274 root  INFO     step 1293.000000 - time: 1.312313, loss: 0.045136, perplexity: 1.046170, precision: 0.921875, batch_len: 112.000000
Train, loss=0.04513600: 1294it [30:38,  1.27s/it]2017-06-02 14:45:37,454 root  INFO     step 1294.000000 - time: 1.081344, loss: 0.042668, perplexity: 1.043591, precision: 0.906250, batch_len: 111.000000
Train, loss=0.04266802: 1295it [30:39,  1.24s/it]2017-06-02 14:45:38,521 root  INFO     step 1295.000000 - time: 1.054638, loss: 0.054248, perplexity: 1.055747, precision: 0.921875, batch_len: 106.000000
Train, loss=0.05424844: 1296it [30:40,  1.19s/it]2017-06-02 14:45:39,625 root  INFO     step 1296.000000 - time: 0.998276, loss: 0.041504, perplexity: 1.042377, precision: 0.890625, batch_len: 109.000000
Train, loss=0.04150414: 1297it [30:41,  1.16s/it]2017-06-02 14:45:40,784 root  INFO     step 1297.000000 - time: 1.140494, loss: 0.024296, perplexity: 1.024593, precision: 0.953125, batch_len: 102.000000
Train, loss=0.02429588: 1298it [30:42,  1.16s/it]2017-06-02 14:45:42,034 root  INFO     step 1298.000000 - time: 1.229679, loss: 0.057287, perplexity: 1.058959, precision: 0.937500, batch_len: 100.000000
Train, loss=0.05728675: 1299it [30:44,  1.19s/it]2017-06-02 14:45:43,059 root  INFO     step 1299.000000 - time: 1.009397, loss: 0.025516, perplexity: 1.025845, precision: 0.937500, batch_len: 97.000000
Train, loss=0.02551618: 1300it [30:45,  1.14s/it]2017-06-02 14:45:44,376 root  INFO     step 1300.000000 - time: 1.139763, loss: 0.051157, perplexity: 1.052488, precision: 0.921875, batch_len: 115.000000
Train, loss=0.05115705: 1301it [30:46,  1.19s/it]2017-06-02 14:45:45,770 root  INFO     step 1301.000000 - time: 1.280474, loss: 0.023331, perplexity: 1.023605, precision: 0.937500, batch_len: 124.000000
Train, loss=0.02333114: 1302it [30:47,  1.25s/it]2017-06-02 14:45:47,021 root  INFO     step 1302.000000 - time: 1.064524, loss: 0.039364, perplexity: 1.040149, precision: 0.921875, batch_len: 86.000000
Train, loss=0.03936437: 1303it [30:48,  1.25s/it]2017-06-02 14:45:48,329 root  INFO     step 1303.000000 - time: 1.295418, loss: 0.086019, perplexity: 1.089827, precision: 0.875000, batch_len: 107.000000
Train, loss=0.08601864: 1304it [30:50,  1.27s/it]2017-06-02 14:45:49,421 root  INFO     step 1304.000000 - time: 0.975547, loss: 0.105986, perplexity: 1.111806, precision: 0.843750, batch_len: 81.000000
Train, loss=0.10598588: 1305it [30:51,  1.22s/it]2017-06-02 14:45:50,614 root  INFO     step 1305.000000 - time: 1.037252, loss: 0.055913, perplexity: 1.057506, precision: 0.890625, batch_len: 85.000000
Train, loss=0.05591293: 1306it [30:52,  1.21s/it]2017-06-02 14:45:51,932 root  INFO     step 1306.000000 - time: 1.294602, loss: 0.020586, perplexity: 1.020799, precision: 0.968750, batch_len: 117.000000
Train, loss=0.02058607: 1307it [30:53,  1.24s/it]2017-06-02 14:45:52,849 root  INFO     step 1307.000000 - time: 0.860764, loss: 0.057066, perplexity: 1.058725, precision: 0.906250, batch_len: 80.000000
Train, loss=0.05706557: 1308it [30:54,  1.14s/it]2017-06-02 14:45:54,133 root  INFO     step 1308.000000 - time: 1.190808, loss: 0.061230, perplexity: 1.063143, precision: 0.937500, batch_len: 94.000000
Train, loss=0.06122976: 1309it [30:56,  1.19s/it]2017-06-02 14:45:55,510 root  INFO     step 1309.000000 - time: 1.345053, loss: 0.051536, perplexity: 1.052887, precision: 0.953125, batch_len: 91.000000
Train, loss=0.05153592: 1310it [30:57,  1.24s/it]2017-06-02 14:45:56,904 root  INFO     step 1310.000000 - time: 1.378767, loss: 0.032225, perplexity: 1.032750, precision: 0.968750, batch_len: 123.000000
Train, loss=0.03222510: 1311it [30:58,  1.29s/it]2017-06-02 14:45:58,290 root  INFO     step 1311.000000 - time: 1.332114, loss: 0.034995, perplexity: 1.035614, precision: 0.921875, batch_len: 116.000000
Train, loss=0.03499470: 1312it [31:00,  1.32s/it]2017-06-02 14:45:59,431 root  INFO     step 1312.000000 - time: 0.986113, loss: 0.044092, perplexity: 1.045079, precision: 0.906250, batch_len: 98.000000
Train, loss=0.04409236: 1313it [31:01,  1.26s/it]2017-06-02 14:46:01,051 root  INFO     step 1313.000000 - time: 1.529382, loss: 0.045473, perplexity: 1.046523, precision: 0.890625, batch_len: 125.000000
Train, loss=0.04547308: 1314it [31:03,  1.37s/it]2017-06-02 14:46:02,610 root  INFO     step 1314.000000 - time: 1.511218, loss: 0.032189, perplexity: 1.032713, precision: 0.937500, batch_len: 121.000000
Train, loss=0.03218925: 1315it [31:04,  1.43s/it]2017-06-02 14:46:03,557 root  INFO     step 1315.000000 - time: 0.940172, loss: 0.032342, perplexity: 1.032871, precision: 0.937500, batch_len: 87.000000
Train, loss=0.03234209: 1316it [31:05,  1.28s/it]2017-06-02 14:46:04,989 root  INFO     step 1316.000000 - time: 1.374415, loss: 0.033124, perplexity: 1.033678, precision: 0.953125, batch_len: 137.000000
Train, loss=0.03312371: 1317it [31:06,  1.33s/it]2017-06-02 14:46:05,925 root  INFO     step 1317.000000 - time: 0.901417, loss: 0.046572, perplexity: 1.047674, precision: 0.937500, batch_len: 84.000000
Train, loss=0.04657219: 1318it [31:07,  1.21s/it]2017-06-02 14:46:07,002 root  INFO     step 1318.000000 - time: 1.042014, loss: 0.057515, perplexity: 1.059201, precision: 0.906250, batch_len: 83.000000
Train, loss=0.05751461: 1319it [31:08,  1.17s/it]2017-06-02 14:46:08,235 root  INFO     step 1319.000000 - time: 1.179158, loss: 0.037670, perplexity: 1.038389, precision: 0.953125, batch_len: 79.000000
Train, loss=0.03767042: 1320it [31:10,  1.19s/it]2017-06-02 14:46:09,380 root  INFO     step 1320.000000 - time: 1.091999, loss: 0.025535, perplexity: 1.025864, precision: 0.953125, batch_len: 99.000000
Train, loss=0.02553525: 1321it [31:11,  1.18s/it]2017-06-02 14:46:10,825 root  INFO     step 1321.000000 - time: 1.385797, loss: 0.029649, perplexity: 1.030093, precision: 0.890625, batch_len: 129.000000
Train, loss=0.02964939: 1322it [31:12,  1.26s/it]2017-06-02 14:46:12,225 root  INFO     step 1322.000000 - time: 1.340131, loss: 0.012642, perplexity: 1.012722, precision: 0.968750, batch_len: 119.000000
Train, loss=0.01264219: 1323it [31:14,  1.30s/it]2017-06-02 14:46:13,568 root  INFO     step 1323.000000 - time: 1.337846, loss: 0.021379, perplexity: 1.021609, precision: 0.937500, batch_len: 126.000000
Train, loss=0.02137856: 1324it [31:15,  1.31s/it]2017-06-02 14:46:15,126 root  INFO     step 1324.000000 - time: 1.361882, loss: 0.046908, perplexity: 1.048026, precision: 0.921875, batch_len: 114.000000
Train, loss=0.04690816: 1325it [31:17,  1.39s/it]2017-06-02 14:46:16,570 root  INFO     step 1325.000000 - time: 1.341799, loss: 0.018850, perplexity: 1.019029, precision: 0.953125, batch_len: 118.000000
Train, loss=0.01885039: 1326it [31:18,  1.40s/it]2017-06-02 14:46:18,124 root  INFO     step 1326.000000 - time: 1.409294, loss: 0.048312, perplexity: 1.049498, precision: 0.937500, batch_len: 135.000000
Train, loss=0.04831194: 1327it [31:20,  1.45s/it]2017-06-02 14:46:19,454 root  INFO     step 1327.000000 - time: 1.270118, loss: 0.019796, perplexity: 1.019993, precision: 0.953125, batch_len: 136.000000
Train, loss=0.01979578: 1328it [31:21,  1.41s/it]2017-06-02 14:46:20,656 root  INFO     step 1328.000000 - time: 1.149524, loss: 0.048468, perplexity: 1.049662, precision: 0.953125, batch_len: 82.000000
Train, loss=0.04846849: 1329it [31:22,  1.35s/it]2017-06-02 14:46:21,875 root  INFO     step 1329.000000 - time: 1.128812, loss: 0.027140, perplexity: 1.027511, precision: 0.953125, batch_len: 78.000000
Train, loss=0.02713986: 1330it [31:23,  1.31s/it]2017-06-02 14:46:23,472 root  INFO     step 1330.000000 - time: 1.428508, loss: 0.039298, perplexity: 1.040080, precision: 0.906250, batch_len: 144.000000
Train, loss=0.03929810: 1331it [31:25,  1.40s/it]2017-06-02 14:46:24,899 root  INFO     step 1331.000000 - time: 1.354076, loss: 0.030821, perplexity: 1.031301, precision: 0.953125, batch_len: 130.000000
Train, loss=0.03082124: 1332it [31:26,  1.41s/it]2017-06-02 14:46:26,344 root  INFO     step 1332.000000 - time: 1.302850, loss: 0.020840, perplexity: 1.021059, precision: 0.953125, batch_len: 133.000000
Train, loss=0.02084042: 1333it [31:28,  1.42s/it]2017-06-02 14:46:27,497 root  INFO     step 1333.000000 - time: 1.142953, loss: 0.028270, perplexity: 1.028674, precision: 0.953125, batch_len: 96.000000
Train, loss=0.02827037: 1334it [31:29,  1.34s/it]2017-06-02 14:46:28,941 root  INFO     step 1334.000000 - time: 1.130878, loss: 0.061976, perplexity: 1.063937, precision: 0.968750, batch_len: 74.000000
Train, loss=0.06197616: 1335it [31:30,  1.37s/it]2017-06-02 14:46:30,528 root  INFO     step 1335.000000 - time: 1.580582, loss: 0.026780, perplexity: 1.027142, precision: 0.953125, batch_len: 138.000000
Train, loss=0.02677999: 1336it [31:32,  1.44s/it]2017-06-02 14:46:32,099 root  INFO     step 1336.000000 - time: 1.422278, loss: 0.039575, perplexity: 1.040368, precision: 0.921875, batch_len: 139.000000
Train, loss=0.03957455: 1337it [31:34,  1.48s/it]2017-06-02 14:46:33,605 root  INFO     step 1337.000000 - time: 1.363178, loss: 0.043019, perplexity: 1.043958, precision: 0.921875, batch_len: 122.000000
Train, loss=0.04301939: 1338it [31:35,  1.48s/it]2017-06-02 14:46:34,493 root  INFO     step 1338.000000 - time: 0.854922, loss: 0.063246, perplexity: 1.065289, precision: 0.906250, batch_len: 76.000000
Train, loss=0.06324612: 1339it [31:36,  1.31s/it]2017-06-02 14:46:36,689 root  INFO     step 1339.000000 - time: 2.157697, loss: 0.073296, perplexity: 1.076049, precision: 0.859375, batch_len: 150.000000
Train, loss=0.07329625: 1340it [31:38,  1.57s/it]2017-06-02 14:46:38,054 root  INFO     step 1340.000000 - time: 1.355844, loss: 0.031912, perplexity: 1.032427, precision: 0.921875, batch_len: 134.000000
Train, loss=0.03191243: 1341it [31:40,  1.51s/it]2017-06-02 14:46:39,498 root  INFO     step 1341.000000 - time: 1.399664, loss: 0.027550, perplexity: 1.027933, precision: 0.953125, batch_len: 132.000000
Train, loss=0.02754983: 1342it [31:41,  1.49s/it]2017-06-02 14:46:40,511 root  INFO     step 1342.000000 - time: 0.938380, loss: 0.020430, perplexity: 1.020640, precision: 0.968750, batch_len: 77.000000
Train, loss=0.02042957: 1343it [31:42,  1.35s/it]2017-06-02 14:46:41,820 root  INFO     step 1343.000000 - time: 1.255227, loss: 0.027443, perplexity: 1.027823, precision: 0.953125, batch_len: 72.000000
Train, loss=0.02744315: 1344it [31:43,  1.34s/it]2017-06-02 14:46:43,997 root  INFO     step 1344.000000 - time: 2.125937, loss: 0.017635, perplexity: 1.017791, precision: 0.968750, batch_len: 152.000000
Train, loss=0.01763467: 1345it [31:45,  1.59s/it]2017-06-02 14:46:45,477 root  INFO     step 1345.000000 - time: 1.351969, loss: 0.031513, perplexity: 1.032015, precision: 0.953125, batch_len: 141.000000
Train, loss=0.03151289: 1346it [31:47,  1.56s/it]2017-06-02 14:46:47,077 root  INFO     step 1346.000000 - time: 1.418599, loss: 0.041848, perplexity: 1.042736, precision: 0.906250, batch_len: 142.000000
Train, loss=0.04184831: 1347it [31:49,  1.57s/it]2017-06-02 14:46:48,581 root  INFO     step 1347.000000 - time: 1.429585, loss: 0.137151, perplexity: 1.147002, precision: 0.921875, batch_len: 131.000000
Train, loss=0.13715115: 1348it [31:50,  1.55s/it]2017-06-02 14:46:49,793 root  INFO     step 1348.000000 - time: 1.170286, loss: 0.222674, perplexity: 1.249414, precision: 0.796875, batch_len: 71.000000
Train, loss=0.22267437: 1349it [31:51,  1.45s/it]2017-06-02 14:46:49,987 root  INFO     Generating first batch)
2017-06-02 14:46:54,223 root  INFO     step 1349.000000 - time: 1.231187, loss: 0.218923, perplexity: 1.244736, precision: 0.734375, batch_len: 96.000000
Train, loss=0.21892315: 1350it [31:56,  2.34s/it]2017-06-02 14:46:55,767 root  INFO     step 1350.000000 - time: 1.134145, loss: 0.187019, perplexity: 1.205651, precision: 0.718750, batch_len: 113.000000
Train, loss=0.18701932: 1351it [31:57,  2.10s/it]2017-06-02 14:46:57,402 root  INFO     step 1351.000000 - time: 1.611013, loss: 0.234403, perplexity: 1.264154, precision: 0.687500, batch_len: 90.000000
Train, loss=0.23440319: 1352it [31:59,  1.96s/it]2017-06-02 14:46:58,430 root  INFO     step 1352.000000 - time: 1.009449, loss: 0.107176, perplexity: 1.113130, precision: 0.843750, batch_len: 109.000000
Train, loss=0.10717628: 1353it [32:00,  1.68s/it]2017-06-02 14:46:59,633 root  INFO     step 1353.000000 - time: 1.186241, loss: 0.072014, perplexity: 1.074670, precision: 0.843750, batch_len: 105.000000
Train, loss=0.07201372: 1354it [32:01,  1.54s/it]2017-06-02 14:47:01,318 root  INFO     step 1354.000000 - time: 1.619112, loss: 0.035532, perplexity: 1.036171, precision: 0.906250, batch_len: 128.000000
Train, loss=0.03553182: 1355it [32:03,  1.58s/it]2017-06-02 14:47:02,457 root  INFO     step 1355.000000 - time: 1.053505, loss: 0.085687, perplexity: 1.089465, precision: 0.890625, batch_len: 101.000000
Train, loss=0.08568653: 1356it [32:04,  1.45s/it]2017-06-02 14:47:03,808 root  INFO     step 1356.000000 - time: 1.336154, loss: 0.065200, perplexity: 1.067373, precision: 0.937500, batch_len: 117.000000
Train, loss=0.06520024: 1357it [32:05,  1.42s/it]2017-06-02 14:47:04,944 root  INFO     step 1357.000000 - time: 1.061554, loss: 0.071716, perplexity: 1.074350, precision: 0.906250, batch_len: 108.000000
Train, loss=0.07171603: 1358it [32:06,  1.33s/it]2017-06-02 14:47:06,043 root  INFO     step 1358.000000 - time: 1.093189, loss: 0.030713, perplexity: 1.031189, precision: 0.921875, batch_len: 106.000000
Train, loss=0.03071260: 1359it [32:08,  1.26s/it]2017-06-02 14:47:07,429 root  INFO     step 1359.000000 - time: 1.299495, loss: 0.082567, perplexity: 1.086071, precision: 0.796875, batch_len: 104.000000
Train, loss=0.08256685: 1360it [32:09,  1.30s/it]2017-06-02 14:47:08,644 root  INFO     step 1360.000000 - time: 1.163457, loss: 0.044549, perplexity: 1.045556, precision: 0.906250, batch_len: 110.000000
Train, loss=0.04454876: 1361it [32:10,  1.27s/it]2017-06-02 14:47:09,756 root  INFO     step 1361.000000 - time: 1.077754, loss: 0.032733, perplexity: 1.033275, precision: 0.937500, batch_len: 100.000000
Train, loss=0.03273350: 1362it [32:11,  1.23s/it]2017-06-02 14:47:10,799 root  INFO     step 1362.000000 - time: 1.019776, loss: 0.061077, perplexity: 1.062981, precision: 0.875000, batch_len: 97.000000
Train, loss=0.06107721: 1363it [32:12,  1.17s/it]2017-06-02 14:47:11,794 root  INFO     step 1363.000000 - time: 0.939944, loss: 0.071916, perplexity: 1.074565, precision: 0.843750, batch_len: 93.000000
Train, loss=0.07191591: 1364it [32:13,  1.12s/it]2017-06-02 14:47:13,299 root  INFO     step 1364.000000 - time: 1.447308, loss: 0.082670, perplexity: 1.086183, precision: 0.859375, batch_len: 120.000000
Train, loss=0.08266960: 1365it [32:15,  1.23s/it]2017-06-02 14:47:14,563 root  INFO     step 1365.000000 - time: 1.203070, loss: 0.053311, perplexity: 1.054758, precision: 0.875000, batch_len: 88.000000
Train, loss=0.05331107: 1366it [32:16,  1.24s/it]2017-06-02 14:47:15,713 root  INFO     step 1366.000000 - time: 1.044940, loss: 0.050752, perplexity: 1.052062, precision: 0.906250, batch_len: 103.000000
Train, loss=0.05075188: 1367it [32:17,  1.22s/it]2017-06-02 14:47:16,714 root  INFO     step 1367.000000 - time: 0.975261, loss: 0.062405, perplexity: 1.064393, precision: 0.875000, batch_len: 89.000000
Train, loss=0.06240511: 1368it [32:18,  1.15s/it]2017-06-02 14:47:17,828 root  INFO     step 1368.000000 - time: 1.098759, loss: 0.062569, perplexity: 1.064568, precision: 0.890625, batch_len: 111.000000
Train, loss=0.06256922: 1369it [32:19,  1.14s/it]2017-06-02 14:47:18,841 root  INFO     step 1369.000000 - time: 0.977880, loss: 0.037049, perplexity: 1.037744, precision: 0.875000, batch_len: 92.000000
Train, loss=0.03704890: 1370it [32:20,  1.10s/it]2017-06-02 14:47:20,336 root  INFO     step 1370.000000 - time: 1.482869, loss: 0.046021, perplexity: 1.047097, precision: 0.921875, batch_len: 125.000000
Train, loss=0.04602136: 1371it [32:22,  1.22s/it]2017-06-02 14:47:21,617 root  INFO     step 1371.000000 - time: 1.213594, loss: 0.039436, perplexity: 1.040224, precision: 0.921875, batch_len: 102.000000
Train, loss=0.03943625: 1372it [32:23,  1.24s/it]2017-06-02 14:47:22,947 root  INFO     step 1372.000000 - time: 1.306454, loss: 0.079802, perplexity: 1.083073, precision: 0.875000, batch_len: 112.000000
Train, loss=0.07980236: 1373it [32:24,  1.27s/it]2017-06-02 14:47:24,645 root  INFO     step 1373.000000 - time: 1.346979, loss: 0.049443, perplexity: 1.050685, precision: 0.843750, batch_len: 121.000000
Train, loss=0.04944256: 1374it [32:26,  1.40s/it]2017-06-02 14:47:25,572 root  INFO     step 1374.000000 - time: 0.844924, loss: 0.086914, perplexity: 1.090803, precision: 0.890625, batch_len: 81.000000
Train, loss=0.08691384: 1375it [32:27,  1.25s/it]2017-06-02 14:47:27,083 root  INFO     step 1375.000000 - time: 1.496878, loss: 0.022268, perplexity: 1.022517, precision: 0.937500, batch_len: 124.000000
Train, loss=0.02226758: 1376it [32:29,  1.33s/it]2017-06-02 14:47:28,515 root  INFO     step 1376.000000 - time: 1.343480, loss: 0.036265, perplexity: 1.036931, precision: 0.890625, batch_len: 115.000000
Train, loss=0.03626545: 1377it [32:30,  1.36s/it]2017-06-02 14:47:29,787 root  INFO     step 1377.000000 - time: 1.245225, loss: 0.038227, perplexity: 1.038967, precision: 0.890625, batch_len: 107.000000
Train, loss=0.03822729: 1378it [32:31,  1.33s/it]2017-06-02 14:47:30,905 root  INFO     step 1378.000000 - time: 1.044639, loss: 0.064653, perplexity: 1.066789, precision: 0.890625, batch_len: 86.000000
Train, loss=0.06465310: 1379it [32:32,  1.27s/it]2017-06-02 14:47:31,976 root  INFO     step 1379.000000 - time: 1.011181, loss: 0.060662, perplexity: 1.062540, precision: 0.906250, batch_len: 85.000000
Train, loss=0.06066226: 1380it [32:33,  1.21s/it]2017-06-02 14:47:33,225 root  INFO     step 1380.000000 - time: 1.087959, loss: 0.053392, perplexity: 1.054843, precision: 0.921875, batch_len: 114.000000
Train, loss=0.05339232: 1381it [32:35,  1.22s/it]2017-06-02 14:47:34,136 root  INFO     step 1381.000000 - time: 0.874942, loss: 0.024953, perplexity: 1.025267, precision: 0.968750, batch_len: 80.000000
Train, loss=0.02495292: 1382it [32:36,  1.13s/it]2017-06-02 14:47:35,735 root  INFO     step 1382.000000 - time: 1.491478, loss: 0.026104, perplexity: 1.026448, precision: 0.953125, batch_len: 123.000000
Train, loss=0.02610384: 1383it [32:37,  1.27s/it]2017-06-02 14:47:36,915 root  INFO     step 1383.000000 - time: 1.082936, loss: 0.020200, perplexity: 1.020406, precision: 0.984375, batch_len: 91.000000
Train, loss=0.02020042: 1384it [32:38,  1.24s/it]2017-06-02 14:47:38,396 root  INFO     step 1384.000000 - time: 1.311587, loss: 0.012286, perplexity: 1.012361, precision: 0.984375, batch_len: 126.000000
Train, loss=0.01228551: 1385it [32:40,  1.31s/it]2017-06-02 14:47:39,386 root  INFO     step 1385.000000 - time: 0.955148, loss: 0.080154, perplexity: 1.083454, precision: 0.875000, batch_len: 83.000000
Train, loss=0.08015391: 1386it [32:41,  1.22s/it]2017-06-02 14:47:40,655 root  INFO     step 1386.000000 - time: 1.241271, loss: 0.034131, perplexity: 1.034720, precision: 0.937500, batch_len: 129.000000
Train, loss=0.03413057: 1387it [32:42,  1.23s/it]2017-06-02 14:47:42,203 root  INFO     step 1387.000000 - time: 1.474383, loss: 0.049486, perplexity: 1.050730, precision: 0.890625, batch_len: 137.000000
Train, loss=0.04948563: 1388it [32:44,  1.33s/it]2017-06-02 14:47:43,715 root  INFO     step 1388.000000 - time: 1.502998, loss: 0.014546, perplexity: 1.014653, precision: 0.984375, batch_len: 116.000000
Train, loss=0.01454646: 1389it [32:45,  1.38s/it]2017-06-02 14:47:45,067 root  INFO     step 1389.000000 - time: 1.341120, loss: 0.009530, perplexity: 1.009575, precision: 0.984375, batch_len: 118.000000
Train, loss=0.00952956: 1390it [32:47,  1.37s/it]2017-06-02 14:47:46,011 root  INFO     step 1390.000000 - time: 0.933100, loss: 0.036309, perplexity: 1.036976, precision: 0.921875, batch_len: 79.000000
Train, loss=0.03630920: 1391it [32:47,  1.24s/it]2017-06-02 14:47:47,022 root  INFO     step 1391.000000 - time: 0.979828, loss: 0.031825, perplexity: 1.032337, precision: 0.921875, batch_len: 98.000000
Train, loss=0.03182510: 1392it [32:48,  1.17s/it]2017-06-02 14:47:48,291 root  INFO     step 1392.000000 - time: 1.168335, loss: 0.081696, perplexity: 1.085126, precision: 0.812500, batch_len: 84.000000
Train, loss=0.08169645: 1393it [32:50,  1.20s/it]2017-06-02 14:47:49,996 root  INFO     step 1393.000000 - time: 1.510046, loss: 0.019927, perplexity: 1.020127, precision: 0.921875, batch_len: 136.000000
Train, loss=0.01992739: 1394it [32:51,  1.35s/it]2017-06-02 14:47:50,963 root  INFO     step 1394.000000 - time: 0.957300, loss: 0.025633, perplexity: 1.025965, precision: 0.937500, batch_len: 87.000000
Train, loss=0.02563318: 1395it [32:52,  1.24s/it]2017-06-02 14:47:52,341 root  INFO     step 1395.000000 - time: 1.335658, loss: 0.052143, perplexity: 1.053526, precision: 0.921875, batch_len: 119.000000
Train, loss=0.05214294: 1396it [32:54,  1.28s/it]2017-06-02 14:47:53,351 root  INFO     step 1396.000000 - time: 0.981771, loss: 0.096804, perplexity: 1.101644, precision: 0.921875, batch_len: 94.000000
Train, loss=0.09680402: 1397it [32:55,  1.20s/it]2017-06-02 14:47:54,426 root  INFO     step 1397.000000 - time: 0.951028, loss: 0.033974, perplexity: 1.034558, precision: 0.921875, batch_len: 99.000000
Train, loss=0.03397419: 1398it [32:56,  1.16s/it]2017-06-02 14:47:56,314 root  INFO     step 1398.000000 - time: 1.731259, loss: 0.051564, perplexity: 1.052916, precision: 0.921875, batch_len: 130.000000
Train, loss=0.05156370: 1399it [32:58,  1.38s/it]2017-06-02 14:47:57,270 root  INFO     step 1399.000000 - time: 0.951496, loss: 0.032553, perplexity: 1.033089, precision: 0.937500, batch_len: 78.000000
Train, loss=0.03255349: 1400it [32:59,  1.25s/it]2017-06-02 14:47:58,708 root  INFO     step 1400.000000 - time: 1.403806, loss: 0.024202, perplexity: 1.024497, precision: 0.937500, batch_len: 133.000000
Train, loss=0.02420170: 1401it [33:00,  1.31s/it]2017-06-02 14:47:59,721 root  INFO     step 1401.000000 - time: 0.923629, loss: 0.041043, perplexity: 1.041897, precision: 0.921875, batch_len: 82.000000
Train, loss=0.04104342: 1402it [33:01,  1.22s/it]2017-06-02 14:48:01,122 root  INFO     step 1402.000000 - time: 1.304858, loss: 0.066034, perplexity: 1.068263, precision: 0.906250, batch_len: 135.000000
Train, loss=0.06603423: 1403it [33:03,  1.27s/it]2017-06-02 14:48:02,941 root  INFO     step 1403.000000 - time: 1.740250, loss: 0.087823, perplexity: 1.091795, precision: 0.859375, batch_len: 144.000000
Train, loss=0.08782306: 1404it [33:04,  1.44s/it]2017-06-02 14:48:04,982 root  INFO     step 1404.000000 - time: 1.875944, loss: 0.052507, perplexity: 1.053910, precision: 0.875000, batch_len: 152.000000
Train, loss=0.05250701: 1405it [33:06,  1.62s/it]2017-06-02 14:48:05,944 root  INFO     step 1405.000000 - time: 0.906811, loss: 0.065591, perplexity: 1.067790, precision: 0.906250, batch_len: 74.000000
Train, loss=0.06559149: 1406it [33:07,  1.42s/it]2017-06-02 14:48:07,246 root  INFO     step 1406.000000 - time: 1.217474, loss: 0.060193, perplexity: 1.062042, precision: 0.890625, batch_len: 122.000000
Train, loss=0.06019309: 1407it [33:09,  1.39s/it]2017-06-02 14:48:08,332 root  INFO     step 1407.000000 - time: 1.071274, loss: 0.020177, perplexity: 1.020382, precision: 0.937500, batch_len: 96.000000
Train, loss=0.02017682: 1408it [33:10,  1.30s/it]2017-06-02 14:48:10,137 root  INFO     step 1408.000000 - time: 1.643527, loss: 0.068256, perplexity: 1.070639, precision: 0.906250, batch_len: 139.000000
Train, loss=0.06825606: 1409it [33:12,  1.45s/it]2017-06-02 14:48:11,305 root  INFO     step 1409.000000 - time: 1.126489, loss: 0.043641, perplexity: 1.044607, precision: 0.921875, batch_len: 72.000000
Train, loss=0.04364088: 1410it [33:13,  1.36s/it]2017-06-02 14:48:12,785 root  INFO     step 1410.000000 - time: 1.400864, loss: 0.039724, perplexity: 1.040523, precision: 0.875000, batch_len: 138.000000
Train, loss=0.03972379: 1411it [33:14,  1.40s/it]2017-06-02 14:48:14,273 root  INFO     step 1411.000000 - time: 1.362980, loss: 0.039345, perplexity: 1.040129, precision: 0.921875, batch_len: 134.000000
Train, loss=0.03934477: 1412it [33:16,  1.43s/it]2017-06-02 14:48:15,476 root  INFO     step 1412.000000 - time: 1.029892, loss: 0.028629, perplexity: 1.029042, precision: 0.968750, batch_len: 71.000000
Train, loss=0.02862853: 1413it [33:17,  1.36s/it]2017-06-02 14:48:17,006 root  INFO     step 1413.000000 - time: 1.522722, loss: 0.090773, perplexity: 1.095020, precision: 0.859375, batch_len: 141.000000
Train, loss=0.09077290: 1414it [33:18,  1.41s/it]2017-06-02 14:48:18,029 root  INFO     step 1414.000000 - time: 0.990772, loss: 0.066967, perplexity: 1.069261, precision: 0.875000, batch_len: 77.000000
Train, loss=0.06696748: 1415it [33:20,  1.29s/it]2017-06-02 14:48:19,651 root  INFO     step 1415.000000 - time: 1.400249, loss: 0.034019, perplexity: 1.034604, precision: 0.875000, batch_len: 142.000000
Train, loss=0.03401857: 1416it [33:21,  1.39s/it]2017-06-02 14:48:21,636 root  INFO     step 1416.000000 - time: 1.923119, loss: 0.031764, perplexity: 1.032274, precision: 0.906250, batch_len: 150.000000
Train, loss=0.03176438: 1417it [33:23,  1.57s/it]2017-06-02 14:48:22,854 root  INFO     step 1417.000000 - time: 1.189081, loss: 0.060793, perplexity: 1.062679, precision: 0.921875, batch_len: 76.000000
Train, loss=0.06079330: 1418it [33:24,  1.46s/it]2017-06-02 14:48:24,258 root  INFO     step 1418.000000 - time: 1.387440, loss: 0.029491, perplexity: 1.029930, precision: 0.921875, batch_len: 132.000000
Train, loss=0.02949091: 1419it [33:26,  1.45s/it]2017-06-02 14:48:25,758 root  INFO     step 1419.000000 - time: 1.381962, loss: 0.077191, perplexity: 1.080249, precision: 0.937500, batch_len: 131.000000
Train, loss=0.07719118: 1420it [33:27,  1.46s/it]2017-06-02 14:48:25,938 root  INFO     Generating first batch)
2017-06-02 14:48:29,618 root  INFO     step 1420.000000 - time: 1.192311, loss: 0.037260, perplexity: 1.037963, precision: 0.921875, batch_len: 96.000000
Train, loss=0.03725997: 1421it [33:31,  2.18s/it]2017-06-02 14:48:31,492 root  INFO     step 1421.000000 - time: 1.073179, loss: 0.023854, perplexity: 1.024141, precision: 0.953125, batch_len: 101.000000
Train, loss=0.02385397: 1422it [33:33,  2.09s/it]2017-06-02 14:48:33,137 root  INFO     step 1422.000000 - time: 1.532357, loss: 0.115451, perplexity: 1.122380, precision: 0.781250, batch_len: 90.000000
Train, loss=0.11545100: 1423it [33:35,  1.96s/it]2017-06-02 14:48:34,410 root  INFO     step 1423.000000 - time: 1.160004, loss: 0.065317, perplexity: 1.067498, precision: 0.828125, batch_len: 104.000000
Train, loss=0.06531720: 1424it [33:36,  1.75s/it]2017-06-02 14:48:35,770 root  INFO     step 1424.000000 - time: 1.340321, loss: 0.065739, perplexity: 1.067948, precision: 0.937500, batch_len: 92.000000
Train, loss=0.06573873: 1425it [33:37,  1.63s/it]2017-06-02 14:48:36,881 root  INFO     step 1425.000000 - time: 0.956038, loss: 0.039012, perplexity: 1.039783, precision: 0.921875, batch_len: 103.000000
Train, loss=0.03901208: 1426it [33:38,  1.48s/it]2017-06-02 14:48:37,892 root  INFO     step 1426.000000 - time: 0.982837, loss: 0.050891, perplexity: 1.052209, precision: 0.859375, batch_len: 102.000000
Train, loss=0.05089138: 1427it [33:39,  1.34s/it]2017-06-02 14:48:39,234 root  INFO     step 1427.000000 - time: 1.336140, loss: 0.024861, perplexity: 1.025172, precision: 0.937500, batch_len: 120.000000
Train, loss=0.02486052: 1428it [33:41,  1.34s/it]2017-06-02 14:48:40,265 root  INFO     step 1428.000000 - time: 0.971166, loss: 0.034083, perplexity: 1.034670, precision: 0.937500, batch_len: 114.000000
Train, loss=0.03408302: 1429it [33:42,  1.25s/it]2017-06-02 14:48:41,691 root  INFO     step 1429.000000 - time: 1.401938, loss: 0.012916, perplexity: 1.012999, precision: 0.984375, batch_len: 124.000000
Train, loss=0.01291561: 1430it [33:43,  1.30s/it]2017-06-02 14:48:43,211 root  INFO     step 1430.000000 - time: 1.499318, loss: 0.007001, perplexity: 1.007026, precision: 1.000000, batch_len: 121.000000
Train, loss=0.00700126: 1431it [33:45,  1.37s/it]2017-06-02 14:48:44,622 root  INFO     step 1431.000000 - time: 1.401197, loss: 0.026953, perplexity: 1.027320, precision: 0.953125, batch_len: 128.000000
Train, loss=0.02695322: 1432it [33:46,  1.38s/it]2017-06-02 14:48:45,722 root  INFO     step 1432.000000 - time: 1.056105, loss: 0.050242, perplexity: 1.051526, precision: 0.921875, batch_len: 105.000000
Train, loss=0.05024227: 1433it [33:47,  1.30s/it]2017-06-02 14:48:46,796 root  INFO     step 1433.000000 - time: 1.054728, loss: 0.036529, perplexity: 1.037204, precision: 0.953125, batch_len: 110.000000
Train, loss=0.03652885: 1434it [33:48,  1.23s/it]2017-06-02 14:48:48,021 root  INFO     step 1434.000000 - time: 1.126297, loss: 0.031941, perplexity: 1.032457, precision: 0.906250, batch_len: 113.000000
Train, loss=0.03194117: 1435it [33:49,  1.23s/it]2017-06-02 14:48:49,264 root  INFO     step 1435.000000 - time: 1.181501, loss: 0.056269, perplexity: 1.057882, precision: 0.921875, batch_len: 93.000000
Train, loss=0.05626862: 1436it [33:51,  1.23s/it]2017-06-02 14:48:50,760 root  INFO     step 1436.000000 - time: 1.302788, loss: 0.039847, perplexity: 1.040652, precision: 0.937500, batch_len: 117.000000
Train, loss=0.03984746: 1437it [33:52,  1.31s/it]2017-06-02 14:48:51,866 root  INFO     step 1437.000000 - time: 1.090393, loss: 0.031737, perplexity: 1.032246, precision: 0.906250, batch_len: 108.000000
Train, loss=0.03173749: 1438it [33:53,  1.25s/it]2017-06-02 14:48:52,911 root  INFO     step 1438.000000 - time: 0.989306, loss: 0.036821, perplexity: 1.037507, precision: 0.953125, batch_len: 97.000000
Train, loss=0.03682109: 1439it [33:54,  1.19s/it]2017-06-02 14:48:53,924 root  INFO     step 1439.000000 - time: 1.004314, loss: 0.048255, perplexity: 1.049438, precision: 0.875000, batch_len: 107.000000
Train, loss=0.04825458: 1440it [33:55,  1.14s/it]2017-06-02 14:48:54,984 root  INFO     step 1440.000000 - time: 1.026445, loss: 0.081983, perplexity: 1.085437, precision: 0.875000, batch_len: 88.000000
Train, loss=0.08198301: 1441it [33:56,  1.11s/it]2017-06-02 14:48:56,385 root  INFO     step 1441.000000 - time: 1.339285, loss: 0.044304, perplexity: 1.045300, precision: 0.906250, batch_len: 112.000000
Train, loss=0.04430411: 1442it [33:58,  1.20s/it]2017-06-02 14:48:57,492 root  INFO     step 1442.000000 - time: 1.095910, loss: 0.068767, perplexity: 1.071187, precision: 0.828125, batch_len: 111.000000
Train, loss=0.06876729: 1443it [33:59,  1.17s/it]2017-06-02 14:48:58,523 root  INFO     step 1443.000000 - time: 1.011084, loss: 0.058659, perplexity: 1.060414, precision: 0.875000, batch_len: 85.000000
Train, loss=0.05865899: 1444it [34:00,  1.13s/it]2017-06-02 14:48:59,599 root  INFO     step 1444.000000 - time: 1.061980, loss: 0.018591, perplexity: 1.018765, precision: 0.968750, batch_len: 100.000000
Train, loss=0.01859121: 1445it [34:01,  1.11s/it]2017-06-02 14:49:00,497 root  INFO     step 1445.000000 - time: 0.873579, loss: 0.041719, perplexity: 1.042601, precision: 0.921875, batch_len: 89.000000
Train, loss=0.04171880: 1446it [34:02,  1.05s/it]2017-06-02 14:49:01,535 root  INFO     step 1446.000000 - time: 1.020650, loss: 0.048484, perplexity: 1.049679, precision: 0.906250, batch_len: 81.000000
Train, loss=0.04848444: 1447it [34:03,  1.05s/it]2017-06-02 14:49:03,250 root  INFO     step 1447.000000 - time: 1.670749, loss: 0.028668, perplexity: 1.029083, precision: 0.937500, batch_len: 116.000000
Train, loss=0.02866795: 1448it [34:05,  1.25s/it]2017-06-02 14:49:04,246 root  INFO     step 1448.000000 - time: 0.935848, loss: 0.034364, perplexity: 1.034961, precision: 0.953125, batch_len: 80.000000
Train, loss=0.03436377: 1449it [34:06,  1.17s/it]2017-06-02 14:49:05,464 root  INFO     step 1449.000000 - time: 1.195642, loss: 0.078703, perplexity: 1.081883, precision: 0.890625, batch_len: 91.000000
Train, loss=0.07870319: 1450it [34:07,  1.19s/it]2017-06-02 14:49:06,949 root  INFO     step 1450.000000 - time: 1.308035, loss: 0.024696, perplexity: 1.025004, precision: 0.937500, batch_len: 125.000000
Train, loss=0.02469609: 1451it [34:08,  1.28s/it]2017-06-02 14:49:08,096 root  INFO     step 1451.000000 - time: 1.118520, loss: 0.034621, perplexity: 1.035227, precision: 0.906250, batch_len: 109.000000
Train, loss=0.03462082: 1452it [34:10,  1.24s/it]2017-06-02 14:49:09,285 root  INFO     step 1452.000000 - time: 1.182972, loss: 0.041130, perplexity: 1.041987, precision: 0.906250, batch_len: 86.000000
Train, loss=0.04112954: 1453it [34:11,  1.22s/it]2017-06-02 14:49:10,540 root  INFO     step 1453.000000 - time: 1.247596, loss: 0.028143, perplexity: 1.028543, precision: 0.921875, batch_len: 106.000000
Train, loss=0.02814309: 1454it [34:12,  1.23s/it]2017-06-02 14:49:11,653 root  INFO     step 1454.000000 - time: 0.944468, loss: 0.056244, perplexity: 1.057856, precision: 0.859375, batch_len: 83.000000
Train, loss=0.05624400: 1455it [34:13,  1.20s/it]2017-06-02 14:49:12,827 root  INFO     step 1455.000000 - time: 1.018963, loss: 0.130422, perplexity: 1.139309, precision: 0.859375, batch_len: 94.000000
Train, loss=0.13042192: 1456it [34:14,  1.19s/it]2017-06-02 14:49:13,859 root  INFO     step 1456.000000 - time: 1.006928, loss: 0.061726, perplexity: 1.063671, precision: 0.875000, batch_len: 115.000000
Train, loss=0.06172648: 1457it [34:15,  1.14s/it]2017-06-02 14:49:15,069 root  INFO     step 1457.000000 - time: 1.096145, loss: 0.031692, perplexity: 1.032200, precision: 0.890625, batch_len: 87.000000
Train, loss=0.03169207: 1458it [34:17,  1.16s/it]2017-06-02 14:49:16,714 root  INFO     step 1458.000000 - time: 1.603908, loss: 0.016275, perplexity: 1.016408, precision: 0.968750, batch_len: 123.000000
Train, loss=0.01627496: 1459it [34:18,  1.31s/it]2017-06-02 14:49:18,176 root  INFO     step 1459.000000 - time: 1.401865, loss: 0.035862, perplexity: 1.036512, precision: 0.906250, batch_len: 129.000000
Train, loss=0.03586160: 1460it [34:20,  1.35s/it]2017-06-02 14:49:19,650 root  INFO     step 1460.000000 - time: 1.300931, loss: 0.046620, perplexity: 1.047724, precision: 0.937500, batch_len: 119.000000
Train, loss=0.04662014: 1461it [34:21,  1.39s/it]2017-06-02 14:49:20,603 root  INFO     step 1461.000000 - time: 0.910608, loss: 0.057892, perplexity: 1.059601, precision: 0.921875, batch_len: 79.000000
Train, loss=0.05789203: 1462it [34:22,  1.26s/it]2017-06-02 14:49:21,844 root  INFO     step 1462.000000 - time: 1.213734, loss: 0.022005, perplexity: 1.022249, precision: 0.968750, batch_len: 98.000000
Train, loss=0.02200539: 1463it [34:23,  1.25s/it]2017-06-02 14:49:23,116 root  INFO     step 1463.000000 - time: 1.235931, loss: 0.028014, perplexity: 1.028410, precision: 0.953125, batch_len: 99.000000
Train, loss=0.02801425: 1464it [34:25,  1.26s/it]2017-06-02 14:49:24,530 root  INFO     step 1464.000000 - time: 1.338986, loss: 0.040495, perplexity: 1.041326, precision: 0.937500, batch_len: 137.000000
Train, loss=0.04049511: 1465it [34:26,  1.31s/it]2017-06-02 14:49:26,095 root  INFO     step 1465.000000 - time: 1.429830, loss: 0.041633, perplexity: 1.042512, precision: 0.906250, batch_len: 133.000000
Train, loss=0.04163290: 1466it [34:28,  1.38s/it]2017-06-02 14:49:27,122 root  INFO     step 1466.000000 - time: 0.953886, loss: 0.061320, perplexity: 1.063239, precision: 0.906250, batch_len: 82.000000
Train, loss=0.06131997: 1467it [34:29,  1.28s/it]2017-06-02 14:49:28,551 root  INFO     step 1467.000000 - time: 1.219587, loss: 0.061156, perplexity: 1.063065, precision: 0.906250, batch_len: 84.000000
Train, loss=0.06115615: 1468it [34:30,  1.32s/it]2017-06-02 14:49:29,937 root  INFO     step 1468.000000 - time: 1.360578, loss: 0.026422, perplexity: 1.026774, precision: 0.937500, batch_len: 126.000000
Train, loss=0.02642164: 1469it [34:31,  1.34s/it]2017-06-02 14:49:31,749 root  INFO     step 1469.000000 - time: 1.703298, loss: 0.065710, perplexity: 1.067917, precision: 0.921875, batch_len: 135.000000
Train, loss=0.06570990: 1470it [34:33,  1.48s/it]2017-06-02 14:49:33,213 root  INFO     step 1470.000000 - time: 1.444118, loss: 0.047685, perplexity: 1.048840, precision: 0.906250, batch_len: 144.000000
Train, loss=0.04768472: 1471it [34:35,  1.48s/it]2017-06-02 14:49:34,690 root  INFO     step 1471.000000 - time: 1.339284, loss: 0.035302, perplexity: 1.035933, precision: 0.890625, batch_len: 130.000000
Train, loss=0.03530208: 1472it [34:36,  1.48s/it]2017-06-02 14:49:35,983 root  INFO     step 1472.000000 - time: 1.274167, loss: 0.024535, perplexity: 1.024839, precision: 0.953125, batch_len: 118.000000
Train, loss=0.02453533: 1473it [34:37,  1.42s/it]2017-06-02 14:49:37,845 root  INFO     step 1473.000000 - time: 1.670446, loss: 0.024442, perplexity: 1.024743, precision: 0.953125, batch_len: 122.000000
Train, loss=0.02444201: 1474it [34:39,  1.55s/it]2017-06-02 14:49:39,303 root  INFO     step 1474.000000 - time: 1.379173, loss: 0.030866, perplexity: 1.031347, precision: 0.937500, batch_len: 141.000000
Train, loss=0.03086576: 1475it [34:41,  1.53s/it]2017-06-02 14:49:40,303 root  INFO     step 1475.000000 - time: 0.947658, loss: 0.025537, perplexity: 1.025865, precision: 0.953125, batch_len: 96.000000
Train, loss=0.02553653: 1476it [34:42,  1.37s/it]2017-06-02 14:49:41,736 root  INFO     step 1476.000000 - time: 1.407288, loss: 0.029332, perplexity: 1.029767, precision: 0.906250, batch_len: 138.000000
Train, loss=0.02933232: 1477it [34:43,  1.39s/it]2017-06-02 14:49:43,118 root  INFO     step 1477.000000 - time: 1.272527, loss: 0.013197, perplexity: 1.013285, precision: 0.984375, batch_len: 136.000000
Train, loss=0.01319720: 1478it [34:45,  1.39s/it]2017-06-02 14:49:44,370 root  INFO     step 1478.000000 - time: 1.098800, loss: 0.026809, perplexity: 1.027172, precision: 0.906250, batch_len: 78.000000
Train, loss=0.02680935: 1479it [34:46,  1.35s/it]2017-06-02 14:49:45,421 root  INFO     step 1479.000000 - time: 1.013145, loss: 0.006065, perplexity: 1.006084, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00606544: 1480it [34:47,  1.26s/it]2017-06-02 14:49:46,795 root  INFO     step 1480.000000 - time: 1.353629, loss: 0.025881, perplexity: 1.026219, precision: 0.921875, batch_len: 134.000000
Train, loss=0.02588132: 1481it [34:48,  1.29s/it]2017-06-02 14:49:48,450 root  INFO     step 1481.000000 - time: 1.408803, loss: 0.022746, perplexity: 1.023007, precision: 0.937500, batch_len: 139.000000
Train, loss=0.02274592: 1482it [34:50,  1.40s/it]2017-06-02 14:49:50,455 root  INFO     step 1482.000000 - time: 1.945037, loss: 0.019837, perplexity: 1.020035, precision: 0.968750, batch_len: 152.000000
Train, loss=0.01983707: 1483it [34:52,  1.58s/it]2017-06-02 14:49:51,869 root  INFO     step 1483.000000 - time: 1.308930, loss: 0.018169, perplexity: 1.018335, precision: 0.984375, batch_len: 72.000000
Train, loss=0.01816886: 1484it [34:53,  1.53s/it]2017-06-02 14:49:52,934 root  INFO     step 1484.000000 - time: 1.032162, loss: 0.050722, perplexity: 1.052031, precision: 0.906250, batch_len: 76.000000
Train, loss=0.05072215: 1485it [34:54,  1.39s/it]2017-06-02 14:49:54,082 root  INFO     step 1485.000000 - time: 0.999099, loss: 0.023295, perplexity: 1.023568, precision: 0.968750, batch_len: 77.000000
Train, loss=0.02329454: 1486it [34:56,  1.32s/it]2017-06-02 14:49:55,853 root  INFO     step 1486.000000 - time: 1.747737, loss: 0.024872, perplexity: 1.025184, precision: 0.953125, batch_len: 150.000000
Train, loss=0.02487239: 1487it [34:57,  1.45s/it]2017-06-02 14:49:57,480 root  INFO     step 1487.000000 - time: 1.547639, loss: 0.015039, perplexity: 1.015152, precision: 0.953125, batch_len: 131.000000
Train, loss=0.01503883: 1488it [34:59,  1.51s/it]2017-06-02 14:49:59,083 root  INFO     step 1488.000000 - time: 1.507923, loss: 0.014209, perplexity: 1.014311, precision: 0.937500, batch_len: 132.000000
Train, loss=0.01420912: 1489it [35:01,  1.54s/it]2017-06-02 14:50:00,701 root  INFO     step 1489.000000 - time: 1.533322, loss: 0.035073, perplexity: 1.035695, precision: 0.937500, batch_len: 142.000000
Train, loss=0.03507292: 1490it [35:02,  1.56s/it]2017-06-02 14:50:01,765 root  INFO     step 1490.000000 - time: 0.859804, loss: 0.010700, perplexity: 1.010757, precision: 0.968750, batch_len: 71.000000
Train, loss=0.01069962: 1491it [35:03,  1.41s/it]2017-06-02 14:50:01,895 root  INFO     Generating first batch)
2017-06-02 14:50:05,790 root  INFO     step 1491.000000 - time: 1.078359, loss: 0.059015, perplexity: 1.060792, precision: 0.906250, batch_len: 105.000000
Train, loss=0.05901538: 1492it [35:07,  2.20s/it]2017-06-02 14:50:06,885 root  INFO     step 1492.000000 - time: 1.060320, loss: 0.029444, perplexity: 1.029881, precision: 0.968750, batch_len: 101.000000
Train, loss=0.02944362: 1493it [35:08,  1.87s/it]2017-06-02 14:50:08,578 root  INFO     step 1493.000000 - time: 1.533539, loss: 0.007816, perplexity: 1.007847, precision: 0.968750, batch_len: 128.000000
Train, loss=0.00781617: 1494it [35:10,  1.81s/it]2017-06-02 14:50:10,025 root  INFO     step 1494.000000 - time: 1.171737, loss: 0.014079, perplexity: 1.014179, precision: 0.953125, batch_len: 96.000000
Train, loss=0.01407909: 1495it [35:11,  1.70s/it]2017-06-02 14:50:11,561 root  INFO     step 1495.000000 - time: 1.368246, loss: 0.026715, perplexity: 1.027076, precision: 0.937500, batch_len: 120.000000
Train, loss=0.02671550: 1496it [35:13,  1.65s/it]2017-06-02 14:50:12,602 root  INFO     step 1496.000000 - time: 0.933035, loss: 0.035173, perplexity: 1.035799, precision: 0.921875, batch_len: 89.000000
Train, loss=0.03517276: 1497it [35:14,  1.47s/it]2017-06-02 14:50:13,725 root  INFO     step 1497.000000 - time: 1.031198, loss: 0.025996, perplexity: 1.026337, precision: 0.984375, batch_len: 106.000000
Train, loss=0.02599647: 1498it [35:15,  1.37s/it]2017-06-02 14:50:14,903 root  INFO     step 1498.000000 - time: 1.164811, loss: 0.010873, perplexity: 1.010933, precision: 0.968750, batch_len: 103.000000
Train, loss=0.01087319: 1499it [35:16,  1.31s/it]2017-06-02 14:50:16,246 root  INFO     step 1499.000000 - time: 1.295903, loss: 0.024712, perplexity: 1.025020, precision: 0.937500, batch_len: 113.000000
Train, loss=0.02471234: 1500it [35:18,  1.32s/it]2017-06-02 14:50:17,317 root  INFO     step 1500.000000 - time: 0.997840, loss: 0.034074, perplexity: 1.034662, precision: 0.968750, batch_len: 92.000000
Train, loss=0.03407439: 1501it [35:19,  1.25s/it]2017-06-02 14:50:18,391 root  INFO     step 1501.000000 - time: 1.059635, loss: 0.045103, perplexity: 1.046136, precision: 0.875000, batch_len: 109.000000
Train, loss=0.04510337: 1502it [35:20,  1.19s/it]2017-06-02 14:50:19,395 root  INFO     step 1502.000000 - time: 0.973243, loss: 0.027882, perplexity: 1.028274, precision: 0.937500, batch_len: 97.000000
Train, loss=0.02788185: 1503it [35:21,  1.14s/it]2017-06-02 14:50:20,496 root  INFO     step 1503.000000 - time: 1.065394, loss: 0.025433, perplexity: 1.025759, precision: 0.937500, batch_len: 110.000000
Train, loss=0.02543321: 1504it [35:22,  1.13s/it]2017-06-02 14:50:21,837 root  INFO     step 1504.000000 - time: 1.323132, loss: 0.081925, perplexity: 1.085375, precision: 0.906250, batch_len: 111.000000
Train, loss=0.08192526: 1505it [35:23,  1.19s/it]2017-06-02 14:50:23,459 root  INFO     step 1505.000000 - time: 1.483593, loss: 0.034006, perplexity: 1.034591, precision: 0.953125, batch_len: 112.000000
Train, loss=0.03400645: 1506it [35:25,  1.32s/it]2017-06-02 14:50:24,634 root  INFO     step 1506.000000 - time: 1.088910, loss: 0.044903, perplexity: 1.045926, precision: 0.921875, batch_len: 108.000000
Train, loss=0.04490282: 1507it [35:26,  1.28s/it]2017-06-02 14:50:25,816 root  INFO     step 1507.000000 - time: 1.112348, loss: 0.024820, perplexity: 1.025130, precision: 0.921875, batch_len: 104.000000
Train, loss=0.02481991: 1508it [35:27,  1.25s/it]2017-06-02 14:50:26,987 root  INFO     step 1508.000000 - time: 1.074589, loss: 0.074094, perplexity: 1.076908, precision: 0.875000, batch_len: 93.000000
Train, loss=0.07409412: 1509it [35:28,  1.22s/it]2017-06-02 14:50:28,134 root  INFO     step 1509.000000 - time: 1.135200, loss: 0.094760, perplexity: 1.099395, precision: 0.812500, batch_len: 88.000000
Train, loss=0.09475973: 1510it [35:30,  1.20s/it]2017-06-02 14:50:29,255 root  INFO     step 1510.000000 - time: 1.090763, loss: 0.066628, perplexity: 1.068898, precision: 0.906250, batch_len: 114.000000
Train, loss=0.06662831: 1511it [35:31,  1.18s/it]2017-06-02 14:50:31,274 root  INFO     step 1511.000000 - time: 1.971949, loss: 0.093323, perplexity: 1.097816, precision: 0.843750, batch_len: 90.000000
Train, loss=0.09332284: 1512it [35:33,  1.43s/it]2017-06-02 14:50:32,692 root  INFO     step 1512.000000 - time: 1.368116, loss: 0.031752, perplexity: 1.032261, precision: 0.921875, batch_len: 117.000000
Train, loss=0.03175179: 1513it [35:34,  1.43s/it]2017-06-02 14:50:33,855 root  INFO     step 1513.000000 - time: 1.089192, loss: 0.037695, perplexity: 1.038415, precision: 0.921875, batch_len: 115.000000
Train, loss=0.03769522: 1514it [35:35,  1.35s/it]2017-06-02 14:50:35,173 root  INFO     step 1514.000000 - time: 1.211713, loss: 0.025826, perplexity: 1.026162, precision: 0.968750, batch_len: 124.000000
Train, loss=0.02582570: 1515it [35:37,  1.34s/it]2017-06-02 14:50:36,434 root  INFO     step 1515.000000 - time: 1.180283, loss: 0.033881, perplexity: 1.034462, precision: 0.921875, batch_len: 85.000000
Train, loss=0.03388138: 1516it [35:38,  1.32s/it]2017-06-02 14:50:38,079 root  INFO     step 1516.000000 - time: 1.576979, loss: 0.007363, perplexity: 1.007390, precision: 0.984375, batch_len: 116.000000
Train, loss=0.00736295: 1517it [35:40,  1.41s/it]2017-06-02 14:50:39,178 root  INFO     step 1517.000000 - time: 0.969173, loss: 0.021201, perplexity: 1.021428, precision: 0.968750, batch_len: 80.000000
Train, loss=0.02120117: 1518it [35:41,  1.32s/it]2017-06-02 14:50:40,245 root  INFO     step 1518.000000 - time: 1.007098, loss: 0.059146, perplexity: 1.060930, precision: 0.890625, batch_len: 102.000000
Train, loss=0.05914565: 1519it [35:42,  1.24s/it]2017-06-02 14:50:41,362 root  INFO     step 1519.000000 - time: 1.066139, loss: 0.074940, perplexity: 1.077820, precision: 0.875000, batch_len: 100.000000
Train, loss=0.07494047: 1520it [35:43,  1.21s/it]2017-06-02 14:50:42,509 root  INFO     step 1520.000000 - time: 1.049373, loss: 0.039604, perplexity: 1.040399, precision: 0.953125, batch_len: 91.000000
Train, loss=0.03960425: 1521it [35:44,  1.19s/it]2017-06-02 14:50:43,599 root  INFO     step 1521.000000 - time: 1.076799, loss: 0.064127, perplexity: 1.066228, precision: 0.937500, batch_len: 83.000000
Train, loss=0.06412736: 1522it [35:45,  1.16s/it]2017-06-02 14:50:45,147 root  INFO     step 1522.000000 - time: 1.515245, loss: 0.038020, perplexity: 1.038752, precision: 0.953125, batch_len: 119.000000
Train, loss=0.03801998: 1523it [35:47,  1.28s/it]2017-06-02 14:50:46,142 root  INFO     step 1523.000000 - time: 0.959476, loss: 0.103088, perplexity: 1.108589, precision: 0.921875, batch_len: 86.000000
Train, loss=0.10308802: 1524it [35:48,  1.19s/it]2017-06-02 14:50:47,622 root  INFO     step 1524.000000 - time: 1.316678, loss: 0.019429, perplexity: 1.019619, precision: 0.953125, batch_len: 118.000000
Train, loss=0.01942931: 1525it [35:49,  1.28s/it]2017-06-02 14:50:49,040 root  INFO     step 1525.000000 - time: 1.334107, loss: 0.021340, perplexity: 1.021569, precision: 0.937500, batch_len: 129.000000
Train, loss=0.02134014: 1526it [35:51,  1.32s/it]2017-06-02 14:50:50,728 root  INFO     step 1526.000000 - time: 1.652619, loss: 0.052339, perplexity: 1.053733, precision: 0.843750, batch_len: 123.000000
Train, loss=0.05233916: 1527it [35:52,  1.43s/it]2017-06-02 14:50:51,788 root  INFO     step 1527.000000 - time: 0.987079, loss: 0.067495, perplexity: 1.069825, precision: 0.906250, batch_len: 81.000000
Train, loss=0.06749503: 1528it [35:53,  1.32s/it]2017-06-02 14:50:52,750 root  INFO     step 1528.000000 - time: 0.943748, loss: 0.088755, perplexity: 1.092813, precision: 0.843750, batch_len: 79.000000
Train, loss=0.08875476: 1529it [35:54,  1.21s/it]2017-06-02 14:50:53,830 root  INFO     step 1529.000000 - time: 1.065517, loss: 0.031208, perplexity: 1.031700, precision: 0.937500, batch_len: 99.000000
Train, loss=0.03120754: 1530it [35:55,  1.17s/it]2017-06-02 14:50:54,901 root  INFO     step 1530.000000 - time: 1.040371, loss: 0.013875, perplexity: 1.013972, precision: 0.968750, batch_len: 98.000000
Train, loss=0.01387491: 1531it [35:56,  1.14s/it]2017-06-02 14:50:55,890 root  INFO     step 1531.000000 - time: 0.911298, loss: 0.065653, perplexity: 1.067856, precision: 0.906250, batch_len: 94.000000
Train, loss=0.06565291: 1532it [35:57,  1.10s/it]2017-06-02 14:50:57,027 root  INFO     step 1532.000000 - time: 1.098462, loss: 0.061931, perplexity: 1.063889, precision: 0.875000, batch_len: 84.000000
Train, loss=0.06193150: 1533it [35:58,  1.11s/it]2017-06-02 14:50:58,578 root  INFO     step 1533.000000 - time: 1.505439, loss: 0.052486, perplexity: 1.053888, precision: 0.937500, batch_len: 137.000000
Train, loss=0.05248591: 1534it [36:00,  1.24s/it]2017-06-02 14:51:00,033 root  INFO     step 1534.000000 - time: 1.448218, loss: 0.054629, perplexity: 1.056149, precision: 0.937500, batch_len: 144.000000
Train, loss=0.05462899: 1535it [36:02,  1.31s/it]2017-06-02 14:51:01,384 root  INFO     step 1535.000000 - time: 1.325041, loss: 0.112774, perplexity: 1.119379, precision: 0.781250, batch_len: 125.000000
Train, loss=0.11277448: 1536it [36:03,  1.32s/it]2017-06-02 14:51:02,728 root  INFO     step 1536.000000 - time: 1.278951, loss: 0.179848, perplexity: 1.197036, precision: 0.750000, batch_len: 121.000000
Train, loss=0.17984840: 1537it [36:04,  1.33s/it]2017-06-02 14:51:04,606 root  INFO     step 1537.000000 - time: 1.715333, loss: 0.709947, perplexity: 2.033883, precision: 0.453125, batch_len: 126.000000
Train, loss=0.70994681: 1538it [36:06,  1.49s/it]2017-06-02 14:51:05,824 root  INFO     step 1538.000000 - time: 1.094119, loss: 0.831366, perplexity: 2.296454, precision: 0.500000, batch_len: 107.000000
Train, loss=0.83136636: 1539it [36:07,  1.41s/it]2017-06-02 14:51:07,017 root  INFO     step 1539.000000 - time: 0.948163, loss: 0.788922, perplexity: 2.201023, precision: 0.500000, batch_len: 87.000000
Train, loss=0.78892231: 1540it [36:08,  1.34s/it]2017-06-02 14:51:08,077 root  INFO     step 1540.000000 - time: 0.889082, loss: 0.557874, perplexity: 1.746954, precision: 0.578125, batch_len: 82.000000
Train, loss=0.55787355: 1541it [36:10,  1.26s/it]2017-06-02 14:51:09,726 root  INFO     step 1541.000000 - time: 1.491992, loss: 0.203738, perplexity: 1.225977, precision: 0.578125, batch_len: 136.000000
Train, loss=0.20373775: 1542it [36:11,  1.38s/it]2017-06-02 14:51:11,372 root  INFO     step 1542.000000 - time: 1.565737, loss: 0.298669, perplexity: 1.348063, precision: 0.625000, batch_len: 122.000000
Train, loss=0.29866910: 1543it [36:13,  1.46s/it]2017-06-02 14:51:12,880 root  INFO     step 1543.000000 - time: 1.408661, loss: 0.260602, perplexity: 1.297711, precision: 0.562500, batch_len: 138.000000
Train, loss=0.26060182: 1544it [36:14,  1.47s/it]2017-06-02 14:51:14,335 root  INFO     step 1544.000000 - time: 1.399448, loss: 0.179409, perplexity: 1.196510, precision: 0.609375, batch_len: 135.000000
Train, loss=0.17940879: 1545it [36:16,  1.47s/it]2017-06-02 14:51:15,239 root  INFO     step 1545.000000 - time: 0.896038, loss: 0.046621, perplexity: 1.047725, precision: 0.890625, batch_len: 96.000000
Train, loss=0.04662119: 1546it [36:17,  1.30s/it]2017-06-02 14:51:16,803 root  INFO     step 1546.000000 - time: 1.532518, loss: 0.059587, perplexity: 1.061398, precision: 0.843750, batch_len: 133.000000
Train, loss=0.05958707: 1547it [36:18,  1.38s/it]2017-06-02 14:51:17,964 root  INFO     step 1547.000000 - time: 1.124024, loss: 0.070748, perplexity: 1.073310, precision: 0.921875, batch_len: 78.000000
Train, loss=0.07074772: 1548it [36:19,  1.31s/it]2017-06-02 14:51:19,497 root  INFO     step 1548.000000 - time: 1.415174, loss: 0.057436, perplexity: 1.059118, precision: 0.875000, batch_len: 130.000000
Train, loss=0.05743650: 1549it [36:21,  1.38s/it]2017-06-02 14:51:21,610 root  INFO     step 1549.000000 - time: 1.873228, loss: 0.036177, perplexity: 1.036839, precision: 0.921875, batch_len: 152.000000
Train, loss=0.03617688: 1550it [36:23,  1.60s/it]2017-06-02 14:51:22,578 root  INFO     step 1550.000000 - time: 0.901574, loss: 0.023621, perplexity: 1.023902, precision: 0.968750, batch_len: 74.000000
Train, loss=0.02362115: 1551it [36:24,  1.41s/it]2017-06-02 14:51:23,745 root  INFO     step 1551.000000 - time: 1.056418, loss: 0.023922, perplexity: 1.024210, precision: 0.984375, batch_len: 71.000000
Train, loss=0.02392171: 1552it [36:25,  1.34s/it]2017-06-02 14:51:25,379 root  INFO     step 1552.000000 - time: 1.535381, loss: 0.020956, perplexity: 1.021177, precision: 0.968750, batch_len: 131.000000
Train, loss=0.02095625: 1553it [36:27,  1.43s/it]2017-06-02 14:51:26,889 root  INFO     step 1553.000000 - time: 1.486576, loss: 0.019018, perplexity: 1.019200, precision: 0.953125, batch_len: 134.000000
Train, loss=0.01901834: 1554it [36:28,  1.45s/it]2017-06-02 14:51:28,352 root  INFO     step 1554.000000 - time: 1.407886, loss: 0.030602, perplexity: 1.031075, precision: 0.953125, batch_len: 132.000000
Train, loss=0.03060200: 1555it [36:30,  1.45s/it]2017-06-02 14:51:29,679 root  INFO     step 1555.000000 - time: 1.302336, loss: 0.036303, perplexity: 1.036970, precision: 0.968750, batch_len: 72.000000
Train, loss=0.03630270: 1556it [36:31,  1.42s/it]2017-06-02 14:51:30,622 root  INFO     step 1556.000000 - time: 0.925648, loss: 0.018214, perplexity: 1.018380, precision: 0.968750, batch_len: 77.000000
Train, loss=0.01821359: 1557it [36:32,  1.27s/it]2017-06-02 14:51:31,749 root  INFO     step 1557.000000 - time: 1.108671, loss: 0.048893, perplexity: 1.050108, precision: 0.937500, batch_len: 76.000000
Train, loss=0.04889323: 1558it [36:33,  1.23s/it]2017-06-02 14:51:33,398 root  INFO     step 1558.000000 - time: 1.561835, loss: 0.026855, perplexity: 1.027219, precision: 0.921875, batch_len: 139.000000
Train, loss=0.02685513: 1559it [36:35,  1.36s/it]2017-06-02 14:51:34,789 root  INFO     step 1559.000000 - time: 1.366027, loss: 0.042331, perplexity: 1.043240, precision: 0.906250, batch_len: 141.000000
Train, loss=0.04233138: 1560it [36:36,  1.37s/it]2017-06-02 14:51:36,828 root  INFO     step 1560.000000 - time: 1.859146, loss: 0.028406, perplexity: 1.028813, precision: 0.953125, batch_len: 150.000000
Train, loss=0.02840578: 1561it [36:38,  1.57s/it]2017-06-02 14:51:38,257 root  INFO     step 1561.000000 - time: 1.407153, loss: 0.029820, perplexity: 1.030270, precision: 0.890625, batch_len: 142.000000
Train, loss=0.02982044: 1562it [36:40,  1.53s/it]2017-06-02 14:51:38,520 root  INFO     Generating first batch)
2017-06-02 14:51:41,911 root  INFO     step 1562.000000 - time: 0.930780, loss: 0.013209, perplexity: 1.013297, precision: 0.984375, batch_len: 96.000000
Train, loss=0.01320920: 1563it [36:43,  2.16s/it]2017-06-02 14:51:43,528 root  INFO     step 1563.000000 - time: 1.128203, loss: 0.021021, perplexity: 1.021244, precision: 0.953125, batch_len: 103.000000
Train, loss=0.02102111: 1564it [36:45,  2.00s/it]2017-06-02 14:51:44,909 root  INFO     step 1564.000000 - time: 1.307418, loss: 0.056782, perplexity: 1.058425, precision: 0.890625, batch_len: 113.000000
Train, loss=0.05678231: 1565it [36:46,  1.81s/it]2017-06-02 14:51:46,148 root  INFO     step 1565.000000 - time: 0.958188, loss: 0.041123, perplexity: 1.041981, precision: 0.937500, batch_len: 86.000000
Train, loss=0.04112342: 1566it [36:48,  1.64s/it]2017-06-02 14:51:47,171 root  INFO     step 1566.000000 - time: 0.979024, loss: 0.018343, perplexity: 1.018512, precision: 0.984375, batch_len: 89.000000
Train, loss=0.01834252: 1567it [36:49,  1.46s/it]2017-06-02 14:51:48,280 root  INFO     step 1567.000000 - time: 1.062325, loss: 0.025866, perplexity: 1.026203, precision: 0.921875, batch_len: 108.000000
Train, loss=0.02586589: 1568it [36:50,  1.35s/it]2017-06-02 14:51:49,514 root  INFO     step 1568.000000 - time: 1.157472, loss: 0.045588, perplexity: 1.046643, precision: 0.906250, batch_len: 105.000000
Train, loss=0.04558793: 1569it [36:51,  1.32s/it]2017-06-02 14:51:50,859 root  INFO     step 1569.000000 - time: 1.288237, loss: 0.035515, perplexity: 1.036153, precision: 0.890625, batch_len: 102.000000
Train, loss=0.03551511: 1570it [36:52,  1.33s/it]2017-06-02 14:51:52,056 root  INFO     step 1570.000000 - time: 1.126328, loss: 0.015386, perplexity: 1.015505, precision: 0.968750, batch_len: 110.000000
Train, loss=0.01538634: 1571it [36:54,  1.29s/it]2017-06-02 14:51:53,078 root  INFO     step 1571.000000 - time: 0.999051, loss: 0.014507, perplexity: 1.014613, precision: 0.984375, batch_len: 97.000000
Train, loss=0.01450711: 1572it [36:55,  1.21s/it]2017-06-02 14:51:54,089 root  INFO     step 1572.000000 - time: 0.970435, loss: 0.018776, perplexity: 1.018953, precision: 0.984375, batch_len: 92.000000
Train, loss=0.01877565: 1573it [36:56,  1.15s/it]2017-06-02 14:51:55,357 root  INFO     step 1573.000000 - time: 1.200333, loss: 0.028028, perplexity: 1.028424, precision: 0.968750, batch_len: 117.000000
Train, loss=0.02802765: 1574it [36:57,  1.18s/it]2017-06-02 14:51:56,895 root  INFO     step 1574.000000 - time: 1.520106, loss: 0.012934, perplexity: 1.013018, precision: 0.984375, batch_len: 128.000000
Train, loss=0.01293373: 1575it [36:58,  1.29s/it]2017-06-02 14:51:58,256 root  INFO     step 1575.000000 - time: 1.271165, loss: 0.034629, perplexity: 1.035236, precision: 0.906250, batch_len: 104.000000
Train, loss=0.03462895: 1576it [37:00,  1.31s/it]2017-06-02 14:51:59,641 root  INFO     step 1576.000000 - time: 1.323501, loss: 0.040005, perplexity: 1.040816, precision: 0.953125, batch_len: 112.000000
Train, loss=0.04000490: 1577it [37:01,  1.33s/it]2017-06-02 14:52:00,831 root  INFO     step 1577.000000 - time: 1.033744, loss: 0.014930, perplexity: 1.015042, precision: 0.968750, batch_len: 100.000000
Train, loss=0.01492958: 1578it [37:02,  1.29s/it]2017-06-02 14:52:02,042 root  INFO     step 1578.000000 - time: 1.189019, loss: 0.054683, perplexity: 1.056206, precision: 0.875000, batch_len: 120.000000
Train, loss=0.05468304: 1579it [37:04,  1.27s/it]2017-06-02 14:52:03,252 root  INFO     step 1579.000000 - time: 1.206374, loss: 0.065058, perplexity: 1.067221, precision: 0.875000, batch_len: 107.000000
Train, loss=0.06505767: 1580it [37:05,  1.25s/it]2017-06-02 14:52:04,529 root  INFO     step 1580.000000 - time: 1.267155, loss: 0.120544, perplexity: 1.128110, precision: 0.828125, batch_len: 93.000000
Train, loss=0.12054355: 1581it [37:06,  1.26s/it]2017-06-02 14:52:05,472 root  INFO     step 1581.000000 - time: 0.929767, loss: 0.053626, perplexity: 1.055090, precision: 0.921875, batch_len: 79.000000
Train, loss=0.05362644: 1582it [37:07,  1.16s/it]2017-06-02 14:52:06,622 root  INFO     step 1582.000000 - time: 1.106613, loss: 0.067421, perplexity: 1.069746, precision: 0.875000, batch_len: 109.000000
Train, loss=0.06742150: 1583it [37:08,  1.16s/it]2017-06-02 14:52:07,752 root  INFO     step 1583.000000 - time: 1.054704, loss: 0.027303, perplexity: 1.027679, precision: 0.921875, batch_len: 106.000000
Train, loss=0.02730272: 1584it [37:09,  1.15s/it]2017-06-02 14:52:08,730 root  INFO     step 1584.000000 - time: 0.954072, loss: 0.026383, perplexity: 1.026734, precision: 0.921875, batch_len: 101.000000
Train, loss=0.02638277: 1585it [37:10,  1.10s/it]2017-06-02 14:52:10,483 root  INFO     step 1585.000000 - time: 1.745374, loss: 0.082615, perplexity: 1.086124, precision: 0.875000, batch_len: 90.000000
Train, loss=0.08261510: 1586it [37:12,  1.29s/it]2017-06-02 14:52:11,796 root  INFO     step 1586.000000 - time: 1.271665, loss: 0.070774, perplexity: 1.073339, precision: 0.875000, batch_len: 111.000000
Train, loss=0.07077418: 1587it [37:13,  1.30s/it]2017-06-02 14:52:13,224 root  INFO     step 1587.000000 - time: 1.370646, loss: 0.038889, perplexity: 1.039655, precision: 0.906250, batch_len: 123.000000
Train, loss=0.03888856: 1588it [37:15,  1.34s/it]2017-06-02 14:52:14,247 root  INFO     step 1588.000000 - time: 1.016998, loss: 0.044376, perplexity: 1.045375, precision: 0.906250, batch_len: 85.000000
Train, loss=0.04437574: 1589it [37:16,  1.24s/it]2017-06-02 14:52:15,204 root  INFO     step 1589.000000 - time: 0.907815, loss: 0.060109, perplexity: 1.061952, precision: 0.921875, batch_len: 88.000000
Train, loss=0.06010912: 1590it [37:17,  1.16s/it]2017-06-02 14:52:16,280 root  INFO     step 1590.000000 - time: 0.994602, loss: 0.052279, perplexity: 1.053669, precision: 0.906250, batch_len: 81.000000
Train, loss=0.05227872: 1591it [37:18,  1.13s/it]2017-06-02 14:52:17,905 root  INFO     step 1591.000000 - time: 1.542823, loss: 0.053460, perplexity: 1.054915, precision: 0.906250, batch_len: 91.000000
Train, loss=0.05345975: 1592it [37:19,  1.28s/it]2017-06-02 14:52:19,336 root  INFO     step 1592.000000 - time: 1.310818, loss: 0.039942, perplexity: 1.040750, precision: 0.859375, batch_len: 125.000000
Train, loss=0.03994153: 1593it [37:21,  1.33s/it]2017-06-02 14:52:20,677 root  INFO     step 1593.000000 - time: 1.333977, loss: 0.028297, perplexity: 1.028701, precision: 0.984375, batch_len: 121.000000
Train, loss=0.02829693: 1594it [37:22,  1.33s/it]2017-06-02 14:52:21,969 root  INFO     step 1594.000000 - time: 1.286703, loss: 0.015447, perplexity: 1.015567, precision: 0.968750, batch_len: 124.000000
Train, loss=0.01544669: 1595it [37:23,  1.32s/it]2017-06-02 14:52:23,259 root  INFO     step 1595.000000 - time: 1.134506, loss: 0.027222, perplexity: 1.027595, precision: 0.921875, batch_len: 94.000000
Train, loss=0.02722151: 1596it [37:25,  1.31s/it]2017-06-02 14:52:24,589 root  INFO     step 1596.000000 - time: 1.263313, loss: 0.030493, perplexity: 1.030963, precision: 0.937500, batch_len: 80.000000
Train, loss=0.03049327: 1597it [37:26,  1.32s/it]2017-06-02 14:52:25,964 root  INFO     step 1597.000000 - time: 1.139754, loss: 0.067178, perplexity: 1.069486, precision: 0.937500, batch_len: 115.000000
Train, loss=0.06717840: 1598it [37:27,  1.33s/it]2017-06-02 14:52:27,057 root  INFO     step 1598.000000 - time: 1.062196, loss: 0.060172, perplexity: 1.062019, precision: 0.906250, batch_len: 83.000000
Train, loss=0.06017156: 1599it [37:29,  1.26s/it]2017-06-02 14:52:28,488 root  INFO     step 1599.000000 - time: 1.422075, loss: 0.052606, perplexity: 1.054014, precision: 0.906250, batch_len: 119.000000
Train, loss=0.05260611: 1600it [37:30,  1.31s/it]2017-06-02 14:52:30,200 root  INFO     step 1600.000000 - time: 1.522882, loss: 0.062923, perplexity: 1.064945, precision: 0.953125, batch_len: 118.000000
Train, loss=0.06292339: 1601it [37:32,  1.43s/it]2017-06-02 14:52:31,688 root  INFO     step 1601.000000 - time: 1.432519, loss: 0.064887, perplexity: 1.067039, precision: 0.890625, batch_len: 135.000000
Train, loss=0.06488708: 1602it [37:33,  1.45s/it]2017-06-02 14:52:33,053 root  INFO     step 1602.000000 - time: 1.334626, loss: 0.111297, perplexity: 1.117727, precision: 0.843750, batch_len: 84.000000
Train, loss=0.11129676: 1603it [37:35,  1.42s/it]2017-06-02 14:52:34,285 root  INFO     step 1603.000000 - time: 1.124093, loss: 0.052400, perplexity: 1.053797, precision: 0.906250, batch_len: 114.000000
Train, loss=0.05240004: 1604it [37:36,  1.37s/it]2017-06-02 14:52:35,224 root  INFO     step 1604.000000 - time: 0.928924, loss: 0.048993, perplexity: 1.050213, precision: 0.890625, batch_len: 87.000000
Train, loss=0.04899328: 1605it [37:37,  1.24s/it]2017-06-02 14:52:36,610 root  INFO     step 1605.000000 - time: 1.334137, loss: 0.023327, perplexity: 1.023601, precision: 0.968750, batch_len: 116.000000
Train, loss=0.02332713: 1606it [37:38,  1.28s/it]2017-06-02 14:52:37,589 root  INFO     step 1606.000000 - time: 0.958297, loss: 0.038053, perplexity: 1.038786, precision: 0.875000, batch_len: 99.000000
Train, loss=0.03805260: 1607it [37:39,  1.19s/it]2017-06-02 14:52:39,132 root  INFO     step 1607.000000 - time: 1.476632, loss: 0.041932, perplexity: 1.042823, precision: 0.921875, batch_len: 137.000000
Train, loss=0.04193188: 1608it [37:41,  1.30s/it]2017-06-02 14:52:40,632 root  INFO     step 1608.000000 - time: 1.461873, loss: 0.040867, perplexity: 1.041714, precision: 0.906250, batch_len: 126.000000
Train, loss=0.04086725: 1609it [37:42,  1.36s/it]2017-06-02 14:52:41,780 root  INFO     step 1609.000000 - time: 1.071861, loss: 0.016409, perplexity: 1.016544, precision: 0.984375, batch_len: 98.000000
Train, loss=0.01640850: 1610it [37:43,  1.29s/it]2017-06-02 14:52:43,500 root  INFO     step 1610.000000 - time: 1.404357, loss: 0.019321, perplexity: 1.019509, precision: 0.937500, batch_len: 129.000000
Train, loss=0.01932113: 1611it [37:45,  1.42s/it]2017-06-02 14:52:44,986 root  INFO     step 1611.000000 - time: 1.384742, loss: 0.011501, perplexity: 1.011567, precision: 0.968750, batch_len: 136.000000
Train, loss=0.01150091: 1612it [37:46,  1.44s/it]2017-06-02 14:52:46,154 root  INFO     step 1612.000000 - time: 1.144037, loss: 0.041251, perplexity: 1.042114, precision: 0.921875, batch_len: 82.000000
Train, loss=0.04125087: 1613it [37:48,  1.36s/it]2017-06-02 14:52:47,631 root  INFO     step 1613.000000 - time: 1.434061, loss: 0.011874, perplexity: 1.011944, precision: 1.000000, batch_len: 130.000000
Train, loss=0.01187363: 1614it [37:49,  1.39s/it]2017-06-02 14:52:49,276 root  INFO     step 1614.000000 - time: 1.400731, loss: 0.008814, perplexity: 1.008853, precision: 0.984375, batch_len: 133.000000
Train, loss=0.00881419: 1615it [37:51,  1.47s/it]2017-06-02 14:52:50,593 root  INFO     step 1615.000000 - time: 1.265221, loss: 0.046592, perplexity: 1.047694, precision: 0.953125, batch_len: 144.000000
Train, loss=0.04659194: 1616it [37:52,  1.42s/it]2017-06-02 14:52:51,779 root  INFO     step 1616.000000 - time: 1.171036, loss: 0.018891, perplexity: 1.019071, precision: 0.953125, batch_len: 96.000000
Train, loss=0.01889127: 1617it [37:53,  1.35s/it]2017-06-02 14:52:53,441 root  INFO     step 1617.000000 - time: 1.561577, loss: 0.031895, perplexity: 1.032409, precision: 0.968750, batch_len: 134.000000
Train, loss=0.03189496: 1618it [37:55,  1.45s/it]2017-06-02 14:52:54,563 root  INFO     step 1618.000000 - time: 0.948914, loss: 0.038443, perplexity: 1.039192, precision: 0.921875, batch_len: 78.000000
Train, loss=0.03844342: 1619it [37:56,  1.35s/it]2017-06-02 14:52:55,977 root  INFO     step 1619.000000 - time: 1.394005, loss: 0.038017, perplexity: 1.038748, precision: 0.937500, batch_len: 122.000000
Train, loss=0.03801653: 1620it [37:57,  1.37s/it]2017-06-02 14:52:56,988 root  INFO     step 1620.000000 - time: 0.889969, loss: 0.040744, perplexity: 1.041585, precision: 0.937500, batch_len: 76.000000
Train, loss=0.04074375: 1621it [37:58,  1.26s/it]2017-06-02 14:52:58,690 root  INFO     step 1621.000000 - time: 1.531694, loss: 0.014937, perplexity: 1.015049, precision: 0.937500, batch_len: 132.000000
Train, loss=0.01493660: 1622it [38:00,  1.39s/it]2017-06-02 14:53:00,785 root  INFO     step 1622.000000 - time: 2.060154, loss: 0.007996, perplexity: 1.008028, precision: 1.000000, batch_len: 152.000000
Train, loss=0.00799558: 1623it [38:02,  1.60s/it]2017-06-02 14:53:02,225 root  INFO     step 1623.000000 - time: 1.398788, loss: 0.016282, perplexity: 1.016415, precision: 0.953125, batch_len: 138.000000
Train, loss=0.01628168: 1624it [38:04,  1.55s/it]2017-06-02 14:53:03,697 root  INFO     step 1624.000000 - time: 1.214939, loss: 0.017605, perplexity: 1.017761, precision: 0.968750, batch_len: 139.000000
Train, loss=0.01760502: 1625it [38:05,  1.53s/it]2017-06-02 14:53:04,964 root  INFO     step 1625.000000 - time: 1.261947, loss: 0.022429, perplexity: 1.022682, precision: 0.984375, batch_len: 72.000000
Train, loss=0.02242883: 1626it [38:06,  1.45s/it]2017-06-02 14:53:06,619 root  INFO     step 1626.000000 - time: 1.535181, loss: 0.030649, perplexity: 1.031124, precision: 0.937500, batch_len: 141.000000
Train, loss=0.03064923: 1627it [38:08,  1.51s/it]2017-06-02 14:53:08,524 root  INFO     step 1627.000000 - time: 1.857921, loss: 0.013168, perplexity: 1.013255, precision: 0.968750, batch_len: 150.000000
Train, loss=0.01316766: 1628it [38:10,  1.63s/it]2017-06-02 14:53:09,939 root  INFO     step 1628.000000 - time: 1.395106, loss: 0.018489, perplexity: 1.018661, precision: 0.968750, batch_len: 142.000000
Train, loss=0.01848892: 1629it [38:11,  1.57s/it]2017-06-02 14:53:10,780 root  INFO     step 1629.000000 - time: 0.807446, loss: 0.006436, perplexity: 1.006456, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00643572: 1630it [38:12,  1.35s/it]2017-06-02 14:53:12,747 root  INFO     step 1630.000000 - time: 1.753942, loss: 0.008802, perplexity: 1.008841, precision: 0.984375, batch_len: 131.000000
Train, loss=0.00880207: 1631it [38:14,  1.53s/it]2017-06-02 14:53:13,771 root  INFO     step 1631.000000 - time: 0.996101, loss: 0.005558, perplexity: 1.005573, precision: 1.000000, batch_len: 77.000000
Train, loss=0.00555763: 1632it [38:15,  1.38s/it]2017-06-02 14:53:14,746 root  INFO     step 1632.000000 - time: 0.913211, loss: 0.012804, perplexity: 1.012887, precision: 0.968750, batch_len: 71.000000
Train, loss=0.01280449: 1633it [38:16,  1.26s/it]2017-06-02 14:53:14,846 root  INFO     Generating first batch)
2017-06-02 14:53:18,564 root  INFO     step 1633.000000 - time: 1.066779, loss: 0.029504, perplexity: 1.029944, precision: 0.968750, batch_len: 96.000000
Train, loss=0.02950418: 1634it [38:20,  2.03s/it]2017-06-02 14:53:19,852 root  INFO     step 1634.000000 - time: 1.100432, loss: 0.052555, perplexity: 1.053960, precision: 0.890625, batch_len: 105.000000
Train, loss=0.05255483: 1635it [38:21,  1.81s/it]2017-06-02 14:53:21,296 root  INFO     step 1635.000000 - time: 1.093536, loss: 0.046379, perplexity: 1.047472, precision: 0.937500, batch_len: 113.000000
Train, loss=0.04637935: 1636it [38:23,  1.70s/it]2017-06-02 14:53:22,967 root  INFO     step 1636.000000 - time: 1.458000, loss: 0.012830, perplexity: 1.012913, precision: 0.953125, batch_len: 128.000000
Train, loss=0.01283020: 1637it [38:24,  1.69s/it]2017-06-02 14:53:24,238 root  INFO     step 1637.000000 - time: 1.265890, loss: 0.041120, perplexity: 1.041977, precision: 0.937500, batch_len: 92.000000
Train, loss=0.04112017: 1638it [38:26,  1.56s/it]2017-06-02 14:53:25,209 root  INFO     step 1638.000000 - time: 0.967899, loss: 0.115824, perplexity: 1.122798, precision: 0.875000, batch_len: 88.000000
Train, loss=0.11582401: 1639it [38:27,  1.39s/it]2017-06-02 14:53:26,535 root  INFO     step 1639.000000 - time: 1.185526, loss: 0.067985, perplexity: 1.070349, precision: 0.843750, batch_len: 108.000000
Train, loss=0.06798506: 1640it [38:28,  1.37s/it]2017-06-02 14:53:27,740 root  INFO     step 1640.000000 - time: 1.180747, loss: 0.055569, perplexity: 1.057142, precision: 0.906250, batch_len: 111.000000
Train, loss=0.05556913: 1641it [38:29,  1.32s/it]2017-06-02 14:53:28,775 root  INFO     step 1641.000000 - time: 0.997837, loss: 0.132585, perplexity: 1.141776, precision: 0.859375, batch_len: 93.000000
Train, loss=0.13258490: 1642it [38:30,  1.23s/it]2017-06-02 14:53:30,056 root  INFO     step 1642.000000 - time: 1.195706, loss: 0.023443, perplexity: 1.023719, precision: 0.984375, batch_len: 104.000000
Train, loss=0.02344250: 1643it [38:32,  1.25s/it]2017-06-02 14:53:31,187 root  INFO     step 1643.000000 - time: 1.065734, loss: 0.038209, perplexity: 1.038949, precision: 0.937500, batch_len: 101.000000
Train, loss=0.03820927: 1644it [38:33,  1.21s/it]2017-06-02 14:53:32,834 root  INFO     step 1644.000000 - time: 1.630077, loss: 0.026977, perplexity: 1.027344, precision: 0.953125, batch_len: 112.000000
Train, loss=0.02697657: 1645it [38:34,  1.34s/it]2017-06-02 14:53:33,960 root  INFO     step 1645.000000 - time: 0.986468, loss: 0.034980, perplexity: 1.035599, precision: 0.906250, batch_len: 102.000000
Train, loss=0.03497957: 1646it [38:35,  1.28s/it]2017-06-02 14:53:35,339 root  INFO     step 1646.000000 - time: 1.365650, loss: 0.046321, perplexity: 1.047410, precision: 0.859375, batch_len: 120.000000
Train, loss=0.04632078: 1647it [38:37,  1.31s/it]2017-06-02 14:53:36,533 root  INFO     step 1647.000000 - time: 1.106533, loss: 0.016997, perplexity: 1.017142, precision: 0.968750, batch_len: 110.000000
Train, loss=0.01699707: 1648it [38:38,  1.27s/it]2017-06-02 14:53:37,551 root  INFO     step 1648.000000 - time: 0.964707, loss: 0.048080, perplexity: 1.049255, precision: 0.953125, batch_len: 115.000000
Train, loss=0.04807994: 1649it [38:39,  1.20s/it]2017-06-02 14:53:38,906 root  INFO     step 1649.000000 - time: 1.259589, loss: 0.068187, perplexity: 1.070565, precision: 0.906250, batch_len: 107.000000
Train, loss=0.06818651: 1650it [38:40,  1.24s/it]2017-06-02 14:53:40,183 root  INFO     step 1650.000000 - time: 1.245942, loss: 0.075823, perplexity: 1.078771, precision: 0.890625, batch_len: 100.000000
Train, loss=0.07582270: 1651it [38:42,  1.25s/it]2017-06-02 14:53:41,277 root  INFO     step 1651.000000 - time: 1.071272, loss: 0.036142, perplexity: 1.036803, precision: 0.953125, batch_len: 106.000000
Train, loss=0.03614179: 1652it [38:43,  1.21s/it]2017-06-02 14:53:42,629 root  INFO     step 1652.000000 - time: 1.338557, loss: 0.023446, perplexity: 1.023723, precision: 0.906250, batch_len: 123.000000
Train, loss=0.02344584: 1653it [38:44,  1.25s/it]2017-06-02 14:53:43,818 root  INFO     step 1653.000000 - time: 1.173635, loss: 0.008995, perplexity: 1.009035, precision: 0.984375, batch_len: 124.000000
Train, loss=0.00899492: 1654it [38:45,  1.23s/it]2017-06-02 14:53:44,980 root  INFO     step 1654.000000 - time: 1.152129, loss: 0.015027, perplexity: 1.015141, precision: 0.953125, batch_len: 89.000000
Train, loss=0.01502742: 1655it [38:46,  1.21s/it]2017-06-02 14:53:46,258 root  INFO     step 1655.000000 - time: 1.265733, loss: 0.081874, perplexity: 1.085319, precision: 0.843750, batch_len: 83.000000
Train, loss=0.08187400: 1656it [38:48,  1.23s/it]2017-06-02 14:53:47,607 root  INFO     step 1656.000000 - time: 1.310878, loss: 0.047925, perplexity: 1.049092, precision: 0.906250, batch_len: 117.000000
Train, loss=0.04792519: 1657it [38:49,  1.27s/it]2017-06-02 14:53:48,671 root  INFO     step 1657.000000 - time: 1.047941, loss: 0.019665, perplexity: 1.019860, precision: 0.937500, batch_len: 98.000000
Train, loss=0.01966541: 1658it [38:50,  1.21s/it]2017-06-02 14:53:49,968 root  INFO     step 1658.000000 - time: 1.279047, loss: 0.020486, perplexity: 1.020698, precision: 0.921875, batch_len: 125.000000
Train, loss=0.02048624: 1659it [38:51,  1.23s/it]2017-06-02 14:53:50,884 root  INFO     step 1659.000000 - time: 0.820503, loss: 0.056960, perplexity: 1.058613, precision: 0.890625, batch_len: 81.000000
Train, loss=0.05695983: 1660it [38:52,  1.14s/it]2017-06-02 14:53:52,360 root  INFO     step 1660.000000 - time: 1.257694, loss: 0.029443, perplexity: 1.029881, precision: 0.921875, batch_len: 109.000000
Train, loss=0.02944335: 1661it [38:54,  1.24s/it]2017-06-02 14:53:53,575 root  INFO     step 1661.000000 - time: 1.115219, loss: 0.062111, perplexity: 1.064081, precision: 0.890625, batch_len: 86.000000
Train, loss=0.06211113: 1662it [38:55,  1.23s/it]2017-06-02 14:53:54,937 root  INFO     step 1662.000000 - time: 1.337112, loss: 0.038468, perplexity: 1.039218, precision: 0.937500, batch_len: 116.000000
Train, loss=0.03846816: 1663it [38:56,  1.27s/it]2017-06-02 14:53:55,965 root  INFO     step 1663.000000 - time: 1.014565, loss: 0.033797, perplexity: 1.034375, precision: 0.921875, batch_len: 103.000000
Train, loss=0.03379707: 1664it [38:57,  1.20s/it]2017-06-02 14:53:57,108 root  INFO     step 1664.000000 - time: 1.101355, loss: 0.188278, perplexity: 1.207169, precision: 0.781250, batch_len: 91.000000
Train, loss=0.18827829: 1665it [38:59,  1.18s/it]2017-06-02 14:53:58,119 root  INFO     step 1665.000000 - time: 0.992456, loss: 0.122036, perplexity: 1.129795, precision: 0.750000, batch_len: 90.000000
Train, loss=0.12203611: 1666it [39:00,  1.13s/it]2017-06-02 14:53:59,508 root  INFO     step 1666.000000 - time: 1.357349, loss: 0.099786, perplexity: 1.104935, precision: 0.765625, batch_len: 114.000000
Train, loss=0.09978632: 1667it [39:01,  1.21s/it]2017-06-02 14:54:00,894 root  INFO     step 1667.000000 - time: 1.368929, loss: 0.059174, perplexity: 1.060960, precision: 0.890625, batch_len: 121.000000
Train, loss=0.05917397: 1668it [39:02,  1.26s/it]2017-06-02 14:54:01,908 root  INFO     step 1668.000000 - time: 1.007933, loss: 0.026662, perplexity: 1.027020, precision: 0.953125, batch_len: 97.000000
Train, loss=0.02666151: 1669it [39:03,  1.19s/it]2017-06-02 14:54:02,956 root  INFO     step 1669.000000 - time: 1.003086, loss: 0.045744, perplexity: 1.046807, precision: 0.921875, batch_len: 85.000000
Train, loss=0.04574412: 1670it [39:04,  1.15s/it]2017-06-02 14:54:03,901 root  INFO     step 1670.000000 - time: 0.898357, loss: 0.064206, perplexity: 1.066312, precision: 0.906250, batch_len: 80.000000
Train, loss=0.06420629: 1671it [39:05,  1.09s/it]2017-06-02 14:54:05,525 root  INFO     step 1671.000000 - time: 1.492396, loss: 0.075787, perplexity: 1.078732, precision: 0.859375, batch_len: 130.000000
Train, loss=0.07578665: 1672it [39:07,  1.25s/it]2017-06-02 14:54:06,697 root  INFO     step 1672.000000 - time: 1.145623, loss: 0.040193, perplexity: 1.041011, precision: 0.890625, batch_len: 87.000000
Train, loss=0.04019268: 1673it [39:08,  1.22s/it]2017-06-02 14:54:07,779 root  INFO     step 1673.000000 - time: 1.021836, loss: 0.134082, perplexity: 1.143487, precision: 0.828125, batch_len: 94.000000
Train, loss=0.13408232: 1674it [39:09,  1.18s/it]2017-06-02 14:54:09,227 root  INFO     step 1674.000000 - time: 1.373980, loss: 0.030602, perplexity: 1.031075, precision: 0.937500, batch_len: 119.000000
Train, loss=0.03060189: 1675it [39:11,  1.26s/it]2017-06-02 14:54:10,499 root  INFO     step 1675.000000 - time: 1.255295, loss: 0.035541, perplexity: 1.036180, precision: 0.937500, batch_len: 118.000000
Train, loss=0.03554057: 1676it [39:12,  1.26s/it]2017-06-02 14:54:11,549 root  INFO     step 1676.000000 - time: 1.041298, loss: 0.024177, perplexity: 1.024471, precision: 0.921875, batch_len: 99.000000
Train, loss=0.02417664: 1677it [39:13,  1.20s/it]2017-06-02 14:54:13,369 root  INFO     step 1677.000000 - time: 1.636750, loss: 0.036148, perplexity: 1.036810, precision: 0.921875, batch_len: 129.000000
Train, loss=0.03614835: 1678it [39:15,  1.39s/it]2017-06-02 14:54:14,423 root  INFO     step 1678.000000 - time: 0.958155, loss: 0.073346, perplexity: 1.076103, precision: 0.906250, batch_len: 79.000000
Train, loss=0.07334643: 1679it [39:16,  1.29s/it]2017-06-02 14:54:15,843 root  INFO     step 1679.000000 - time: 1.354637, loss: 0.069899, perplexity: 1.072400, precision: 0.875000, batch_len: 137.000000
Train, loss=0.06989920: 1680it [39:17,  1.33s/it]2017-06-02 14:54:17,868 root  INFO     step 1680.000000 - time: 1.776827, loss: 0.026850, perplexity: 1.027214, precision: 0.921875, batch_len: 152.000000
Train, loss=0.02684991: 1681it [39:19,  1.54s/it]2017-06-02 14:54:19,657 root  INFO     step 1681.000000 - time: 1.713183, loss: 0.015348, perplexity: 1.015466, precision: 0.953125, batch_len: 133.000000
Train, loss=0.01534792: 1682it [39:21,  1.61s/it]2017-06-02 14:54:21,000 root  INFO     step 1682.000000 - time: 1.305336, loss: 0.028787, perplexity: 1.029205, precision: 0.937500, batch_len: 126.000000
Train, loss=0.02878661: 1683it [39:22,  1.53s/it]2017-06-02 14:54:22,538 root  INFO     step 1683.000000 - time: 1.444209, loss: 0.040851, perplexity: 1.041697, precision: 0.921875, batch_len: 144.000000
Train, loss=0.04085073: 1684it [39:24,  1.53s/it]2017-06-02 14:54:23,574 root  INFO     step 1684.000000 - time: 0.951655, loss: 0.066310, perplexity: 1.068558, precision: 0.890625, batch_len: 84.000000
Train, loss=0.06630967: 1685it [39:25,  1.38s/it]2017-06-02 14:54:25,300 root  INFO     step 1685.000000 - time: 1.441930, loss: 0.018662, perplexity: 1.018837, precision: 0.921875, batch_len: 136.000000
Train, loss=0.01866217: 1686it [39:27,  1.49s/it]2017-06-02 14:54:26,579 root  INFO     step 1686.000000 - time: 1.161634, loss: 0.040327, perplexity: 1.041152, precision: 0.890625, batch_len: 78.000000
Train, loss=0.04032743: 1687it [39:28,  1.42s/it]2017-06-02 14:54:28,130 root  INFO     step 1687.000000 - time: 1.458827, loss: 0.039255, perplexity: 1.040036, precision: 0.906250, batch_len: 122.000000
Train, loss=0.03925540: 1688it [39:30,  1.46s/it]2017-06-02 14:54:29,663 root  INFO     step 1688.000000 - time: 1.438982, loss: 0.046707, perplexity: 1.047815, precision: 0.953125, batch_len: 135.000000
Train, loss=0.04670723: 1689it [39:31,  1.48s/it]2017-06-02 14:54:30,643 root  INFO     step 1689.000000 - time: 0.972388, loss: 0.088488, perplexity: 1.092521, precision: 0.875000, batch_len: 82.000000
Train, loss=0.08848824: 1690it [39:32,  1.33s/it]2017-06-02 14:54:32,060 root  INFO     step 1690.000000 - time: 1.334676, loss: 0.101750, perplexity: 1.107107, precision: 0.875000, batch_len: 141.000000
Train, loss=0.10175028: 1691it [39:34,  1.36s/it]2017-06-02 14:54:33,294 root  INFO     step 1691.000000 - time: 1.077573, loss: 0.029321, perplexity: 1.029755, precision: 0.890625, batch_len: 96.000000
Train, loss=0.02932058: 1692it [39:35,  1.32s/it]2017-06-02 14:54:35,096 root  INFO     step 1692.000000 - time: 1.633753, loss: 0.052994, perplexity: 1.054423, precision: 0.906250, batch_len: 138.000000
Train, loss=0.05299373: 1693it [39:37,  1.47s/it]2017-06-02 14:54:36,131 root  INFO     step 1693.000000 - time: 0.910059, loss: 0.013476, perplexity: 1.013567, precision: 0.984375, batch_len: 74.000000
Train, loss=0.01347606: 1694it [39:38,  1.34s/it]2017-06-02 14:54:37,545 root  INFO     step 1694.000000 - time: 1.385456, loss: 0.033972, perplexity: 1.034556, precision: 0.906250, batch_len: 132.000000
Train, loss=0.03397197: 1695it [39:39,  1.36s/it]2017-06-02 14:54:38,727 root  INFO     step 1695.000000 - time: 1.031842, loss: 0.058187, perplexity: 1.059913, precision: 0.890625, batch_len: 72.000000
Train, loss=0.05818650: 1696it [39:40,  1.31s/it]2017-06-02 14:54:40,510 root  INFO     step 1696.000000 - time: 1.722803, loss: 0.058688, perplexity: 1.060445, precision: 0.890625, batch_len: 139.000000
Train, loss=0.05868822: 1697it [39:42,  1.45s/it]2017-06-02 14:54:42,084 root  INFO     step 1697.000000 - time: 1.536166, loss: 0.026291, perplexity: 1.026640, precision: 0.937500, batch_len: 134.000000
Train, loss=0.02629111: 1698it [39:44,  1.49s/it]2017-06-02 14:54:43,198 root  INFO     step 1698.000000 - time: 0.997132, loss: 0.054700, perplexity: 1.056224, precision: 0.921875, batch_len: 76.000000
Train, loss=0.05469988: 1699it [39:45,  1.37s/it]2017-06-02 14:54:44,273 root  INFO     step 1699.000000 - time: 1.003168, loss: 0.036300, perplexity: 1.036967, precision: 0.953125, batch_len: 77.000000
Train, loss=0.03630032: 1700it [39:46,  1.29s/it]2017-06-02 14:54:45,570 root  INFO     step 1700.000000 - time: 1.287995, loss: 0.029766, perplexity: 1.030214, precision: 0.890625, batch_len: 142.000000
Train, loss=0.02976612: 1701it [39:47,  1.29s/it]2017-06-02 14:54:47,712 root  INFO     step 1701.000000 - time: 2.114480, loss: 0.014114, perplexity: 1.014214, precision: 0.968750, batch_len: 150.000000
Train, loss=0.01411385: 1702it [39:49,  1.54s/it]2017-06-02 14:54:48,772 root  INFO     step 1702.000000 - time: 1.021199, loss: 0.014022, perplexity: 1.014120, precision: 1.000000, batch_len: 71.000000
Train, loss=0.01402172: 1703it [39:50,  1.40s/it]2017-06-02 14:54:50,184 root  INFO     step 1703.000000 - time: 1.358222, loss: 0.017065, perplexity: 1.017211, precision: 0.968750, batch_len: 131.000000
Train, loss=0.01706456: 1704it [39:52,  1.40s/it]2017-06-02 14:54:50,396 root  INFO     Generating first batch)
2017-06-02 14:54:54,221 root  INFO     step 1704.000000 - time: 1.008689, loss: 0.103188, perplexity: 1.108699, precision: 0.875000, batch_len: 93.000000
Train, loss=0.10318752: 1705it [39:56,  2.19s/it]2017-06-02 14:54:55,674 root  INFO     step 1705.000000 - time: 1.018715, loss: 0.019892, perplexity: 1.020092, precision: 0.937500, batch_len: 96.000000
Train, loss=0.01989247: 1706it [39:57,  1.97s/it]2017-06-02 14:54:56,699 root  INFO     step 1706.000000 - time: 0.896509, loss: 0.030465, perplexity: 1.030933, precision: 0.937500, batch_len: 92.000000
Train, loss=0.03046452: 1707it [39:58,  1.69s/it]2017-06-02 14:54:57,960 root  INFO     step 1707.000000 - time: 1.192172, loss: 0.055390, perplexity: 1.056953, precision: 0.937500, batch_len: 100.000000
Train, loss=0.05539035: 1708it [39:59,  1.56s/it]2017-06-02 14:54:59,378 root  INFO     step 1708.000000 - time: 1.405411, loss: 0.045891, perplexity: 1.046961, precision: 0.859375, batch_len: 110.000000
Train, loss=0.04589124: 1709it [40:01,  1.52s/it]2017-06-02 14:55:00,832 root  INFO     step 1709.000000 - time: 1.435159, loss: 0.014340, perplexity: 1.014444, precision: 0.953125, batch_len: 128.000000
Train, loss=0.01434029: 1710it [40:02,  1.50s/it]2017-06-02 14:55:01,970 root  INFO     step 1710.000000 - time: 1.093967, loss: 0.039423, perplexity: 1.040210, precision: 0.921875, batch_len: 105.000000
Train, loss=0.03942299: 1711it [40:03,  1.39s/it]2017-06-02 14:55:03,184 root  INFO     step 1711.000000 - time: 1.110154, loss: 0.045970, perplexity: 1.047043, precision: 0.921875, batch_len: 113.000000
Train, loss=0.04597009: 1712it [40:05,  1.34s/it]2017-06-02 14:55:04,666 root  INFO     step 1712.000000 - time: 1.436187, loss: 0.023802, perplexity: 1.024087, precision: 0.937500, batch_len: 117.000000
Train, loss=0.02380190: 1713it [40:06,  1.38s/it]2017-06-02 14:55:06,102 root  INFO     step 1713.000000 - time: 1.368610, loss: 0.021606, perplexity: 1.021841, precision: 0.953125, batch_len: 106.000000
Train, loss=0.02160552: 1714it [40:08,  1.40s/it]2017-06-02 14:55:07,099 root  INFO     step 1714.000000 - time: 0.957689, loss: 0.124954, perplexity: 1.133096, precision: 0.906250, batch_len: 88.000000
Train, loss=0.12495391: 1715it [40:09,  1.28s/it]2017-06-02 14:55:08,141 root  INFO     step 1715.000000 - time: 0.948686, loss: 0.147066, perplexity: 1.158430, precision: 0.812500, batch_len: 91.000000
Train, loss=0.14706558: 1716it [40:10,  1.21s/it]2017-06-02 14:55:09,130 root  INFO     step 1716.000000 - time: 0.969968, loss: 0.103119, perplexity: 1.108623, precision: 0.812500, batch_len: 102.000000
Train, loss=0.10311864: 1717it [40:11,  1.14s/it]2017-06-02 14:55:10,084 root  INFO     step 1717.000000 - time: 0.874887, loss: 0.058985, perplexity: 1.060760, precision: 0.875000, batch_len: 81.000000
Train, loss=0.05898548: 1718it [40:12,  1.09s/it]2017-06-02 14:55:11,155 root  INFO     step 1718.000000 - time: 1.010722, loss: 0.017348, perplexity: 1.017499, precision: 0.968750, batch_len: 103.000000
Train, loss=0.01734797: 1719it [40:13,  1.08s/it]2017-06-02 14:55:12,435 root  INFO     step 1719.000000 - time: 1.223633, loss: 0.031915, perplexity: 1.032429, precision: 0.937500, batch_len: 101.000000
Train, loss=0.03191469: 1720it [40:14,  1.14s/it]2017-06-02 14:55:13,876 root  INFO     step 1720.000000 - time: 1.411835, loss: 0.029054, perplexity: 1.029481, precision: 0.937500, batch_len: 112.000000
Train, loss=0.02905441: 1721it [40:15,  1.23s/it]2017-06-02 14:55:15,006 root  INFO     step 1721.000000 - time: 1.116715, loss: 0.033422, perplexity: 1.033987, precision: 0.953125, batch_len: 108.000000
Train, loss=0.03342232: 1722it [40:16,  1.20s/it]2017-06-02 14:55:16,049 root  INFO     step 1722.000000 - time: 0.982678, loss: 0.030716, perplexity: 1.031193, precision: 0.953125, batch_len: 90.000000
Train, loss=0.03071617: 1723it [40:18,  1.15s/it]2017-06-02 14:55:17,038 root  INFO     step 1723.000000 - time: 0.981135, loss: 0.029133, perplexity: 1.029562, precision: 0.968750, batch_len: 107.000000
Train, loss=0.02913309: 1724it [40:19,  1.10s/it]2017-06-02 14:55:18,162 root  INFO     step 1724.000000 - time: 1.118793, loss: 0.014944, perplexity: 1.015056, precision: 0.968750, batch_len: 104.000000
Train, loss=0.01494375: 1725it [40:20,  1.11s/it]2017-06-02 14:55:19,529 root  INFO     step 1725.000000 - time: 1.297612, loss: 0.040399, perplexity: 1.041226, precision: 0.906250, batch_len: 97.000000
Train, loss=0.04039853: 1726it [40:21,  1.19s/it]2017-06-02 14:55:20,695 root  INFO     step 1726.000000 - time: 1.102956, loss: 0.040070, perplexity: 1.040883, precision: 0.968750, batch_len: 114.000000
Train, loss=0.04006952: 1727it [40:22,  1.18s/it]2017-06-02 14:55:21,716 root  INFO     step 1727.000000 - time: 1.013301, loss: 0.064484, perplexity: 1.066609, precision: 0.921875, batch_len: 94.000000
Train, loss=0.06448410: 1728it [40:23,  1.13s/it]2017-06-02 14:55:22,868 root  INFO     step 1728.000000 - time: 1.099318, loss: 0.046318, perplexity: 1.047407, precision: 0.890625, batch_len: 111.000000
Train, loss=0.04631795: 1729it [40:24,  1.14s/it]2017-06-02 14:55:24,152 root  INFO     step 1729.000000 - time: 1.222200, loss: 0.033116, perplexity: 1.033670, precision: 0.921875, batch_len: 120.000000
Train, loss=0.03311557: 1730it [40:26,  1.18s/it]2017-06-02 14:55:25,688 root  INFO     step 1730.000000 - time: 1.526367, loss: 0.035934, perplexity: 1.036588, precision: 0.875000, batch_len: 125.000000
Train, loss=0.03593447: 1731it [40:27,  1.29s/it]2017-06-02 14:55:26,851 root  INFO     step 1731.000000 - time: 1.125764, loss: 0.024549, perplexity: 1.024853, precision: 0.937500, batch_len: 109.000000
Train, loss=0.02454890: 1732it [40:28,  1.25s/it]2017-06-02 14:55:28,560 root  INFO     step 1732.000000 - time: 1.609760, loss: 0.018949, perplexity: 1.019130, precision: 0.937500, batch_len: 124.000000
Train, loss=0.01894900: 1733it [40:30,  1.39s/it]2017-06-02 14:55:29,618 root  INFO     step 1733.000000 - time: 1.000616, loss: 0.050289, perplexity: 1.051575, precision: 0.984375, batch_len: 80.000000
Train, loss=0.05028861: 1734it [40:31,  1.29s/it]2017-06-02 14:55:30,641 root  INFO     step 1734.000000 - time: 1.003539, loss: 0.030320, perplexity: 1.030784, precision: 0.937500, batch_len: 89.000000
Train, loss=0.03031969: 1735it [40:32,  1.21s/it]2017-06-02 14:55:32,130 root  INFO     step 1735.000000 - time: 1.416538, loss: 0.019184, perplexity: 1.019369, precision: 0.953125, batch_len: 116.000000
Train, loss=0.01918389: 1736it [40:34,  1.29s/it]2017-06-02 14:55:33,565 root  INFO     step 1736.000000 - time: 1.420623, loss: 0.015408, perplexity: 1.015527, precision: 0.921875, batch_len: 123.000000
Train, loss=0.01540790: 1737it [40:35,  1.34s/it]2017-06-02 14:55:34,932 root  INFO     step 1737.000000 - time: 1.296279, loss: 0.021837, perplexity: 1.022077, precision: 0.953125, batch_len: 85.000000
Train, loss=0.02183701: 1738it [40:36,  1.35s/it]2017-06-02 14:55:36,319 root  INFO     step 1738.000000 - time: 1.321913, loss: 0.021509, perplexity: 1.021742, precision: 0.953125, batch_len: 121.000000
Train, loss=0.02150944: 1739it [40:38,  1.36s/it]2017-06-02 14:55:37,366 root  INFO     step 1739.000000 - time: 0.968699, loss: 0.079418, perplexity: 1.082657, precision: 0.859375, batch_len: 83.000000
Train, loss=0.07941793: 1740it [40:39,  1.26s/it]2017-06-02 14:55:38,339 root  INFO     step 1740.000000 - time: 0.925141, loss: 0.050041, perplexity: 1.051315, precision: 0.921875, batch_len: 86.000000
Train, loss=0.05004131: 1741it [40:40,  1.18s/it]2017-06-02 14:55:39,464 root  INFO     step 1741.000000 - time: 1.049702, loss: 0.046848, perplexity: 1.047963, precision: 0.937500, batch_len: 115.000000
Train, loss=0.04684839: 1742it [40:41,  1.16s/it]2017-06-02 14:55:40,797 root  INFO     step 1742.000000 - time: 1.259167, loss: 0.031378, perplexity: 1.031876, precision: 0.937500, batch_len: 98.000000
Train, loss=0.03137812: 1743it [40:42,  1.21s/it]2017-06-02 14:55:42,469 root  INFO     step 1743.000000 - time: 1.490226, loss: 0.031802, perplexity: 1.032313, precision: 0.937500, batch_len: 119.000000
Train, loss=0.03180218: 1744it [40:44,  1.35s/it]2017-06-02 14:55:43,864 root  INFO     step 1744.000000 - time: 1.336622, loss: 0.014578, perplexity: 1.014684, precision: 0.984375, batch_len: 118.000000
Train, loss=0.01457770: 1745it [40:45,  1.36s/it]2017-06-02 14:55:45,353 root  INFO     step 1745.000000 - time: 1.335902, loss: 0.013733, perplexity: 1.013828, precision: 0.984375, batch_len: 129.000000
Train, loss=0.01373279: 1746it [40:47,  1.40s/it]2017-06-02 14:55:46,342 root  INFO     step 1746.000000 - time: 0.940593, loss: 0.024911, perplexity: 1.025224, precision: 0.937500, batch_len: 87.000000
Train, loss=0.02491107: 1747it [40:48,  1.28s/it]2017-06-02 14:55:47,782 root  INFO     step 1747.000000 - time: 1.223310, loss: 0.011502, perplexity: 1.011568, precision: 0.968750, batch_len: 99.000000
Train, loss=0.01150202: 1748it [40:49,  1.33s/it]2017-06-02 14:55:48,831 root  INFO     step 1748.000000 - time: 0.969240, loss: 0.028406, perplexity: 1.028814, precision: 0.921875, batch_len: 79.000000
Train, loss=0.02840630: 1749it [40:50,  1.24s/it]2017-06-02 14:55:50,342 root  INFO     step 1749.000000 - time: 1.398686, loss: 0.007577, perplexity: 1.007606, precision: 1.000000, batch_len: 133.000000
Train, loss=0.00757713: 1750it [40:52,  1.32s/it]2017-06-02 14:55:51,742 root  INFO     step 1750.000000 - time: 1.343218, loss: 0.029306, perplexity: 1.029740, precision: 0.921875, batch_len: 137.000000
Train, loss=0.02930641: 1751it [40:53,  1.35s/it]2017-06-02 14:55:52,739 root  INFO     step 1751.000000 - time: 0.892374, loss: 0.030482, perplexity: 1.030951, precision: 0.968750, batch_len: 82.000000
Train, loss=0.03048163: 1752it [40:54,  1.24s/it]2017-06-02 14:55:54,051 root  INFO     step 1752.000000 - time: 1.194657, loss: 0.073953, perplexity: 1.076756, precision: 0.843750, batch_len: 84.000000
Train, loss=0.07395276: 1753it [40:56,  1.26s/it]2017-06-02 14:55:55,646 root  INFO     step 1753.000000 - time: 1.480362, loss: 0.022005, perplexity: 1.022249, precision: 0.937500, batch_len: 136.000000
Train, loss=0.02200522: 1754it [40:57,  1.36s/it]2017-06-02 14:55:57,165 root  INFO     step 1754.000000 - time: 1.453845, loss: 0.036987, perplexity: 1.037679, precision: 0.953125, batch_len: 138.000000
Train, loss=0.03698654: 1755it [40:59,  1.41s/it]2017-06-02 14:55:58,162 root  INFO     step 1755.000000 - time: 0.976035, loss: 0.064770, perplexity: 1.066914, precision: 0.843750, batch_len: 96.000000
Train, loss=0.06477046: 1756it [41:00,  1.29s/it]2017-06-02 14:55:59,527 root  INFO     step 1756.000000 - time: 1.361369, loss: 0.103593, perplexity: 1.109149, precision: 0.828125, batch_len: 144.000000
Train, loss=0.10359329: 1757it [41:01,  1.31s/it]2017-06-02 14:56:01,321 root  INFO     step 1757.000000 - time: 1.753572, loss: 0.252264, perplexity: 1.286936, precision: 0.687500, batch_len: 135.000000
Train, loss=0.25226381: 1758it [41:03,  1.45s/it]2017-06-02 14:56:02,832 root  INFO     step 1758.000000 - time: 1.447804, loss: 0.805465, perplexity: 2.237737, precision: 0.468750, batch_len: 130.000000
Train, loss=0.80546498: 1759it [41:04,  1.47s/it]2017-06-02 14:56:04,130 root  INFO     step 1759.000000 - time: 1.293333, loss: 0.285257, perplexity: 1.330104, precision: 0.515625, batch_len: 126.000000
Train, loss=0.28525686: 1760it [41:06,  1.42s/it]2017-06-02 14:56:05,490 root  INFO     step 1760.000000 - time: 1.340220, loss: 0.150401, perplexity: 1.162300, precision: 0.718750, batch_len: 122.000000
Train, loss=0.15040052: 1761it [41:07,  1.40s/it]2017-06-02 14:56:06,910 root  INFO     step 1761.000000 - time: 1.034065, loss: 0.108863, perplexity: 1.115009, precision: 0.812500, batch_len: 78.000000
Train, loss=0.10886281: 1762it [41:08,  1.41s/it]2017-06-02 14:56:08,475 root  INFO     step 1762.000000 - time: 1.517192, loss: 0.164724, perplexity: 1.179068, precision: 0.828125, batch_len: 141.000000
Train, loss=0.16472441: 1763it [41:10,  1.45s/it]2017-06-02 14:56:09,472 root  INFO     step 1763.000000 - time: 0.948690, loss: 0.054800, perplexity: 1.056329, precision: 0.890625, batch_len: 74.000000
Train, loss=0.05479977: 1764it [41:11,  1.32s/it]2017-06-02 14:56:11,504 root  INFO     step 1764.000000 - time: 1.820453, loss: 0.018307, perplexity: 1.018475, precision: 0.968750, batch_len: 152.000000
Train, loss=0.01830678: 1765it [41:13,  1.53s/it]2017-06-02 14:56:12,783 root  INFO     step 1765.000000 - time: 1.225693, loss: 0.045145, perplexity: 1.046179, precision: 0.875000, batch_len: 139.000000
Train, loss=0.04514475: 1766it [41:14,  1.46s/it]2017-06-02 14:56:14,522 root  INFO     step 1766.000000 - time: 1.659073, loss: 0.022150, perplexity: 1.022397, precision: 0.906250, batch_len: 132.000000
Train, loss=0.02215030: 1767it [41:16,  1.54s/it]2017-06-02 14:56:15,717 root  INFO     step 1767.000000 - time: 1.159776, loss: 0.029553, perplexity: 1.029994, precision: 0.968750, batch_len: 72.000000
Train, loss=0.02955284: 1768it [41:17,  1.44s/it]2017-06-02 14:56:17,570 root  INFO     step 1768.000000 - time: 1.841531, loss: 0.026516, perplexity: 1.026871, precision: 0.921875, batch_len: 150.000000
Train, loss=0.02651604: 1769it [41:19,  1.56s/it]2017-06-02 14:56:18,688 root  INFO     step 1769.000000 - time: 0.977307, loss: 0.028481, perplexity: 1.028890, precision: 0.968750, batch_len: 77.000000
Train, loss=0.02848088: 1770it [41:20,  1.43s/it]2017-06-02 14:56:19,570 root  INFO     step 1770.000000 - time: 0.865458, loss: 0.015062, perplexity: 1.015176, precision: 0.984375, batch_len: 71.000000
Train, loss=0.01506242: 1771it [41:21,  1.26s/it]2017-06-02 14:56:21,385 root  INFO     step 1771.000000 - time: 1.700735, loss: 0.022677, perplexity: 1.022936, precision: 0.921875, batch_len: 134.000000
Train, loss=0.02267651: 1772it [41:23,  1.43s/it]2017-06-02 14:56:22,379 root  INFO     step 1772.000000 - time: 0.988521, loss: 0.049712, perplexity: 1.050968, precision: 0.921875, batch_len: 76.000000
Train, loss=0.04971194: 1773it [41:24,  1.30s/it]2017-06-02 14:56:24,006 root  INFO     step 1773.000000 - time: 1.422181, loss: 0.026818, perplexity: 1.027181, precision: 0.921875, batch_len: 142.000000
Train, loss=0.02681784: 1774it [41:25,  1.40s/it]2017-06-02 14:56:25,366 root  INFO     step 1774.000000 - time: 1.309424, loss: 0.034380, perplexity: 1.034978, precision: 0.906250, batch_len: 131.000000
Train, loss=0.03438004: 1775it [41:27,  1.39s/it]2017-06-02 14:56:25,564 root  INFO     Generating first batch)
2017-06-02 14:56:29,462 root  INFO     step 1775.000000 - time: 1.099353, loss: 0.027049, perplexity: 1.027418, precision: 0.921875, batch_len: 110.000000
Train, loss=0.02704922: 1776it [41:31,  2.20s/it]2017-06-02 14:56:30,502 root  INFO     step 1776.000000 - time: 1.001020, loss: 0.018298, perplexity: 1.018466, precision: 0.968750, batch_len: 96.000000
Train, loss=0.01829751: 1777it [41:32,  1.85s/it]2017-06-02 14:56:31,851 root  INFO     step 1777.000000 - time: 1.089822, loss: 0.022039, perplexity: 1.022284, precision: 0.968750, batch_len: 101.000000
Train, loss=0.02203918: 1778it [41:33,  1.70s/it]2017-06-02 14:56:32,889 root  INFO     step 1778.000000 - time: 0.909016, loss: 0.045848, perplexity: 1.046915, precision: 0.906250, batch_len: 93.000000
Train, loss=0.04584805: 1779it [41:34,  1.50s/it]2017-06-02 14:56:34,469 root  INFO     step 1779.000000 - time: 1.568798, loss: 0.030471, perplexity: 1.030940, precision: 0.953125, batch_len: 120.000000
Train, loss=0.03047100: 1780it [41:36,  1.53s/it]2017-06-02 14:56:35,985 root  INFO     step 1780.000000 - time: 1.417094, loss: 0.045176, perplexity: 1.046212, precision: 0.953125, batch_len: 128.000000
Train, loss=0.04517645: 1781it [41:37,  1.52s/it]2017-06-02 14:56:37,093 root  INFO     step 1781.000000 - time: 0.977450, loss: 0.041192, perplexity: 1.042053, precision: 0.921875, batch_len: 88.000000
Train, loss=0.04119233: 1782it [41:39,  1.40s/it]2017-06-02 14:56:38,229 root  INFO     step 1782.000000 - time: 1.056678, loss: 0.050346, perplexity: 1.051635, precision: 0.828125, batch_len: 105.000000
Train, loss=0.05034621: 1783it [41:40,  1.32s/it]2017-06-02 14:56:39,280 root  INFO     step 1783.000000 - time: 1.029362, loss: 0.015163, perplexity: 1.015278, precision: 0.968750, batch_len: 113.000000
Train, loss=0.01516265: 1784it [41:41,  1.24s/it]2017-06-02 14:56:40,822 root  INFO     step 1784.000000 - time: 1.327416, loss: 0.034008, perplexity: 1.034593, precision: 0.875000, batch_len: 111.000000
Train, loss=0.03400809: 1785it [41:42,  1.33s/it]2017-06-02 14:56:42,617 root  INFO     step 1785.000000 - time: 1.738684, loss: 0.066006, perplexity: 1.068234, precision: 0.875000, batch_len: 90.000000
Train, loss=0.06600645: 1786it [41:44,  1.47s/it]2017-06-02 14:56:43,567 root  INFO     step 1786.000000 - time: 0.927104, loss: 0.056674, perplexity: 1.058310, precision: 0.921875, batch_len: 81.000000
Train, loss=0.05667369: 1787it [41:45,  1.31s/it]2017-06-02 14:56:44,625 root  INFO     step 1787.000000 - time: 1.054685, loss: 0.060865, perplexity: 1.062755, precision: 0.937500, batch_len: 100.000000
Train, loss=0.06086492: 1788it [41:46,  1.24s/it]2017-06-02 14:56:45,655 root  INFO     step 1788.000000 - time: 0.979530, loss: 0.024898, perplexity: 1.025211, precision: 0.968750, batch_len: 106.000000
Train, loss=0.02489844: 1789it [41:47,  1.17s/it]2017-06-02 14:56:46,872 root  INFO     step 1789.000000 - time: 1.210311, loss: 0.021102, perplexity: 1.021326, precision: 0.953125, batch_len: 108.000000
Train, loss=0.02110166: 1790it [41:48,  1.19s/it]2017-06-02 14:56:48,250 root  INFO     step 1790.000000 - time: 1.286284, loss: 0.033337, perplexity: 1.033898, precision: 0.953125, batch_len: 112.000000
Train, loss=0.03333655: 1791it [41:50,  1.24s/it]2017-06-02 14:56:49,579 root  INFO     step 1791.000000 - time: 1.121260, loss: 0.023731, perplexity: 1.024014, precision: 0.968750, batch_len: 115.000000
Train, loss=0.02373056: 1792it [41:51,  1.27s/it]2017-06-02 14:56:50,766 root  INFO     step 1792.000000 - time: 1.138121, loss: 0.022146, perplexity: 1.022393, precision: 0.937500, batch_len: 109.000000
Train, loss=0.02214596: 1793it [41:52,  1.24s/it]2017-06-02 14:56:51,867 root  INFO     step 1793.000000 - time: 1.073065, loss: 0.019026, perplexity: 1.019208, precision: 0.953125, batch_len: 104.000000
Train, loss=0.01902592: 1794it [41:53,  1.20s/it]2017-06-02 14:56:53,177 root  INFO     step 1794.000000 - time: 1.263387, loss: 0.018220, perplexity: 1.018387, precision: 0.968750, batch_len: 125.000000
Train, loss=0.01822043: 1795it [41:55,  1.23s/it]2017-06-02 14:56:54,833 root  INFO     step 1795.000000 - time: 1.630847, loss: 0.007574, perplexity: 1.007603, precision: 0.984375, batch_len: 116.000000
Train, loss=0.00757436: 1796it [41:56,  1.36s/it]2017-06-02 14:56:55,924 root  INFO     step 1796.000000 - time: 1.039032, loss: 0.021544, perplexity: 1.021778, precision: 0.984375, batch_len: 85.000000
Train, loss=0.02154407: 1797it [41:57,  1.28s/it]2017-06-02 14:56:56,913 root  INFO     step 1797.000000 - time: 0.936623, loss: 0.033811, perplexity: 1.034389, precision: 0.906250, batch_len: 86.000000
Train, loss=0.03381089: 1798it [41:58,  1.19s/it]2017-06-02 14:56:57,999 root  INFO     step 1798.000000 - time: 1.017325, loss: 0.038337, perplexity: 1.039081, precision: 0.921875, batch_len: 97.000000
Train, loss=0.03833689: 1799it [41:59,  1.16s/it]2017-06-02 14:56:58,944 root  INFO     step 1799.000000 - time: 0.927395, loss: 0.026098, perplexity: 1.026441, precision: 0.953125, batch_len: 102.000000
Train, loss=0.02609786: 1800it [42:00,  1.10s/it]2017-06-02 14:57:00,239 root  INFO     step 1800.000000 - time: 1.133114, loss: 0.026316, perplexity: 1.026666, precision: 0.953125, batch_len: 103.000000
Train, loss=0.02631649: 1801it [42:02,  1.16s/it]2017-06-02 14:57:01,810 root  INFO     step 1801.000000 - time: 1.523891, loss: 0.009493, perplexity: 1.009538, precision: 1.000000, batch_len: 117.000000
Train, loss=0.00949250: 1802it [42:03,  1.28s/it]2017-06-02 14:57:02,968 root  INFO     step 1802.000000 - time: 1.080962, loss: 0.010968, perplexity: 1.011028, precision: 0.984375, batch_len: 98.000000
Train, loss=0.01096787: 1803it [42:04,  1.24s/it]2017-06-02 14:57:04,040 root  INFO     step 1803.000000 - time: 0.954957, loss: 0.015975, perplexity: 1.016103, precision: 0.953125, batch_len: 87.000000
Train, loss=0.01597453: 1804it [42:06,  1.19s/it]2017-06-02 14:57:05,394 root  INFO     step 1804.000000 - time: 1.342146, loss: 0.009046, perplexity: 1.009087, precision: 0.984375, batch_len: 123.000000
Train, loss=0.00904568: 1805it [42:07,  1.24s/it]2017-06-02 14:57:06,361 root  INFO     step 1805.000000 - time: 0.916871, loss: 0.039183, perplexity: 1.039961, precision: 0.937500, batch_len: 80.000000
Train, loss=0.03918291: 1806it [42:08,  1.16s/it]2017-06-02 14:57:07,569 root  INFO     step 1806.000000 - time: 1.145432, loss: 0.030683, perplexity: 1.031159, precision: 0.921875, batch_len: 92.000000
Train, loss=0.03068297: 1807it [42:09,  1.17s/it]2017-06-02 14:57:09,113 root  INFO     step 1807.000000 - time: 1.510411, loss: 0.019927, perplexity: 1.020127, precision: 0.968750, batch_len: 121.000000
Train, loss=0.01992668: 1808it [42:11,  1.28s/it]2017-06-02 14:57:10,099 root  INFO     step 1808.000000 - time: 0.968004, loss: 0.022308, perplexity: 1.022559, precision: 0.921875, batch_len: 89.000000
Train, loss=0.02230837: 1809it [42:12,  1.20s/it]2017-06-02 14:57:11,330 root  INFO     step 1809.000000 - time: 1.116729, loss: 0.046624, perplexity: 1.047728, precision: 0.906250, batch_len: 114.000000
Train, loss=0.04662392: 1810it [42:13,  1.21s/it]2017-06-02 14:57:12,752 root  INFO     step 1810.000000 - time: 1.298275, loss: 0.027048, perplexity: 1.027417, precision: 0.937500, batch_len: 129.000000
Train, loss=0.02704785: 1811it [42:14,  1.27s/it]2017-06-02 14:57:13,852 root  INFO     step 1811.000000 - time: 1.089397, loss: 0.082176, perplexity: 1.085647, precision: 0.875000, batch_len: 83.000000
Train, loss=0.08217578: 1812it [42:15,  1.22s/it]2017-06-02 14:57:15,339 root  INFO     step 1812.000000 - time: 1.464838, loss: 0.037515, perplexity: 1.038228, precision: 0.953125, batch_len: 124.000000
Train, loss=0.03751540: 1813it [42:17,  1.30s/it]2017-06-02 14:57:16,299 root  INFO     step 1813.000000 - time: 0.941281, loss: 0.075983, perplexity: 1.078945, precision: 0.921875, batch_len: 79.000000
Train, loss=0.07598330: 1814it [42:18,  1.20s/it]2017-06-02 14:57:17,285 root  INFO     step 1814.000000 - time: 0.977934, loss: 0.070544, perplexity: 1.073092, precision: 0.890625, batch_len: 94.000000
Train, loss=0.07054420: 1815it [42:19,  1.13s/it]2017-06-02 14:57:18,446 root  INFO     step 1815.000000 - time: 1.047687, loss: 0.080733, perplexity: 1.084082, precision: 0.875000, batch_len: 107.000000
Train, loss=0.08073317: 1816it [42:20,  1.14s/it]2017-06-02 14:57:19,589 root  INFO     step 1816.000000 - time: 1.084080, loss: 0.069949, perplexity: 1.072453, precision: 0.875000, batch_len: 91.000000
Train, loss=0.06994865: 1817it [42:21,  1.14s/it]2017-06-02 14:57:21,543 root  INFO     step 1817.000000 - time: 1.738025, loss: 0.134882, perplexity: 1.144401, precision: 0.875000, batch_len: 144.000000
Train, loss=0.13488153: 1818it [42:23,  1.39s/it]2017-06-02 14:57:22,884 root  INFO     step 1818.000000 - time: 1.277411, loss: 0.116451, perplexity: 1.123502, precision: 0.796875, batch_len: 126.000000
Train, loss=0.11645097: 1819it [42:24,  1.37s/it]2017-06-02 14:57:24,010 root  INFO     step 1819.000000 - time: 1.017588, loss: 0.105267, perplexity: 1.111007, precision: 0.781250, batch_len: 84.000000
Train, loss=0.10526696: 1820it [42:25,  1.30s/it]2017-06-02 14:57:25,390 root  INFO     step 1820.000000 - time: 1.329698, loss: 0.049234, perplexity: 1.050466, precision: 0.890625, batch_len: 119.000000
Train, loss=0.04923432: 1821it [42:27,  1.32s/it]2017-06-02 14:57:27,164 root  INFO     step 1821.000000 - time: 1.602986, loss: 0.044117, perplexity: 1.045105, precision: 0.921875, batch_len: 133.000000
Train, loss=0.04411726: 1822it [42:29,  1.46s/it]2017-06-02 14:57:28,375 root  INFO     step 1822.000000 - time: 1.075902, loss: 0.019824, perplexity: 1.020021, precision: 0.968750, batch_len: 99.000000
Train, loss=0.01982362: 1823it [42:30,  1.38s/it]2017-06-02 14:57:29,533 root  INFO     step 1823.000000 - time: 1.123586, loss: 0.043939, perplexity: 1.044918, precision: 0.921875, batch_len: 82.000000
Train, loss=0.04393884: 1824it [42:31,  1.32s/it]2017-06-02 14:57:30,988 root  INFO     step 1824.000000 - time: 1.441654, loss: 0.038407, perplexity: 1.039154, precision: 0.937500, batch_len: 137.000000
Train, loss=0.03840733: 1825it [42:32,  1.36s/it]2017-06-02 14:57:32,495 root  INFO     step 1825.000000 - time: 1.465896, loss: 0.050258, perplexity: 1.051542, precision: 0.843750, batch_len: 130.000000
Train, loss=0.05025753: 1826it [42:34,  1.40s/it]2017-06-02 14:57:33,937 root  INFO     step 1826.000000 - time: 1.358948, loss: 0.108082, perplexity: 1.114139, precision: 0.921875, batch_len: 135.000000
Train, loss=0.10808165: 1827it [42:35,  1.41s/it]2017-06-02 14:57:35,390 root  INFO     step 1827.000000 - time: 1.414034, loss: 0.015174, perplexity: 1.015289, precision: 0.937500, batch_len: 118.000000
Train, loss=0.01517360: 1828it [42:37,  1.43s/it]2017-06-02 14:57:37,144 root  INFO     step 1828.000000 - time: 1.522349, loss: 0.029437, perplexity: 1.029874, precision: 0.953125, batch_len: 122.000000
Train, loss=0.02943671: 1829it [42:39,  1.52s/it]2017-06-02 14:57:38,321 root  INFO     step 1829.000000 - time: 0.972040, loss: 0.021141, perplexity: 1.021366, precision: 0.937500, batch_len: 96.000000
Train, loss=0.02114074: 1830it [42:40,  1.42s/it]2017-06-02 14:57:39,273 root  INFO     step 1830.000000 - time: 0.919259, loss: 0.030452, perplexity: 1.030920, precision: 0.937500, batch_len: 78.000000
Train, loss=0.03045189: 1831it [42:41,  1.28s/it]2017-06-02 14:57:40,587 root  INFO     step 1831.000000 - time: 1.241568, loss: 0.008220, perplexity: 1.008254, precision: 1.000000, batch_len: 136.000000
Train, loss=0.00822041: 1832it [42:42,  1.29s/it]2017-06-02 14:57:42,243 root  INFO     step 1832.000000 - time: 1.544295, loss: 0.015062, perplexity: 1.015176, precision: 0.968750, batch_len: 138.000000
Train, loss=0.01506162: 1833it [42:44,  1.40s/it]2017-06-02 14:57:44,001 root  INFO     step 1833.000000 - time: 1.654578, loss: 0.026462, perplexity: 1.026816, precision: 0.968750, batch_len: 139.000000
Train, loss=0.02646237: 1834it [42:45,  1.51s/it]2017-06-02 14:57:45,405 root  INFO     step 1834.000000 - time: 1.364299, loss: 0.042054, perplexity: 1.042951, precision: 0.937500, batch_len: 141.000000
Train, loss=0.04205424: 1835it [42:47,  1.48s/it]2017-06-02 14:57:46,492 root  INFO     step 1835.000000 - time: 0.953940, loss: 0.015049, perplexity: 1.015162, precision: 0.984375, batch_len: 77.000000
Train, loss=0.01504852: 1836it [42:48,  1.36s/it]2017-06-02 14:57:48,320 root  INFO     step 1836.000000 - time: 1.806451, loss: 0.008312, perplexity: 1.008347, precision: 0.968750, batch_len: 152.000000
Train, loss=0.00831242: 1837it [42:50,  1.50s/it]2017-06-02 14:57:50,024 root  INFO     step 1837.000000 - time: 1.675825, loss: 0.036497, perplexity: 1.037171, precision: 0.968750, batch_len: 132.000000
Train, loss=0.03649705: 1838it [42:51,  1.56s/it]2017-06-02 14:57:51,586 root  INFO     step 1838.000000 - time: 1.408363, loss: 0.006684, perplexity: 1.006707, precision: 0.984375, batch_len: 134.000000
Train, loss=0.00668415: 1839it [42:53,  1.56s/it]2017-06-02 14:57:52,651 root  INFO     step 1839.000000 - time: 0.947043, loss: 0.016577, perplexity: 1.016715, precision: 0.937500, batch_len: 74.000000
Train, loss=0.01657708: 1840it [42:54,  1.41s/it]2017-06-02 14:57:53,778 root  INFO     step 1840.000000 - time: 1.081536, loss: 0.012839, perplexity: 1.012922, precision: 0.984375, batch_len: 72.000000
Train, loss=0.01283900: 1841it [42:55,  1.33s/it]2017-06-02 14:57:54,763 root  INFO     step 1841.000000 - time: 0.954182, loss: 0.027021, perplexity: 1.027389, precision: 0.937500, batch_len: 76.000000
Train, loss=0.02702068: 1842it [42:56,  1.22s/it]2017-06-02 14:57:56,966 root  INFO     step 1842.000000 - time: 2.171597, loss: 0.017115, perplexity: 1.017263, precision: 0.968750, batch_len: 150.000000
Train, loss=0.01711543: 1843it [42:58,  1.52s/it]2017-06-02 14:57:58,538 root  INFO     step 1843.000000 - time: 1.446476, loss: 0.016367, perplexity: 1.016502, precision: 0.968750, batch_len: 142.000000
Train, loss=0.01636685: 1844it [43:00,  1.53s/it]2017-06-02 14:57:59,898 root  INFO     step 1844.000000 - time: 1.350431, loss: 0.007323, perplexity: 1.007350, precision: 0.984375, batch_len: 131.000000
Train, loss=0.00732345: 1845it [43:01,  1.48s/it]2017-06-02 14:58:00,922 root  INFO     step 1845.000000 - time: 0.806381, loss: 0.007337, perplexity: 1.007364, precision: 1.000000, batch_len: 71.000000
Train, loss=0.00733653: 1846it [43:02,  1.34s/it]2017-06-02 14:58:00,978 root  INFO     Generating first batch)
2017-06-02 14:58:04,772 root  INFO     step 1846.000000 - time: 1.001522, loss: 0.020825, perplexity: 1.021043, precision: 0.937500, batch_len: 96.000000
Train, loss=0.02082508: 1847it [43:06,  2.10s/it]2017-06-02 14:58:06,356 root  INFO     step 1847.000000 - time: 1.467417, loss: 0.082735, perplexity: 1.086254, precision: 0.906250, batch_len: 90.000000
Train, loss=0.08273499: 1848it [43:08,  1.94s/it]2017-06-02 14:58:07,565 root  INFO     step 1848.000000 - time: 1.160976, loss: 0.073187, perplexity: 1.075932, precision: 0.890625, batch_len: 93.000000
Train, loss=0.07318701: 1849it [43:09,  1.72s/it]2017-06-02 14:58:08,846 root  INFO     step 1849.000000 - time: 1.037140, loss: 0.035918, perplexity: 1.036571, precision: 0.921875, batch_len: 92.000000
Train, loss=0.03591768: 1850it [43:10,  1.59s/it]2017-06-02 14:58:09,985 root  INFO     step 1850.000000 - time: 1.000207, loss: 0.017505, perplexity: 1.017659, precision: 0.968750, batch_len: 102.000000
Train, loss=0.01750489: 1851it [43:11,  1.45s/it]2017-06-02 14:58:11,107 root  INFO     step 1851.000000 - time: 1.084243, loss: 0.024140, perplexity: 1.024434, precision: 0.953125, batch_len: 105.000000
Train, loss=0.02413995: 1852it [43:13,  1.36s/it]2017-06-02 14:58:12,100 root  INFO     step 1852.000000 - time: 0.967446, loss: 0.022745, perplexity: 1.023006, precision: 0.968750, batch_len: 101.000000
Train, loss=0.02274550: 1853it [43:14,  1.25s/it]2017-06-02 14:58:13,302 root  INFO     step 1853.000000 - time: 1.113660, loss: 0.022702, perplexity: 1.022962, precision: 0.953125, batch_len: 108.000000
Train, loss=0.02270216: 1854it [43:15,  1.23s/it]2017-06-02 14:58:14,678 root  INFO     step 1854.000000 - time: 1.346031, loss: 0.030213, perplexity: 1.030674, precision: 0.921875, batch_len: 113.000000
Train, loss=0.03021322: 1855it [43:16,  1.28s/it]2017-06-02 14:58:15,823 root  INFO     step 1855.000000 - time: 1.074878, loss: 0.049428, perplexity: 1.050669, precision: 0.921875, batch_len: 100.000000
Train, loss=0.04942751: 1856it [43:17,  1.24s/it]2017-06-02 14:58:16,983 root  INFO     step 1856.000000 - time: 1.119064, loss: 0.038740, perplexity: 1.039500, precision: 0.953125, batch_len: 111.000000
Train, loss=0.03873961: 1857it [43:18,  1.21s/it]2017-06-02 14:58:18,300 root  INFO     step 1857.000000 - time: 1.311110, loss: 0.029719, perplexity: 1.030165, precision: 0.953125, batch_len: 117.000000
Train, loss=0.02971900: 1858it [43:20,  1.24s/it]2017-06-02 14:58:19,239 root  INFO     step 1858.000000 - time: 0.911527, loss: 0.029136, perplexity: 1.029565, precision: 0.937500, batch_len: 97.000000
Train, loss=0.02913610: 1859it [43:21,  1.15s/it]2017-06-02 14:58:20,514 root  INFO     step 1859.000000 - time: 1.205469, loss: 0.031877, perplexity: 1.032390, precision: 0.890625, batch_len: 110.000000
Train, loss=0.03187684: 1860it [43:22,  1.19s/it]2017-06-02 14:58:21,710 root  INFO     step 1860.000000 - time: 1.172166, loss: 0.066211, perplexity: 1.068452, precision: 0.890625, batch_len: 88.000000
Train, loss=0.06621079: 1861it [43:23,  1.19s/it]2017-06-02 14:58:22,719 root  INFO     step 1861.000000 - time: 1.001424, loss: 0.018408, perplexity: 1.018579, precision: 0.937500, batch_len: 103.000000
Train, loss=0.01840849: 1862it [43:24,  1.14s/it]2017-06-02 14:58:24,205 root  INFO     step 1862.000000 - time: 1.401049, loss: 0.032314, perplexity: 1.032842, precision: 0.937500, batch_len: 128.000000
Train, loss=0.03231417: 1863it [43:26,  1.24s/it]2017-06-02 14:58:25,589 root  INFO     step 1863.000000 - time: 1.335263, loss: 0.019195, perplexity: 1.019381, precision: 0.968750, batch_len: 120.000000
Train, loss=0.01919530: 1864it [43:27,  1.28s/it]2017-06-02 14:58:27,072 root  INFO     step 1864.000000 - time: 1.389064, loss: 0.016046, perplexity: 1.016175, precision: 0.953125, batch_len: 125.000000
Train, loss=0.01604570: 1865it [43:29,  1.34s/it]2017-06-02 14:58:27,993 root  INFO     step 1865.000000 - time: 0.846475, loss: 0.037717, perplexity: 1.038437, precision: 0.890625, batch_len: 80.000000
Train, loss=0.03771660: 1866it [43:29,  1.22s/it]2017-06-02 14:58:29,311 root  INFO     step 1866.000000 - time: 1.303213, loss: 0.017101, perplexity: 1.017248, precision: 0.968750, batch_len: 104.000000
Train, loss=0.01710091: 1867it [43:31,  1.25s/it]2017-06-02 14:58:30,908 root  INFO     step 1867.000000 - time: 1.511952, loss: 0.023252, perplexity: 1.023524, precision: 0.937500, batch_len: 121.000000
Train, loss=0.02325160: 1868it [43:32,  1.35s/it]2017-06-02 14:58:31,976 root  INFO     step 1868.000000 - time: 1.051790, loss: 0.051731, perplexity: 1.053093, precision: 0.921875, batch_len: 85.000000
Train, loss=0.05173147: 1869it [43:33,  1.27s/it]2017-06-02 14:58:33,071 root  INFO     step 1869.000000 - time: 1.072886, loss: 0.043236, perplexity: 1.044184, precision: 0.906250, batch_len: 109.000000
Train, loss=0.04323556: 1870it [43:35,  1.22s/it]2017-06-02 14:58:34,198 root  INFO     step 1870.000000 - time: 1.093316, loss: 0.011000, perplexity: 1.011060, precision: 0.953125, batch_len: 112.000000
Train, loss=0.01099961: 1871it [43:36,  1.19s/it]2017-06-02 14:58:35,128 root  INFO     step 1871.000000 - time: 0.913653, loss: 0.053906, perplexity: 1.055385, precision: 0.859375, batch_len: 89.000000
Train, loss=0.05390608: 1872it [43:37,  1.11s/it]2017-06-02 14:58:36,714 root  INFO     step 1872.000000 - time: 1.437632, loss: 0.035743, perplexity: 1.036389, precision: 0.937500, batch_len: 91.000000
Train, loss=0.03574283: 1873it [43:38,  1.25s/it]2017-06-02 14:58:37,878 root  INFO     step 1873.000000 - time: 1.140378, loss: 0.037753, perplexity: 1.038474, precision: 0.906250, batch_len: 115.000000
Train, loss=0.03775267: 1874it [43:39,  1.23s/it]2017-06-02 14:58:39,131 root  INFO     step 1874.000000 - time: 1.093562, loss: 0.079442, perplexity: 1.082683, precision: 0.828125, batch_len: 107.000000
Train, loss=0.07944194: 1875it [43:41,  1.23s/it]2017-06-02 14:58:40,180 root  INFO     step 1875.000000 - time: 0.934023, loss: 0.090742, perplexity: 1.094987, precision: 0.906250, batch_len: 81.000000
Train, loss=0.09074225: 1876it [43:42,  1.18s/it]2017-06-02 14:58:41,493 root  INFO     step 1876.000000 - time: 1.269634, loss: 0.024751, perplexity: 1.025059, precision: 0.953125, batch_len: 116.000000
Train, loss=0.02475065: 1877it [43:43,  1.22s/it]2017-06-02 14:58:42,847 root  INFO     step 1877.000000 - time: 1.276552, loss: 0.046614, perplexity: 1.047718, precision: 0.921875, batch_len: 114.000000
Train, loss=0.04661448: 1878it [43:44,  1.26s/it]2017-06-02 14:58:44,012 root  INFO     step 1878.000000 - time: 1.132069, loss: 0.068328, perplexity: 1.070716, precision: 0.890625, batch_len: 86.000000
Train, loss=0.06832762: 1879it [43:45,  1.23s/it]2017-06-02 14:58:45,208 root  INFO     step 1879.000000 - time: 1.080707, loss: 0.062405, perplexity: 1.064394, precision: 0.890625, batch_len: 106.000000
Train, loss=0.06240539: 1880it [43:47,  1.22s/it]2017-06-02 14:58:46,492 root  INFO     step 1880.000000 - time: 1.277644, loss: 0.023446, perplexity: 1.023723, precision: 0.937500, batch_len: 124.000000
Train, loss=0.02344583: 1881it [43:48,  1.24s/it]2017-06-02 14:58:47,447 root  INFO     step 1881.000000 - time: 0.883455, loss: 0.088716, perplexity: 1.092771, precision: 0.875000, batch_len: 79.000000
Train, loss=0.08871649: 1882it [43:49,  1.15s/it]2017-06-02 14:58:48,494 root  INFO     step 1882.000000 - time: 0.959093, loss: 0.046781, perplexity: 1.047892, precision: 0.921875, batch_len: 87.000000
Train, loss=0.04678062: 1883it [43:50,  1.12s/it]2017-06-02 14:58:50,202 root  INFO     step 1883.000000 - time: 1.658012, loss: 0.055260, perplexity: 1.056815, precision: 0.906250, batch_len: 119.000000
Train, loss=0.05525979: 1884it [43:52,  1.30s/it]2017-06-02 14:58:51,222 root  INFO     step 1884.000000 - time: 0.959158, loss: 0.062484, perplexity: 1.064478, precision: 0.875000, batch_len: 83.000000
Train, loss=0.06248415: 1885it [43:53,  1.21s/it]2017-06-02 14:58:52,355 root  INFO     step 1885.000000 - time: 1.096943, loss: 0.032859, perplexity: 1.033405, precision: 0.890625, batch_len: 98.000000
Train, loss=0.03285877: 1886it [43:54,  1.19s/it]2017-06-02 14:58:53,430 root  INFO     step 1886.000000 - time: 1.011593, loss: 0.047641, perplexity: 1.048794, precision: 0.906250, batch_len: 84.000000
Train, loss=0.04764139: 1887it [43:55,  1.16s/it]2017-06-02 14:58:54,798 root  INFO     step 1887.000000 - time: 1.263430, loss: 0.023558, perplexity: 1.023838, precision: 0.937500, batch_len: 123.000000
Train, loss=0.02355841: 1888it [43:56,  1.22s/it]2017-06-02 14:58:56,168 root  INFO     step 1888.000000 - time: 1.218207, loss: 0.015825, perplexity: 1.015951, precision: 0.968750, batch_len: 99.000000
Train, loss=0.01582539: 1889it [43:58,  1.26s/it]2017-06-02 14:58:57,809 root  INFO     step 1889.000000 - time: 1.592522, loss: 0.060553, perplexity: 1.062424, precision: 0.937500, batch_len: 144.000000
Train, loss=0.06055322: 1890it [43:59,  1.38s/it]2017-06-02 14:58:59,004 root  INFO     step 1890.000000 - time: 1.021629, loss: 0.068447, perplexity: 1.070844, precision: 0.859375, batch_len: 94.000000
Train, loss=0.06844699: 1891it [44:00,  1.32s/it]2017-06-02 14:59:00,379 root  INFO     step 1891.000000 - time: 1.337108, loss: 0.056158, perplexity: 1.057765, precision: 0.859375, batch_len: 137.000000
Train, loss=0.05615830: 1892it [44:02,  1.34s/it]2017-06-02 14:59:01,669 root  INFO     step 1892.000000 - time: 1.163647, loss: 0.028416, perplexity: 1.028824, precision: 0.937500, batch_len: 129.000000
Train, loss=0.02841603: 1893it [44:03,  1.32s/it]2017-06-02 14:59:02,885 root  INFO     step 1893.000000 - time: 1.163742, loss: 0.059516, perplexity: 1.061323, precision: 0.890625, batch_len: 82.000000
Train, loss=0.05951583: 1894it [44:04,  1.29s/it]2017-06-02 14:59:04,439 root  INFO     step 1894.000000 - time: 1.492251, loss: 0.008935, perplexity: 1.008975, precision: 0.984375, batch_len: 118.000000
Train, loss=0.00893527: 1895it [44:06,  1.37s/it]2017-06-02 14:59:05,776 root  INFO     step 1895.000000 - time: 1.266932, loss: 0.064666, perplexity: 1.066803, precision: 0.921875, batch_len: 126.000000
Train, loss=0.06466606: 1896it [44:07,  1.36s/it]2017-06-02 14:59:07,273 root  INFO     step 1896.000000 - time: 1.398383, loss: 0.057139, perplexity: 1.058803, precision: 0.921875, batch_len: 133.000000
Train, loss=0.05713904: 1897it [44:09,  1.40s/it]2017-06-02 14:59:08,349 root  INFO     step 1897.000000 - time: 0.951237, loss: 0.048338, perplexity: 1.049525, precision: 0.890625, batch_len: 78.000000
Train, loss=0.04833778: 1898it [44:10,  1.30s/it]2017-06-02 14:59:10,194 root  INFO     step 1898.000000 - time: 1.699638, loss: 0.036132, perplexity: 1.036793, precision: 0.937500, batch_len: 130.000000
Train, loss=0.03613241: 1899it [44:12,  1.47s/it]2017-06-02 14:59:11,569 root  INFO     step 1899.000000 - time: 1.353480, loss: 0.028937, perplexity: 1.029360, precision: 0.890625, batch_len: 122.000000
Train, loss=0.02893743: 1900it [44:13,  1.44s/it]2017-06-02 14:59:12,993 root  INFO     step 1900.000000 - time: 1.401798, loss: 0.058176, perplexity: 1.059902, precision: 0.921875, batch_len: 135.000000
Train, loss=0.05817612: 1901it [44:14,  1.43s/it]2017-06-02 14:59:14,291 root  INFO     step 1901.000000 - time: 1.151081, loss: 0.012754, perplexity: 1.012836, precision: 0.953125, batch_len: 136.000000
Train, loss=0.01275402: 1902it [44:16,  1.39s/it]2017-06-02 14:59:15,506 root  INFO     step 1902.000000 - time: 1.097005, loss: 0.016579, perplexity: 1.016717, precision: 0.953125, batch_len: 96.000000
Train, loss=0.01657904: 1903it [44:17,  1.34s/it]2017-06-02 14:59:17,274 root  INFO     step 1903.000000 - time: 1.520521, loss: 0.034264, perplexity: 1.034857, precision: 0.906250, batch_len: 141.000000
Train, loss=0.03426353: 1904it [44:19,  1.47s/it]2017-06-02 14:59:19,156 root  INFO     step 1904.000000 - time: 1.862627, loss: 0.017971, perplexity: 1.018133, precision: 0.968750, batch_len: 150.000000
Train, loss=0.01797080: 1905it [44:21,  1.59s/it]2017-06-02 14:59:20,344 root  INFO     step 1905.000000 - time: 1.002661, loss: 0.046171, perplexity: 1.047254, precision: 0.906250, batch_len: 76.000000
Train, loss=0.04617129: 1906it [44:22,  1.47s/it]2017-06-02 14:59:21,635 root  INFO     step 1906.000000 - time: 1.270604, loss: 0.011557, perplexity: 1.011624, precision: 0.984375, batch_len: 132.000000
Train, loss=0.01155680: 1907it [44:23,  1.42s/it]2017-06-02 14:59:23,423 root  INFO     step 1907.000000 - time: 1.768589, loss: 0.011748, perplexity: 1.011817, precision: 0.968750, batch_len: 134.000000
Train, loss=0.01174753: 1908it [44:25,  1.53s/it]2017-06-02 14:59:24,609 root  INFO     step 1908.000000 - time: 1.156291, loss: 0.026092, perplexity: 1.026435, precision: 0.953125, batch_len: 72.000000
Train, loss=0.02609161: 1909it [44:26,  1.43s/it]2017-06-02 14:59:26,552 root  INFO     step 1909.000000 - time: 1.877082, loss: 0.008411, perplexity: 1.008446, precision: 0.984375, batch_len: 152.000000
Train, loss=0.00841057: 1910it [44:28,  1.58s/it]2017-06-02 14:59:27,970 root  INFO     step 1910.000000 - time: 1.413924, loss: 0.010916, perplexity: 1.010976, precision: 0.968750, batch_len: 138.000000
Train, loss=0.01091610: 1911it [44:29,  1.53s/it]2017-06-02 14:59:29,152 root  INFO     step 1911.000000 - time: 1.080543, loss: 0.008620, perplexity: 1.008658, precision: 0.984375, batch_len: 74.000000
Train, loss=0.00862026: 1912it [44:31,  1.43s/it]2017-06-02 14:59:30,343 root  INFO     step 1912.000000 - time: 1.004762, loss: 0.006838, perplexity: 1.006861, precision: 1.000000, batch_len: 77.000000
Train, loss=0.00683790: 1913it [44:32,  1.36s/it]2017-06-02 14:59:32,060 root  INFO     step 1913.000000 - time: 1.691417, loss: 0.021627, perplexity: 1.021862, precision: 0.953125, batch_len: 142.000000
Train, loss=0.02162662: 1914it [44:34,  1.46s/it]2017-06-02 14:59:33,612 root  INFO     step 1914.000000 - time: 1.425780, loss: 0.014992, perplexity: 1.015105, precision: 0.968750, batch_len: 139.000000
Train, loss=0.01499234: 1915it [44:35,  1.49s/it]2017-06-02 14:59:35,116 root  INFO     step 1915.000000 - time: 1.344382, loss: 0.015218, perplexity: 1.015334, precision: 0.968750, batch_len: 131.000000
Train, loss=0.01521811: 1916it [44:37,  1.49s/it]2017-06-02 14:59:36,016 root  INFO     step 1916.000000 - time: 0.816891, loss: 0.008831, perplexity: 1.008870, precision: 1.000000, batch_len: 71.000000
Train, loss=0.00883060: 1917it [44:37,  1.32s/it]2017-06-02 14:59:36,090 root  INFO     Generating first batch)
2017-06-02 14:59:39,572 root  INFO     step 1917.000000 - time: 1.002292, loss: 0.007841, perplexity: 1.007872, precision: 0.984375, batch_len: 96.000000
Train, loss=0.00784077: 1918it [44:41,  1.99s/it]2017-06-02 14:59:41,277 root  INFO     step 1918.000000 - time: 1.285627, loss: 0.029528, perplexity: 1.029969, precision: 0.984375, batch_len: 128.000000
Train, loss=0.02952847: 1919it [44:43,  1.90s/it]2017-06-02 14:59:42,575 root  INFO     step 1919.000000 - time: 1.231691, loss: 0.082862, perplexity: 1.086391, precision: 0.937500, batch_len: 113.000000
Train, loss=0.08286165: 1920it [44:44,  1.72s/it]2017-06-02 14:59:43,918 root  INFO     step 1920.000000 - time: 1.259360, loss: 0.104877, perplexity: 1.110574, precision: 0.875000, batch_len: 105.000000
Train, loss=0.10487711: 1921it [44:45,  1.61s/it]2017-06-02 14:59:45,020 root  INFO     step 1921.000000 - time: 1.078501, loss: 0.086749, perplexity: 1.090623, precision: 0.828125, batch_len: 101.000000
Train, loss=0.08674918: 1922it [44:46,  1.46s/it]2017-06-02 14:59:46,160 root  INFO     step 1922.000000 - time: 0.964991, loss: 0.034145, perplexity: 1.034735, precision: 0.921875, batch_len: 102.000000
Train, loss=0.03414532: 1923it [44:48,  1.36s/it]2017-06-02 14:59:47,367 root  INFO     step 1923.000000 - time: 1.047449, loss: 0.033648, perplexity: 1.034220, precision: 0.875000, batch_len: 104.000000
Train, loss=0.03364800: 1924it [44:49,  1.32s/it]2017-06-02 14:59:48,346 root  INFO     step 1924.000000 - time: 0.974092, loss: 0.083694, perplexity: 1.087296, precision: 0.843750, batch_len: 88.000000
Train, loss=0.08369422: 1925it [44:50,  1.21s/it]2017-06-02 14:59:50,142 root  INFO     step 1925.000000 - time: 1.727377, loss: 0.084131, perplexity: 1.087772, precision: 0.890625, batch_len: 120.000000
Train, loss=0.08413124: 1926it [44:52,  1.39s/it]2017-06-02 14:59:51,314 root  INFO     step 1926.000000 - time: 1.083138, loss: 0.185560, perplexity: 1.203893, precision: 0.843750, batch_len: 100.000000
Train, loss=0.18556011: 1927it [44:53,  1.32s/it]2017-06-02 14:59:52,840 root  INFO     step 1927.000000 - time: 1.339719, loss: 0.063467, perplexity: 1.065525, precision: 0.843750, batch_len: 121.000000
Train, loss=0.06346749: 1928it [44:54,  1.38s/it]2017-06-02 14:59:53,950 root  INFO     step 1928.000000 - time: 1.092882, loss: 0.188987, perplexity: 1.208025, precision: 0.781250, batch_len: 111.000000
Train, loss=0.18898697: 1929it [44:55,  1.30s/it]2017-06-02 14:59:55,100 root  INFO     step 1929.000000 - time: 1.132344, loss: 0.113909, perplexity: 1.120651, precision: 0.843750, batch_len: 114.000000
Train, loss=0.11390933: 1930it [44:57,  1.26s/it]2017-06-02 14:59:56,280 root  INFO     step 1930.000000 - time: 1.128157, loss: 0.162528, perplexity: 1.176481, precision: 0.843750, batch_len: 81.000000
Train, loss=0.16252758: 1931it [44:58,  1.23s/it]2017-06-02 14:59:57,486 root  INFO     step 1931.000000 - time: 1.044578, loss: 0.110977, perplexity: 1.117370, precision: 0.765625, batch_len: 89.000000
Train, loss=0.11097746: 1932it [44:59,  1.23s/it]2017-06-02 14:59:58,652 root  INFO     step 1932.000000 - time: 1.105374, loss: 0.081908, perplexity: 1.085356, precision: 0.843750, batch_len: 110.000000
Train, loss=0.08190814: 1933it [45:00,  1.21s/it]2017-06-02 15:00:00,004 root  INFO     step 1933.000000 - time: 1.334514, loss: 0.035339, perplexity: 1.035970, precision: 0.890625, batch_len: 124.000000
Train, loss=0.03533862: 1934it [45:01,  1.25s/it]2017-06-02 15:00:00,938 root  INFO     step 1934.000000 - time: 0.915522, loss: 0.067981, perplexity: 1.070345, precision: 0.843750, batch_len: 93.000000
Train, loss=0.06798082: 1935it [45:02,  1.16s/it]2017-06-02 15:00:02,495 root  INFO     step 1935.000000 - time: 1.521442, loss: 0.060845, perplexity: 1.062734, precision: 0.875000, batch_len: 112.000000
Train, loss=0.06084454: 1936it [45:04,  1.28s/it]2017-06-02 15:00:03,812 root  INFO     step 1936.000000 - time: 1.202351, loss: 0.020486, perplexity: 1.020697, precision: 0.968750, batch_len: 92.000000
Train, loss=0.02048595: 1937it [45:05,  1.29s/it]2017-06-02 15:00:04,968 root  INFO     step 1937.000000 - time: 1.105104, loss: 0.069641, perplexity: 1.072123, precision: 0.843750, batch_len: 108.000000
Train, loss=0.06964087: 1938it [45:06,  1.25s/it]2017-06-02 15:00:06,360 root  INFO     step 1938.000000 - time: 1.362634, loss: 0.027186, perplexity: 1.027559, precision: 0.953125, batch_len: 117.000000
Train, loss=0.02718626: 1939it [45:08,  1.29s/it]2017-06-02 15:00:07,400 root  INFO     step 1939.000000 - time: 0.974109, loss: 0.053475, perplexity: 1.054931, precision: 0.890625, batch_len: 94.000000
Train, loss=0.05347493: 1940it [45:09,  1.22s/it]2017-06-02 15:00:08,348 root  INFO     step 1940.000000 - time: 0.941353, loss: 0.056734, perplexity: 1.058375, precision: 0.890625, batch_len: 90.000000
Train, loss=0.05673437: 1941it [45:10,  1.14s/it]2017-06-02 15:00:09,687 root  INFO     step 1941.000000 - time: 1.290243, loss: 0.069825, perplexity: 1.072321, precision: 0.828125, batch_len: 109.000000
Train, loss=0.06982521: 1942it [45:11,  1.20s/it]2017-06-02 15:00:11,334 root  INFO     step 1942.000000 - time: 1.526493, loss: 0.054164, perplexity: 1.055658, precision: 0.953125, batch_len: 133.000000
Train, loss=0.05416398: 1943it [45:13,  1.33s/it]2017-06-02 15:00:12,715 root  INFO     step 1943.000000 - time: 1.344909, loss: 0.057973, perplexity: 1.059686, precision: 0.890625, batch_len: 116.000000
Train, loss=0.05797289: 1944it [45:14,  1.35s/it]2017-06-02 15:00:13,748 root  INFO     step 1944.000000 - time: 1.005032, loss: 0.055166, perplexity: 1.056716, precision: 0.875000, batch_len: 85.000000
Train, loss=0.05516578: 1945it [45:15,  1.25s/it]2017-06-02 15:00:14,862 root  INFO     step 1945.000000 - time: 1.092277, loss: 0.054547, perplexity: 1.056062, precision: 0.906250, batch_len: 91.000000
Train, loss=0.05454655: 1946it [45:16,  1.21s/it]2017-06-02 15:00:16,064 root  INFO     step 1946.000000 - time: 1.121495, loss: 0.068713, perplexity: 1.071129, precision: 0.921875, batch_len: 103.000000
Train, loss=0.06871321: 1947it [45:18,  1.21s/it]2017-06-02 15:00:17,209 root  INFO     step 1947.000000 - time: 1.119392, loss: 0.043892, perplexity: 1.044870, precision: 0.937500, batch_len: 80.000000
Train, loss=0.04389219: 1948it [45:19,  1.19s/it]2017-06-02 15:00:18,248 root  INFO     step 1948.000000 - time: 1.004752, loss: 0.113050, perplexity: 1.119688, precision: 0.781250, batch_len: 97.000000
Train, loss=0.11305024: 1949it [45:20,  1.14s/it]2017-06-02 15:00:19,241 root  INFO     step 1949.000000 - time: 0.984949, loss: 0.032425, perplexity: 1.032957, precision: 0.937500, batch_len: 86.000000
Train, loss=0.03242521: 1950it [45:21,  1.10s/it]2017-06-02 15:00:20,329 root  INFO     step 1950.000000 - time: 1.019962, loss: 0.027995, perplexity: 1.028390, precision: 0.953125, batch_len: 106.000000
Train, loss=0.02799470: 1951it [45:22,  1.10s/it]2017-06-02 15:00:21,454 root  INFO     step 1951.000000 - time: 1.038896, loss: 0.031070, perplexity: 1.031558, precision: 0.921875, batch_len: 115.000000
Train, loss=0.03107046: 1952it [45:23,  1.10s/it]2017-06-02 15:00:22,836 root  INFO     step 1952.000000 - time: 1.265033, loss: 0.019305, perplexity: 1.019492, precision: 0.953125, batch_len: 99.000000
Train, loss=0.01930454: 1953it [45:24,  1.19s/it]2017-06-02 15:00:24,372 root  INFO     step 1953.000000 - time: 1.528369, loss: 0.020828, perplexity: 1.021046, precision: 0.937500, batch_len: 123.000000
Train, loss=0.02082801: 1954it [45:26,  1.29s/it]2017-06-02 15:00:25,550 root  INFO     step 1954.000000 - time: 1.072470, loss: 0.037943, perplexity: 1.038672, precision: 0.921875, batch_len: 107.000000
Train, loss=0.03794331: 1955it [45:27,  1.26s/it]2017-06-02 15:00:26,976 root  INFO     step 1955.000000 - time: 1.421016, loss: 0.034304, perplexity: 1.034899, precision: 0.953125, batch_len: 119.000000
Train, loss=0.03430380: 1956it [45:28,  1.31s/it]2017-06-02 15:00:28,399 root  INFO     step 1956.000000 - time: 1.365597, loss: 0.024464, perplexity: 1.024766, precision: 0.937500, batch_len: 126.000000
Train, loss=0.02446409: 1957it [45:30,  1.34s/it]2017-06-02 15:00:29,777 root  INFO     step 1957.000000 - time: 1.330146, loss: 0.013186, perplexity: 1.013273, precision: 0.984375, batch_len: 125.000000
Train, loss=0.01318588: 1958it [45:31,  1.35s/it]2017-06-02 15:00:30,911 root  INFO     step 1958.000000 - time: 1.085374, loss: 0.036453, perplexity: 1.037126, precision: 0.953125, batch_len: 79.000000
Train, loss=0.03645309: 1959it [45:32,  1.29s/it]2017-06-02 15:00:32,653 root  INFO     step 1959.000000 - time: 1.609902, loss: 0.047621, perplexity: 1.048773, precision: 0.906250, batch_len: 144.000000
Train, loss=0.04762081: 1960it [45:34,  1.42s/it]2017-06-02 15:00:33,783 root  INFO     step 1960.000000 - time: 1.011966, loss: 0.051124, perplexity: 1.052453, precision: 0.843750, batch_len: 84.000000
Train, loss=0.05112395: 1961it [45:35,  1.34s/it]2017-06-02 15:00:35,225 root  INFO     step 1961.000000 - time: 1.343780, loss: 0.027042, perplexity: 1.027411, precision: 0.953125, batch_len: 137.000000
Train, loss=0.02704205: 1962it [45:37,  1.37s/it]2017-06-02 15:00:36,242 root  INFO     step 1962.000000 - time: 0.881054, loss: 0.021255, perplexity: 1.021483, precision: 0.921875, batch_len: 87.000000
Train, loss=0.02125543: 1963it [45:38,  1.26s/it]2017-06-02 15:00:37,771 root  INFO     step 1963.000000 - time: 1.485317, loss: 0.012415, perplexity: 1.012492, precision: 0.984375, batch_len: 136.000000
Train, loss=0.01241487: 1964it [45:39,  1.34s/it]2017-06-02 15:00:39,338 root  INFO     step 1964.000000 - time: 1.552637, loss: 0.019413, perplexity: 1.019602, precision: 0.968750, batch_len: 130.000000
Train, loss=0.01941266: 1965it [45:41,  1.41s/it]2017-06-02 15:00:40,303 root  INFO     step 1965.000000 - time: 0.961973, loss: 0.025272, perplexity: 1.025594, precision: 0.937500, batch_len: 83.000000
Train, loss=0.02527200: 1966it [45:42,  1.28s/it]2017-06-02 15:00:41,480 root  INFO     step 1966.000000 - time: 1.083874, loss: 0.007438, perplexity: 1.007466, precision: 0.984375, batch_len: 98.000000
Train, loss=0.00743833: 1967it [45:43,  1.25s/it]2017-06-02 15:00:42,873 root  INFO     step 1967.000000 - time: 1.310639, loss: 0.021091, perplexity: 1.021315, precision: 0.921875, batch_len: 129.000000
Train, loss=0.02109149: 1968it [45:44,  1.29s/it]2017-06-02 15:00:44,470 root  INFO     step 1968.000000 - time: 1.557030, loss: 0.012669, perplexity: 1.012749, precision: 0.953125, batch_len: 118.000000
Train, loss=0.01266875: 1969it [45:46,  1.38s/it]2017-06-02 15:00:46,224 root  INFO     step 1969.000000 - time: 1.547095, loss: 0.032803, perplexity: 1.033347, precision: 0.953125, batch_len: 135.000000
Train, loss=0.03280329: 1970it [45:48,  1.49s/it]2017-06-02 15:00:47,404 root  INFO     step 1970.000000 - time: 0.903301, loss: 0.026148, perplexity: 1.026492, precision: 0.968750, batch_len: 82.000000
Train, loss=0.02614758: 1971it [45:49,  1.40s/it]2017-06-02 15:00:48,553 root  INFO     step 1971.000000 - time: 1.019188, loss: 0.015660, perplexity: 1.015783, precision: 0.953125, batch_len: 96.000000
Train, loss=0.01565985: 1972it [45:50,  1.32s/it]2017-06-02 15:00:50,443 root  INFO     step 1972.000000 - time: 1.875631, loss: 0.015329, perplexity: 1.015447, precision: 0.968750, batch_len: 150.000000
Train, loss=0.01532879: 1973it [45:52,  1.49s/it]2017-06-02 15:00:51,747 root  INFO     step 1973.000000 - time: 1.210534, loss: 0.028532, perplexity: 1.028943, precision: 0.953125, batch_len: 76.000000
Train, loss=0.02853172: 1974it [45:53,  1.44s/it]2017-06-02 15:00:53,199 root  INFO     step 1974.000000 - time: 1.403682, loss: 0.018703, perplexity: 1.018879, precision: 0.953125, batch_len: 122.000000
Train, loss=0.01870305: 1975it [45:55,  1.44s/it]2017-06-02 15:00:54,880 root  INFO     step 1975.000000 - time: 1.412372, loss: 0.008502, perplexity: 1.008539, precision: 0.984375, batch_len: 138.000000
Train, loss=0.00850236: 1976it [45:56,  1.51s/it]2017-06-02 15:00:55,927 root  INFO     step 1976.000000 - time: 0.885097, loss: 0.004015, perplexity: 1.004023, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00401472: 1977it [45:57,  1.37s/it]2017-06-02 15:00:57,326 root  INFO     step 1977.000000 - time: 1.367572, loss: 0.021377, perplexity: 1.021607, precision: 0.968750, batch_len: 72.000000
Train, loss=0.02137658: 1978it [45:59,  1.38s/it]2017-06-02 15:00:59,398 root  INFO     step 1978.000000 - time: 2.034464, loss: 0.009905, perplexity: 1.009954, precision: 0.968750, batch_len: 152.000000
Train, loss=0.00990452: 1979it [46:01,  1.59s/it]2017-06-02 15:01:00,786 root  INFO     step 1979.000000 - time: 1.378290, loss: 0.009648, perplexity: 1.009695, precision: 0.984375, batch_len: 134.000000
Train, loss=0.00964793: 1980it [46:02,  1.53s/it]2017-06-02 15:01:02,178 root  INFO     step 1980.000000 - time: 1.385998, loss: 0.053603, perplexity: 1.055065, precision: 0.843750, batch_len: 132.000000
Train, loss=0.05360279: 1981it [46:04,  1.49s/it]2017-06-02 15:01:03,090 root  INFO     step 1981.000000 - time: 0.907179, loss: 0.010579, perplexity: 1.010635, precision: 0.984375, batch_len: 77.000000
Train, loss=0.01057868: 1982it [46:05,  1.31s/it]2017-06-02 15:01:04,225 root  INFO     step 1982.000000 - time: 1.121224, loss: 0.023666, perplexity: 1.023948, precision: 0.937500, batch_len: 78.000000
Train, loss=0.02366572: 1983it [46:06,  1.26s/it]2017-06-02 15:01:05,881 root  INFO     step 1983.000000 - time: 1.491496, loss: 0.034943, perplexity: 1.035560, precision: 0.921875, batch_len: 141.000000
Train, loss=0.03494260: 1984it [46:07,  1.38s/it]2017-06-02 15:01:07,412 root  INFO     step 1984.000000 - time: 1.443934, loss: 0.023528, perplexity: 1.023807, precision: 0.953125, batch_len: 142.000000
Train, loss=0.02352772: 1985it [46:09,  1.43s/it]2017-06-02 15:01:08,863 root  INFO     step 1985.000000 - time: 1.418399, loss: 0.021813, perplexity: 1.022053, precision: 0.968750, batch_len: 139.000000
Train, loss=0.02181289: 1986it [46:10,  1.43s/it]2017-06-02 15:01:09,968 root  INFO     step 1986.000000 - time: 0.948910, loss: 0.013534, perplexity: 1.013626, precision: 0.984375, batch_len: 71.000000
Train, loss=0.01353373: 1987it [46:11,  1.33s/it]2017-06-02 15:01:11,820 root  INFO     step 1987.000000 - time: 1.712764, loss: 0.010701, perplexity: 1.010758, precision: 0.984375, batch_len: 131.000000
Train, loss=0.01070080: 1988it [46:13,  1.49s/it]2017-06-02 15:01:11,872 root  INFO     Generating first batch)
2017-06-02 15:01:15,395 root  INFO     step 1988.000000 - time: 1.163666, loss: 0.023805, perplexity: 1.024091, precision: 0.968750, batch_len: 96.000000
Train, loss=0.02380513: 1989it [46:17,  2.12s/it]2017-06-02 15:01:17,131 root  INFO     step 1989.000000 - time: 1.257840, loss: 0.018218, perplexity: 1.018385, precision: 0.968750, batch_len: 101.000000
Train, loss=0.01821846: 1990it [46:19,  2.00s/it]2017-06-02 15:01:18,740 root  INFO     step 1990.000000 - time: 1.378829, loss: 0.038420, perplexity: 1.039167, precision: 0.921875, batch_len: 128.000000
Train, loss=0.03841967: 1991it [46:20,  1.88s/it]2017-06-02 15:01:19,949 root  INFO     step 1991.000000 - time: 1.094317, loss: 0.030783, perplexity: 1.031262, precision: 0.937500, batch_len: 110.000000
Train, loss=0.03078311: 1992it [46:21,  1.68s/it]2017-06-02 15:01:21,049 root  INFO     step 1992.000000 - time: 1.092899, loss: 0.021604, perplexity: 1.021839, precision: 0.968750, batch_len: 113.000000
Train, loss=0.02160425: 1993it [46:23,  1.51s/it]2017-06-02 15:01:22,440 root  INFO     step 1993.000000 - time: 1.313788, loss: 0.027186, perplexity: 1.027559, precision: 0.968750, batch_len: 108.000000
Train, loss=0.02718596: 1994it [46:24,  1.47s/it]2017-06-02 15:01:23,728 root  INFO     step 1994.000000 - time: 1.061724, loss: 0.025518, perplexity: 1.025846, precision: 0.968750, batch_len: 92.000000
Train, loss=0.02551777: 1995it [46:25,  1.42s/it]2017-06-02 15:01:24,853 root  INFO     step 1995.000000 - time: 1.066448, loss: 0.029245, perplexity: 1.029677, precision: 0.921875, batch_len: 104.000000
Train, loss=0.02924539: 1996it [46:26,  1.33s/it]2017-06-02 15:01:26,222 root  INFO     step 1996.000000 - time: 1.340811, loss: 0.147993, perplexity: 1.159505, precision: 0.890625, batch_len: 121.000000
Train, loss=0.14799313: 1997it [46:28,  1.34s/it]2017-06-02 15:01:27,980 root  INFO     step 1997.000000 - time: 1.672280, loss: 0.108110, perplexity: 1.114170, precision: 0.890625, batch_len: 90.000000
Train, loss=0.10810973: 1998it [46:29,  1.47s/it]2017-06-02 15:01:29,113 root  INFO     step 1998.000000 - time: 1.116585, loss: 0.054271, perplexity: 1.055770, precision: 0.921875, batch_len: 88.000000
Train, loss=0.05427081: 1999it [46:31,  1.37s/it]2017-06-02 15:01:30,690 root  INFO     step 1999.000000 - time: 1.563268, loss: 0.045462, perplexity: 1.046512, precision: 0.921875, batch_len: 120.000000
Train, loss=0.04546226: 2000it [46:32,  1.43s/it]2017-06-02 15:01:31,210 root  INFO     global step 4000 step-time 1.28 loss 0.110443  perplexity 1.12
2017-06-02 15:01:31,210 root  INFO     Saving model, current_step: 2000
2017-06-02 15:01:41,388 root  INFO     step 2000.000000 - time: 0.976537, loss: 0.034425, perplexity: 1.035025, precision: 0.953125, batch_len: 86.000000
Train, loss=0.03442534: 2001it [46:43,  4.21s/it]2017-06-02 15:01:42,419 root  INFO     step 2001.000000 - time: 0.968903, loss: 0.028732, perplexity: 1.029149, precision: 0.875000, batch_len: 97.000000
Train, loss=0.02873219: 2002it [46:44,  3.26s/it]2017-06-02 15:01:43,429 root  INFO     step 2002.000000 - time: 0.949040, loss: 0.040773, perplexity: 1.041616, precision: 0.921875, batch_len: 105.000000
Train, loss=0.04077289: 2003it [46:45,  2.58s/it]2017-06-02 15:01:44,590 root  INFO     step 2003.000000 - time: 1.137966, loss: 0.035106, perplexity: 1.035729, precision: 0.937500, batch_len: 102.000000
Train, loss=0.03510594: 2004it [46:46,  2.16s/it]2017-06-02 15:01:45,779 root  INFO     step 2004.000000 - time: 1.184252, loss: 0.080805, perplexity: 1.084159, precision: 0.859375, batch_len: 93.000000
Train, loss=0.08080474: 2005it [46:47,  1.87s/it]2017-06-02 15:01:46,888 root  INFO     step 2005.000000 - time: 0.972660, loss: 0.020455, perplexity: 1.020666, precision: 0.984375, batch_len: 80.000000
Train, loss=0.02045503: 2006it [46:48,  1.64s/it]2017-06-02 15:01:48,322 root  INFO     step 2006.000000 - time: 1.333066, loss: 0.012289, perplexity: 1.012365, precision: 0.984375, batch_len: 112.000000
Train, loss=0.01228903: 2007it [46:50,  1.58s/it]2017-06-02 15:01:49,406 root  INFO     step 2007.000000 - time: 1.076623, loss: 0.068295, perplexity: 1.070681, precision: 0.875000, batch_len: 111.000000
Train, loss=0.06829506: 2008it [46:51,  1.43s/it]2017-06-02 15:01:50,811 root  INFO     step 2008.000000 - time: 1.239574, loss: 0.027187, perplexity: 1.027560, precision: 0.953125, batch_len: 107.000000
Train, loss=0.02718688: 2009it [46:52,  1.42s/it]2017-06-02 15:01:52,127 root  INFO     step 2009.000000 - time: 1.247430, loss: 0.034782, perplexity: 1.035394, precision: 0.953125, batch_len: 81.000000
Train, loss=0.03478209: 2010it [46:54,  1.39s/it]2017-06-02 15:01:53,226 root  INFO     step 2010.000000 - time: 1.055508, loss: 0.058079, perplexity: 1.059798, precision: 0.968750, batch_len: 106.000000
Train, loss=0.05807852: 2011it [46:55,  1.30s/it]2017-06-02 15:01:54,372 root  INFO     step 2011.000000 - time: 1.123795, loss: 0.177372, perplexity: 1.194076, precision: 0.828125, batch_len: 114.000000
Train, loss=0.17737231: 2012it [46:56,  1.26s/it]2017-06-02 15:01:55,726 root  INFO     step 2012.000000 - time: 1.291429, loss: 0.284963, perplexity: 1.329713, precision: 0.765625, batch_len: 124.000000
Train, loss=0.28496319: 2013it [46:57,  1.29s/it]2017-06-02 15:01:56,643 root  INFO     step 2013.000000 - time: 0.889585, loss: 0.247718, perplexity: 1.281099, precision: 0.750000, batch_len: 89.000000
Train, loss=0.24771816: 2014it [46:58,  1.17s/it]2017-06-02 15:01:58,131 root  INFO     step 2014.000000 - time: 1.463797, loss: 0.139596, perplexity: 1.149809, precision: 0.734375, batch_len: 118.000000
Train, loss=0.13959575: 2015it [47:00,  1.27s/it]2017-06-02 15:01:59,368 root  INFO     step 2015.000000 - time: 1.178805, loss: 0.176676, perplexity: 1.193244, precision: 0.750000, batch_len: 94.000000
Train, loss=0.17667559: 2016it [47:01,  1.26s/it]2017-06-02 15:02:00,579 root  INFO     step 2016.000000 - time: 1.143301, loss: 0.122955, perplexity: 1.130833, precision: 0.703125, batch_len: 109.000000
Train, loss=0.12295453: 2017it [47:02,  1.24s/it]2017-06-02 15:02:01,893 root  INFO     step 2017.000000 - time: 1.293169, loss: 0.067262, perplexity: 1.069576, precision: 0.843750, batch_len: 117.000000
Train, loss=0.06726196: 2018it [47:03,  1.27s/it]2017-06-02 15:02:02,864 root  INFO     step 2018.000000 - time: 0.909801, loss: 0.092802, perplexity: 1.097245, precision: 0.890625, batch_len: 91.000000
Train, loss=0.09280238: 2019it [47:04,  1.18s/it]2017-06-02 15:02:04,080 root  INFO     step 2019.000000 - time: 1.169115, loss: 0.041679, perplexity: 1.042560, precision: 0.953125, batch_len: 100.000000
Train, loss=0.04167934: 2020it [47:06,  1.19s/it]2017-06-02 15:02:05,773 root  INFO     step 2020.000000 - time: 1.661190, loss: 0.014331, perplexity: 1.014434, precision: 0.984375, batch_len: 123.000000
Train, loss=0.01433090: 2021it [47:07,  1.34s/it]2017-06-02 15:02:06,738 root  INFO     step 2021.000000 - time: 0.947470, loss: 0.061928, perplexity: 1.063885, precision: 0.937500, batch_len: 79.000000
Train, loss=0.06192762: 2022it [47:08,  1.23s/it]2017-06-02 15:02:07,760 root  INFO     step 2022.000000 - time: 0.997953, loss: 0.041515, perplexity: 1.042389, precision: 0.921875, batch_len: 103.000000
Train, loss=0.04151496: 2023it [47:09,  1.17s/it]2017-06-02 15:02:09,146 root  INFO     step 2023.000000 - time: 1.342831, loss: 0.016479, perplexity: 1.016616, precision: 0.968750, batch_len: 137.000000
Train, loss=0.01647917: 2024it [47:11,  1.23s/it]2017-06-02 15:02:10,122 root  INFO     step 2024.000000 - time: 0.878542, loss: 0.019543, perplexity: 1.019736, precision: 0.968750, batch_len: 87.000000
Train, loss=0.01954335: 2025it [47:12,  1.16s/it]2017-06-02 15:02:11,325 root  INFO     step 2025.000000 - time: 1.130136, loss: 0.063016, perplexity: 1.065044, precision: 0.843750, batch_len: 83.000000
Train, loss=0.06301607: 2026it [47:13,  1.17s/it]2017-06-02 15:02:12,708 root  INFO     step 2026.000000 - time: 1.187668, loss: 0.052140, perplexity: 1.053524, precision: 0.875000, batch_len: 84.000000
Train, loss=0.05214036: 2027it [47:14,  1.23s/it]2017-06-02 15:02:13,780 root  INFO     step 2027.000000 - time: 0.996112, loss: 0.029753, perplexity: 1.030200, precision: 0.921875, batch_len: 85.000000
Train, loss=0.02975300: 2028it [47:15,  1.18s/it]2017-06-02 15:02:14,939 root  INFO     step 2028.000000 - time: 1.140045, loss: 0.036493, perplexity: 1.037167, precision: 0.921875, batch_len: 115.000000
Train, loss=0.03649271: 2029it [47:16,  1.18s/it]2017-06-02 15:02:16,071 root  INFO     step 2029.000000 - time: 1.045693, loss: 0.013161, perplexity: 1.013248, precision: 0.984375, batch_len: 98.000000
Train, loss=0.01316138: 2030it [47:18,  1.16s/it]2017-06-02 15:02:17,440 root  INFO     step 2030.000000 - time: 1.353712, loss: 0.030714, perplexity: 1.031191, precision: 0.906250, batch_len: 125.000000
Train, loss=0.03071403: 2031it [47:19,  1.23s/it]2017-06-02 15:02:19,185 root  INFO     step 2031.000000 - time: 1.632920, loss: 0.018478, perplexity: 1.018650, precision: 0.937500, batch_len: 130.000000
Train, loss=0.01847813: 2032it [47:21,  1.38s/it]2017-06-02 15:02:20,700 root  INFO     step 2032.000000 - time: 1.406042, loss: 0.014794, perplexity: 1.014904, precision: 0.953125, batch_len: 138.000000
Train, loss=0.01479441: 2033it [47:22,  1.42s/it]2017-06-02 15:02:22,025 root  INFO     step 2033.000000 - time: 1.308849, loss: 0.027184, perplexity: 1.027557, precision: 0.921875, batch_len: 126.000000
Train, loss=0.02718376: 2034it [47:23,  1.39s/it]2017-06-02 15:02:23,346 root  INFO     step 2034.000000 - time: 1.261903, loss: 0.012280, perplexity: 1.012356, precision: 0.984375, batch_len: 133.000000
Train, loss=0.01228000: 2035it [47:25,  1.37s/it]2017-06-02 15:02:24,625 root  INFO     step 2035.000000 - time: 1.266761, loss: 0.018312, perplexity: 1.018481, precision: 0.921875, batch_len: 99.000000
Train, loss=0.01831198: 2036it [47:26,  1.34s/it]2017-06-02 15:02:26,445 root  INFO     step 2036.000000 - time: 1.538297, loss: 0.031329, perplexity: 1.031825, precision: 0.968750, batch_len: 119.000000
Train, loss=0.03132888: 2037it [47:28,  1.49s/it]2017-06-02 15:02:27,935 root  INFO     step 2037.000000 - time: 1.448196, loss: 0.015576, perplexity: 1.015698, precision: 0.953125, batch_len: 116.000000
Train, loss=0.01557564: 2038it [47:29,  1.49s/it]2017-06-02 15:02:29,442 root  INFO     step 2038.000000 - time: 1.395937, loss: 0.007370, perplexity: 1.007397, precision: 0.968750, batch_len: 136.000000
Train, loss=0.00737010: 2039it [47:31,  1.49s/it]2017-06-02 15:02:31,088 root  INFO     step 2039.000000 - time: 1.551301, loss: 0.017378, perplexity: 1.017530, precision: 0.984375, batch_len: 144.000000
Train, loss=0.01737809: 2040it [47:33,  1.54s/it]2017-06-02 15:02:31,967 root  INFO     step 2040.000000 - time: 0.840038, loss: 0.027499, perplexity: 1.027880, precision: 0.937500, batch_len: 82.000000
Train, loss=0.02749858: 2041it [47:33,  1.34s/it]2017-06-02 15:02:33,109 root  INFO     step 2041.000000 - time: 1.137833, loss: 0.004870, perplexity: 1.004881, precision: 1.000000, batch_len: 96.000000
Train, loss=0.00486956: 2042it [47:35,  1.28s/it]2017-06-02 15:02:34,783 root  INFO     step 2042.000000 - time: 1.572012, loss: 0.012409, perplexity: 1.012486, precision: 0.953125, batch_len: 129.000000
Train, loss=0.01240852: 2043it [47:36,  1.40s/it]2017-06-02 15:02:36,349 root  INFO     step 2043.000000 - time: 1.372171, loss: 0.009016, perplexity: 1.009057, precision: 0.953125, batch_len: 122.000000
Train, loss=0.00901611: 2044it [47:38,  1.45s/it]2017-06-02 15:02:37,837 root  INFO     step 2044.000000 - time: 1.399702, loss: 0.019835, perplexity: 1.020033, precision: 0.968750, batch_len: 135.000000
Train, loss=0.01983484: 2045it [47:39,  1.46s/it]2017-06-02 15:02:38,854 root  INFO     step 2045.000000 - time: 0.944051, loss: 0.021269, perplexity: 1.021496, precision: 0.953125, batch_len: 78.000000
Train, loss=0.02126856: 2046it [47:40,  1.33s/it]2017-06-02 15:02:39,996 root  INFO     step 2046.000000 - time: 1.121669, loss: 0.004681, perplexity: 1.004692, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00468103: 2047it [47:41,  1.27s/it]2017-06-02 15:02:42,153 root  INFO     step 2047.000000 - time: 1.996842, loss: 0.027661, perplexity: 1.028047, precision: 0.953125, batch_len: 152.000000
Train, loss=0.02766134: 2048it [47:44,  1.54s/it]2017-06-02 15:02:43,587 root  INFO     step 2048.000000 - time: 1.346452, loss: 0.060640, perplexity: 1.062517, precision: 0.890625, batch_len: 141.000000
Train, loss=0.06064020: 2049it [47:45,  1.51s/it]2017-06-02 15:02:44,920 root  INFO     step 2049.000000 - time: 1.234986, loss: 0.022973, perplexity: 1.023239, precision: 0.968750, batch_len: 134.000000
Train, loss=0.02297312: 2050it [47:46,  1.45s/it]2017-06-02 15:02:46,136 root  INFO     step 2050.000000 - time: 1.117396, loss: 0.009071, perplexity: 1.009112, precision: 0.968750, batch_len: 77.000000
Train, loss=0.00907071: 2051it [47:48,  1.38s/it]2017-06-02 15:02:47,943 root  INFO     step 2051.000000 - time: 1.772980, loss: 0.040441, perplexity: 1.041270, precision: 0.937500, batch_len: 139.000000
Train, loss=0.04044146: 2052it [47:49,  1.51s/it]2017-06-02 15:02:49,003 root  INFO     step 2052.000000 - time: 1.027855, loss: 0.037648, perplexity: 1.038365, precision: 0.953125, batch_len: 76.000000
Train, loss=0.03764764: 2053it [47:50,  1.38s/it]2017-06-02 15:02:50,278 root  INFO     step 2053.000000 - time: 1.140624, loss: 0.021780, perplexity: 1.022019, precision: 0.984375, batch_len: 72.000000
Train, loss=0.02177997: 2054it [47:52,  1.35s/it]2017-06-02 15:02:52,038 root  INFO     step 2054.000000 - time: 1.722132, loss: 0.011426, perplexity: 1.011491, precision: 0.953125, batch_len: 150.000000
Train, loss=0.01142591: 2055it [47:54,  1.47s/it]2017-06-02 15:02:53,937 root  INFO     step 2055.000000 - time: 1.725165, loss: 0.012722, perplexity: 1.012804, precision: 0.984375, batch_len: 142.000000
Train, loss=0.01272225: 2056it [47:55,  1.60s/it]2017-06-02 15:02:55,378 root  INFO     step 2056.000000 - time: 1.394442, loss: 0.005461, perplexity: 1.005476, precision: 1.000000, batch_len: 132.000000
Train, loss=0.00546133: 2057it [47:57,  1.55s/it]2017-06-02 15:02:57,010 root  INFO     step 2057.000000 - time: 1.500354, loss: 0.008771, perplexity: 1.008809, precision: 1.000000, batch_len: 131.000000
Train, loss=0.00877057: 2058it [47:58,  1.58s/it]2017-06-02 15:02:58,009 root  INFO     step 2058.000000 - time: 0.833893, loss: 0.004935, perplexity: 1.004947, precision: 1.000000, batch_len: 71.000000
Train, loss=0.00493470: 2059it [47:59,  1.40s/it]2017-06-02 15:02:58,207 root  INFO     Generating first batch)
2017-06-02 15:03:01,345 root  INFO     step 2059.000000 - time: 0.980426, loss: 0.010599, perplexity: 1.010655, precision: 0.968750, batch_len: 96.000000
Train, loss=0.01059891: 2060it [48:03,  1.98s/it]2017-06-02 15:03:03,343 root  INFO     step 2060.000000 - time: 0.995410, loss: 0.039379, perplexity: 1.040165, precision: 0.953125, batch_len: 110.000000
Train, loss=0.03937926: 2061it [48:05,  1.99s/it]2017-06-02 15:03:04,725 root  INFO     step 2061.000000 - time: 1.280173, loss: 0.025216, perplexity: 1.025536, precision: 0.968750, batch_len: 101.000000
Train, loss=0.02521584: 2062it [48:06,  1.81s/it]2017-06-02 15:03:05,981 root  INFO     step 2062.000000 - time: 1.251629, loss: 0.022056, perplexity: 1.022301, precision: 0.968750, batch_len: 104.000000
Train, loss=0.02205628: 2063it [48:07,  1.64s/it]2017-06-02 15:03:07,085 root  INFO     step 2063.000000 - time: 1.085116, loss: 0.032591, perplexity: 1.033128, precision: 0.953125, batch_len: 105.000000
Train, loss=0.03259139: 2064it [48:09,  1.48s/it]2017-06-02 15:03:08,484 root  INFO     step 2064.000000 - time: 1.325979, loss: 0.056766, perplexity: 1.058408, precision: 0.906250, batch_len: 121.000000
Train, loss=0.05676587: 2065it [48:10,  1.46s/it]2017-06-02 15:03:09,464 root  INFO     step 2065.000000 - time: 0.944920, loss: 0.046763, perplexity: 1.047874, precision: 0.937500, batch_len: 93.000000
Train, loss=0.04676308: 2066it [48:11,  1.31s/it]2017-06-02 15:03:10,889 root  INFO     step 2066.000000 - time: 1.418901, loss: 0.032211, perplexity: 1.032735, precision: 0.937500, batch_len: 120.000000
Train, loss=0.03221074: 2067it [48:12,  1.35s/it]2017-06-02 15:03:12,315 root  INFO     step 2067.000000 - time: 1.374864, loss: 0.009106, perplexity: 1.009147, precision: 1.000000, batch_len: 112.000000
Train, loss=0.00910582: 2068it [48:14,  1.37s/it]2017-06-02 15:03:13,695 root  INFO     step 2068.000000 - time: 1.349876, loss: 0.007867, perplexity: 1.007898, precision: 0.968750, batch_len: 128.000000
Train, loss=0.00786740: 2069it [48:15,  1.37s/it]2017-06-02 15:03:14,719 root  INFO     step 2069.000000 - time: 0.969289, loss: 0.030319, perplexity: 1.030783, precision: 0.921875, batch_len: 90.000000
Train, loss=0.03031902: 2070it [48:16,  1.27s/it]2017-06-02 15:03:15,683 root  INFO     step 2070.000000 - time: 0.934723, loss: 0.033438, perplexity: 1.034003, precision: 0.937500, batch_len: 88.000000
Train, loss=0.03343779: 2071it [48:17,  1.18s/it]2017-06-02 15:03:16,661 root  INFO     step 2071.000000 - time: 0.940018, loss: 0.017275, perplexity: 1.017425, precision: 0.921875, batch_len: 97.000000
Train, loss=0.01727480: 2072it [48:18,  1.12s/it]2017-06-02 15:03:17,897 root  INFO     step 2072.000000 - time: 1.220882, loss: 0.013100, perplexity: 1.013186, precision: 0.984375, batch_len: 111.000000
Train, loss=0.01309993: 2073it [48:19,  1.15s/it]2017-06-02 15:03:19,253 root  INFO     step 2073.000000 - time: 1.292974, loss: 0.016379, perplexity: 1.016514, precision: 0.968750, batch_len: 108.000000
Train, loss=0.01637888: 2074it [48:21,  1.21s/it]2017-06-02 15:03:20,478 root  INFO     step 2074.000000 - time: 1.207500, loss: 0.048480, perplexity: 1.049675, precision: 0.937500, batch_len: 91.000000
Train, loss=0.04848041: 2075it [48:22,  1.22s/it]2017-06-02 15:03:21,643 root  INFO     step 2075.000000 - time: 1.076500, loss: 0.033128, perplexity: 1.033683, precision: 0.968750, batch_len: 100.000000
Train, loss=0.03312828: 2076it [48:23,  1.20s/it]2017-06-02 15:03:22,621 root  INFO     step 2076.000000 - time: 0.928958, loss: 0.018241, perplexity: 1.018408, precision: 0.984375, batch_len: 80.000000
Train, loss=0.01824058: 2077it [48:24,  1.13s/it]2017-06-02 15:03:23,627 root  INFO     step 2077.000000 - time: 0.914163, loss: 0.016461, perplexity: 1.016597, precision: 0.984375, batch_len: 115.000000
Train, loss=0.01646053: 2078it [48:25,  1.10s/it]2017-06-02 15:03:24,794 root  INFO     step 2078.000000 - time: 1.154706, loss: 0.059223, perplexity: 1.061012, precision: 0.906250, batch_len: 86.000000
Train, loss=0.05922310: 2079it [48:26,  1.12s/it]2017-06-02 15:03:25,996 root  INFO     step 2079.000000 - time: 1.139017, loss: 0.047736, perplexity: 1.048893, precision: 0.921875, batch_len: 81.000000
Train, loss=0.04773551: 2080it [48:27,  1.14s/it]2017-06-02 15:03:27,285 root  INFO     step 2080.000000 - time: 1.184922, loss: 0.072783, perplexity: 1.075497, precision: 0.921875, batch_len: 106.000000
Train, loss=0.07278293: 2081it [48:29,  1.19s/it]2017-06-02 15:03:28,734 root  INFO     step 2081.000000 - time: 1.363429, loss: 0.051658, perplexity: 1.053015, precision: 0.937500, batch_len: 117.000000
Train, loss=0.05165792: 2082it [48:30,  1.27s/it]2017-06-02 15:03:29,816 root  INFO     step 2082.000000 - time: 1.046942, loss: 0.105508, perplexity: 1.111275, precision: 0.843750, batch_len: 107.000000
Train, loss=0.10550819: 2083it [48:31,  1.21s/it]2017-06-02 15:03:31,174 root  INFO     step 2083.000000 - time: 1.304345, loss: 0.048748, perplexity: 1.049956, precision: 0.859375, batch_len: 113.000000
Train, loss=0.04874848: 2084it [48:33,  1.25s/it]2017-06-02 15:03:32,265 root  INFO     step 2084.000000 - time: 1.070454, loss: 0.068874, perplexity: 1.071301, precision: 0.921875, batch_len: 114.000000
Train, loss=0.06887356: 2085it [48:34,  1.21s/it]2017-06-02 15:03:33,615 root  INFO     step 2085.000000 - time: 1.214193, loss: 0.096594, perplexity: 1.101413, precision: 0.859375, batch_len: 103.000000
Train, loss=0.09659363: 2086it [48:35,  1.25s/it]2017-06-02 15:03:34,809 root  INFO     step 2086.000000 - time: 1.164947, loss: 0.062114, perplexity: 1.064083, precision: 0.906250, batch_len: 102.000000
Train, loss=0.06211355: 2087it [48:36,  1.23s/it]2017-06-02 15:03:35,812 root  INFO     step 2087.000000 - time: 0.963935, loss: 0.060700, perplexity: 1.062580, precision: 0.843750, batch_len: 89.000000
Train, loss=0.06070026: 2088it [48:37,  1.16s/it]2017-06-02 15:03:37,201 root  INFO     step 2088.000000 - time: 1.361956, loss: 0.059035, perplexity: 1.060812, precision: 0.953125, batch_len: 116.000000
Train, loss=0.05903481: 2089it [48:39,  1.23s/it]2017-06-02 15:03:38,324 root  INFO     step 2089.000000 - time: 0.990360, loss: 0.082121, perplexity: 1.085587, precision: 0.890625, batch_len: 109.000000
Train, loss=0.08212116: 2090it [48:40,  1.20s/it]2017-06-02 15:03:39,700 root  INFO     step 2090.000000 - time: 1.363682, loss: 0.024217, perplexity: 1.024513, precision: 0.968750, batch_len: 124.000000
Train, loss=0.02421730: 2091it [48:41,  1.25s/it]2017-06-02 15:03:41,089 root  INFO     step 2091.000000 - time: 1.259543, loss: 0.062350, perplexity: 1.064335, precision: 0.921875, batch_len: 92.000000
Train, loss=0.06234974: 2092it [48:43,  1.29s/it]2017-06-02 15:03:42,183 root  INFO     step 2092.000000 - time: 0.951905, loss: 0.088737, perplexity: 1.092793, precision: 0.812500, batch_len: 83.000000
Train, loss=0.08873703: 2093it [48:44,  1.23s/it]2017-06-02 15:03:43,593 root  INFO     step 2093.000000 - time: 1.334932, loss: 0.035044, perplexity: 1.035666, precision: 0.906250, batch_len: 123.000000
Train, loss=0.03504448: 2094it [48:45,  1.29s/it]2017-06-02 15:03:44,549 root  INFO     step 2094.000000 - time: 0.914892, loss: 0.095110, perplexity: 1.099779, precision: 0.890625, batch_len: 79.000000
Train, loss=0.09510954: 2095it [48:46,  1.19s/it]2017-06-02 15:03:45,829 root  INFO     step 2095.000000 - time: 1.193823, loss: 0.051251, perplexity: 1.052587, precision: 0.906250, batch_len: 125.000000
Train, loss=0.05125130: 2096it [48:47,  1.22s/it]2017-06-02 15:03:47,138 root  INFO     step 2096.000000 - time: 1.256537, loss: 0.023977, perplexity: 1.024267, precision: 0.937500, batch_len: 98.000000
Train, loss=0.02397719: 2097it [48:49,  1.24s/it]2017-06-02 15:03:48,360 root  INFO     step 2097.000000 - time: 1.117827, loss: 0.031076, perplexity: 1.031564, precision: 0.953125, batch_len: 87.000000
Train, loss=0.03107595: 2098it [48:50,  1.24s/it]2017-06-02 15:03:49,779 root  INFO     step 2098.000000 - time: 1.390497, loss: 0.018593, perplexity: 1.018766, precision: 0.968750, batch_len: 129.000000
Train, loss=0.01859252: 2099it [48:51,  1.29s/it]2017-06-02 15:03:51,134 root  INFO     step 2099.000000 - time: 1.310558, loss: 0.036793, perplexity: 1.037478, precision: 0.921875, batch_len: 126.000000
Train, loss=0.03679316: 2100it [48:53,  1.31s/it]2017-06-02 15:03:52,646 root  INFO     step 2100.000000 - time: 1.323462, loss: 0.024908, perplexity: 1.025220, precision: 0.953125, batch_len: 144.000000
Train, loss=0.02490767: 2101it [48:54,  1.37s/it]2017-06-02 15:03:53,939 root  INFO     step 2101.000000 - time: 1.246833, loss: 0.086982, perplexity: 1.090878, precision: 0.875000, batch_len: 94.000000
Train, loss=0.08698244: 2102it [48:55,  1.35s/it]2017-06-02 15:03:55,093 root  INFO     step 2102.000000 - time: 1.114365, loss: 0.033108, perplexity: 1.033662, precision: 0.890625, batch_len: 84.000000
Train, loss=0.03310774: 2103it [48:57,  1.29s/it]2017-06-02 15:03:56,566 root  INFO     step 2103.000000 - time: 1.416533, loss: 0.031045, perplexity: 1.031532, precision: 0.937500, batch_len: 135.000000
Train, loss=0.03104511: 2104it [48:58,  1.34s/it]2017-06-02 15:03:57,975 root  INFO     step 2104.000000 - time: 1.324404, loss: 0.023243, perplexity: 1.023515, precision: 0.937500, batch_len: 118.000000
Train, loss=0.02324297: 2105it [48:59,  1.36s/it]2017-06-02 15:03:59,005 root  INFO     step 2105.000000 - time: 0.938213, loss: 0.049239, perplexity: 1.050472, precision: 0.875000, batch_len: 85.000000
Train, loss=0.04923943: 2106it [49:00,  1.26s/it]2017-06-02 15:04:00,519 root  INFO     step 2106.000000 - time: 1.480998, loss: 0.030640, perplexity: 1.031114, precision: 0.953125, batch_len: 137.000000
Train, loss=0.03063957: 2107it [49:02,  1.34s/it]2017-06-02 15:04:02,052 root  INFO     step 2107.000000 - time: 1.476999, loss: 0.009785, perplexity: 1.009833, precision: 0.984375, batch_len: 136.000000
Train, loss=0.00978531: 2108it [49:04,  1.40s/it]2017-06-02 15:04:03,047 root  INFO     step 2108.000000 - time: 0.949113, loss: 0.035902, perplexity: 1.036554, precision: 0.953125, batch_len: 82.000000
Train, loss=0.03590192: 2109it [49:05,  1.28s/it]2017-06-02 15:04:04,138 root  INFO     step 2109.000000 - time: 1.045119, loss: 0.020364, perplexity: 1.020573, precision: 0.921875, batch_len: 99.000000
Train, loss=0.02036377: 2110it [49:06,  1.22s/it]2017-06-02 15:04:05,132 root  INFO     step 2110.000000 - time: 0.908543, loss: 0.012600, perplexity: 1.012680, precision: 0.953125, batch_len: 96.000000
Train, loss=0.01259988: 2111it [49:07,  1.15s/it]2017-06-02 15:04:06,820 root  INFO     step 2111.000000 - time: 1.601644, loss: 0.013202, perplexity: 1.013290, precision: 0.984375, batch_len: 130.000000
Train, loss=0.01320203: 2112it [49:08,  1.31s/it]2017-06-02 15:04:08,401 root  INFO     step 2112.000000 - time: 1.544310, loss: 0.009515, perplexity: 1.009561, precision: 0.984375, batch_len: 133.000000
Train, loss=0.00951531: 2113it [49:10,  1.39s/it]2017-06-02 15:04:10,058 root  INFO     step 2113.000000 - time: 1.331383, loss: 0.024632, perplexity: 1.024938, precision: 0.968750, batch_len: 119.000000
Train, loss=0.02463224: 2114it [49:12,  1.47s/it]2017-06-02 15:04:11,500 root  INFO     step 2114.000000 - time: 1.287430, loss: 0.016551, perplexity: 1.016689, precision: 0.984375, batch_len: 141.000000
Train, loss=0.01655148: 2115it [49:13,  1.46s/it]2017-06-02 15:04:12,716 root  INFO     step 2115.000000 - time: 1.019090, loss: 0.004661, perplexity: 1.004671, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00466059: 2116it [49:14,  1.39s/it]2017-06-02 15:04:13,819 root  INFO     step 2116.000000 - time: 1.073567, loss: 0.017324, perplexity: 1.017475, precision: 0.968750, batch_len: 78.000000
Train, loss=0.01732417: 2117it [49:15,  1.30s/it]2017-06-02 15:04:15,359 root  INFO     step 2117.000000 - time: 1.496959, loss: 0.011328, perplexity: 1.011392, precision: 1.000000, batch_len: 142.000000
Train, loss=0.01132781: 2118it [49:17,  1.37s/it]2017-06-02 15:04:16,845 root  INFO     step 2118.000000 - time: 1.403267, loss: 0.005507, perplexity: 1.005523, precision: 0.984375, batch_len: 132.000000
Train, loss=0.00550739: 2119it [49:18,  1.41s/it]2017-06-02 15:04:18,245 root  INFO     step 2119.000000 - time: 1.255736, loss: 0.018780, perplexity: 1.018957, precision: 0.921875, batch_len: 122.000000
Train, loss=0.01877973: 2120it [49:20,  1.41s/it]2017-06-02 15:04:19,807 root  INFO     step 2120.000000 - time: 1.555716, loss: 0.006138, perplexity: 1.006157, precision: 1.000000, batch_len: 138.000000
Train, loss=0.00613838: 2121it [49:21,  1.45s/it]2017-06-02 15:04:21,477 root  INFO     step 2121.000000 - time: 1.544029, loss: 0.004230, perplexity: 1.004239, precision: 0.984375, batch_len: 134.000000
Train, loss=0.00422997: 2122it [49:23,  1.52s/it]2017-06-02 15:04:22,522 root  INFO     step 2122.000000 - time: 0.985815, loss: 0.019584, perplexity: 1.019777, precision: 0.968750, batch_len: 76.000000
Train, loss=0.01958365: 2123it [49:24,  1.38s/it]2017-06-02 15:04:24,389 root  INFO     step 2123.000000 - time: 1.840155, loss: 0.011502, perplexity: 1.011569, precision: 0.968750, batch_len: 150.000000
Train, loss=0.01150233: 2124it [49:26,  1.52s/it]2017-06-02 15:04:25,920 root  INFO     step 2124.000000 - time: 1.385187, loss: 0.006389, perplexity: 1.006409, precision: 0.984375, batch_len: 131.000000
Train, loss=0.00638874: 2125it [49:27,  1.53s/it]2017-06-02 15:04:27,171 root  INFO     step 2125.000000 - time: 1.212159, loss: 0.020126, perplexity: 1.020330, precision: 0.984375, batch_len: 72.000000
Train, loss=0.02012610: 2126it [49:29,  1.44s/it]2017-06-02 15:04:29,462 root  INFO     step 2126.000000 - time: 2.260184, loss: 0.008862, perplexity: 1.008902, precision: 0.984375, batch_len: 152.000000
Train, loss=0.00886217: 2127it [49:31,  1.70s/it]2017-06-02 15:04:30,698 root  INFO     step 2127.000000 - time: 1.072354, loss: 0.015821, perplexity: 1.015946, precision: 0.953125, batch_len: 77.000000
Train, loss=0.01582056: 2128it [49:32,  1.56s/it]2017-06-02 15:04:32,284 root  INFO     step 2128.000000 - time: 1.463799, loss: 0.018842, perplexity: 1.019021, precision: 0.968750, batch_len: 139.000000
Train, loss=0.01884224: 2129it [49:34,  1.57s/it]2017-06-02 15:04:33,415 root  INFO     step 2129.000000 - time: 0.926672, loss: 0.005753, perplexity: 1.005769, precision: 1.000000, batch_len: 71.000000
Train, loss=0.00575277: 2130it [49:35,  1.44s/it]
input_tensor dim: (?, 1, 32, ?)
CNN outdim before squeeze: (?, 1, ?, 512)
CNN outdim: (?, ?, 512)
using GRU CELL in decoder
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:02:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3c56350
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:04:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3c5a190
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:83:00.0
Total memory: 11.17GiB
Free memory: 2.04GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3c5dfd0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: 
name: Tesla K40c
major: 3 minor: 5 memoryClockRate (GHz) 0.745
pciBusID 0000:84:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K40c, pci bus id: 0000:04:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K40c, pci bus id: 0000:83:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K40c, pci bus id: 0000:84:00.0)
2017-06-02 16:49:29,611 root  INFO     loading data
2017-06-02 16:49:29,655 root  INFO     phase: train
2017-06-02 16:49:29,655 root  INFO     model_dir: model_01_16
2017-06-02 16:49:29,656 root  INFO     load_model: True
2017-06-02 16:49:29,656 root  INFO     output_dir: results
2017-06-02 16:49:29,656 root  INFO     steps_per_checkpoint: 2000
2017-06-02 16:49:29,656 root  INFO     batch_size: 64
2017-06-02 16:49:29,656 root  INFO     num_epoch: 30
2017-06-02 16:49:29,656 root  INFO     learning_rate: 1
2017-06-02 16:49:29,656 root  INFO     reg_val: 0
2017-06-02 16:49:29,656 root  INFO     max_gradient_norm: 5.000000
2017-06-02 16:49:29,656 root  INFO     clip_gradients: True
2017-06-02 16:49:29,656 root  INFO     valid_target_length inf
2017-06-02 16:49:29,656 root  INFO     target_vocab_size: 39
2017-06-02 16:49:29,656 root  INFO     target_embedding_size: 10.000000
2017-06-02 16:49:29,656 root  INFO     attn_num_hidden: 256
2017-06-02 16:49:29,656 root  INFO     attn_num_layers: 2
2017-06-02 16:49:29,656 root  INFO     visualize: True
2017-06-02 16:49:29,657 root  INFO     buckets
2017-06-02 16:49:29,657 root  INFO     [(16, 11), (27, 17), (35, 19), (64, 22), (80, 32)]
2017-06-02 16:49:29,657 root  INFO     ues GRU in the decoder.
2017-06-02 16:50:45,523 root  INFO     Reading model parameters from model_01_16/translate.ckpt-4000
Train: :   0%|          | 0/156 [00:00<?, ?it/s]2017-06-02 16:51:08,689 root  INFO     Generating first batch)
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.40GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=1325 evicted_count=1000 eviction_rate=0.754717 and unsatisfied allocation rate=0.913892
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.39GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.13GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.40GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.34GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.07GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2017-06-02 16:51:17,831 root  INFO     step 0.000000 - time: 6.985547, loss: 0.040995, perplexity: 1.041847, precision: 0.921875, batch_len: 96.000000
Train, loss=0.04099516:   1%|          | 1/156 [00:09<23:38,  9.15s/it]2017-06-02 16:51:22,717 root  INFO     step 1.000000 - time: 4.231743, loss: 0.034180, perplexity: 1.034771, precision: 0.875000, batch_len: 104.000000
Train, loss=0.03418025:   1%|1         | 2/156 [00:14<20:11,  7.87s/it]2017-06-02 16:51:26,887 root  INFO     step 2.000000 - time: 4.133644, loss: 0.012498, perplexity: 1.012576, precision: 0.953125, batch_len: 110.000000
Train, loss=0.01249778:   2%|1         | 3/156 [00:18<17:14,  6.76s/it]2017-06-02 16:51:32,745 root  INFO     step 3.000000 - time: 5.725601, loss: 0.024111, perplexity: 1.024404, precision: 0.937500, batch_len: 128.000000
Train, loss=0.02411108:   3%|2         | 4/156 [00:24<16:26,  6.49s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1014 evicted_count=1000 eviction_rate=0.986193 and unsatisfied allocation rate=0
2017-06-02 16:51:37,120 root  INFO     step 4.000000 - time: 4.212401, loss: 0.004673, perplexity: 1.004684, precision: 1.000000, batch_len: 101.000000
Train, loss=0.00467294:   3%|3         | 5/156 [00:28<14:44,  5.85s/it]2017-06-02 16:51:41,034 root  INFO     step 5.000000 - time: 3.817218, loss: 0.030790, perplexity: 1.031269, precision: 0.937500, batch_len: 92.000000
Train, loss=0.03078997:   4%|3         | 6/156 [00:32<13:10,  5.27s/it]2017-06-02 16:51:42,676 root  INFO     step 6.000000 - time: 1.540243, loss: 0.049497, perplexity: 1.050743, precision: 0.890625, batch_len: 105.000000
Train, loss=0.04949737:   4%|4         | 7/156 [00:33<10:23,  4.18s/it]2017-06-02 16:51:46,786 root  INFO     step 7.000000 - time: 4.076987, loss: 0.032166, perplexity: 1.032689, precision: 0.968750, batch_len: 113.000000
Train, loss=0.03216589:   5%|5         | 8/156 [00:38<10:15,  4.16s/it]2017-06-02 16:51:48,528 root  INFO     step 8.000000 - time: 1.595179, loss: 0.011681, perplexity: 1.011749, precision: 0.984375, batch_len: 103.000000
Train, loss=0.01168078:   6%|5         | 9/156 [00:39<08:25,  3.44s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=2479 evicted_count=2000 eviction_rate=0.806777 and unsatisfied allocation rate=0.824723
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 233 to 256
2017-06-02 16:51:50,164 root  INFO     step 9.000000 - time: 1.608632, loss: 0.044959, perplexity: 1.045985, precision: 0.843750, batch_len: 93.000000
Train, loss=0.04495931:   6%|6         | 10/156 [00:41<07:02,  2.90s/it]2017-06-02 16:51:53,979 root  INFO     step 10.000000 - time: 3.643000, loss: 0.053878, perplexity: 1.055356, precision: 0.937500, batch_len: 80.000000
Train, loss=0.05387778:   7%|7         | 11/156 [00:45<07:39,  3.17s/it]2017-06-02 16:51:58,032 root  INFO     step 11.000000 - time: 4.024095, loss: 0.091388, perplexity: 1.095694, precision: 0.859375, batch_len: 88.000000
Train, loss=0.09138811:   8%|7         | 12/156 [00:49<08:14,  3.44s/it]2017-06-02 16:51:59,691 root  INFO     step 12.000000 - time: 1.620063, loss: 0.045081, perplexity: 1.046113, precision: 0.937500, batch_len: 106.000000
Train, loss=0.04508092:   8%|8         | 13/156 [00:51<06:55,  2.90s/it]2017-06-02 16:52:01,405 root  INFO     step 13.000000 - time: 1.675070, loss: 0.030491, perplexity: 1.030961, precision: 0.953125, batch_len: 108.000000
Train, loss=0.03049131:   9%|8         | 14/156 [00:52<06:01,  2.55s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=2630 evicted_count=2000 eviction_rate=0.760456 and unsatisfied allocation rate=0.771527
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 372 to 409
2017-06-02 16:52:02,966 root  INFO     step 14.000000 - time: 1.500666, loss: 0.040777, perplexity: 1.041620, precision: 0.921875, batch_len: 97.000000
Train, loss=0.04077681:  10%|9         | 15/156 [00:54<05:17,  2.25s/it]2017-06-02 16:52:07,456 root  INFO     step 15.000000 - time: 4.271062, loss: 0.015681, perplexity: 1.015804, precision: 0.968750, batch_len: 125.000000
Train, loss=0.01568062:  10%|#         | 16/156 [00:58<06:49,  2.92s/it]2017-06-02 16:52:08,611 root  INFO     step 16.000000 - time: 1.115226, loss: 0.022931, perplexity: 1.023196, precision: 0.937500, batch_len: 89.000000
Train, loss=0.02293076:  11%|#         | 17/156 [00:59<05:32,  2.39s/it]2017-06-02 16:52:10,219 root  INFO     step 17.000000 - time: 1.461020, loss: 0.056444, perplexity: 1.058067, precision: 0.921875, batch_len: 100.000000
Train, loss=0.05644356:  12%|#1        | 18/156 [01:01<04:57,  2.16s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1054 evicted_count=1000 eviction_rate=0.948767 and unsatisfied allocation rate=0
2017-06-02 16:52:11,975 root  INFO     step 18.000000 - time: 1.714685, loss: 0.033087, perplexity: 1.033640, precision: 0.859375, batch_len: 112.000000
Train, loss=0.03308682:  12%|#2        | 19/156 [01:03<04:39,  2.04s/it]2017-06-02 16:52:17,058 root  INFO     step 19.000000 - time: 5.045098, loss: 0.066925, perplexity: 1.069216, precision: 0.890625, batch_len: 90.000000
Train, loss=0.06692532:  13%|#2        | 20/156 [01:08<06:41,  2.95s/it]2017-06-02 16:52:21,091 root  INFO     step 20.000000 - time: 3.982702, loss: 0.073164, perplexity: 1.075907, precision: 0.859375, batch_len: 120.000000
Train, loss=0.07316439:  13%|#3        | 21/156 [01:12<07:22,  3.28s/it]2017-06-02 16:52:22,964 root  INFO     step 21.000000 - time: 1.861860, loss: 0.023755, perplexity: 1.024040, precision: 0.921875, batch_len: 124.000000
Train, loss=0.02375524:  14%|#4        | 22/156 [01:14<06:22,  2.85s/it]2017-06-02 16:52:24,719 root  INFO     step 22.000000 - time: 1.667643, loss: 0.036131, perplexity: 1.036792, precision: 0.937500, batch_len: 114.000000
Train, loss=0.03613116:  15%|#4        | 23/156 [01:16<05:35,  2.52s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2613 get requests, put_count=2175 evicted_count=1000 eviction_rate=0.45977 and unsatisfied allocation rate=0.580559
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 871 to 958
2017-06-02 16:52:26,128 root  INFO     step 23.000000 - time: 1.352273, loss: 0.041718, perplexity: 1.042601, precision: 0.906250, batch_len: 107.000000
Train, loss=0.04171807:  15%|#5        | 24/156 [01:17<04:49,  2.19s/it]2017-06-02 16:52:30,196 root  INFO     step 24.000000 - time: 4.015630, loss: 0.010711, perplexity: 1.010769, precision: 0.984375, batch_len: 117.000000
Train, loss=0.01071117:  16%|#6        | 25/156 [01:21<06:00,  2.75s/it]2017-06-02 16:52:31,868 root  INFO     step 25.000000 - time: 1.638626, loss: 0.033325, perplexity: 1.033886, precision: 0.937500, batch_len: 115.000000
Train, loss=0.03332462:  17%|#6        | 26/156 [01:23<05:15,  2.43s/it]2017-06-02 16:52:33,222 root  INFO     step 26.000000 - time: 1.289835, loss: 0.033670, perplexity: 1.034243, precision: 0.953125, batch_len: 102.000000
Train, loss=0.03367019:  17%|#7        | 27/156 [01:24<04:31,  2.11s/it]2017-06-02 16:52:34,639 root  INFO     step 27.000000 - time: 1.388569, loss: 0.040023, perplexity: 1.040834, precision: 0.937500, batch_len: 111.000000
Train, loss=0.04002251:  18%|#7        | 28/156 [01:25<04:03,  1.90s/it]2017-06-02 16:52:38,786 root  INFO     step 28.000000 - time: 4.126560, loss: 0.031101, perplexity: 1.031589, precision: 0.921875, batch_len: 86.000000
Train, loss=0.03110065:  19%|#8        | 29/156 [01:30<05:26,  2.57s/it]2017-06-02 16:52:40,097 root  INFO     step 29.000000 - time: 1.189924, loss: 0.042183, perplexity: 1.043085, precision: 0.906250, batch_len: 81.000000
Train, loss=0.04218253:  19%|#9        | 30/156 [01:31<04:36,  2.20s/it]2017-06-02 16:52:41,725 root  INFO     step 30.000000 - time: 1.609967, loss: 0.032172, perplexity: 1.032695, precision: 0.953125, batch_len: 116.000000
Train, loss=0.03217204:  20%|#9        | 31/156 [01:33<04:13,  2.02s/it]2017-06-02 16:52:42,999 root  INFO     step 31.000000 - time: 1.224999, loss: 0.031594, perplexity: 1.032098, precision: 0.953125, batch_len: 109.000000
Train, loss=0.03159369:  21%|##        | 32/156 [01:34<03:43,  1.80s/it]I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 5746 get requests, put_count=5367 evicted_count=1000 eviction_rate=0.186324 and unsatisfied allocation rate=0.269405
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1863 to 2049
2017-06-02 16:52:44,649 root  INFO     step 32.000000 - time: 1.621189, loss: 0.049756, perplexity: 1.051015, precision: 0.937500, batch_len: 91.000000
Train, loss=0.04975627:  21%|##1       | 33/156 [01:35<03:35,  1.75s/it]2017-06-02 16:52:46,074 root  INFO     step 33.000000 - time: 1.404635, loss: 0.020309, perplexity: 1.020517, precision: 0.984375, batch_len: 87.000000
Train, loss=0.02030893:  22%|##1       | 34/156 [01:37<03:22,  1.66s/it]2017-06-02 16:52:49,796 root  INFO     step 34.000000 - time: 3.614687, loss: 0.092489, perplexity: 1.096901, precision: 0.859375, batch_len: 79.000000
Train, loss=0.09248884:  22%|##2       | 35/156 [01:41<04:35,  2.28s/it]2017-06-02 16:52:52,179 root  INFO     step 35.000000 - time: 2.197973, loss: 0.023724, perplexity: 1.024007, precision: 0.953125, batch_len: 123.000000
Train, loss=0.02372364:  23%|##3       | 36/156 [01:43<04:36,  2.31s/it]2017-06-02 16:52:53,776 root  INFO     step 36.000000 - time: 1.562763, loss: 0.017862, perplexity: 1.018022, precision: 0.953125, batch_len: 98.000000
Train, loss=0.01786182:  24%|##3       | 37/156 [01:45<04:09,  2.09s/it]2017-06-02 16:52:55,694 root  INFO     step 37.000000 - time: 1.854512, loss: 0.013506, perplexity: 1.013598, precision: 0.968750, batch_len: 118.000000
Train, loss=0.01350604:  24%|##4       | 38/156 [01:47<04:00,  2.04s/it]2017-06-02 16:52:57,248 root  INFO     step 38.000000 - time: 1.504378, loss: 0.034541, perplexity: 1.035144, precision: 0.937500, batch_len: 94.000000
Train, loss=0.03454062:  25%|##5       | 39/156 [01:48<03:41,  1.90s/it]2017-06-02 16:52:59,119 root  INFO     step 39.000000 - time: 1.831079, loss: 0.013229, perplexity: 1.013316, precision: 0.937500, batch_len: 129.000000
Train, loss=0.01322855:  26%|##5       | 40/156 [01:50<03:39,  1.89s/it]2017-06-02 16:53:00,611 root  INFO     step 40.000000 - time: 1.405287, loss: 0.037713, perplexity: 1.038433, precision: 0.937500, batch_len: 85.000000
Train, loss=0.03771320:  26%|##6       | 41/156 [01:51<03:23,  1.77s/it]2017-06-02 16:53:02,472 root  INFO     step 41.000000 - time: 1.762179, loss: 0.025616, perplexity: 1.025947, precision: 0.937500, batch_len: 126.000000
Train, loss=0.02561571:  27%|##6       | 42/156 [01:53<03:24,  1.80s/it]2017-06-02 16:53:04,058 root  INFO     step 42.000000 - time: 1.425901, loss: 0.046085, perplexity: 1.047163, precision: 0.921875, batch_len: 83.000000
Train, loss=0.04608470:  28%|##7       | 43/156 [01:55<03:15,  1.73s/it]2017-06-02 16:53:06,041 root  INFO     step 43.000000 - time: 1.940448, loss: 0.058998, perplexity: 1.060773, precision: 0.937500, batch_len: 121.000000
Train, loss=0.05899786:  28%|##8       | 44/156 [01:57<03:22,  1.81s/it]2017-06-02 16:53:07,425 root  INFO     step 44.000000 - time: 1.236696, loss: 0.061983, perplexity: 1.063944, precision: 0.843750, batch_len: 84.000000
Train, loss=0.06198288:  29%|##8       | 45/156 [01:58<03:06,  1.68s/it]2017-06-02 16:53:11,560 root  INFO     step 45.000000 - time: 4.116357, loss: 0.065858, perplexity: 1.068075, precision: 0.843750, batch_len: 133.000000
Train, loss=0.06585807:  29%|##9       | 46/156 [02:02<04:25,  2.42s/it]2017-06-02 16:53:13,010 root  INFO     step 46.000000 - time: 1.445750, loss: 0.014929, perplexity: 1.015041, precision: 0.968750, batch_len: 82.000000
Train, loss=0.01492889:  30%|###       | 47/156 [02:04<03:51,  2.13s/it]2017-06-02 16:53:14,647 root  INFO     step 47.000000 - time: 1.624175, loss: 0.043593, perplexity: 1.044558, precision: 0.937500, batch_len: 119.000000
Train, loss=0.04359342:  31%|###       | 48/156 [02:05<03:33,  1.98s/it]2017-06-02 16:53:19,188 root  INFO     step 48.000000 - time: 4.340920, loss: 0.039035, perplexity: 1.039807, precision: 0.906250, batch_len: 144.000000
Train, loss=0.03903529:  31%|###1      | 49/156 [02:10<04:54,  2.75s/it]2017-06-02 16:53:23,584 root  INFO     step 49.000000 - time: 4.377655, loss: 0.035493, perplexity: 1.036130, precision: 0.953125, batch_len: 137.000000
Train, loss=0.03549305:  32%|###2      | 50/156 [02:14<05:43,  3.24s/it]2017-06-02 16:53:24,941 root  INFO     step 50.000000 - time: 1.063907, loss: 0.031607, perplexity: 1.032111, precision: 0.953125, batch_len: 78.000000
Train, loss=0.03160669:  33%|###2      | 51/156 [02:16<04:41,  2.68s/it]2017-06-02 16:53:26,156 root  INFO     step 51.000000 - time: 1.129000, loss: 0.015958, perplexity: 1.016086, precision: 0.953125, batch_len: 96.000000
Train, loss=0.01595805:  33%|###3      | 52/156 [02:17<03:52,  2.24s/it]2017-06-02 16:53:28,108 root  INFO     step 52.000000 - time: 1.904763, loss: 0.009860, perplexity: 1.009909, precision: 0.984375, batch_len: 136.000000
Train, loss=0.00986025:  34%|###3      | 53/156 [02:19<03:41,  2.15s/it]2017-06-02 16:53:29,994 root  INFO     step 53.000000 - time: 1.865982, loss: 0.010829, perplexity: 1.010888, precision: 0.968750, batch_len: 130.000000
Train, loss=0.01082932:  35%|###4      | 54/156 [02:21<03:31,  2.07s/it]2017-06-02 16:53:31,510 root  INFO     step 54.000000 - time: 1.379066, loss: 0.007460, perplexity: 1.007488, precision: 1.000000, batch_len: 99.000000
Train, loss=0.00746007:  35%|###5      | 55/156 [02:22<03:12,  1.91s/it]2017-06-02 16:53:32,986 root  INFO     step 55.000000 - time: 1.318448, loss: 0.032591, perplexity: 1.033128, precision: 0.953125, batch_len: 76.000000
Train, loss=0.03259107:  36%|###5      | 56/156 [02:24<02:57,  1.78s/it]2017-06-02 16:53:35,157 root  INFO     step 56.000000 - time: 2.025721, loss: 0.032234, perplexity: 1.032759, precision: 0.937500, batch_len: 135.000000
Train, loss=0.03223431:  37%|###6      | 57/156 [02:26<03:07,  1.90s/it]2017-06-02 16:53:39,988 root  INFO     step 57.000000 - time: 4.673739, loss: 0.007264, perplexity: 1.007290, precision: 0.968750, batch_len: 152.000000
Train, loss=0.00726363:  37%|###7      | 58/156 [02:31<04:32,  2.78s/it]2017-06-02 16:53:42,012 root  INFO     step 58.000000 - time: 2.014467, loss: 0.023335, perplexity: 1.023609, precision: 0.937500, batch_len: 139.000000
Train, loss=0.02333456:  38%|###7      | 59/156 [02:33<04:07,  2.55s/it]2017-06-02 16:53:43,743 root  INFO     step 59.000000 - time: 1.633910, loss: 0.007680, perplexity: 1.007710, precision: 1.000000, batch_len: 138.000000
Train, loss=0.00768026:  38%|###8      | 60/156 [02:35<03:41,  2.30s/it]2017-06-02 16:53:45,324 root  INFO     step 60.000000 - time: 1.568966, loss: 0.011961, perplexity: 1.012033, precision: 0.953125, batch_len: 122.000000
Train, loss=0.01196145:  39%|###9      | 61/156 [02:36<03:18,  2.09s/it]2017-06-02 16:53:50,297 root  INFO     step 61.000000 - time: 4.807519, loss: 0.021543, perplexity: 1.021777, precision: 0.968750, batch_len: 141.000000
Train, loss=0.02154344:  40%|###9      | 62/156 [02:41<04:37,  2.95s/it]2017-06-02 16:53:53,903 root  INFO     step 62.000000 - time: 3.593598, loss: 0.003656, perplexity: 1.003663, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00365641:  40%|####      | 63/156 [02:45<04:52,  3.15s/it]2017-06-02 16:53:55,629 root  INFO     step 63.000000 - time: 1.709735, loss: 0.014894, perplexity: 1.015005, precision: 0.984375, batch_len: 72.000000
Train, loss=0.01489385:  41%|####1     | 64/156 [02:46<04:10,  2.72s/it]2017-06-02 16:53:57,402 root  INFO     step 64.000000 - time: 1.673815, loss: 0.037268, perplexity: 1.037971, precision: 0.968750, batch_len: 134.000000
Train, loss=0.03726828:  42%|####1     | 65/156 [02:48<03:41,  2.44s/it]2017-06-02 16:53:58,962 root  INFO     step 65.000000 - time: 1.494134, loss: 0.032301, perplexity: 1.032828, precision: 0.953125, batch_len: 132.000000
Train, loss=0.03230115:  42%|####2     | 66/156 [02:50<03:15,  2.17s/it]2017-06-02 16:54:00,265 root  INFO     step 66.000000 - time: 1.124814, loss: 0.016475, perplexity: 1.016612, precision: 0.968750, batch_len: 77.000000
Train, loss=0.01647541:  43%|####2     | 67/156 [02:51<02:50,  1.91s/it]2017-06-02 16:54:02,018 root  INFO     step 67.000000 - time: 1.671377, loss: 0.003617, perplexity: 1.003623, precision: 1.000000, batch_len: 131.000000
Train, loss=0.00361677:  44%|####3     | 68/156 [02:53<02:44,  1.87s/it]2017-06-02 16:54:05,968 root  INFO     step 68.000000 - time: 3.749478, loss: 0.006911, perplexity: 1.006935, precision: 1.000000, batch_len: 71.000000
Train, loss=0.00691065:  44%|####4     | 69/156 [02:57<03:36,  2.49s/it]2017-06-02 16:54:10,922 root  INFO     step 69.000000 - time: 4.918732, loss: 0.015204, perplexity: 1.015320, precision: 0.921875, batch_len: 150.000000
Train, loss=0.01520417:  45%|####4     | 70/156 [03:02<04:37,  3.23s/it]2017-06-02 16:54:12,745 root  INFO     step 70.000000 - time: 1.800462, loss: 0.013843, perplexity: 1.013939, precision: 0.984375, batch_len: 142.000000
Train, loss=0.01384284:  46%|####5     | 71/156 [03:04<03:58,  2.81s/it]2017-06-02 16:54:12,879 root  INFO     Generating first batch)
2017-06-02 16:54:16,841 root  INFO     step 71.000000 - time: 1.089235, loss: 0.012436, perplexity: 1.012514, precision: 0.968750, batch_len: 101.000000
Train, loss=0.01243632:  46%|####6     | 72/156 [03:08<04:28,  3.19s/it]2017-06-02 16:54:17,882 root  INFO     step 72.000000 - time: 1.034125, loss: 0.053350, perplexity: 1.054799, precision: 0.937500, batch_len: 88.000000
Train, loss=0.05335033:  47%|####6     | 73/156 [03:09<03:31,  2.55s/it]2017-06-02 16:54:19,252 root  INFO     step 73.000000 - time: 1.209236, loss: 0.007551, perplexity: 1.007579, precision: 0.984375, batch_len: 128.000000
Train, loss=0.00755063:  47%|####7     | 74/156 [03:10<02:59,  2.19s/it]2017-06-02 16:54:20,629 root  INFO     step 74.000000 - time: 1.256936, loss: 0.017949, perplexity: 1.018112, precision: 0.968750, batch_len: 108.000000
Train, loss=0.01794949:  48%|####8     | 75/156 [03:11<02:37,  1.95s/it]2017-06-02 16:54:21,978 root  INFO     step 75.000000 - time: 1.290079, loss: 0.010129, perplexity: 1.010180, precision: 0.953125, batch_len: 110.000000
Train, loss=0.01012857:  49%|####8     | 76/156 [03:13<02:21,  1.77s/it]2017-06-02 16:54:23,046 root  INFO     step 76.000000 - time: 1.041436, loss: 0.021343, perplexity: 1.021572, precision: 0.906250, batch_len: 102.000000
Train, loss=0.02134252:  49%|####9     | 77/156 [03:14<02:03,  1.56s/it]2017-06-02 16:54:24,175 root  INFO     step 77.000000 - time: 1.087052, loss: 0.077918, perplexity: 1.081034, precision: 0.937500, batch_len: 100.000000
Train, loss=0.07791834:  50%|#####     | 78/156 [03:15<01:51,  1.43s/it]2017-06-02 16:54:25,215 root  INFO     step 78.000000 - time: 1.032368, loss: 0.027989, perplexity: 1.028385, precision: 0.921875, batch_len: 106.000000
Train, loss=0.02798923:  51%|#####     | 79/156 [03:16<01:41,  1.31s/it]2017-06-02 16:54:26,370 root  INFO     step 79.000000 - time: 1.111813, loss: 0.057504, perplexity: 1.059189, precision: 0.906250, batch_len: 105.000000
Train, loss=0.05750399:  51%|#####1    | 80/156 [03:17<01:36,  1.27s/it]2017-06-02 16:54:27,650 root  INFO     step 80.000000 - time: 1.210662, loss: 0.042618, perplexity: 1.043539, precision: 0.906250, batch_len: 96.000000
Train, loss=0.04261811:  52%|#####1    | 81/156 [03:18<01:35,  1.27s/it]2017-06-02 16:54:28,961 root  INFO     step 81.000000 - time: 1.207728, loss: 0.022767, perplexity: 1.023028, precision: 0.937500, batch_len: 113.000000
Train, loss=0.02276686:  53%|#####2    | 82/156 [03:20<01:34,  1.28s/it]2017-06-02 16:54:30,237 root  INFO     step 82.000000 - time: 1.260884, loss: 0.076795, perplexity: 1.079821, precision: 0.890625, batch_len: 93.000000
Train, loss=0.07679504:  53%|#####3    | 83/156 [03:21<01:33,  1.28s/it]2017-06-02 16:54:31,429 root  INFO     step 83.000000 - time: 1.119818, loss: 0.014061, perplexity: 1.014160, precision: 0.953125, batch_len: 114.000000
Train, loss=0.01406056:  54%|#####3    | 84/156 [03:22<01:30,  1.25s/it]2017-06-02 16:54:32,759 root  INFO     step 84.000000 - time: 1.312482, loss: 0.036048, perplexity: 1.036706, precision: 0.953125, batch_len: 112.000000
Train, loss=0.03604826:  54%|#####4    | 85/156 [03:24<01:30,  1.28s/it]2017-06-02 16:54:34,322 root  INFO     step 85.000000 - time: 1.529334, loss: 0.049707, perplexity: 1.050963, precision: 0.937500, batch_len: 90.000000
Train, loss=0.04970711:  55%|#####5    | 86/156 [03:25<01:35,  1.36s/it]2017-06-02 16:54:35,619 root  INFO     step 86.000000 - time: 1.156321, loss: 0.016489, perplexity: 1.016625, precision: 0.953125, batch_len: 104.000000
Train, loss=0.01648862:  56%|#####5    | 87/156 [03:26<01:32,  1.34s/it]2017-06-02 16:54:37,075 root  INFO     step 87.000000 - time: 1.348923, loss: 0.029323, perplexity: 1.029758, precision: 0.906250, batch_len: 109.000000
Train, loss=0.02932339:  56%|#####6    | 88/156 [03:28<01:33,  1.38s/it]2017-06-02 16:54:38,533 root  INFO     step 88.000000 - time: 1.352344, loss: 0.018423, perplexity: 1.018594, precision: 0.968750, batch_len: 120.000000
Train, loss=0.01842318:  57%|#####7    | 89/156 [03:29<01:33,  1.40s/it]2017-06-02 16:54:39,633 root  INFO     step 89.000000 - time: 1.079239, loss: 0.024798, perplexity: 1.025108, precision: 0.953125, batch_len: 97.000000
Train, loss=0.02479816:  58%|#####7    | 90/156 [03:30<01:26,  1.31s/it]2017-06-02 16:54:40,661 root  INFO     step 90.000000 - time: 1.007053, loss: 0.027517, perplexity: 1.027899, precision: 0.921875, batch_len: 85.000000
Train, loss=0.02751696:  58%|#####8    | 91/156 [03:31<01:19,  1.23s/it]2017-06-02 16:54:41,753 root  INFO     step 91.000000 - time: 0.966064, loss: 0.013633, perplexity: 1.013726, precision: 0.968750, batch_len: 89.000000
Train, loss=0.01363285:  59%|#####8    | 92/156 [03:33<01:15,  1.19s/it]2017-06-02 16:54:43,025 root  INFO     step 92.000000 - time: 1.237263, loss: 0.031382, perplexity: 1.031880, precision: 0.921875, batch_len: 86.000000
Train, loss=0.03138210:  60%|#####9    | 93/156 [03:34<01:16,  1.21s/it]2017-06-02 16:54:44,236 root  INFO     step 93.000000 - time: 1.154163, loss: 0.021019, perplexity: 1.021241, precision: 0.984375, batch_len: 92.000000
Train, loss=0.02101886:  60%|######    | 94/156 [03:35<01:15,  1.21s/it]2017-06-02 16:54:45,655 root  INFO     step 94.000000 - time: 1.338290, loss: 0.022568, perplexity: 1.022825, precision: 0.968750, batch_len: 121.000000
Train, loss=0.02256803:  61%|######    | 95/156 [03:36<01:17,  1.27s/it]2017-06-02 16:54:46,788 root  INFO     step 95.000000 - time: 0.972879, loss: 0.012632, perplexity: 1.012712, precision: 0.968750, batch_len: 87.000000
Train, loss=0.01263184:  62%|######1   | 96/156 [03:38<01:13,  1.23s/it]2017-06-02 16:54:47,920 root  INFO     step 96.000000 - time: 1.065322, loss: 0.028133, perplexity: 1.028533, precision: 0.953125, batch_len: 111.000000
Train, loss=0.02813349:  62%|######2   | 97/156 [03:39<01:10,  1.20s/it]2017-06-02 16:54:49,186 root  INFO     step 97.000000 - time: 1.232207, loss: 0.046164, perplexity: 1.047246, precision: 0.937500, batch_len: 107.000000
Train, loss=0.04616409:  63%|######2   | 98/156 [03:40<01:10,  1.22s/it]2017-06-02 16:54:50,753 root  INFO     step 98.000000 - time: 1.521912, loss: 0.015628, perplexity: 1.015751, precision: 0.937500, batch_len: 125.000000
Train, loss=0.01562792:  63%|######3   | 99/156 [03:42<01:15,  1.32s/it]2017-06-02 16:54:51,835 root  INFO     step 99.000000 - time: 1.016060, loss: 0.024398, perplexity: 1.024698, precision: 0.968750, batch_len: 103.000000
Train, loss=0.02439753:  64%|######4   | 100/156 [03:43<01:10,  1.25s/it]2017-06-02 16:54:53,278 root  INFO     step 100.000000 - time: 1.348869, loss: 0.011320, perplexity: 1.011384, precision: 0.953125, batch_len: 117.000000
Train, loss=0.01131997:  65%|######4   | 101/156 [03:44<01:12,  1.31s/it]2017-06-02 16:54:54,204 root  INFO     step 101.000000 - time: 0.882788, loss: 0.028650, perplexity: 1.029064, precision: 0.953125, batch_len: 81.000000
Train, loss=0.02864959:  65%|######5   | 102/156 [03:45<01:04,  1.19s/it]2017-06-02 16:54:55,963 root  INFO     step 102.000000 - time: 1.626304, loss: 0.028598, perplexity: 1.029011, precision: 0.968750, batch_len: 137.000000
Train, loss=0.02859776:  66%|######6   | 103/156 [03:47<01:12,  1.36s/it]2017-06-02 16:54:57,473 root  INFO     step 103.000000 - time: 1.454757, loss: 0.020187, perplexity: 1.020392, precision: 0.937500, batch_len: 129.000000
Train, loss=0.02018687:  67%|######6   | 104/156 [03:48<01:13,  1.41s/it]2017-06-02 16:54:58,941 root  INFO     step 104.000000 - time: 1.348958, loss: 0.012372, perplexity: 1.012449, precision: 0.937500, batch_len: 116.000000
Train, loss=0.01237168:  67%|######7   | 105/156 [03:50<01:12,  1.43s/it]2017-06-02 16:55:00,275 root  INFO     step 105.000000 - time: 1.328787, loss: 0.035052, perplexity: 1.035674, precision: 0.953125, batch_len: 135.000000
Train, loss=0.03505246:  68%|######7   | 106/156 [03:51<01:09,  1.40s/it]2017-06-02 16:55:01,346 root  INFO     step 106.000000 - time: 1.010574, loss: 0.027864, perplexity: 1.028256, precision: 0.921875, batch_len: 115.000000
Train, loss=0.02786423:  69%|######8   | 107/156 [03:52<01:03,  1.30s/it]2017-06-02 16:55:02,556 root  INFO     step 107.000000 - time: 1.195452, loss: 0.033797, perplexity: 1.034374, precision: 0.968750, batch_len: 94.000000
Train, loss=0.03379669:  69%|######9   | 108/156 [03:53<01:01,  1.27s/it]2017-06-02 16:55:03,734 root  INFO     step 108.000000 - time: 1.126460, loss: 0.036677, perplexity: 1.037358, precision: 0.921875, batch_len: 79.000000
Train, loss=0.03667713:  70%|######9   | 109/156 [03:55<00:58,  1.24s/it]2017-06-02 16:55:05,115 root  INFO     step 109.000000 - time: 1.304203, loss: 0.026483, perplexity: 1.026837, precision: 0.953125, batch_len: 123.000000
Train, loss=0.02648344:  71%|#######   | 110/156 [03:56<00:59,  1.29s/it]2017-06-02 16:55:06,549 root  INFO     step 110.000000 - time: 1.377105, loss: 0.005403, perplexity: 1.005418, precision: 1.000000, batch_len: 124.000000
Train, loss=0.00540328:  71%|#######1  | 111/156 [03:57<00:59,  1.33s/it]2017-06-02 16:55:07,445 root  INFO     step 111.000000 - time: 0.891698, loss: 0.034491, perplexity: 1.035093, precision: 0.906250, batch_len: 80.000000
Train, loss=0.03449085:  72%|#######1  | 112/156 [03:58<00:52,  1.20s/it]2017-06-02 16:55:08,466 root  INFO     step 112.000000 - time: 1.015543, loss: 0.065191, perplexity: 1.067363, precision: 0.921875, batch_len: 83.000000
Train, loss=0.06519106:  72%|#######2  | 113/156 [03:59<00:49,  1.15s/it]2017-06-02 16:55:09,754 root  INFO     step 113.000000 - time: 1.270103, loss: 0.083709, perplexity: 1.087313, precision: 0.859375, batch_len: 98.000000
Train, loss=0.08370932:  73%|#######3  | 114/156 [04:01<00:49,  1.19s/it]2017-06-02 16:55:10,921 root  INFO     step 114.000000 - time: 1.049251, loss: 0.088735, perplexity: 1.092791, precision: 0.859375, batch_len: 91.000000
Train, loss=0.08873506:  74%|#######3  | 115/156 [04:02<00:48,  1.18s/it]2017-06-02 16:55:12,127 root  INFO     step 115.000000 - time: 1.030366, loss: 0.109335, perplexity: 1.115536, precision: 0.796875, batch_len: 84.000000
Train, loss=0.10933491:  74%|#######4  | 116/156 [04:03<00:47,  1.19s/it]2017-06-02 16:55:13,558 root  INFO     step 116.000000 - time: 1.380555, loss: 0.038002, perplexity: 1.038733, precision: 0.875000, batch_len: 136.000000
Train, loss=0.03800198:  75%|#######5  | 117/156 [04:04<00:49,  1.26s/it]2017-06-02 16:55:14,882 root  INFO     step 117.000000 - time: 1.312896, loss: 0.034572, perplexity: 1.035177, precision: 0.859375, batch_len: 118.000000
Train, loss=0.03457239:  76%|#######5  | 118/156 [04:06<00:48,  1.28s/it]2017-06-02 16:55:16,655 root  INFO     step 118.000000 - time: 1.683926, loss: 0.041265, perplexity: 1.042129, precision: 0.906250, batch_len: 119.000000
Train, loss=0.04126534:  76%|#######6  | 119/156 [04:07<00:52,  1.43s/it]2017-06-02 16:55:17,725 root  INFO     step 119.000000 - time: 1.055534, loss: 0.036681, perplexity: 1.037362, precision: 0.906250, batch_len: 99.000000
Train, loss=0.03668110:  77%|#######6  | 120/156 [04:09<00:47,  1.32s/it]2017-06-02 16:55:19,244 root  INFO     step 120.000000 - time: 1.411234, loss: 0.024112, perplexity: 1.024405, precision: 0.890625, batch_len: 126.000000
Train, loss=0.02411183:  78%|#######7  | 121/156 [04:10<00:48,  1.38s/it]2017-06-02 16:55:20,795 root  INFO     step 121.000000 - time: 1.349385, loss: 0.013045, perplexity: 1.013130, precision: 0.968750, batch_len: 144.000000
Train, loss=0.01304497:  78%|#######8  | 122/156 [04:12<00:48,  1.43s/it]2017-06-02 16:55:22,593 root  INFO     step 122.000000 - time: 1.439042, loss: 0.024709, perplexity: 1.025017, precision: 0.937500, batch_len: 122.000000
Train, loss=0.02470916:  79%|#######8  | 123/156 [04:13<00:50,  1.54s/it]2017-06-02 16:55:23,838 root  INFO     step 123.000000 - time: 1.210632, loss: 0.030834, perplexity: 1.031314, precision: 0.906250, batch_len: 82.000000
Train, loss=0.03083361:  79%|#######9  | 124/156 [04:15<00:46,  1.45s/it]2017-06-02 16:55:25,357 root  INFO     step 124.000000 - time: 1.405430, loss: 0.021177, perplexity: 1.021403, precision: 0.968750, batch_len: 130.000000
Train, loss=0.02117683:  80%|########  | 125/156 [04:16<00:45,  1.47s/it]2017-06-02 16:55:27,319 root  INFO     step 125.000000 - time: 1.799265, loss: 0.026540, perplexity: 1.026895, precision: 0.890625, batch_len: 152.000000
Train, loss=0.02653972:  81%|########  | 126/156 [04:18<00:48,  1.62s/it]2017-06-02 16:55:28,860 root  INFO     step 126.000000 - time: 1.529762, loss: 0.017985, perplexity: 1.018148, precision: 0.968750, batch_len: 133.000000
Train, loss=0.01798544:  81%|########1 | 127/156 [04:20<00:46,  1.60s/it]2017-06-02 16:55:30,628 root  INFO     step 127.000000 - time: 1.634301, loss: 0.034691, perplexity: 1.035300, precision: 0.906250, batch_len: 141.000000
Train, loss=0.03469114:  82%|########2 | 128/156 [04:21<00:46,  1.65s/it]2017-06-02 16:55:31,958 root  INFO     step 128.000000 - time: 1.226811, loss: 0.047091, perplexity: 1.048218, precision: 0.953125, batch_len: 96.000000
Train, loss=0.04709119:  83%|########2 | 129/156 [04:23<00:41,  1.55s/it]2017-06-02 16:55:33,026 root  INFO     step 129.000000 - time: 0.995840, loss: 0.036972, perplexity: 1.037664, precision: 0.953125, batch_len: 78.000000
Train, loss=0.03697161:  83%|########3 | 130/156 [04:24<00:36,  1.41s/it]2017-06-02 16:55:34,492 root  INFO     step 130.000000 - time: 1.440843, loss: 0.026108, perplexity: 1.026452, precision: 0.937500, batch_len: 138.000000
Train, loss=0.02610833:  84%|########3 | 131/156 [04:25<00:35,  1.42s/it]2017-06-02 16:55:35,766 root  INFO     step 131.000000 - time: 1.250671, loss: 0.033925, perplexity: 1.034507, precision: 0.921875, batch_len: 134.000000
Train, loss=0.03392470:  85%|########4 | 132/156 [04:27<00:33,  1.38s/it]2017-06-02 16:55:36,902 root  INFO     step 132.000000 - time: 1.089473, loss: 0.055859, perplexity: 1.057448, precision: 0.875000, batch_len: 76.000000
Train, loss=0.05585875:  85%|########5 | 133/156 [04:28<00:30,  1.31s/it]2017-06-02 16:55:38,300 root  INFO     step 133.000000 - time: 1.187268, loss: 0.029626, perplexity: 1.030069, precision: 0.921875, batch_len: 77.000000
Train, loss=0.02962577:  86%|########5 | 134/156 [04:29<00:29,  1.33s/it]2017-06-02 16:55:39,663 root  INFO     step 134.000000 - time: 1.289237, loss: 0.027469, perplexity: 1.027850, precision: 0.906250, batch_len: 132.000000
Train, loss=0.02746902:  87%|########6 | 135/156 [04:30<00:28,  1.34s/it]2017-06-02 16:55:40,909 root  INFO     step 135.000000 - time: 1.183890, loss: 0.030348, perplexity: 1.030813, precision: 0.937500, batch_len: 72.000000
Train, loss=0.03034816:  87%|########7 | 136/156 [04:32<00:26,  1.31s/it]2017-06-02 16:55:42,738 root  INFO     step 136.000000 - time: 1.718683, loss: 0.015715, perplexity: 1.015839, precision: 0.953125, batch_len: 150.000000
Train, loss=0.01571453:  88%|########7 | 137/156 [04:34<00:27,  1.47s/it]2017-06-02 16:55:43,751 root  INFO     step 137.000000 - time: 1.008615, loss: 0.005221, perplexity: 1.005234, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00522065:  88%|########8 | 138/156 [04:35<00:23,  1.33s/it]2017-06-02 16:55:45,023 root  INFO     step 138.000000 - time: 1.122075, loss: 0.020441, perplexity: 1.020651, precision: 0.937500, batch_len: 71.000000
Train, loss=0.02044052:  89%|########9 | 139/156 [04:36<00:22,  1.31s/it]2017-06-02 16:55:46,476 root  INFO     step 139.000000 - time: 1.444196, loss: 0.013888, perplexity: 1.013985, precision: 0.968750, batch_len: 139.000000
Train, loss=0.01388771:  90%|########9 | 140/156 [04:37<00:21,  1.36s/it]2017-06-02 16:55:48,080 root  INFO     step 140.000000 - time: 1.424037, loss: 0.021780, perplexity: 1.022019, precision: 0.968750, batch_len: 142.000000
Train, loss=0.02177995:  90%|######### | 141/156 [04:39<00:21,  1.43s/it]2017-06-02 16:55:49,453 root  INFO     step 141.000000 - time: 1.311901, loss: 0.040514, perplexity: 1.041346, precision: 0.968750, batch_len: 131.000000
Train, loss=0.04051365:  91%|#########1| 142/156 [04:40<00:19,  1.41s/it]2017-06-02 16:55:49,655 root  INFO     Generating first batch)
2017-06-02 16:55:53,189 root  INFO     step 142.000000 - time: 1.057132, loss: 0.013130, perplexity: 1.013216, precision: 0.953125, batch_len: 96.000000
Train, loss=0.01312953:  92%|#########1| 143/156 [04:44<00:27,  2.11s/it]2017-06-02 16:55:55,439 root  INFO     step 143.000000 - time: 1.505179, loss: 0.017556, perplexity: 1.017711, precision: 0.968750, batch_len: 120.000000
Train, loss=0.01755647:  92%|#########2| 144/156 [04:46<00:25,  2.15s/it]2017-06-02 16:55:56,672 root  INFO     step 144.000000 - time: 1.178601, loss: 0.031787, perplexity: 1.032297, precision: 0.984375, batch_len: 92.000000
Train, loss=0.03178686:  93%|#########2| 145/156 [04:47<00:20,  1.88s/it]2017-06-02 16:55:57,905 root  INFO     step 145.000000 - time: 1.020279, loss: 0.036987, perplexity: 1.037679, precision: 0.906250, batch_len: 102.000000
Train, loss=0.03698675:  94%|#########3| 146/156 [04:49<00:16,  1.68s/it]2017-06-02 16:55:59,084 root  INFO     step 146.000000 - time: 1.097009, loss: 0.064295, perplexity: 1.066407, precision: 0.906250, batch_len: 100.000000
Train, loss=0.06429513:  94%|#########4| 147/156 [04:50<00:13,  1.53s/it]2017-06-02 16:56:00,269 root  INFO     step 147.000000 - time: 1.175001, loss: 0.016036, perplexity: 1.016165, precision: 0.968750, batch_len: 128.000000
Train, loss=0.01603622:  95%|#########4| 148/156 [04:51<00:11,  1.43s/it]2017-06-02 16:56:01,485 root  INFO     step 148.000000 - time: 1.209220, loss: 0.045876, perplexity: 1.046945, precision: 0.875000, batch_len: 110.000000
Train, loss=0.04587622:  96%|#########5| 149/156 [04:52<00:09,  1.36s/it]2017-06-02 16:56:02,800 root  INFO     step 149.000000 - time: 1.246759, loss: 0.038652, perplexity: 1.039408, precision: 0.875000, batch_len: 88.000000
Train, loss=0.03865154:  96%|#########6| 150/156 [04:54<00:08,  1.35s/it]2017-06-02 16:56:03,970 root  INFO     step 150.000000 - time: 1.132970, loss: 0.052368, perplexity: 1.053763, precision: 0.843750, batch_len: 113.000000
Train, loss=0.05236798:  97%|#########6| 151/156 [04:55<00:06,  1.30s/it]2017-06-02 16:56:05,344 root  INFO     step 151.000000 - time: 1.359508, loss: 0.022811, perplexity: 1.023073, precision: 0.968750, batch_len: 124.000000
Train, loss=0.02281055:  97%|#########7| 152/156 [04:56<00:05,  1.32s/it]2017-06-02 16:56:06,379 root  INFO     step 152.000000 - time: 0.988776, loss: 0.063470, perplexity: 1.065527, precision: 0.890625, batch_len: 93.000000
Train, loss=0.06346954:  98%|#########8| 153/156 [04:57<00:03,  1.23s/it]2017-06-02 16:56:07,765 root  INFO     step 153.000000 - time: 1.313239, loss: 0.020807, perplexity: 1.021025, precision: 0.968750, batch_len: 121.000000
Train, loss=0.02080684:  99%|#########8| 154/156 [04:59<00:02,  1.28s/it]2017-06-02 16:56:09,158 root  INFO     step 154.000000 - time: 1.373608, loss: 0.019304, perplexity: 1.019491, precision: 0.937500, batch_len: 104.000000
Train, loss=0.01930393:  99%|#########9| 155/156 [05:00<00:01,  1.31s/it]2017-06-02 16:56:10,390 root  INFO     step 155.000000 - time: 1.141236, loss: 0.023113, perplexity: 1.023383, precision: 0.953125, batch_len: 112.000000
Train, loss=0.02311346: 100%|##########| 156/156 [05:01<00:00,  1.29s/it]2017-06-02 16:56:11,434 root  INFO     step 156.000000 - time: 1.017365, loss: 0.034333, perplexity: 1.034929, precision: 0.921875, batch_len: 86.000000
Train, loss=0.03433316: 157it [05:02,  1.22s/it]                         2017-06-02 16:56:12,650 root  INFO     step 157.000000 - time: 1.078956, loss: 0.016068, perplexity: 1.016198, precision: 0.953125, batch_len: 101.000000
Train, loss=0.01606790: 158it [05:03,  1.22s/it]2017-06-02 16:56:13,696 root  INFO     step 158.000000 - time: 1.026392, loss: 0.047971, perplexity: 1.049141, precision: 0.875000, batch_len: 111.000000
Train, loss=0.04797141: 159it [05:05,  1.16s/it]2017-06-02 16:56:15,045 root  INFO     step 159.000000 - time: 1.337242, loss: 0.018142, perplexity: 1.018307, precision: 0.937500, batch_len: 108.000000
Train, loss=0.01814182: 160it [05:06,  1.22s/it]2017-06-02 16:56:16,384 root  INFO     step 160.000000 - time: 1.254835, loss: 0.040596, perplexity: 1.041431, precision: 0.906250, batch_len: 114.000000
Train, loss=0.04059584: 161it [05:07,  1.26s/it]2017-06-02 16:56:17,479 root  INFO     step 161.000000 - time: 1.092462, loss: 0.039702, perplexity: 1.040501, precision: 0.906250, batch_len: 106.000000
Train, loss=0.03970233: 162it [05:08,  1.21s/it]2017-06-02 16:56:18,450 root  INFO     step 162.000000 - time: 0.926613, loss: 0.044035, perplexity: 1.045019, precision: 0.937500, batch_len: 81.000000
Train, loss=0.04403539: 163it [05:09,  1.14s/it]2017-06-02 16:56:19,525 root  INFO     step 163.000000 - time: 1.051683, loss: 0.028095, perplexity: 1.028494, precision: 0.906250, batch_len: 97.000000
Train, loss=0.02809541: 164it [05:10,  1.12s/it]2017-06-02 16:56:20,606 root  INFO     step 164.000000 - time: 1.042045, loss: 0.038492, perplexity: 1.039242, precision: 0.906250, batch_len: 105.000000
Train, loss=0.03849187: 165it [05:11,  1.11s/it]2017-06-02 16:56:22,356 root  INFO     step 165.000000 - time: 1.743675, loss: 0.147589, perplexity: 1.159036, precision: 0.828125, batch_len: 90.000000
Train, loss=0.14758886: 166it [05:13,  1.30s/it]2017-06-02 16:56:23,742 root  INFO     step 166.000000 - time: 1.315881, loss: 0.054790, perplexity: 1.056319, precision: 0.843750, batch_len: 109.000000
Train, loss=0.05478976: 167it [05:15,  1.33s/it]2017-06-02 16:56:25,279 root  INFO     step 167.000000 - time: 1.348370, loss: 0.045260, perplexity: 1.046300, precision: 0.890625, batch_len: 117.000000
Train, loss=0.04526041: 168it [05:16,  1.39s/it]2017-06-02 16:56:26,543 root  INFO     step 168.000000 - time: 1.236460, loss: 0.091549, perplexity: 1.095871, precision: 0.843750, batch_len: 91.000000
Train, loss=0.09154925: 169it [05:17,  1.35s/it]2017-06-02 16:56:27,570 root  INFO     step 169.000000 - time: 0.983407, loss: 0.054036, perplexity: 1.055522, precision: 0.859375, batch_len: 85.000000
Train, loss=0.05403587: 170it [05:18,  1.25s/it]2017-06-02 16:56:28,795 root  INFO     step 170.000000 - time: 1.133917, loss: 0.042006, perplexity: 1.042901, precision: 0.921875, batch_len: 103.000000
Train, loss=0.04200584: 171it [05:20,  1.25s/it]2017-06-02 16:56:29,957 root  INFO     step 171.000000 - time: 1.148889, loss: 0.169178, perplexity: 1.184331, precision: 0.812500, batch_len: 89.000000
Train, loss=0.16917805: 172it [05:21,  1.22s/it]2017-06-02 16:56:31,277 root  INFO     step 172.000000 - time: 1.297212, loss: 0.237280, perplexity: 1.267796, precision: 0.734375, batch_len: 87.000000
Train, loss=0.23727983: 173it [05:22,  1.25s/it]2017-06-02 16:56:32,670 root  INFO     step 173.000000 - time: 1.348937, loss: 0.107402, perplexity: 1.113382, precision: 0.765625, batch_len: 116.000000
Train, loss=0.10740227: 174it [05:23,  1.29s/it]2017-06-02 16:56:33,748 root  INFO     step 174.000000 - time: 0.952205, loss: 0.101177, perplexity: 1.106472, precision: 0.781250, batch_len: 79.000000
Train, loss=0.10117698: 175it [05:25,  1.23s/it]2017-06-02 16:56:35,106 root  INFO     step 175.000000 - time: 1.269074, loss: 0.075196, perplexity: 1.078096, precision: 0.875000, batch_len: 123.000000
Train, loss=0.07519624: 176it [05:26,  1.27s/it]2017-06-02 16:56:35,965 root  INFO     step 176.000000 - time: 0.790328, loss: 0.059281, perplexity: 1.061073, precision: 0.906250, batch_len: 80.000000
Train, loss=0.05928055: 177it [05:27,  1.14s/it]2017-06-02 16:56:37,548 root  INFO     step 177.000000 - time: 1.463535, loss: 0.021174, perplexity: 1.021400, precision: 0.953125, batch_len: 129.000000
Train, loss=0.02117421: 178it [05:28,  1.28s/it]2017-06-02 16:56:38,780 root  INFO     step 178.000000 - time: 1.210723, loss: 0.092318, perplexity: 1.096714, precision: 0.921875, batch_len: 94.000000
Train, loss=0.09231845: 179it [05:30,  1.26s/it]2017-06-02 16:56:39,901 root  INFO     step 179.000000 - time: 1.089645, loss: 0.036730, perplexity: 1.037413, precision: 0.921875, batch_len: 107.000000
Train, loss=0.03673006: 180it [05:31,  1.22s/it]2017-06-02 16:56:41,257 root  INFO     step 180.000000 - time: 1.346207, loss: 0.034962, perplexity: 1.035580, precision: 0.953125, batch_len: 133.000000
Train, loss=0.03496174: 181it [05:32,  1.26s/it]2017-06-02 16:56:42,365 root  INFO     step 181.000000 - time: 0.961835, loss: 0.034607, perplexity: 1.035212, precision: 0.937500, batch_len: 99.000000
Train, loss=0.03460656: 182it [05:33,  1.22s/it]2017-06-02 16:56:44,139 root  INFO     step 182.000000 - time: 1.715281, loss: 0.038823, perplexity: 1.039587, precision: 0.906250, batch_len: 144.000000
Train, loss=0.03882313: 183it [05:35,  1.38s/it]2017-06-02 16:56:45,752 root  INFO     step 183.000000 - time: 1.535964, loss: 0.111235, perplexity: 1.117658, precision: 0.875000, batch_len: 126.000000
Train, loss=0.11123527: 184it [05:37,  1.45s/it]2017-06-02 16:56:47,193 root  INFO     step 184.000000 - time: 1.365594, loss: 0.054468, perplexity: 1.055978, precision: 0.875000, batch_len: 125.000000
Train, loss=0.05446781: 185it [05:38,  1.45s/it]2017-06-02 16:56:48,332 root  INFO     step 185.000000 - time: 1.014343, loss: 0.037291, perplexity: 1.037995, precision: 0.906250, batch_len: 98.000000
Train, loss=0.03729126: 186it [05:39,  1.36s/it]2017-06-02 16:56:50,035 root  INFO     step 186.000000 - time: 1.473991, loss: 0.039538, perplexity: 1.040330, precision: 0.921875, batch_len: 137.000000
Train, loss=0.03953769: 187it [05:41,  1.46s/it]2017-06-02 16:56:51,718 root  INFO     step 187.000000 - time: 1.653008, loss: 0.016621, perplexity: 1.016759, precision: 0.968750, batch_len: 136.000000
Train, loss=0.01662057: 188it [05:43,  1.53s/it]2017-06-02 16:56:52,821 root  INFO     step 188.000000 - time: 1.054100, loss: 0.046760, perplexity: 1.047871, precision: 0.890625, batch_len: 84.000000
Train, loss=0.04676033: 189it [05:44,  1.40s/it]2017-06-02 16:56:53,963 root  INFO     step 189.000000 - time: 1.134079, loss: 0.044246, perplexity: 1.045240, precision: 0.890625, batch_len: 115.000000
Train, loss=0.04424626: 190it [05:45,  1.32s/it]2017-06-02 16:56:55,005 root  INFO     step 190.000000 - time: 1.001236, loss: 0.096985, perplexity: 1.101844, precision: 0.906250, batch_len: 83.000000
Train, loss=0.09698538: 191it [05:46,  1.24s/it]2017-06-02 16:56:56,274 root  INFO     step 191.000000 - time: 1.245555, loss: 0.006499, perplexity: 1.006520, precision: 1.000000, batch_len: 135.000000
Train, loss=0.00649908: 192it [05:47,  1.25s/it]2017-06-02 16:56:58,097 root  INFO     step 192.000000 - time: 1.652297, loss: 0.095337, perplexity: 1.100030, precision: 0.859375, batch_len: 119.000000
Train, loss=0.09533717: 193it [05:49,  1.42s/it]2017-06-02 16:56:59,266 root  INFO     step 193.000000 - time: 0.959461, loss: 0.061033, perplexity: 1.062934, precision: 0.906250, batch_len: 78.000000
Train, loss=0.06103317: 194it [05:50,  1.34s/it]2017-06-02 16:57:00,706 root  INFO     step 194.000000 - time: 1.377381, loss: 0.012473, perplexity: 1.012551, precision: 0.968750, batch_len: 118.000000
Train, loss=0.01247337: 195it [05:52,  1.37s/it]2017-06-02 16:57:01,800 root  INFO     step 195.000000 - time: 0.907275, loss: 0.040557, perplexity: 1.041391, precision: 0.921875, batch_len: 82.000000
Train, loss=0.04055703: 196it [05:53,  1.29s/it]2017-06-02 16:57:03,478 root  INFO     step 196.000000 - time: 1.576215, loss: 0.056248, perplexity: 1.057860, precision: 0.921875, batch_len: 130.000000
Train, loss=0.05624834: 197it [05:54,  1.41s/it]2017-06-02 16:57:04,814 root  INFO     step 197.000000 - time: 1.191299, loss: 0.021543, perplexity: 1.021777, precision: 0.937500, batch_len: 96.000000
Train, loss=0.02154329: 198it [05:56,  1.39s/it]2017-06-02 16:57:06,324 root  INFO     step 198.000000 - time: 1.450065, loss: 0.019174, perplexity: 1.019359, precision: 0.921875, batch_len: 138.000000
Train, loss=0.01917392: 199it [05:57,  1.42s/it]2017-06-02 16:57:07,905 root  INFO     step 199.000000 - time: 1.423555, loss: 0.050701, perplexity: 1.052008, precision: 0.921875, batch_len: 139.000000
Train, loss=0.05070069: 200it [05:59,  1.47s/it]2017-06-02 16:57:10,028 root  INFO     step 200.000000 - time: 1.938378, loss: 0.017434, perplexity: 1.017587, precision: 0.937500, batch_len: 152.000000
Train, loss=0.01743425: 201it [06:01,  1.67s/it]2017-06-02 16:57:11,216 root  INFO     step 201.000000 - time: 1.093182, loss: 0.009967, perplexity: 1.010017, precision: 0.968750, batch_len: 74.000000
Train, loss=0.00996707: 202it [06:02,  1.52s/it]2017-06-02 16:57:12,528 root  INFO     step 202.000000 - time: 1.292210, loss: 0.039537, perplexity: 1.040329, precision: 0.906250, batch_len: 122.000000
Train, loss=0.03953739: 203it [06:03,  1.46s/it]2017-06-02 16:57:13,599 root  INFO     step 203.000000 - time: 0.927225, loss: 0.093243, perplexity: 1.097729, precision: 0.875000, batch_len: 76.000000
Train, loss=0.09324314: 204it [06:04,  1.34s/it]2017-06-02 16:57:14,884 root  INFO     step 204.000000 - time: 1.244194, loss: 0.018629, perplexity: 1.018803, precision: 0.953125, batch_len: 134.000000
Train, loss=0.01862858: 205it [06:06,  1.33s/it]2017-06-02 16:57:16,374 root  INFO     step 205.000000 - time: 1.410056, loss: 0.038629, perplexity: 1.039384, precision: 0.937500, batch_len: 132.000000
Train, loss=0.03862854: 206it [06:07,  1.37s/it]2017-06-02 16:57:18,092 root  INFO     step 206.000000 - time: 1.628673, loss: 0.107836, perplexity: 1.113866, precision: 0.828125, batch_len: 141.000000
Train, loss=0.10783642: 207it [06:09,  1.48s/it]2017-06-02 16:57:19,331 root  INFO     step 207.000000 - time: 1.205207, loss: 0.048299, perplexity: 1.049485, precision: 0.890625, batch_len: 72.000000
Train, loss=0.04829929: 208it [06:10,  1.41s/it]2017-06-02 16:57:21,223 root  INFO     step 208.000000 - time: 1.865261, loss: 0.024593, perplexity: 1.024897, precision: 0.937500, batch_len: 150.000000
Train, loss=0.02459253: 209it [06:12,  1.55s/it]2017-06-02 16:57:22,540 root  INFO     step 209.000000 - time: 1.249874, loss: 0.049162, perplexity: 1.050391, precision: 0.875000, batch_len: 142.000000
Train, loss=0.04916243: 210it [06:13,  1.48s/it]2017-06-02 16:57:23,687 root  INFO     step 210.000000 - time: 1.141083, loss: 0.016104, perplexity: 1.016235, precision: 1.000000, batch_len: 71.000000
Train, loss=0.01610446: 211it [06:14,  1.38s/it]2017-06-02 16:57:24,863 root  INFO     step 211.000000 - time: 1.111897, loss: 0.022514, perplexity: 1.022770, precision: 0.921875, batch_len: 77.000000
Train, loss=0.02251415: 212it [06:16,  1.32s/it]2017-06-02 16:57:26,452 root  INFO     step 212.000000 - time: 1.469318, loss: 0.009950, perplexity: 1.010000, precision: 0.968750, batch_len: 131.000000
Train, loss=0.00995036: 213it [06:17,  1.40s/it]2017-06-02 16:57:26,815 root  INFO     Generating first batch)
2017-06-02 16:57:30,920 root  INFO     step 213.000000 - time: 1.173695, loss: 0.008700, perplexity: 1.008738, precision: 0.984375, batch_len: 96.000000
Train, loss=0.00869966: 214it [06:22,  2.32s/it]2017-06-02 16:57:32,380 root  INFO     step 214.000000 - time: 1.130228, loss: 0.024876, perplexity: 1.025188, precision: 0.937500, batch_len: 101.000000
Train, loss=0.02487611: 215it [06:23,  2.06s/it]2017-06-02 16:57:33,585 root  INFO     step 215.000000 - time: 1.059362, loss: 0.029682, perplexity: 1.030127, precision: 0.937500, batch_len: 102.000000
Train, loss=0.02968204: 216it [06:24,  1.81s/it]2017-06-02 16:57:35,039 root  INFO     step 216.000000 - time: 1.260871, loss: 0.034782, perplexity: 1.035394, precision: 0.875000, batch_len: 121.000000
Train, loss=0.03478247: 217it [06:26,  1.70s/it]2017-06-02 16:57:36,397 root  INFO     step 217.000000 - time: 1.282240, loss: 0.018957, perplexity: 1.019137, precision: 0.953125, batch_len: 110.000000
Train, loss=0.01895668: 218it [06:27,  1.60s/it]2017-06-02 16:57:37,711 root  INFO     step 218.000000 - time: 1.282224, loss: 0.038234, perplexity: 1.038975, precision: 0.937500, batch_len: 113.000000
Train, loss=0.03823422: 219it [06:29,  1.51s/it]2017-06-02 16:57:38,849 root  INFO     step 219.000000 - time: 0.996072, loss: 0.085123, perplexity: 1.088851, precision: 0.875000, batch_len: 93.000000
Train, loss=0.08512307: 220it [06:30,  1.40s/it]2017-06-02 16:57:40,007 root  INFO     step 220.000000 - time: 1.064722, loss: 0.045163, perplexity: 1.046198, precision: 0.875000, batch_len: 106.000000
Train, loss=0.04516259: 221it [06:31,  1.33s/it]2017-06-02 16:57:41,033 root  INFO     step 221.000000 - time: 0.993100, loss: 0.040630, perplexity: 1.041466, precision: 0.921875, batch_len: 107.000000
Train, loss=0.04062979: 222it [06:32,  1.24s/it]2017-06-02 16:57:42,095 root  INFO     step 222.000000 - time: 1.053341, loss: 0.024472, perplexity: 1.024774, precision: 0.937500, batch_len: 105.000000
Train, loss=0.02447186: 223it [06:33,  1.18s/it]2017-06-02 16:57:43,425 root  INFO     step 223.000000 - time: 1.319298, loss: 0.016726, perplexity: 1.016867, precision: 0.937500, batch_len: 109.000000
Train, loss=0.01672614: 224it [06:34,  1.23s/it]2017-06-02 16:57:44,835 root  INFO     step 224.000000 - time: 1.382197, loss: 0.022292, perplexity: 1.022543, precision: 0.937500, batch_len: 128.000000
Train, loss=0.02229234: 225it [06:36,  1.28s/it]2017-06-02 16:57:45,984 root  INFO     step 225.000000 - time: 1.102166, loss: 0.045759, perplexity: 1.046822, precision: 0.875000, batch_len: 104.000000
Train, loss=0.04575866: 226it [06:37,  1.24s/it]2017-06-02 16:57:46,973 root  INFO     step 226.000000 - time: 0.979571, loss: 0.021555, perplexity: 1.021789, precision: 0.953125, batch_len: 90.000000
Train, loss=0.02155533: 227it [06:38,  1.17s/it]2017-06-02 16:57:48,037 root  INFO     step 227.000000 - time: 1.019041, loss: 0.020214, perplexity: 1.020420, precision: 0.953125, batch_len: 108.000000
Train, loss=0.02021388: 228it [06:39,  1.14s/it]2017-06-02 16:57:49,217 root  INFO     step 228.000000 - time: 1.161448, loss: 0.005378, perplexity: 1.005392, precision: 0.984375, batch_len: 92.000000
Train, loss=0.00537787: 229it [06:40,  1.15s/it]2017-06-02 16:57:50,624 root  INFO     step 229.000000 - time: 1.370757, loss: 0.024594, perplexity: 1.024899, precision: 0.906250, batch_len: 100.000000
Train, loss=0.02459382: 230it [06:41,  1.23s/it]2017-06-02 16:57:51,836 root  INFO     step 230.000000 - time: 1.148329, loss: 0.024638, perplexity: 1.024945, precision: 0.937500, batch_len: 111.000000
Train, loss=0.02463850: 231it [06:43,  1.22s/it]2017-06-02 16:57:52,916 root  INFO     step 231.000000 - time: 1.031720, loss: 0.055225, perplexity: 1.056779, precision: 0.906250, batch_len: 88.000000
Train, loss=0.05522515: 232it [06:44,  1.18s/it]2017-06-02 16:57:54,012 root  INFO     step 232.000000 - time: 1.035661, loss: 0.043544, perplexity: 1.044506, precision: 0.906250, batch_len: 97.000000
Train, loss=0.04354417: 233it [06:45,  1.15s/it]2017-06-02 16:57:55,057 root  INFO     step 233.000000 - time: 0.949866, loss: 0.018889, perplexity: 1.019068, precision: 0.953125, batch_len: 103.000000
Train, loss=0.01888892: 234it [06:46,  1.12s/it]2017-06-02 16:57:56,676 root  INFO     step 234.000000 - time: 1.592801, loss: 0.018143, perplexity: 1.018308, precision: 0.984375, batch_len: 120.000000
Train, loss=0.01814255: 235it [06:47,  1.27s/it]2017-06-02 16:57:58,024 root  INFO     step 235.000000 - time: 1.300157, loss: 0.037017, perplexity: 1.037711, precision: 0.906250, batch_len: 114.000000
Train, loss=0.03701738: 236it [06:49,  1.29s/it]2017-06-02 16:57:58,972 root  INFO     step 236.000000 - time: 0.938245, loss: 0.048650, perplexity: 1.049853, precision: 0.906250, batch_len: 81.000000
Train, loss=0.04864998: 237it [06:50,  1.19s/it]2017-06-02 16:58:00,355 root  INFO     step 237.000000 - time: 1.350061, loss: 0.046561, perplexity: 1.047662, precision: 0.875000, batch_len: 112.000000
Train, loss=0.04656117: 238it [06:51,  1.25s/it]2017-06-02 16:58:01,702 root  INFO     step 238.000000 - time: 1.254483, loss: 0.019576, perplexity: 1.019769, precision: 0.953125, batch_len: 124.000000
Train, loss=0.01957567: 239it [06:53,  1.28s/it]2017-06-02 16:58:03,044 root  INFO     step 239.000000 - time: 1.209134, loss: 0.048126, perplexity: 1.049303, precision: 0.859375, batch_len: 85.000000
Train, loss=0.04812649: 240it [06:54,  1.30s/it]2017-06-02 16:58:04,718 root  INFO     step 240.000000 - time: 1.508663, loss: 0.015674, perplexity: 1.015798, precision: 0.937500, batch_len: 118.000000
Train, loss=0.01567447: 241it [06:56,  1.41s/it]2017-06-02 16:58:05,773 root  INFO     step 241.000000 - time: 1.020295, loss: 0.128205, perplexity: 1.136786, precision: 0.937500, batch_len: 89.000000
Train, loss=0.12820475: 242it [06:57,  1.30s/it]2017-06-02 16:58:06,972 root  INFO     step 242.000000 - time: 1.070491, loss: 0.169660, perplexity: 1.184901, precision: 0.796875, batch_len: 94.000000
Train, loss=0.16965955: 243it [06:58,  1.27s/it]2017-06-02 16:58:07,892 root  INFO     step 243.000000 - time: 0.862447, loss: 0.070410, perplexity: 1.072948, precision: 0.859375, batch_len: 80.000000
Train, loss=0.07041016: 244it [06:59,  1.17s/it]2017-06-02 16:58:09,300 root  INFO     step 244.000000 - time: 1.365136, loss: 0.142516, perplexity: 1.153171, precision: 0.781250, batch_len: 91.000000
Train, loss=0.14251564: 245it [07:00,  1.24s/it]2017-06-02 16:58:10,639 root  INFO     step 245.000000 - time: 1.305432, loss: 0.058239, perplexity: 1.059968, precision: 0.859375, batch_len: 115.000000
Train, loss=0.05823875: 246it [07:01,  1.27s/it]2017-06-02 16:58:12,009 root  INFO     step 246.000000 - time: 1.345927, loss: 0.105672, perplexity: 1.111458, precision: 0.921875, batch_len: 117.000000
Train, loss=0.10567249: 247it [07:03,  1.30s/it]2017-06-02 16:58:13,142 root  INFO     step 247.000000 - time: 0.971139, loss: 0.094133, perplexity: 1.098706, precision: 0.921875, batch_len: 86.000000
Train, loss=0.09413345: 248it [07:04,  1.25s/it]2017-06-02 16:58:14,465 root  INFO     step 248.000000 - time: 1.204608, loss: 0.068178, perplexity: 1.070556, precision: 0.890625, batch_len: 123.000000
Train, loss=0.06817797: 249it [07:05,  1.27s/it]2017-06-02 16:58:16,072 root  INFO     step 249.000000 - time: 1.472190, loss: 0.077733, perplexity: 1.080834, precision: 0.859375, batch_len: 125.000000
Train, loss=0.07773284: 250it [07:07,  1.37s/it]2017-06-02 16:58:17,323 root  INFO     step 250.000000 - time: 1.198895, loss: 0.060278, perplexity: 1.062132, precision: 0.796875, batch_len: 98.000000
Train, loss=0.06027815: 251it [07:08,  1.34s/it]2017-06-02 16:58:18,451 root  INFO     step 251.000000 - time: 1.028100, loss: 0.062038, perplexity: 1.064002, precision: 0.812500, batch_len: 84.000000
Train, loss=0.06203764: 252it [07:09,  1.27s/it]2017-06-02 16:58:19,877 root  INFO     step 252.000000 - time: 1.329970, loss: 0.042090, perplexity: 1.042988, precision: 0.890625, batch_len: 116.000000
Train, loss=0.04209004: 253it [07:11,  1.32s/it]2017-06-02 16:58:20,871 root  INFO     step 253.000000 - time: 0.957010, loss: 0.083215, perplexity: 1.086776, precision: 0.859375, batch_len: 83.000000
Train, loss=0.08321531: 254it [07:12,  1.22s/it]2017-06-02 16:58:22,560 root  INFO     step 254.000000 - time: 1.630026, loss: 0.040626, perplexity: 1.041462, precision: 0.937500, batch_len: 137.000000
Train, loss=0.04062556: 255it [07:13,  1.36s/it]2017-06-02 16:58:23,869 root  INFO     step 255.000000 - time: 1.214909, loss: 0.023225, perplexity: 1.023497, precision: 0.968750, batch_len: 99.000000
Train, loss=0.02322474: 256it [07:15,  1.35s/it]2017-06-02 16:58:25,245 root  INFO     step 256.000000 - time: 1.361272, loss: 0.045337, perplexity: 1.046380, precision: 0.921875, batch_len: 119.000000
Train, loss=0.04533651: 257it [07:16,  1.36s/it]2017-06-02 16:58:26,617 root  INFO     step 257.000000 - time: 1.078783, loss: 0.056018, perplexity: 1.057617, precision: 0.890625, batch_len: 87.000000
Train, loss=0.05601836: 258it [07:17,  1.36s/it]2017-06-02 16:58:27,545 root  INFO     step 258.000000 - time: 0.903798, loss: 0.059505, perplexity: 1.061311, precision: 0.875000, batch_len: 79.000000
Train, loss=0.05950483: 259it [07:18,  1.23s/it]2017-06-02 16:58:29,239 root  INFO     step 259.000000 - time: 1.670247, loss: 0.030655, perplexity: 1.031130, precision: 0.937500, batch_len: 144.000000
Train, loss=0.03065548: 260it [07:20,  1.37s/it]2017-06-02 16:58:30,714 root  INFO     step 260.000000 - time: 1.414659, loss: 0.022908, perplexity: 1.023172, precision: 0.953125, batch_len: 129.000000
Train, loss=0.02290775: 261it [07:22,  1.40s/it]2017-06-02 16:58:32,353 root  INFO     step 261.000000 - time: 1.562555, loss: 0.046145, perplexity: 1.047227, precision: 0.953125, batch_len: 135.000000
Train, loss=0.04614542: 262it [07:23,  1.47s/it]2017-06-02 16:58:33,878 root  INFO     step 262.000000 - time: 1.427039, loss: 0.024299, perplexity: 1.024597, precision: 0.953125, batch_len: 130.000000
Train, loss=0.02429917: 263it [07:25,  1.49s/it]2017-06-02 16:58:34,876 root  INFO     step 263.000000 - time: 0.990269, loss: 0.018874, perplexity: 1.019053, precision: 0.984375, batch_len: 82.000000
Train, loss=0.01887380: 264it [07:26,  1.34s/it]2017-06-02 16:58:36,205 root  INFO     step 264.000000 - time: 1.312936, loss: 0.035247, perplexity: 1.035876, precision: 0.921875, batch_len: 141.000000
Train, loss=0.03524740: 265it [07:27,  1.34s/it]2017-06-02 16:58:37,771 root  INFO     step 265.000000 - time: 1.471031, loss: 0.028963, perplexity: 1.029387, precision: 0.937500, batch_len: 133.000000
Train, loss=0.02896318: 266it [07:29,  1.41s/it]2017-06-02 16:58:39,052 root  INFO     step 266.000000 - time: 1.118458, loss: 0.029353, perplexity: 1.029788, precision: 0.937500, batch_len: 78.000000
Train, loss=0.02935293: 267it [07:30,  1.37s/it]2017-06-02 16:58:40,523 root  INFO     step 267.000000 - time: 1.462310, loss: 0.018786, perplexity: 1.018964, precision: 0.937500, batch_len: 126.000000
Train, loss=0.01878611: 268it [07:31,  1.40s/it]2017-06-02 16:58:42,159 root  INFO     step 268.000000 - time: 1.399881, loss: 0.011112, perplexity: 1.011174, precision: 0.968750, batch_len: 136.000000
Train, loss=0.01111190: 269it [07:33,  1.47s/it]2017-06-02 16:58:43,101 root  INFO     step 269.000000 - time: 0.841132, loss: 0.005631, perplexity: 1.005647, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00563093: 270it [07:34,  1.31s/it]2017-06-02 16:58:44,758 root  INFO     step 270.000000 - time: 1.518561, loss: 0.027142, perplexity: 1.027514, precision: 0.937500, batch_len: 122.000000
Train, loss=0.02714184: 271it [07:36,  1.42s/it]2017-06-02 16:58:46,336 root  INFO     step 271.000000 - time: 1.477266, loss: 0.045476, perplexity: 1.046526, precision: 0.953125, batch_len: 138.000000
Train, loss=0.04547587: 272it [07:37,  1.46s/it]2017-06-02 16:58:47,300 root  INFO     step 272.000000 - time: 0.944262, loss: 0.101551, perplexity: 1.106886, precision: 0.968750, batch_len: 77.000000
Train, loss=0.10155080: 273it [07:38,  1.31s/it]2017-06-02 16:58:49,205 root  INFO     step 273.000000 - time: 1.896049, loss: 0.046562, perplexity: 1.047663, precision: 0.921875, batch_len: 152.000000
Train, loss=0.04656183: 274it [07:40,  1.49s/it]2017-06-02 16:58:50,311 root  INFO     step 274.000000 - time: 1.082496, loss: 0.021429, perplexity: 1.021660, precision: 0.937500, batch_len: 96.000000
Train, loss=0.02142900: 275it [07:41,  1.38s/it]2017-06-02 16:58:52,076 root  INFO     step 275.000000 - time: 1.705597, loss: 0.014239, perplexity: 1.014341, precision: 0.953125, batch_len: 132.000000
Train, loss=0.01423884: 276it [07:43,  1.49s/it]2017-06-02 16:58:53,526 root  INFO     step 276.000000 - time: 1.427942, loss: 0.020325, perplexity: 1.020533, precision: 0.953125, batch_len: 139.000000
Train, loss=0.02032524: 277it [07:44,  1.48s/it]2017-06-02 16:58:54,862 root  INFO     step 277.000000 - time: 1.291864, loss: 0.015511, perplexity: 1.015632, precision: 0.968750, batch_len: 134.000000
Train, loss=0.01551081: 278it [07:46,  1.44s/it]2017-06-02 16:58:55,866 root  INFO     step 278.000000 - time: 0.865601, loss: 0.038501, perplexity: 1.039252, precision: 0.921875, batch_len: 76.000000
Train, loss=0.03850139: 279it [07:47,  1.31s/it]2017-06-02 16:58:58,124 root  INFO     step 279.000000 - time: 2.086842, loss: 0.014483, perplexity: 1.014589, precision: 0.953125, batch_len: 150.000000
Train, loss=0.01448344: 280it [07:49,  1.59s/it]2017-06-02 16:58:59,339 root  INFO     step 280.000000 - time: 1.093608, loss: 0.007181, perplexity: 1.007207, precision: 0.984375, batch_len: 71.000000
Train, loss=0.00718138: 281it [07:50,  1.48s/it]2017-06-02 16:59:00,813 root  INFO     step 281.000000 - time: 1.427270, loss: 0.020451, perplexity: 1.020661, precision: 0.953125, batch_len: 142.000000
Train, loss=0.02045091: 282it [07:52,  1.48s/it]2017-06-02 16:59:02,077 root  INFO     step 282.000000 - time: 1.158783, loss: 0.022644, perplexity: 1.022902, precision: 0.953125, batch_len: 72.000000
Train, loss=0.02264360: 283it [07:53,  1.41s/it]2017-06-02 16:59:03,555 root  INFO     step 283.000000 - time: 1.258274, loss: 0.006977, perplexity: 1.007001, precision: 1.000000, batch_len: 131.000000
Train, loss=0.00697702: 284it [07:54,  1.43s/it]2017-06-02 16:59:03,634 root  INFO     Generating first batch)
2017-06-02 16:59:07,289 root  INFO     step 284.000000 - time: 1.044148, loss: 0.006481, perplexity: 1.006502, precision: 0.984375, batch_len: 96.000000
Train, loss=0.00648091: 285it [07:58,  2.12s/it]2017-06-02 16:59:08,562 root  INFO     step 285.000000 - time: 1.041298, loss: 0.033631, perplexity: 1.034203, precision: 0.921875, batch_len: 92.000000
Train, loss=0.03363148: 286it [07:59,  1.87s/it]2017-06-02 16:59:09,827 root  INFO     step 286.000000 - time: 1.238852, loss: 0.043323, perplexity: 1.044276, precision: 0.906250, batch_len: 93.000000
Train, loss=0.04332336: 287it [08:01,  1.69s/it]2017-06-02 16:59:11,086 root  INFO     step 287.000000 - time: 1.139147, loss: 0.012567, perplexity: 1.012646, precision: 0.984375, batch_len: 104.000000
Train, loss=0.01256707: 288it [08:02,  1.56s/it]2017-06-02 16:59:12,261 root  INFO     step 288.000000 - time: 1.072513, loss: 0.016984, perplexity: 1.017129, precision: 0.968750, batch_len: 101.000000
Train, loss=0.01698387: 289it [08:03,  1.44s/it]2017-06-02 16:59:13,359 root  INFO     step 289.000000 - time: 1.088189, loss: 0.035092, perplexity: 1.035715, precision: 0.906250, batch_len: 105.000000
Train, loss=0.03509183: 290it [08:04,  1.34s/it]2017-06-02 16:59:14,685 root  INFO     step 290.000000 - time: 1.221971, loss: 0.037758, perplexity: 1.038480, precision: 0.968750, batch_len: 128.000000
Train, loss=0.03775848: 291it [08:05,  1.34s/it]2017-06-02 16:59:16,064 root  INFO     step 291.000000 - time: 1.330914, loss: 0.023691, perplexity: 1.023974, precision: 0.937500, batch_len: 113.000000
Train, loss=0.02369147: 292it [08:07,  1.35s/it]2017-06-02 16:59:17,585 root  INFO     step 292.000000 - time: 1.220103, loss: 0.033324, perplexity: 1.033885, precision: 0.953125, batch_len: 114.000000
Train, loss=0.03332373: 293it [08:08,  1.40s/it]2017-06-02 16:59:18,739 root  INFO     step 293.000000 - time: 1.147579, loss: 0.012240, perplexity: 1.012315, precision: 0.968750, batch_len: 110.000000
Train, loss=0.01224009: 294it [08:10,  1.33s/it]2017-06-02 16:59:19,942 root  INFO     step 294.000000 - time: 1.044538, loss: 0.013962, perplexity: 1.014060, precision: 0.937500, batch_len: 100.000000
Train, loss=0.01396214: 295it [08:11,  1.29s/it]2017-06-02 16:59:20,908 root  INFO     step 295.000000 - time: 0.943435, loss: 0.024736, perplexity: 1.025045, precision: 0.906250, batch_len: 97.000000
Train, loss=0.02473642: 296it [08:12,  1.19s/it]2017-06-02 16:59:22,222 root  INFO     step 296.000000 - time: 1.231295, loss: 0.032876, perplexity: 1.033422, precision: 0.937500, batch_len: 89.000000
Train, loss=0.03287604: 297it [08:13,  1.23s/it]2017-06-02 16:59:23,480 root  INFO     step 297.000000 - time: 1.219008, loss: 0.092692, perplexity: 1.097124, precision: 0.921875, batch_len: 88.000000
Train, loss=0.09269241: 298it [08:14,  1.24s/it]2017-06-02 16:59:24,476 root  INFO     step 298.000000 - time: 0.950205, loss: 0.087811, perplexity: 1.091782, precision: 0.875000, batch_len: 81.000000
Train, loss=0.08781104: 299it [08:15,  1.17s/it]2017-06-02 16:59:25,605 root  INFO     step 299.000000 - time: 1.081661, loss: 0.033783, perplexity: 1.034361, precision: 0.953125, batch_len: 107.000000
Train, loss=0.03378341: 300it [08:16,  1.15s/it]2017-06-02 16:59:27,051 root  INFO     step 300.000000 - time: 1.442960, loss: 0.037812, perplexity: 1.038536, precision: 0.906250, batch_len: 120.000000
Train, loss=0.03781182: 301it [08:18,  1.24s/it]2017-06-02 16:59:28,161 root  INFO     step 301.000000 - time: 1.101873, loss: 0.029709, perplexity: 1.030155, precision: 0.968750, batch_len: 106.000000
Train, loss=0.02970940: 302it [08:19,  1.20s/it]2017-06-02 16:59:29,438 root  INFO     step 302.000000 - time: 1.217646, loss: 0.016427, perplexity: 1.016562, precision: 0.968750, batch_len: 112.000000
Train, loss=0.01642683: 303it [08:20,  1.22s/it]2017-06-02 16:59:30,821 root  INFO     step 303.000000 - time: 1.349457, loss: 0.073344, perplexity: 1.076101, precision: 0.859375, batch_len: 91.000000
Train, loss=0.07334433: 304it [08:22,  1.27s/it]2017-06-02 16:59:32,180 root  INFO     step 304.000000 - time: 1.309469, loss: 0.024142, perplexity: 1.024436, precision: 0.921875, batch_len: 111.000000
Train, loss=0.02414211: 305it [08:23,  1.30s/it]2017-06-02 16:59:33,249 root  INFO     step 305.000000 - time: 0.967753, loss: 0.069852, perplexity: 1.072350, precision: 0.937500, batch_len: 90.000000
Train, loss=0.06985240: 306it [08:24,  1.23s/it]2017-06-02 16:59:34,237 root  INFO     step 306.000000 - time: 0.940005, loss: 0.018026, perplexity: 1.018190, precision: 0.937500, batch_len: 80.000000
Train, loss=0.01802628: 307it [08:25,  1.16s/it]2017-06-02 16:59:35,359 root  INFO     step 307.000000 - time: 0.969900, loss: 0.033272, perplexity: 1.033832, precision: 0.937500, batch_len: 86.000000
Train, loss=0.03327243: 308it [08:26,  1.15s/it]2017-06-02 16:59:36,384 root  INFO     step 308.000000 - time: 0.917194, loss: 0.047100, perplexity: 1.048227, precision: 0.875000, batch_len: 102.000000
Train, loss=0.04710008: 309it [08:27,  1.11s/it]2017-06-02 16:59:37,660 root  INFO     step 309.000000 - time: 1.242919, loss: 0.036161, perplexity: 1.036823, precision: 0.921875, batch_len: 115.000000
Train, loss=0.03616138: 310it [08:28,  1.16s/it]2017-06-02 16:59:38,982 root  INFO     step 310.000000 - time: 1.288780, loss: 0.037111, perplexity: 1.037808, precision: 0.921875, batch_len: 108.000000
Train, loss=0.03711091: 311it [08:30,  1.21s/it]2017-06-02 16:59:40,422 root  INFO     step 311.000000 - time: 1.396679, loss: 0.038764, perplexity: 1.039525, precision: 0.953125, batch_len: 124.000000
Train, loss=0.03876386: 312it [08:31,  1.28s/it]2017-06-02 16:59:41,472 root  INFO     step 312.000000 - time: 1.019572, loss: 0.048080, perplexity: 1.049255, precision: 0.843750, batch_len: 85.000000
Train, loss=0.04808047: 313it [08:32,  1.21s/it]2017-06-02 16:59:42,728 root  INFO     step 313.000000 - time: 1.241463, loss: 0.027610, perplexity: 1.027995, precision: 0.953125, batch_len: 116.000000
Train, loss=0.02761029: 314it [08:34,  1.22s/it]2017-06-02 16:59:44,300 root  INFO     step 314.000000 - time: 1.560687, loss: 0.091003, perplexity: 1.095272, precision: 0.953125, batch_len: 125.000000
Train, loss=0.09100299: 315it [08:35,  1.33s/it]2017-06-02 16:59:45,851 root  INFO     step 315.000000 - time: 1.501427, loss: 0.031604, perplexity: 1.032109, precision: 0.921875, batch_len: 121.000000
Train, loss=0.03160422: 316it [08:37,  1.40s/it]2017-06-02 16:59:46,827 root  INFO     step 316.000000 - time: 0.938976, loss: 0.089704, perplexity: 1.093851, precision: 0.843750, batch_len: 79.000000
Train, loss=0.08970450: 317it [08:38,  1.27s/it]2017-06-02 16:59:48,204 root  INFO     step 317.000000 - time: 1.356930, loss: 0.047650, perplexity: 1.048803, precision: 0.890625, batch_len: 117.000000
Train, loss=0.04764988: 318it [08:39,  1.30s/it]2017-06-02 16:59:49,478 root  INFO     step 318.000000 - time: 1.238811, loss: 0.031651, perplexity: 1.032157, precision: 0.890625, batch_len: 118.000000
Train, loss=0.03165062: 319it [08:40,  1.29s/it]2017-06-02 16:59:50,626 root  INFO     step 319.000000 - time: 1.107092, loss: 0.096414, perplexity: 1.101215, precision: 0.875000, batch_len: 103.000000
Train, loss=0.09641425: 320it [08:41,  1.25s/it]2017-06-02 16:59:52,137 root  INFO     step 320.000000 - time: 1.356706, loss: 0.048031, perplexity: 1.049203, precision: 0.906250, batch_len: 109.000000
Train, loss=0.04803095: 321it [08:43,  1.33s/it]2017-06-02 16:59:53,244 root  INFO     step 321.000000 - time: 1.031192, loss: 0.095390, perplexity: 1.100088, precision: 0.875000, batch_len: 82.000000
Train, loss=0.09539001: 322it [08:44,  1.26s/it]2017-06-02 16:59:54,365 root  INFO     step 322.000000 - time: 0.980665, loss: 0.137924, perplexity: 1.147889, precision: 0.875000, batch_len: 83.000000
Train, loss=0.13792440: 323it [08:45,  1.22s/it]2017-06-02 16:59:55,823 root  INFO     step 323.000000 - time: 1.328995, loss: 0.104577, perplexity: 1.110241, precision: 0.859375, batch_len: 126.000000
Train, loss=0.10457715: 324it [08:47,  1.29s/it]2017-06-02 16:59:57,178 root  INFO     step 324.000000 - time: 1.186150, loss: 0.097568, perplexity: 1.102486, precision: 0.828125, batch_len: 94.000000
Train, loss=0.09756751: 325it [08:48,  1.31s/it]2017-06-02 16:59:58,815 root  INFO     step 325.000000 - time: 1.472363, loss: 0.054869, perplexity: 1.056402, precision: 0.859375, batch_len: 129.000000
Train, loss=0.05486878: 326it [08:50,  1.41s/it]2017-06-02 16:59:59,782 root  INFO     step 326.000000 - time: 0.933952, loss: 0.061091, perplexity: 1.062996, precision: 0.875000, batch_len: 78.000000
Train, loss=0.06109094: 327it [08:51,  1.28s/it]2017-06-02 17:00:00,992 root  INFO     step 327.000000 - time: 1.076013, loss: 0.047732, perplexity: 1.048889, precision: 0.890625, batch_len: 99.000000
Train, loss=0.04773161: 328it [08:52,  1.26s/it]2017-06-02 17:00:02,311 root  INFO     step 328.000000 - time: 1.236289, loss: 0.038182, perplexity: 1.038920, precision: 0.890625, batch_len: 123.000000
Train, loss=0.03818177: 329it [08:53,  1.28s/it]2017-06-02 17:00:03,809 root  INFO     step 329.000000 - time: 1.462357, loss: 0.039522, perplexity: 1.040314, precision: 0.937500, batch_len: 119.000000
Train, loss=0.03952214: 330it [08:55,  1.34s/it]2017-06-02 17:00:05,395 root  INFO     step 330.000000 - time: 1.549865, loss: 0.029530, perplexity: 1.029971, precision: 0.906250, batch_len: 133.000000
Train, loss=0.02953047: 331it [08:56,  1.42s/it]2017-06-02 17:00:06,510 root  INFO     step 331.000000 - time: 1.029554, loss: 0.022072, perplexity: 1.022318, precision: 0.953125, batch_len: 98.000000
Train, loss=0.02207216: 332it [08:57,  1.33s/it]2017-06-02 17:00:08,041 root  INFO     step 332.000000 - time: 1.416833, loss: 0.029497, perplexity: 1.029937, precision: 0.968750, batch_len: 137.000000
Train, loss=0.02949743: 333it [08:59,  1.39s/it]2017-06-02 17:00:09,307 root  INFO     step 333.000000 - time: 1.241698, loss: 0.029270, perplexity: 1.029703, precision: 0.937500, batch_len: 135.000000
Train, loss=0.02927024: 334it [09:00,  1.35s/it]2017-06-02 17:00:10,529 root  INFO     step 334.000000 - time: 1.191131, loss: 0.047205, perplexity: 1.048337, precision: 0.921875, batch_len: 84.000000
Train, loss=0.04720496: 335it [09:01,  1.31s/it]2017-06-02 17:00:12,192 root  INFO     step 335.000000 - time: 1.610576, loss: 0.037574, perplexity: 1.038289, precision: 0.906250, batch_len: 130.000000
Train, loss=0.03757421: 336it [09:03,  1.42s/it]2017-06-02 17:00:13,292 root  INFO     step 336.000000 - time: 1.006443, loss: 0.033824, perplexity: 1.034403, precision: 0.937500, batch_len: 87.000000
Train, loss=0.03382435: 337it [09:04,  1.32s/it]2017-06-02 17:00:14,758 root  INFO     step 337.000000 - time: 1.427332, loss: 0.041104, perplexity: 1.041961, precision: 0.890625, batch_len: 144.000000
Train, loss=0.04110411: 338it [09:06,  1.37s/it]2017-06-02 17:00:16,127 root  INFO     step 338.000000 - time: 0.988281, loss: 0.009666, perplexity: 1.009712, precision: 0.968750, batch_len: 96.000000
Train, loss=0.00966559: 339it [09:07,  1.37s/it]2017-06-02 17:00:17,671 root  INFO     step 339.000000 - time: 1.524312, loss: 0.012224, perplexity: 1.012299, precision: 0.968750, batch_len: 136.000000
Train, loss=0.01222351: 340it [09:08,  1.42s/it]2017-06-02 17:00:19,262 root  INFO     step 340.000000 - time: 1.463465, loss: 0.011413, perplexity: 1.011479, precision: 0.953125, batch_len: 134.000000
Train, loss=0.01141320: 341it [09:10,  1.47s/it]2017-06-02 17:00:20,759 root  INFO     step 341.000000 - time: 1.444804, loss: 0.031666, perplexity: 1.032172, precision: 0.984375, batch_len: 138.000000
Train, loss=0.03166551: 342it [09:12,  1.48s/it]2017-06-02 17:00:22,291 root  INFO     step 342.000000 - time: 1.350815, loss: 0.155272, perplexity: 1.167976, precision: 0.828125, batch_len: 141.000000
Train, loss=0.15527207: 343it [09:13,  1.49s/it]2017-06-02 17:00:23,502 root  INFO     step 343.000000 - time: 1.192895, loss: 0.112640, perplexity: 1.119229, precision: 0.859375, batch_len: 72.000000
Train, loss=0.11263992: 344it [09:14,  1.41s/it]2017-06-02 17:00:25,121 root  INFO     step 344.000000 - time: 1.577359, loss: 0.107590, perplexity: 1.113591, precision: 0.828125, batch_len: 122.000000
Train, loss=0.10759025: 345it [09:16,  1.47s/it]2017-06-02 17:00:26,607 root  INFO     step 345.000000 - time: 1.379335, loss: 0.116301, perplexity: 1.123334, precision: 0.718750, batch_len: 132.000000
Train, loss=0.11630061: 346it [09:17,  1.48s/it]2017-06-02 17:00:28,719 root  INFO     step 346.000000 - time: 1.981313, loss: 0.087933, perplexity: 1.091914, precision: 0.828125, batch_len: 152.000000
Train, loss=0.08793251: 347it [09:20,  1.67s/it]2017-06-02 17:00:30,273 root  INFO     step 347.000000 - time: 1.463275, loss: 0.070753, perplexity: 1.073316, precision: 0.812500, batch_len: 139.000000
Train, loss=0.07075328: 348it [09:21,  1.63s/it]2017-06-02 17:00:31,199 root  INFO     step 348.000000 - time: 0.902947, loss: 0.017035, perplexity: 1.017180, precision: 0.937500, batch_len: 74.000000
Train, loss=0.01703455: 349it [09:22,  1.42s/it]2017-06-02 17:00:33,263 root  INFO     step 349.000000 - time: 1.768829, loss: 0.016304, perplexity: 1.016437, precision: 0.968750, batch_len: 142.000000
Train, loss=0.01630359: 350it [09:24,  1.61s/it]2017-06-02 17:00:34,243 root  INFO     step 350.000000 - time: 0.931854, loss: 0.045940, perplexity: 1.047012, precision: 0.906250, batch_len: 76.000000
Train, loss=0.04594019: 351it [09:25,  1.42s/it]2017-06-02 17:00:36,172 root  INFO     step 351.000000 - time: 1.880249, loss: 0.016872, perplexity: 1.017015, precision: 0.984375, batch_len: 150.000000
Train, loss=0.01687191: 352it [09:27,  1.58s/it]2017-06-02 17:00:37,162 root  INFO     step 352.000000 - time: 0.908196, loss: 0.023113, perplexity: 1.023382, precision: 0.921875, batch_len: 71.000000
Train, loss=0.02311319: 353it [09:28,  1.40s/it]2017-06-02 17:00:38,169 root  INFO     step 353.000000 - time: 0.926413, loss: 0.019661, perplexity: 1.019856, precision: 0.968750, batch_len: 77.000000
Train, loss=0.01966104: 354it [09:29,  1.28s/it]2017-06-02 17:00:40,028 root  INFO     step 354.000000 - time: 1.768280, loss: 0.021329, perplexity: 1.021558, precision: 0.953125, batch_len: 131.000000
Train, loss=0.02132861: 355it [09:31,  1.45s/it]2017-06-02 17:00:40,157 root  INFO     Generating first batch)
2017-06-02 17:00:43,922 root  INFO     step 355.000000 - time: 1.146567, loss: 0.009148, perplexity: 1.009190, precision: 0.984375, batch_len: 96.000000
Train, loss=0.00914837: 356it [09:35,  2.19s/it]2017-06-02 17:00:45,594 root  INFO     step 356.000000 - time: 1.491164, loss: 0.023249, perplexity: 1.023521, precision: 0.921875, batch_len: 120.000000
Train, loss=0.02324860: 357it [09:36,  2.03s/it]2017-06-02 17:00:47,049 root  INFO     step 357.000000 - time: 1.297570, loss: 0.023384, perplexity: 1.023659, precision: 0.953125, batch_len: 128.000000
Train, loss=0.02338367: 358it [09:38,  1.86s/it]2017-06-02 17:00:48,219 root  INFO     step 358.000000 - time: 1.099411, loss: 0.027176, perplexity: 1.027549, precision: 0.937500, batch_len: 110.000000
Train, loss=0.02717615: 359it [09:39,  1.65s/it]2017-06-02 17:00:49,277 root  INFO     step 359.000000 - time: 0.939865, loss: 0.017739, perplexity: 1.017897, precision: 0.968750, batch_len: 102.000000
Train, loss=0.01773903: 360it [09:40,  1.47s/it]2017-06-02 17:00:50,708 root  INFO     step 360.000000 - time: 1.308506, loss: 0.018900, perplexity: 1.019080, precision: 0.937500, batch_len: 101.000000
Train, loss=0.01890004: 361it [09:42,  1.46s/it]2017-06-02 17:00:52,258 root  INFO     step 361.000000 - time: 1.509503, loss: 0.014799, perplexity: 1.014909, precision: 0.968750, batch_len: 117.000000
Train, loss=0.01479910: 362it [09:43,  1.49s/it]2017-06-02 17:00:53,471 root  INFO     step 362.000000 - time: 1.088630, loss: 0.021321, perplexity: 1.021550, precision: 0.968750, batch_len: 105.000000
Train, loss=0.02132146: 363it [09:44,  1.41s/it]2017-06-02 17:00:54,666 root  INFO     step 363.000000 - time: 1.111581, loss: 0.035650, perplexity: 1.036293, precision: 0.937500, batch_len: 108.000000
Train, loss=0.03564959: 364it [09:45,  1.34s/it]2017-06-02 17:00:55,967 root  INFO     step 364.000000 - time: 1.222094, loss: 0.035289, perplexity: 1.035919, precision: 0.937500, batch_len: 112.000000
Train, loss=0.03528858: 365it [09:47,  1.33s/it]2017-06-02 17:00:57,156 root  INFO     step 365.000000 - time: 1.146449, loss: 0.050121, perplexity: 1.051398, precision: 0.859375, batch_len: 93.000000
Train, loss=0.05012089: 366it [09:48,  1.29s/it]2017-06-02 17:00:58,468 root  INFO     step 366.000000 - time: 1.245773, loss: 0.036693, perplexity: 1.037375, precision: 0.968750, batch_len: 100.000000
Train, loss=0.03669346: 367it [09:49,  1.29s/it]2017-06-02 17:00:59,595 root  INFO     step 367.000000 - time: 1.008067, loss: 0.033470, perplexity: 1.034036, precision: 0.937500, batch_len: 83.000000
Train, loss=0.03346971: 368it [09:50,  1.24s/it]2017-06-02 17:01:00,747 root  INFO     step 368.000000 - time: 1.097572, loss: 0.033335, perplexity: 1.033897, precision: 0.906250, batch_len: 113.000000
Train, loss=0.03333487: 369it [09:52,  1.22s/it]2017-06-02 17:01:01,691 root  INFO     step 369.000000 - time: 0.940011, loss: 0.021323, perplexity: 1.021552, precision: 0.953125, batch_len: 92.000000
Train, loss=0.02132281: 370it [09:53,  1.14s/it]2017-06-02 17:01:02,695 root  INFO     step 370.000000 - time: 0.971740, loss: 0.033244, perplexity: 1.033803, precision: 0.937500, batch_len: 97.000000
Train, loss=0.03324411: 371it [09:54,  1.10s/it]2017-06-02 17:01:04,101 root  INFO     step 371.000000 - time: 1.345001, loss: 0.033163, perplexity: 1.033719, precision: 0.953125, batch_len: 114.000000
Train, loss=0.03316323: 372it [09:55,  1.19s/it]2017-06-02 17:01:05,390 root  INFO     step 372.000000 - time: 1.228053, loss: 0.033615, perplexity: 1.034187, precision: 0.937500, batch_len: 111.000000
Train, loss=0.03361524: 373it [09:56,  1.22s/it]2017-06-02 17:01:06,384 root  INFO     step 373.000000 - time: 0.959003, loss: 0.039857, perplexity: 1.040662, precision: 0.937500, batch_len: 91.000000
Train, loss=0.03985688: 374it [09:57,  1.15s/it]2017-06-02 17:01:07,444 root  INFO     step 374.000000 - time: 1.010169, loss: 0.062504, perplexity: 1.064499, precision: 0.859375, batch_len: 88.000000
Train, loss=0.06250381: 375it [09:58,  1.12s/it]2017-06-02 17:01:08,540 root  INFO     step 375.000000 - time: 1.067522, loss: 0.015985, perplexity: 1.016113, precision: 0.968750, batch_len: 104.000000
Train, loss=0.01598456: 376it [09:59,  1.12s/it]2017-06-02 17:01:09,802 root  INFO     step 376.000000 - time: 1.168916, loss: 0.018140, perplexity: 1.018306, precision: 0.984375, batch_len: 106.000000
Train, loss=0.01814020: 377it [10:01,  1.16s/it]2017-06-02 17:01:11,081 root  INFO     step 377.000000 - time: 1.212713, loss: 0.028755, perplexity: 1.029172, precision: 0.921875, batch_len: 90.000000
Train, loss=0.02875460: 378it [10:02,  1.20s/it]2017-06-02 17:01:12,216 root  INFO     step 378.000000 - time: 1.119438, loss: 0.017365, perplexity: 1.017517, precision: 0.984375, batch_len: 85.000000
Train, loss=0.01736508: 379it [10:03,  1.18s/it]2017-06-02 17:01:13,676 root  INFO     step 379.000000 - time: 1.386132, loss: 0.016181, perplexity: 1.016313, precision: 0.953125, batch_len: 124.000000
Train, loss=0.01618125: 380it [10:04,  1.26s/it]2017-06-02 17:01:14,655 root  INFO     step 380.000000 - time: 0.958664, loss: 0.028352, perplexity: 1.028758, precision: 0.953125, batch_len: 81.000000
Train, loss=0.02835193: 381it [10:05,  1.18s/it]2017-06-02 17:01:15,664 root  INFO     step 381.000000 - time: 0.889830, loss: 0.024549, perplexity: 1.024853, precision: 0.968750, batch_len: 80.000000
Train, loss=0.02454906: 382it [10:06,  1.13s/it]2017-06-02 17:01:17,203 root  INFO     step 382.000000 - time: 1.474433, loss: 0.007483, perplexity: 1.007511, precision: 0.984375, batch_len: 121.000000
Train, loss=0.00748341: 383it [10:08,  1.25s/it]2017-06-02 17:01:18,429 root  INFO     step 383.000000 - time: 1.203151, loss: 0.014931, perplexity: 1.015043, precision: 0.968750, batch_len: 87.000000
Train, loss=0.01493078: 384it [10:09,  1.24s/it]2017-06-02 17:01:19,747 root  INFO     step 384.000000 - time: 1.285032, loss: 0.008135, perplexity: 1.008168, precision: 0.984375, batch_len: 123.000000
Train, loss=0.00813507: 385it [10:11,  1.27s/it]2017-06-02 17:01:20,764 root  INFO     step 385.000000 - time: 1.007549, loss: 0.033978, perplexity: 1.034561, precision: 0.968750, batch_len: 103.000000
Train, loss=0.03397764: 386it [10:12,  1.19s/it]2017-06-02 17:01:21,705 root  INFO     step 386.000000 - time: 0.889928, loss: 0.027489, perplexity: 1.027870, precision: 0.968750, batch_len: 79.000000
Train, loss=0.02748909: 387it [10:13,  1.12s/it]2017-06-02 17:01:22,834 root  INFO     step 387.000000 - time: 0.978334, loss: 0.098049, perplexity: 1.103017, precision: 0.890625, batch_len: 94.000000
Train, loss=0.09804878: 388it [10:14,  1.12s/it]2017-06-02 17:01:24,023 root  INFO     step 388.000000 - time: 1.177416, loss: 0.017592, perplexity: 1.017747, precision: 0.953125, batch_len: 89.000000
Train, loss=0.01759183: 389it [10:15,  1.14s/it]2017-06-02 17:01:25,470 root  INFO     step 389.000000 - time: 1.229122, loss: 0.033592, perplexity: 1.034162, precision: 0.937500, batch_len: 107.000000
Train, loss=0.03359177: 390it [10:16,  1.23s/it]2017-06-02 17:01:26,746 root  INFO     step 390.000000 - time: 1.193136, loss: 0.032019, perplexity: 1.032537, precision: 0.953125, batch_len: 115.000000
Train, loss=0.03201862: 391it [10:18,  1.25s/it]2017-06-02 17:01:27,908 root  INFO     step 391.000000 - time: 1.124233, loss: 0.022377, perplexity: 1.022629, precision: 0.937500, batch_len: 109.000000
Train, loss=0.02237709: 392it [10:19,  1.22s/it]2017-06-02 17:01:29,411 root  INFO     step 392.000000 - time: 1.426575, loss: 0.078711, perplexity: 1.081892, precision: 0.921875, batch_len: 125.000000
Train, loss=0.07871107: 393it [10:20,  1.31s/it]2017-06-02 17:01:30,943 root  INFO     step 393.000000 - time: 1.481868, loss: 0.044360, perplexity: 1.045359, precision: 0.937500, batch_len: 119.000000
Train, loss=0.04436037: 394it [10:22,  1.37s/it]2017-06-02 17:01:32,542 root  INFO     step 394.000000 - time: 1.583223, loss: 0.013156, perplexity: 1.013243, precision: 0.953125, batch_len: 118.000000
Train, loss=0.01315647: 395it [10:23,  1.44s/it]2017-06-02 17:01:34,146 root  INFO     step 395.000000 - time: 1.504786, loss: 0.029590, perplexity: 1.030032, precision: 0.921875, batch_len: 130.000000
Train, loss=0.02959011: 396it [10:25,  1.49s/it]2017-06-02 17:01:35,530 root  INFO     step 396.000000 - time: 1.367929, loss: 0.023002, perplexity: 1.023268, precision: 0.937500, batch_len: 126.000000
Train, loss=0.02300159: 397it [10:26,  1.46s/it]2017-06-02 17:01:36,674 root  INFO     step 397.000000 - time: 1.003216, loss: 0.021030, perplexity: 1.021253, precision: 0.937500, batch_len: 86.000000
Train, loss=0.02102988: 398it [10:27,  1.36s/it]2017-06-02 17:01:37,916 root  INFO     step 398.000000 - time: 1.209099, loss: 0.010939, perplexity: 1.010999, precision: 0.937500, batch_len: 133.000000
Train, loss=0.01093895: 399it [10:29,  1.33s/it]2017-06-02 17:01:39,108 root  INFO     step 399.000000 - time: 1.180983, loss: 0.015537, perplexity: 1.015658, precision: 0.953125, batch_len: 99.000000
Train, loss=0.01553687: 400it [10:30,  1.29s/it]2017-06-02 17:01:40,737 root  INFO     step 400.000000 - time: 1.580947, loss: 0.009496, perplexity: 1.009542, precision: 0.984375, batch_len: 137.000000
Train, loss=0.00949637: 401it [10:32,  1.39s/it]2017-06-02 17:01:42,230 root  INFO     step 401.000000 - time: 1.361094, loss: 0.004858, perplexity: 1.004870, precision: 1.000000, batch_len: 116.000000
Train, loss=0.00485816: 402it [10:33,  1.42s/it]2017-06-02 17:01:43,588 root  INFO     step 402.000000 - time: 1.305067, loss: 0.022660, perplexity: 1.022918, precision: 0.968750, batch_len: 129.000000
Train, loss=0.02265971: 403it [10:34,  1.40s/it]2017-06-02 17:01:44,756 root  INFO     step 403.000000 - time: 1.054659, loss: 0.008467, perplexity: 1.008503, precision: 0.984375, batch_len: 98.000000
Train, loss=0.00846722: 404it [10:36,  1.33s/it]2017-06-02 17:01:45,982 root  INFO     step 404.000000 - time: 1.212507, loss: 0.025052, perplexity: 1.025369, precision: 0.937500, batch_len: 84.000000
Train, loss=0.02505221: 405it [10:37,  1.30s/it]2017-06-02 17:01:47,586 root  INFO     step 405.000000 - time: 1.580902, loss: 0.009870, perplexity: 1.009919, precision: 0.968750, batch_len: 136.000000
Train, loss=0.00987046: 406it [10:38,  1.39s/it]2017-06-02 17:01:48,778 root  INFO     step 406.000000 - time: 0.970491, loss: 0.022661, perplexity: 1.022920, precision: 0.953125, batch_len: 82.000000
Train, loss=0.02266112: 407it [10:40,  1.33s/it]2017-06-02 17:01:50,132 root  INFO     step 407.000000 - time: 1.300990, loss: 0.023290, perplexity: 1.023564, precision: 0.984375, batch_len: 135.000000
Train, loss=0.02329019: 408it [10:41,  1.34s/it]2017-06-02 17:01:51,677 root  INFO     step 408.000000 - time: 1.419331, loss: 0.017821, perplexity: 1.017981, precision: 0.953125, batch_len: 144.000000
Train, loss=0.01782137: 409it [10:42,  1.40s/it]2017-06-02 17:01:53,258 root  INFO     step 409.000000 - time: 1.531168, loss: 0.009295, perplexity: 1.009338, precision: 0.984375, batch_len: 122.000000
Train, loss=0.00929495: 410it [10:44,  1.45s/it]2017-06-02 17:01:54,351 root  INFO     step 410.000000 - time: 0.967044, loss: 0.017416, perplexity: 1.017569, precision: 0.953125, batch_len: 78.000000
Train, loss=0.01741602: 411it [10:45,  1.35s/it]2017-06-02 17:01:56,104 root  INFO     step 411.000000 - time: 1.421184, loss: 0.005445, perplexity: 1.005460, precision: 0.984375, batch_len: 138.000000
Train, loss=0.00544486: 412it [10:47,  1.47s/it]2017-06-02 17:01:57,406 root  INFO     step 412.000000 - time: 1.245659, loss: 0.005226, perplexity: 1.005239, precision: 1.000000, batch_len: 134.000000
Train, loss=0.00522562: 413it [10:48,  1.42s/it]2017-06-02 17:01:58,726 root  INFO     step 413.000000 - time: 1.187329, loss: 0.010749, perplexity: 1.010807, precision: 0.984375, batch_len: 96.000000
Train, loss=0.01074898: 414it [10:50,  1.39s/it]2017-06-02 17:02:00,808 root  INFO     step 414.000000 - time: 2.052686, loss: 0.002872, perplexity: 1.002876, precision: 1.000000, batch_len: 152.000000
Train, loss=0.00287164: 415it [10:52,  1.60s/it]2017-06-02 17:02:01,982 root  INFO     step 415.000000 - time: 0.930119, loss: 0.007565, perplexity: 1.007594, precision: 0.968750, batch_len: 74.000000
Train, loss=0.00756503: 416it [10:53,  1.47s/it]2017-06-02 17:02:03,396 root  INFO     step 416.000000 - time: 1.396049, loss: 0.006718, perplexity: 1.006740, precision: 1.000000, batch_len: 139.000000
Train, loss=0.00671788: 417it [10:54,  1.45s/it]2017-06-02 17:02:04,617 root  INFO     step 417.000000 - time: 1.210668, loss: 0.041053, perplexity: 1.041908, precision: 0.953125, batch_len: 72.000000
Train, loss=0.04105339: 418it [10:55,  1.38s/it]2017-06-02 17:02:06,331 root  INFO     step 418.000000 - time: 1.655722, loss: 0.074326, perplexity: 1.077158, precision: 0.859375, batch_len: 141.000000
Train, loss=0.07432634: 419it [10:57,  1.48s/it]2017-06-02 17:02:07,389 root  INFO     step 419.000000 - time: 0.962853, loss: 0.039381, perplexity: 1.040167, precision: 0.890625, batch_len: 76.000000
Train, loss=0.03938124: 420it [10:58,  1.36s/it]2017-06-02 17:02:08,731 root  INFO     step 420.000000 - time: 1.336342, loss: 0.019804, perplexity: 1.020001, precision: 0.953125, batch_len: 132.000000
Train, loss=0.01980358: 421it [11:00,  1.35s/it]2017-06-02 17:02:09,782 root  INFO     step 421.000000 - time: 0.911312, loss: 0.009978, perplexity: 1.010028, precision: 1.000000, batch_len: 77.000000
Train, loss=0.00997838: 422it [11:01,  1.26s/it]2017-06-02 17:02:11,998 root  INFO     step 422.000000 - time: 2.034610, loss: 0.012712, perplexity: 1.012793, precision: 0.968750, batch_len: 150.000000
Train, loss=0.01271219: 423it [11:03,  1.55s/it]2017-06-02 17:02:13,292 root  INFO     step 423.000000 - time: 1.147774, loss: 0.008274, perplexity: 1.008308, precision: 0.984375, batch_len: 71.000000
Train, loss=0.00827393: 424it [11:04,  1.47s/it]2017-06-02 17:02:14,741 root  INFO     step 424.000000 - time: 1.420305, loss: 0.011943, perplexity: 1.012015, precision: 0.968750, batch_len: 142.000000
Train, loss=0.01194336: 425it [11:06,  1.46s/it]2017-06-02 17:02:16,118 root  INFO     step 425.000000 - time: 1.368511, loss: 0.005271, perplexity: 1.005284, precision: 0.984375, batch_len: 131.000000
Train, loss=0.00527052: 426it [11:07,  1.44s/it]2017-06-02 17:02:16,286 root  INFO     Generating first batch)
2017-06-02 17:02:20,092 root  INFO     step 426.000000 - time: 1.059460, loss: 0.015223, perplexity: 1.015340, precision: 0.968750, batch_len: 100.000000
Train, loss=0.01522309: 427it [11:11,  2.20s/it]2017-06-02 17:02:21,219 root  INFO     step 427.000000 - time: 1.050557, loss: 0.049029, perplexity: 1.050251, precision: 0.890625, batch_len: 105.000000
Train, loss=0.04902893: 428it [11:12,  1.88s/it]2017-06-02 17:02:22,293 root  INFO     step 428.000000 - time: 0.980684, loss: 0.020736, perplexity: 1.020953, precision: 0.937500, batch_len: 96.000000
Train, loss=0.02073644: 429it [11:13,  1.64s/it]2017-06-02 17:02:23,632 root  INFO     step 429.000000 - time: 1.257073, loss: 0.008275, perplexity: 1.008309, precision: 0.984375, batch_len: 113.000000
Train, loss=0.00827454: 430it [11:14,  1.55s/it]2017-06-02 17:02:24,934 root  INFO     step 430.000000 - time: 1.180309, loss: 0.034467, perplexity: 1.035068, precision: 0.953125, batch_len: 93.000000
Train, loss=0.03446728: 431it [11:16,  1.47s/it]2017-06-02 17:02:26,269 root  INFO     step 431.000000 - time: 1.323257, loss: 0.018699, perplexity: 1.018875, precision: 0.984375, batch_len: 128.000000
Train, loss=0.01869945: 432it [11:17,  1.43s/it]2017-06-02 17:02:27,554 root  INFO     step 432.000000 - time: 1.190469, loss: 0.021226, perplexity: 1.021453, precision: 0.953125, batch_len: 112.000000
Train, loss=0.02122569: 433it [11:18,  1.39s/it]2017-06-02 17:02:28,715 root  INFO     step 433.000000 - time: 1.109759, loss: 0.010364, perplexity: 1.010418, precision: 0.984375, batch_len: 110.000000
Train, loss=0.01036377: 434it [11:20,  1.32s/it]2017-06-02 17:02:30,065 root  INFO     step 434.000000 - time: 1.193983, loss: 0.017336, perplexity: 1.017487, precision: 0.953125, batch_len: 102.000000
Train, loss=0.01733606: 435it [11:21,  1.33s/it]2017-06-02 17:02:31,246 root  INFO     step 435.000000 - time: 1.138419, loss: 0.019775, perplexity: 1.019971, precision: 0.984375, batch_len: 101.000000
Train, loss=0.01977460: 436it [11:22,  1.28s/it]2017-06-02 17:02:32,683 root  INFO     step 436.000000 - time: 1.312967, loss: 0.007829, perplexity: 1.007859, precision: 1.000000, batch_len: 92.000000
Train, loss=0.00782863: 437it [11:23,  1.33s/it]2017-06-02 17:02:34,106 root  INFO     step 437.000000 - time: 1.371685, loss: 0.042376, perplexity: 1.043287, precision: 0.890625, batch_len: 120.000000
Train, loss=0.04237632: 438it [11:25,  1.36s/it]2017-06-02 17:02:35,801 root  INFO     step 438.000000 - time: 1.595470, loss: 0.043050, perplexity: 1.043991, precision: 0.937500, batch_len: 90.000000
Train, loss=0.04305048: 439it [11:27,  1.46s/it]2017-06-02 17:02:36,740 root  INFO     step 439.000000 - time: 0.872799, loss: 0.050438, perplexity: 1.051732, precision: 0.906250, batch_len: 81.000000
Train, loss=0.05043794: 440it [11:28,  1.30s/it]2017-06-02 17:02:37,865 root  INFO     step 440.000000 - time: 1.059082, loss: 0.014386, perplexity: 1.014490, precision: 0.953125, batch_len: 103.000000
Train, loss=0.01438576: 441it [11:29,  1.25s/it]2017-06-02 17:02:39,313 root  INFO     step 441.000000 - time: 1.399624, loss: 0.036709, perplexity: 1.037391, precision: 0.921875, batch_len: 106.000000
Train, loss=0.03670854: 442it [11:30,  1.31s/it]2017-06-02 17:02:40,500 root  INFO     step 442.000000 - time: 1.123085, loss: 0.031130, perplexity: 1.031619, precision: 0.953125, batch_len: 108.000000
Train, loss=0.03112951: 443it [11:31,  1.27s/it]2017-06-02 17:02:41,755 root  INFO     step 443.000000 - time: 1.115644, loss: 0.029253, perplexity: 1.029685, precision: 0.953125, batch_len: 114.000000
Train, loss=0.02925342: 444it [11:33,  1.27s/it]2017-06-02 17:02:43,066 root  INFO     step 444.000000 - time: 1.150840, loss: 0.033911, perplexity: 1.034493, precision: 0.906250, batch_len: 111.000000
Train, loss=0.03391140: 445it [11:34,  1.28s/it]2017-06-02 17:02:44,059 root  INFO     step 445.000000 - time: 0.929713, loss: 0.036292, perplexity: 1.036958, precision: 0.953125, batch_len: 88.000000
Train, loss=0.03629184: 446it [11:35,  1.19s/it]2017-06-02 17:02:45,260 root  INFO     step 446.000000 - time: 1.148925, loss: 0.030500, perplexity: 1.030970, precision: 0.921875, batch_len: 97.000000
Train, loss=0.03050014: 447it [11:36,  1.20s/it]2017-06-02 17:02:46,509 root  INFO     step 447.000000 - time: 1.204177, loss: 0.010746, perplexity: 1.010804, precision: 0.984375, batch_len: 98.000000
Train, loss=0.01074610: 448it [11:37,  1.21s/it]2017-06-02 17:02:47,927 root  INFO     step 448.000000 - time: 1.362448, loss: 0.017147, perplexity: 1.017295, precision: 0.968750, batch_len: 117.000000
Train, loss=0.01714737: 449it [11:39,  1.27s/it]2017-06-02 17:02:49,032 root  INFO     step 449.000000 - time: 1.003947, loss: 0.018047, perplexity: 1.018211, precision: 0.984375, batch_len: 86.000000
Train, loss=0.01804673: 450it [11:40,  1.22s/it]2017-06-02 17:02:50,263 root  INFO     step 450.000000 - time: 1.221376, loss: 0.013020, perplexity: 1.013105, precision: 0.968750, batch_len: 125.000000
Train, loss=0.01302024: 451it [11:41,  1.23s/it]2017-06-02 17:02:51,438 root  INFO     step 451.000000 - time: 1.136046, loss: 0.025157, perplexity: 1.025476, precision: 0.937500, batch_len: 85.000000
Train, loss=0.02515706: 452it [11:42,  1.21s/it]2017-06-02 17:02:53,124 root  INFO     step 452.000000 - time: 1.641423, loss: 0.016299, perplexity: 1.016432, precision: 0.953125, batch_len: 137.000000
Train, loss=0.01629881: 453it [11:44,  1.35s/it]2017-06-02 17:02:54,524 root  INFO     step 453.000000 - time: 1.390513, loss: 0.018227, perplexity: 1.018394, precision: 0.968750, batch_len: 116.000000
Train, loss=0.01822703: 454it [11:45,  1.37s/it]2017-06-02 17:02:55,782 root  INFO     step 454.000000 - time: 1.112299, loss: 0.021188, perplexity: 1.021414, precision: 0.984375, batch_len: 115.000000
Train, loss=0.02118805: 455it [11:47,  1.33s/it]2017-06-02 17:02:56,749 root  INFO     step 455.000000 - time: 0.952915, loss: 0.005791, perplexity: 1.005807, precision: 0.984375, batch_len: 104.000000
Train, loss=0.00579051: 456it [11:48,  1.22s/it]2017-06-02 17:02:57,857 root  INFO     step 456.000000 - time: 1.078281, loss: 0.021594, perplexity: 1.021829, precision: 0.953125, batch_len: 99.000000
Train, loss=0.02159381: 457it [11:49,  1.19s/it]2017-06-02 17:02:59,534 root  INFO     step 457.000000 - time: 1.590370, loss: 0.010587, perplexity: 1.010644, precision: 0.984375, batch_len: 121.000000
Train, loss=0.01058728: 458it [11:50,  1.34s/it]2017-06-02 17:03:00,609 root  INFO     step 458.000000 - time: 1.061798, loss: 0.025067, perplexity: 1.025383, precision: 0.921875, batch_len: 89.000000
Train, loss=0.02506667: 459it [11:51,  1.26s/it]2017-06-02 17:03:01,782 root  INFO     step 459.000000 - time: 1.166600, loss: 0.018043, perplexity: 1.018207, precision: 0.968750, batch_len: 91.000000
Train, loss=0.01804315: 460it [11:53,  1.23s/it]2017-06-02 17:03:02,753 root  INFO     step 460.000000 - time: 0.921902, loss: 0.026284, perplexity: 1.026632, precision: 0.953125, batch_len: 79.000000
Train, loss=0.02628360: 461it [11:54,  1.15s/it]2017-06-02 17:03:03,806 root  INFO     step 461.000000 - time: 1.009527, loss: 0.035492, perplexity: 1.036130, precision: 0.953125, batch_len: 107.000000
Train, loss=0.03549219: 462it [11:55,  1.12s/it]2017-06-02 17:03:05,395 root  INFO     step 462.000000 - time: 1.523522, loss: 0.009254, perplexity: 1.009297, precision: 0.968750, batch_len: 123.000000
Train, loss=0.00925367: 463it [11:56,  1.26s/it]2017-06-02 17:03:06,691 root  INFO     step 463.000000 - time: 1.270228, loss: 0.020887, perplexity: 1.021107, precision: 0.953125, batch_len: 109.000000
Train, loss=0.02088685: 464it [11:58,  1.27s/it]2017-06-02 17:03:07,745 root  INFO     step 464.000000 - time: 1.019029, loss: 0.068465, perplexity: 1.070863, precision: 0.859375, batch_len: 83.000000
Train, loss=0.06846485: 465it [11:59,  1.21s/it]2017-06-02 17:03:08,790 root  INFO     step 465.000000 - time: 0.947466, loss: 0.139047, perplexity: 1.149178, precision: 0.953125, batch_len: 80.000000
Train, loss=0.13904704: 466it [12:00,  1.16s/it]2017-06-02 17:03:10,193 root  INFO     step 466.000000 - time: 1.315277, loss: 0.041941, perplexity: 1.042833, precision: 0.875000, batch_len: 124.000000
Train, loss=0.04194066: 467it [12:01,  1.23s/it]2017-06-02 17:03:11,874 root  INFO     step 467.000000 - time: 1.509207, loss: 0.033114, perplexity: 1.033668, precision: 0.890625, batch_len: 133.000000
Train, loss=0.03311381: 468it [12:03,  1.37s/it]2017-06-02 17:03:13,456 root  INFO     step 468.000000 - time: 1.502653, loss: 0.047770, perplexity: 1.048929, precision: 0.906250, batch_len: 119.000000
Train, loss=0.04776964: 469it [12:04,  1.43s/it]2017-06-02 17:03:14,621 root  INFO     step 469.000000 - time: 1.057234, loss: 0.144470, perplexity: 1.155427, precision: 0.859375, batch_len: 94.000000
Train, loss=0.14447011: 470it [12:05,  1.35s/it]2017-06-02 17:03:16,030 root  INFO     step 470.000000 - time: 1.304497, loss: 0.024718, perplexity: 1.025026, precision: 0.953125, batch_len: 118.000000
Train, loss=0.02471806: 471it [12:07,  1.37s/it]2017-06-02 17:03:16,982 root  INFO     step 471.000000 - time: 0.923784, loss: 0.028833, perplexity: 1.029253, precision: 0.921875, batch_len: 87.000000
Train, loss=0.02883347: 472it [12:08,  1.24s/it]2017-06-02 17:03:18,631 root  INFO     step 472.000000 - time: 1.592203, loss: 0.065999, perplexity: 1.068225, precision: 0.875000, batch_len: 130.000000
Train, loss=0.06599850: 473it [12:09,  1.37s/it]2017-06-02 17:03:20,132 root  INFO     step 473.000000 - time: 1.483888, loss: 0.035382, perplexity: 1.036015, precision: 0.984375, batch_len: 135.000000
Train, loss=0.03538206: 474it [12:11,  1.41s/it]2017-06-02 17:03:21,645 root  INFO     step 474.000000 - time: 1.425681, loss: 0.023382, perplexity: 1.023657, precision: 0.968750, batch_len: 144.000000
Train, loss=0.02338189: 475it [12:12,  1.44s/it]2017-06-02 17:03:23,057 root  INFO     step 475.000000 - time: 1.285398, loss: 0.010218, perplexity: 1.010271, precision: 0.984375, batch_len: 129.000000
Train, loss=0.01021819: 476it [12:14,  1.43s/it]2017-06-02 17:03:24,160 root  INFO     step 476.000000 - time: 0.943104, loss: 0.031530, perplexity: 1.032032, precision: 0.906250, batch_len: 84.000000
Train, loss=0.03152996: 477it [12:15,  1.33s/it]2017-06-02 17:03:26,005 root  INFO     step 477.000000 - time: 1.688715, loss: 0.025603, perplexity: 1.025933, precision: 0.921875, batch_len: 126.000000
Train, loss=0.02560282: 478it [12:17,  1.49s/it]2017-06-02 17:03:27,728 root  INFO     step 478.000000 - time: 1.519489, loss: 0.007817, perplexity: 1.007847, precision: 1.000000, batch_len: 136.000000
Train, loss=0.00781675: 479it [12:19,  1.56s/it]2017-06-02 17:03:28,762 root  INFO     step 479.000000 - time: 1.014498, loss: 0.021348, perplexity: 1.021577, precision: 0.953125, batch_len: 78.000000
Train, loss=0.02134765: 480it [12:20,  1.40s/it]2017-06-02 17:03:30,186 root  INFO     step 480.000000 - time: 1.302269, loss: 0.013937, perplexity: 1.014034, precision: 0.953125, batch_len: 122.000000
Train, loss=0.01393674: 481it [12:21,  1.41s/it]2017-06-02 17:03:31,733 root  INFO     step 481.000000 - time: 1.453991, loss: 0.006793, perplexity: 1.006816, precision: 0.984375, batch_len: 134.000000
Train, loss=0.00679296: 482it [12:23,  1.45s/it]2017-06-02 17:03:33,058 root  INFO     step 482.000000 - time: 1.225402, loss: 0.067816, perplexity: 1.070168, precision: 0.937500, batch_len: 82.000000
Train, loss=0.06781559: 483it [12:24,  1.41s/it]2017-06-02 17:03:34,291 root  INFO     step 483.000000 - time: 1.177124, loss: 0.089239, perplexity: 1.093342, precision: 0.859375, batch_len: 76.000000
Train, loss=0.08923909: 484it [12:25,  1.36s/it]2017-06-02 17:03:35,741 root  INFO     step 484.000000 - time: 1.428571, loss: 0.049025, perplexity: 1.050247, precision: 0.875000, batch_len: 138.000000
Train, loss=0.04902494: 485it [12:27,  1.39s/it]2017-06-02 17:03:37,422 root  INFO     step 485.000000 - time: 1.475993, loss: 0.020199, perplexity: 1.020405, precision: 0.953125, batch_len: 139.000000
Train, loss=0.02019949: 486it [12:28,  1.47s/it]2017-06-02 17:03:39,343 root  INFO     step 486.000000 - time: 1.822965, loss: 0.129959, perplexity: 1.138782, precision: 0.890625, batch_len: 152.000000
Train, loss=0.12995897: 487it [12:30,  1.61s/it]2017-06-02 17:03:40,705 root  INFO     step 487.000000 - time: 1.319927, loss: 0.019713, perplexity: 1.019908, precision: 0.953125, batch_len: 96.000000
Train, loss=0.01971259: 488it [12:32,  1.53s/it]2017-06-02 17:03:42,253 root  INFO     step 488.000000 - time: 1.442517, loss: 0.024091, perplexity: 1.024384, precision: 0.968750, batch_len: 141.000000
Train, loss=0.02409103: 489it [12:33,  1.54s/it]2017-06-02 17:03:44,200 root  INFO     step 489.000000 - time: 1.834635, loss: 0.023678, perplexity: 1.023961, precision: 0.968750, batch_len: 150.000000
Train, loss=0.02367830: 490it [12:35,  1.66s/it]2017-06-02 17:03:45,128 root  INFO     step 490.000000 - time: 0.906593, loss: 0.015545, perplexity: 1.015666, precision: 0.953125, batch_len: 77.000000
Train, loss=0.01554495: 491it [12:36,  1.44s/it]2017-06-02 17:03:46,212 root  INFO     step 491.000000 - time: 1.072303, loss: 0.008783, perplexity: 1.008822, precision: 0.984375, batch_len: 74.000000
Train, loss=0.00878328: 492it [12:37,  1.33s/it]2017-06-02 17:03:47,900 root  INFO     step 492.000000 - time: 1.513450, loss: 0.007609, perplexity: 1.007638, precision: 0.984375, batch_len: 132.000000
Train, loss=0.00760866: 493it [12:39,  1.44s/it]2017-06-02 17:03:49,175 root  INFO     step 493.000000 - time: 1.207694, loss: 0.017813, perplexity: 1.017973, precision: 0.968750, batch_len: 72.000000
Train, loss=0.01781321: 494it [12:40,  1.39s/it]2017-06-02 17:03:50,615 root  INFO     step 494.000000 - time: 1.355733, loss: 0.012068, perplexity: 1.012142, precision: 0.968750, batch_len: 131.000000
Train, loss=0.01206840: 495it [12:41,  1.41s/it]2017-06-02 17:03:52,163 root  INFO     step 495.000000 - time: 1.408995, loss: 0.019115, perplexity: 1.019299, precision: 0.953125, batch_len: 142.000000
Train, loss=0.01911533: 496it [12:43,  1.45s/it]2017-06-02 17:03:53,443 root  INFO     step 496.000000 - time: 1.173251, loss: 0.009647, perplexity: 1.009694, precision: 0.984375, batch_len: 71.000000
Train, loss=0.00964691: 497it [12:44,  1.40s/it]2017-06-02 17:03:53,596 root  INFO     Generating first batch)
2017-06-02 17:03:57,101 root  INFO     step 497.000000 - time: 1.015097, loss: 0.039696, perplexity: 1.040495, precision: 0.906250, batch_len: 113.000000
Train, loss=0.03969616: 498it [12:48,  2.08s/it]2017-06-02 17:03:58,694 root  INFO     step 498.000000 - time: 1.272132, loss: 0.009682, perplexity: 1.009729, precision: 0.968750, batch_len: 96.000000
Train, loss=0.00968229: 499it [12:50,  1.93s/it]2017-06-02 17:03:59,961 root  INFO     step 499.000000 - time: 1.062684, loss: 0.037469, perplexity: 1.038180, precision: 0.921875, batch_len: 104.000000
Train, loss=0.03746884: 500it [12:51,  1.73s/it]2017-06-02 17:04:01,362 root  INFO     step 500.000000 - time: 1.273502, loss: 0.025997, perplexity: 1.026338, precision: 0.937500, batch_len: 128.000000
Train, loss=0.02599739: 501it [12:52,  1.63s/it]2017-06-02 17:04:02,483 root  INFO     step 501.000000 - time: 1.065925, loss: 0.031928, perplexity: 1.032443, precision: 0.890625, batch_len: 105.000000
Train, loss=0.03192756: 502it [12:53,  1.48s/it]2017-06-02 17:04:03,426 root  INFO     step 502.000000 - time: 0.921680, loss: 0.034716, perplexity: 1.035325, precision: 0.937500, batch_len: 88.000000
Train, loss=0.03471550: 503it [12:54,  1.32s/it]2017-06-02 17:04:04,770 root  INFO     step 503.000000 - time: 1.241242, loss: 0.019900, perplexity: 1.020099, precision: 0.953125, batch_len: 102.000000
Train, loss=0.01989960: 504it [12:56,  1.33s/it]2017-06-02 17:04:05,981 root  INFO     step 504.000000 - time: 1.163186, loss: 0.012547, perplexity: 1.012626, precision: 0.968750, batch_len: 92.000000
Train, loss=0.01254654: 505it [12:57,  1.29s/it]2017-06-02 17:04:07,208 root  INFO     step 505.000000 - time: 1.091816, loss: 0.031200, perplexity: 1.031691, precision: 0.921875, batch_len: 106.000000
Train, loss=0.03119952: 506it [12:58,  1.27s/it]2017-06-02 17:04:08,661 root  INFO     step 506.000000 - time: 1.347360, loss: 0.039158, perplexity: 1.039935, precision: 0.890625, batch_len: 120.000000
Train, loss=0.03915789: 507it [12:59,  1.33s/it]2017-06-02 17:04:10,020 root  INFO     step 507.000000 - time: 1.317313, loss: 0.007085, perplexity: 1.007110, precision: 0.953125, batch_len: 124.000000
Train, loss=0.00708483: 508it [13:01,  1.34s/it]2017-06-02 17:04:11,256 root  INFO     step 508.000000 - time: 1.193688, loss: 0.055997, perplexity: 1.057594, precision: 0.906250, batch_len: 93.000000
Train, loss=0.05599670: 509it [13:02,  1.31s/it]2017-06-02 17:04:12,598 root  INFO     step 509.000000 - time: 1.314744, loss: 0.008663, perplexity: 1.008700, precision: 1.000000, batch_len: 110.000000
Train, loss=0.00866272: 510it [13:03,  1.32s/it]2017-06-02 17:04:13,747 root  INFO     step 510.000000 - time: 1.082804, loss: 0.019067, perplexity: 1.019250, precision: 0.937500, batch_len: 101.000000
Train, loss=0.01906739: 511it [13:05,  1.27s/it]2017-06-02 17:04:14,898 root  INFO     step 511.000000 - time: 1.116582, loss: 0.024006, perplexity: 1.024296, precision: 0.937500, batch_len: 111.000000
Train, loss=0.02400565: 512it [13:06,  1.23s/it]2017-06-02 17:04:15,912 root  INFO     step 512.000000 - time: 0.931325, loss: 0.022070, perplexity: 1.022316, precision: 0.937500, batch_len: 90.000000
Train, loss=0.02207042: 513it [13:07,  1.17s/it]2017-06-02 17:04:17,143 root  INFO     step 513.000000 - time: 1.141077, loss: 0.042225, perplexity: 1.043129, precision: 0.859375, batch_len: 100.000000
Train, loss=0.04222500: 514it [13:08,  1.19s/it]2017-06-02 17:04:18,557 root  INFO     step 514.000000 - time: 1.396316, loss: 0.047654, perplexity: 1.048808, precision: 0.937500, batch_len: 114.000000
Train, loss=0.04765409: 515it [13:09,  1.25s/it]2017-06-02 17:04:19,752 root  INFO     step 515.000000 - time: 1.145035, loss: 0.022479, perplexity: 1.022733, precision: 0.937500, batch_len: 108.000000
Train, loss=0.02247859: 516it [13:11,  1.24s/it]2017-06-02 17:04:21,149 root  INFO     step 516.000000 - time: 1.339273, loss: 0.030856, perplexity: 1.031337, precision: 0.937500, batch_len: 121.000000
Train, loss=0.03085591: 517it [13:12,  1.28s/it]2017-06-02 17:04:22,491 root  INFO     step 517.000000 - time: 1.326955, loss: 0.035425, perplexity: 1.036060, precision: 0.921875, batch_len: 112.000000
Train, loss=0.03542471: 518it [13:13,  1.30s/it]2017-06-02 17:04:23,707 root  INFO     step 518.000000 - time: 0.975211, loss: 0.013190, perplexity: 1.013278, precision: 1.000000, batch_len: 86.000000
Train, loss=0.01319025: 519it [13:15,  1.28s/it]2017-06-02 17:04:24,982 root  INFO     step 519.000000 - time: 1.173716, loss: 0.030760, perplexity: 1.031238, precision: 0.875000, batch_len: 97.000000
Train, loss=0.03076016: 520it [13:16,  1.28s/it]2017-06-02 17:04:26,159 root  INFO     step 520.000000 - time: 1.166955, loss: 0.103828, perplexity: 1.109410, precision: 0.875000, batch_len: 83.000000
Train, loss=0.10382843: 521it [13:17,  1.25s/it]2017-06-02 17:04:27,620 root  INFO     step 521.000000 - time: 1.443082, loss: 0.014819, perplexity: 1.014930, precision: 0.953125, batch_len: 117.000000
Train, loss=0.01481926: 522it [13:18,  1.31s/it]2017-06-02 17:04:28,728 root  INFO     step 522.000000 - time: 1.086276, loss: 0.017926, perplexity: 1.018088, precision: 0.937500, batch_len: 103.000000
Train, loss=0.01792592: 523it [13:20,  1.25s/it]2017-06-02 17:04:29,860 root  INFO     step 523.000000 - time: 1.090493, loss: 0.016392, perplexity: 1.016527, precision: 0.937500, batch_len: 109.000000
Train, loss=0.01639207: 524it [13:21,  1.21s/it]2017-06-02 17:04:30,921 root  INFO     step 524.000000 - time: 1.054804, loss: 0.025301, perplexity: 1.025623, precision: 0.921875, batch_len: 81.000000
Train, loss=0.02530067: 525it [13:22,  1.17s/it]2017-06-02 17:04:32,211 root  INFO     step 525.000000 - time: 1.254153, loss: 0.018862, perplexity: 1.019041, precision: 0.953125, batch_len: 123.000000
Train, loss=0.01886206: 526it [13:23,  1.21s/it]2017-06-02 17:04:33,550 root  INFO     step 526.000000 - time: 1.283981, loss: 0.082066, perplexity: 1.085528, precision: 0.875000, batch_len: 94.000000
Train, loss=0.08206607: 527it [13:24,  1.25s/it]2017-06-02 17:04:34,839 root  INFO     step 527.000000 - time: 1.173780, loss: 0.039116, perplexity: 1.039892, precision: 0.921875, batch_len: 107.000000
Train, loss=0.03911643: 528it [13:26,  1.26s/it]2017-06-02 17:04:35,931 root  INFO     step 528.000000 - time: 0.950740, loss: 0.035096, perplexity: 1.035720, precision: 0.921875, batch_len: 80.000000
Train, loss=0.03509647: 529it [13:27,  1.21s/it]2017-06-02 17:04:37,041 root  INFO     step 529.000000 - time: 1.018304, loss: 0.010238, perplexity: 1.010290, precision: 0.968750, batch_len: 87.000000
Train, loss=0.01023750: 530it [13:28,  1.18s/it]2017-06-02 17:04:37,992 root  INFO     step 530.000000 - time: 0.861103, loss: 0.046862, perplexity: 1.047977, precision: 0.921875, batch_len: 79.000000
Train, loss=0.04686201: 531it [13:29,  1.11s/it]2017-06-02 17:04:39,004 root  INFO     step 531.000000 - time: 1.004503, loss: 0.025813, perplexity: 1.026149, precision: 0.937500, batch_len: 89.000000
Train, loss=0.02581278: 532it [13:30,  1.08s/it]2017-06-02 17:04:40,177 root  INFO     step 532.000000 - time: 1.152996, loss: 0.021625, perplexity: 1.021861, precision: 0.937500, batch_len: 85.000000
Train, loss=0.02162503: 533it [13:31,  1.11s/it]2017-06-02 17:04:41,487 root  INFO     step 533.000000 - time: 1.273174, loss: 0.027148, perplexity: 1.027520, precision: 0.937500, batch_len: 115.000000
Train, loss=0.02714807: 534it [13:32,  1.17s/it]2017-06-02 17:04:42,917 root  INFO     step 534.000000 - time: 1.349820, loss: 0.012722, perplexity: 1.012804, precision: 0.968750, batch_len: 116.000000
Train, loss=0.01272247: 535it [13:34,  1.25s/it]2017-06-02 17:04:44,008 root  INFO     step 535.000000 - time: 1.047848, loss: 0.019335, perplexity: 1.019523, precision: 0.937500, batch_len: 98.000000
Train, loss=0.01933495: 536it [13:35,  1.20s/it]2017-06-02 17:04:45,387 root  INFO     step 536.000000 - time: 1.227849, loss: 0.010827, perplexity: 1.010886, precision: 0.968750, batch_len: 119.000000
Train, loss=0.01082679: 537it [13:36,  1.25s/it]2017-06-02 17:04:47,208 root  INFO     step 537.000000 - time: 1.705302, loss: 0.008232, perplexity: 1.008266, precision: 0.984375, batch_len: 125.000000
Train, loss=0.00823160: 538it [13:38,  1.42s/it]2017-06-02 17:04:48,559 root  INFO     step 538.000000 - time: 1.297821, loss: 0.005139, perplexity: 1.005152, precision: 1.000000, batch_len: 129.000000
Train, loss=0.00513868: 539it [13:39,  1.40s/it]2017-06-02 17:04:49,746 root  INFO     step 539.000000 - time: 1.042736, loss: 0.023969, perplexity: 1.024259, precision: 0.937500, batch_len: 84.000000
Train, loss=0.02396922: 540it [13:41,  1.34s/it]2017-06-02 17:04:50,915 root  INFO     step 540.000000 - time: 1.152648, loss: 0.024334, perplexity: 1.024632, precision: 0.937500, batch_len: 91.000000
Train, loss=0.02433383: 541it [13:42,  1.29s/it]2017-06-02 17:04:52,168 root  INFO     step 541.000000 - time: 1.041896, loss: 0.011424, perplexity: 1.011489, precision: 0.968750, batch_len: 99.000000
Train, loss=0.01142351: 542it [13:43,  1.28s/it]2017-06-02 17:04:53,893 root  INFO     step 542.000000 - time: 1.676626, loss: 0.007154, perplexity: 1.007179, precision: 0.984375, batch_len: 126.000000
Train, loss=0.00715361: 543it [13:45,  1.41s/it]2017-06-02 17:04:55,579 root  INFO     step 543.000000 - time: 1.569219, loss: 0.012673, perplexity: 1.012754, precision: 0.984375, batch_len: 144.000000
Train, loss=0.01267316: 544it [13:46,  1.49s/it]2017-06-02 17:04:57,005 root  INFO     step 544.000000 - time: 1.317780, loss: 0.009606, perplexity: 1.009652, precision: 0.953125, batch_len: 118.000000
Train, loss=0.00960592: 545it [13:48,  1.47s/it]2017-06-02 17:04:58,298 root  INFO     step 545.000000 - time: 1.277962, loss: 0.043920, perplexity: 1.044899, precision: 0.921875, batch_len: 135.000000
Train, loss=0.04391990: 546it [13:49,  1.42s/it]2017-06-02 17:05:00,142 root  INFO     step 546.000000 - time: 1.717954, loss: 0.032655, perplexity: 1.033194, precision: 0.937500, batch_len: 137.000000
Train, loss=0.03265530: 547it [13:51,  1.55s/it]2017-06-02 17:05:01,463 root  INFO     step 547.000000 - time: 1.298384, loss: 0.012380, perplexity: 1.012457, precision: 0.968750, batch_len: 122.000000
Train, loss=0.01238046: 548it [13:52,  1.48s/it]2017-06-02 17:05:02,578 root  INFO     step 548.000000 - time: 0.920534, loss: 0.015815, perplexity: 1.015941, precision: 0.984375, batch_len: 78.000000
Train, loss=0.01581548: 549it [13:53,  1.37s/it]2017-06-02 17:05:04,019 root  INFO     step 549.000000 - time: 1.388864, loss: 0.019700, perplexity: 1.019895, precision: 0.937500, batch_len: 138.000000
Train, loss=0.01970007: 550it [13:55,  1.39s/it]2017-06-02 17:05:04,976 root  INFO     step 550.000000 - time: 0.934718, loss: 0.013674, perplexity: 1.013768, precision: 0.984375, batch_len: 82.000000
Train, loss=0.01367397: 551it [13:56,  1.26s/it]2017-06-02 17:05:06,776 root  INFO     step 551.000000 - time: 1.649105, loss: 0.003074, perplexity: 1.003079, precision: 1.000000, batch_len: 133.000000
Train, loss=0.00307401: 552it [13:58,  1.42s/it]2017-06-02 17:05:08,289 root  INFO     step 552.000000 - time: 1.384098, loss: 0.004737, perplexity: 1.004749, precision: 1.000000, batch_len: 130.000000
Train, loss=0.00473731: 553it [13:59,  1.45s/it]2017-06-02 17:05:09,779 root  INFO     step 553.000000 - time: 1.402147, loss: 0.006867, perplexity: 1.006891, precision: 0.984375, batch_len: 136.000000
Train, loss=0.00686738: 554it [14:01,  1.46s/it]2017-06-02 17:05:11,616 root  INFO     step 554.000000 - time: 1.795213, loss: 0.044700, perplexity: 1.045714, precision: 0.953125, batch_len: 152.000000
Train, loss=0.04469975: 555it [14:02,  1.57s/it]2017-06-02 17:05:12,807 root  INFO     step 555.000000 - time: 1.156000, loss: 0.009297, perplexity: 1.009340, precision: 0.968750, batch_len: 96.000000
Train, loss=0.00929711: 556it [14:04,  1.46s/it]2017-06-02 17:05:14,106 root  INFO     step 556.000000 - time: 1.155994, loss: 0.004604, perplexity: 1.004615, precision: 0.984375, batch_len: 72.000000
Train, loss=0.00460438: 557it [14:05,  1.41s/it]2017-06-02 17:05:15,528 root  INFO     step 557.000000 - time: 1.296648, loss: 0.004600, perplexity: 1.004610, precision: 0.984375, batch_len: 134.000000
Train, loss=0.00459986: 558it [14:06,  1.41s/it]2017-06-02 17:05:16,966 root  INFO     step 558.000000 - time: 1.398241, loss: 0.012510, perplexity: 1.012588, precision: 0.968750, batch_len: 141.000000
Train, loss=0.01250952: 559it [14:08,  1.42s/it]2017-06-02 17:05:17,875 root  INFO     step 559.000000 - time: 0.867628, loss: 0.002735, perplexity: 1.002739, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00273497: 560it [14:09,  1.27s/it]2017-06-02 17:05:19,656 root  INFO     step 560.000000 - time: 1.576017, loss: 0.012512, perplexity: 1.012591, precision: 0.953125, batch_len: 139.000000
Train, loss=0.01251238: 561it [14:10,  1.42s/it]2017-06-02 17:05:21,127 root  INFO     step 561.000000 - time: 1.394727, loss: 0.009563, perplexity: 1.009609, precision: 0.984375, batch_len: 132.000000
Train, loss=0.00956342: 562it [14:12,  1.44s/it]2017-06-02 17:05:22,088 root  INFO     step 562.000000 - time: 0.930805, loss: 0.004618, perplexity: 1.004629, precision: 1.000000, batch_len: 77.000000
Train, loss=0.00461813: 563it [14:13,  1.29s/it]2017-06-02 17:05:23,655 root  INFO     step 563.000000 - time: 1.417713, loss: 0.006564, perplexity: 1.006586, precision: 0.984375, batch_len: 142.000000
Train, loss=0.00656393: 564it [14:14,  1.38s/it]2017-06-02 17:05:24,636 root  INFO     step 564.000000 - time: 0.911611, loss: 0.071933, perplexity: 1.074584, precision: 0.937500, batch_len: 76.000000
Train, loss=0.07193334: 565it [14:15,  1.26s/it]2017-06-02 17:05:26,611 root  INFO     step 565.000000 - time: 1.955819, loss: 0.021855, perplexity: 1.022096, precision: 0.953125, batch_len: 150.000000
Train, loss=0.02185540: 566it [14:17,  1.47s/it]2017-06-02 17:05:28,106 root  INFO     step 566.000000 - time: 1.253294, loss: 0.009326, perplexity: 1.009369, precision: 0.984375, batch_len: 71.000000
Train, loss=0.00932585: 567it [14:19,  1.48s/it]2017-06-02 17:05:29,658 root  INFO     step 567.000000 - time: 1.534886, loss: 0.006303, perplexity: 1.006323, precision: 0.984375, batch_len: 131.000000
Train, loss=0.00630327: 568it [14:20,  1.50s/it]2017-06-02 17:05:29,818 root  INFO     Generating first batch)
2017-06-02 17:05:33,601 root  INFO     step 568.000000 - time: 1.304942, loss: 0.008425, perplexity: 1.008461, precision: 0.984375, batch_len: 96.000000
Train, loss=0.00842516: 569it [14:24,  2.23s/it]2017-06-02 17:05:35,132 root  INFO     step 569.000000 - time: 1.133166, loss: 0.020890, perplexity: 1.021110, precision: 0.937500, batch_len: 113.000000
Train, loss=0.02088998: 570it [14:26,  2.02s/it]2017-06-02 17:05:36,657 root  INFO     step 570.000000 - time: 1.339776, loss: 0.015555, perplexity: 1.015677, precision: 0.968750, batch_len: 120.000000
Train, loss=0.01555525: 571it [14:27,  1.87s/it]2017-06-02 17:05:37,651 root  INFO     step 571.000000 - time: 0.913921, loss: 0.016958, perplexity: 1.017103, precision: 0.953125, batch_len: 102.000000
Train, loss=0.01695813: 572it [14:28,  1.61s/it]2017-06-02 17:05:39,148 root  INFO     step 572.000000 - time: 1.482614, loss: 0.020827, perplexity: 1.021045, precision: 0.968750, batch_len: 112.000000
Train, loss=0.02082683: 573it [14:30,  1.58s/it]2017-06-02 17:05:40,440 root  INFO     step 573.000000 - time: 1.264150, loss: 0.013934, perplexity: 1.014032, precision: 0.968750, batch_len: 101.000000
Train, loss=0.01393413: 574it [14:31,  1.49s/it]2017-06-02 17:05:41,879 root  INFO     step 574.000000 - time: 1.328930, loss: 0.009562, perplexity: 1.009608, precision: 0.968750, batch_len: 117.000000
Train, loss=0.00956182: 575it [14:33,  1.48s/it]2017-06-02 17:05:43,033 root  INFO     step 575.000000 - time: 1.060134, loss: 0.005824, perplexity: 1.005841, precision: 0.984375, batch_len: 104.000000
Train, loss=0.00582411: 576it [14:34,  1.38s/it]2017-06-02 17:05:44,381 root  INFO     step 576.000000 - time: 1.222815, loss: 0.008540, perplexity: 1.008576, precision: 0.968750, batch_len: 128.000000
Train, loss=0.00853988: 577it [14:35,  1.37s/it]2017-06-02 17:05:45,531 root  INFO     step 577.000000 - time: 1.118942, loss: 0.047299, perplexity: 1.048435, precision: 0.953125, batch_len: 93.000000
Train, loss=0.04729860: 578it [14:36,  1.30s/it]2017-06-02 17:05:46,857 root  INFO     step 578.000000 - time: 1.273514, loss: 0.010496, perplexity: 1.010551, precision: 0.984375, batch_len: 108.000000
Train, loss=0.01049573: 579it [14:38,  1.31s/it]2017-06-02 17:05:48,018 root  INFO     step 579.000000 - time: 1.046261, loss: 0.023926, perplexity: 1.024215, precision: 0.937500, batch_len: 86.000000
Train, loss=0.02392621: 580it [14:39,  1.27s/it]2017-06-02 17:05:49,156 root  INFO     step 580.000000 - time: 1.109682, loss: 0.010266, perplexity: 1.010319, precision: 0.984375, batch_len: 110.000000
Train, loss=0.01026627: 581it [14:40,  1.23s/it]2017-06-02 17:05:50,293 root  INFO     step 581.000000 - time: 1.082221, loss: 0.023692, perplexity: 1.023975, precision: 0.937500, batch_len: 105.000000
Train, loss=0.02369233: 582it [14:41,  1.20s/it]2017-06-02 17:05:51,345 root  INFO     step 582.000000 - time: 0.985104, loss: 0.036737, perplexity: 1.037420, precision: 0.953125, batch_len: 92.000000
Train, loss=0.03673724: 583it [14:42,  1.16s/it]2017-06-02 17:05:53,384 root  INFO     step 583.000000 - time: 1.857045, loss: 0.052501, perplexity: 1.053903, precision: 0.937500, batch_len: 90.000000
Train, loss=0.05250062: 584it [14:44,  1.42s/it]2017-06-02 17:05:54,510 root  INFO     step 584.000000 - time: 1.106598, loss: 0.036816, perplexity: 1.037502, precision: 0.937500, batch_len: 106.000000
Train, loss=0.03681635: 585it [14:45,  1.33s/it]2017-06-02 17:05:55,628 root  INFO     step 585.000000 - time: 1.063521, loss: 0.053977, perplexity: 1.055460, precision: 0.906250, batch_len: 89.000000
Train, loss=0.05397655: 586it [14:46,  1.27s/it]2017-06-02 17:05:56,674 root  INFO     step 586.000000 - time: 1.010165, loss: 0.029831, perplexity: 1.030280, precision: 0.968750, batch_len: 88.000000
Train, loss=0.02983070: 587it [14:47,  1.20s/it]2017-06-02 17:05:57,817 root  INFO     step 587.000000 - time: 1.038160, loss: 0.049173, perplexity: 1.050402, precision: 0.890625, batch_len: 111.000000
Train, loss=0.04917305: 588it [14:49,  1.18s/it]2017-06-02 17:05:59,502 root  INFO     step 588.000000 - time: 1.656071, loss: 0.005613, perplexity: 1.005629, precision: 0.984375, batch_len: 124.000000
Train, loss=0.00561285: 589it [14:50,  1.33s/it]2017-06-02 17:06:00,527 root  INFO     step 589.000000 - time: 0.955397, loss: 0.047263, perplexity: 1.048398, precision: 0.906250, batch_len: 81.000000
Train, loss=0.04726305: 590it [14:51,  1.24s/it]2017-06-02 17:06:01,748 root  INFO     step 590.000000 - time: 1.162072, loss: 0.021863, perplexity: 1.022104, precision: 0.953125, batch_len: 91.000000
Train, loss=0.02186332: 591it [14:53,  1.24s/it]2017-06-02 17:06:02,885 root  INFO     step 591.000000 - time: 1.094253, loss: 0.033441, perplexity: 1.034007, precision: 0.953125, batch_len: 115.000000
Train, loss=0.03344116: 592it [14:54,  1.21s/it]2017-06-02 17:06:04,202 root  INFO     step 592.000000 - time: 1.263755, loss: 0.014690, perplexity: 1.014798, precision: 0.968750, batch_len: 121.000000
Train, loss=0.01468967: 593it [14:55,  1.24s/it]2017-06-02 17:06:05,350 root  INFO     step 593.000000 - time: 1.122459, loss: 0.032644, perplexity: 1.033182, precision: 0.937500, batch_len: 79.000000
Train, loss=0.03264352: 594it [14:56,  1.21s/it]2017-06-02 17:06:06,671 root  INFO     step 594.000000 - time: 1.270881, loss: 0.023025, perplexity: 1.023293, precision: 0.953125, batch_len: 100.000000
Train, loss=0.02302538: 595it [14:57,  1.24s/it]2017-06-02 17:06:07,776 root  INFO     step 595.000000 - time: 1.076497, loss: 0.024876, perplexity: 1.025188, precision: 0.921875, batch_len: 97.000000
Train, loss=0.02487599: 596it [14:59,  1.20s/it]2017-06-02 17:06:09,233 root  INFO     step 596.000000 - time: 1.433594, loss: 0.084374, perplexity: 1.088035, precision: 0.921875, batch_len: 125.000000
Train, loss=0.08437356: 597it [15:00,  1.28s/it]2017-06-02 17:06:10,250 root  INFO     step 597.000000 - time: 0.984375, loss: 0.041219, perplexity: 1.042080, precision: 0.906250, batch_len: 83.000000
Train, loss=0.04121879: 598it [15:01,  1.20s/it]2017-06-02 17:06:11,297 root  INFO     step 598.000000 - time: 0.993867, loss: 0.044780, perplexity: 1.045798, precision: 0.937500, batch_len: 103.000000
Train, loss=0.04477987: 599it [15:02,  1.15s/it]2017-06-02 17:06:12,719 root  INFO     step 599.000000 - time: 1.333104, loss: 0.113524, perplexity: 1.120219, precision: 0.843750, batch_len: 114.000000
Train, loss=0.11352439: 600it [15:04,  1.23s/it]2017-06-02 17:06:13,792 root  INFO     step 600.000000 - time: 1.007440, loss: 0.050796, perplexity: 1.052108, precision: 0.921875, batch_len: 80.000000
Train, loss=0.05079604: 601it [15:05,  1.19s/it]2017-06-02 17:06:14,839 root  INFO     step 601.000000 - time: 1.031684, loss: 0.070364, perplexity: 1.072899, precision: 0.906250, batch_len: 85.000000
Train, loss=0.07036430: 602it [15:06,  1.14s/it]2017-06-02 17:06:15,886 root  INFO     step 602.000000 - time: 0.982984, loss: 0.037328, perplexity: 1.038033, precision: 0.953125, batch_len: 82.000000
Train, loss=0.03732798: 603it [15:07,  1.12s/it]2017-06-02 17:06:17,003 root  INFO     step 603.000000 - time: 1.085383, loss: 0.032287, perplexity: 1.032814, precision: 0.906250, batch_len: 107.000000
Train, loss=0.03228712: 604it [15:08,  1.12s/it]2017-06-02 17:06:18,378 root  INFO     step 604.000000 - time: 1.310553, loss: 0.023036, perplexity: 1.023304, precision: 0.937500, batch_len: 123.000000
Train, loss=0.02303631: 605it [15:09,  1.19s/it]2017-06-02 17:06:19,975 root  INFO     step 605.000000 - time: 1.553473, loss: 0.025552, perplexity: 1.025881, precision: 0.953125, batch_len: 116.000000
Train, loss=0.02555219: 606it [15:11,  1.31s/it]2017-06-02 17:06:21,112 root  INFO     step 606.000000 - time: 1.128586, loss: 0.036115, perplexity: 1.036775, precision: 0.906250, batch_len: 109.000000
Train, loss=0.03611459: 607it [15:12,  1.26s/it]2017-06-02 17:06:22,304 root  INFO     step 607.000000 - time: 0.988325, loss: 0.063018, perplexity: 1.065046, precision: 0.843750, batch_len: 84.000000
Train, loss=0.06301825: 608it [15:13,  1.24s/it]2017-06-02 17:06:23,384 root  INFO     step 608.000000 - time: 1.057335, loss: 0.097924, perplexity: 1.102879, precision: 0.859375, batch_len: 94.000000
Train, loss=0.09792403: 609it [15:14,  1.19s/it]2017-06-02 17:06:24,798 root  INFO     step 609.000000 - time: 1.240186, loss: 0.011678, perplexity: 1.011747, precision: 0.953125, batch_len: 129.000000
Train, loss=0.01167848: 610it [15:16,  1.26s/it]2017-06-02 17:06:26,213 root  INFO     step 610.000000 - time: 1.389243, loss: 0.011397, perplexity: 1.011463, precision: 0.968750, batch_len: 118.000000
Train, loss=0.01139742: 611it [15:17,  1.31s/it]2017-06-02 17:06:27,494 root  INFO     step 611.000000 - time: 1.219456, loss: 0.006350, perplexity: 1.006371, precision: 1.000000, batch_len: 98.000000
Train, loss=0.00635035: 612it [15:18,  1.30s/it]2017-06-02 17:06:28,868 root  INFO     step 612.000000 - time: 1.214797, loss: 0.011706, perplexity: 1.011775, precision: 0.968750, batch_len: 87.000000
Train, loss=0.01170599: 613it [15:20,  1.32s/it]2017-06-02 17:06:30,060 root  INFO     step 613.000000 - time: 1.120519, loss: 0.023054, perplexity: 1.023322, precision: 0.937500, batch_len: 99.000000
Train, loss=0.02305380: 614it [15:21,  1.28s/it]2017-06-02 17:06:31,927 root  INFO     step 614.000000 - time: 1.506438, loss: 0.018436, perplexity: 1.018607, precision: 0.984375, batch_len: 144.000000
Train, loss=0.01843593: 615it [15:23,  1.46s/it]2017-06-02 17:06:33,429 root  INFO     step 615.000000 - time: 1.433758, loss: 0.464788, perplexity: 1.591677, precision: 0.765625, batch_len: 126.000000
Train, loss=0.46478826: 616it [15:24,  1.47s/it]2017-06-02 17:06:35,030 root  INFO     step 616.000000 - time: 1.556284, loss: 0.705613, perplexity: 2.025089, precision: 0.546875, batch_len: 119.000000
Train, loss=0.70561349: 617it [15:26,  1.51s/it]2017-06-02 17:06:36,421 root  INFO     step 617.000000 - time: 1.333815, loss: 0.580789, perplexity: 1.787448, precision: 0.281250, batch_len: 135.000000
Train, loss=0.58078909: 618it [15:27,  1.47s/it]2017-06-02 17:06:37,857 root  INFO     step 618.000000 - time: 1.385679, loss: 0.583881, perplexity: 1.792983, precision: 0.312500, batch_len: 130.000000
Train, loss=0.58388066: 619it [15:29,  1.46s/it]2017-06-02 17:06:39,348 root  INFO     step 619.000000 - time: 1.315801, loss: 0.257114, perplexity: 1.293193, precision: 0.593750, batch_len: 137.000000
Train, loss=0.25711417: 620it [15:30,  1.47s/it]2017-06-02 17:06:40,840 root  INFO     step 620.000000 - time: 1.466162, loss: 0.067330, perplexity: 1.069649, precision: 0.812500, batch_len: 133.000000
Train, loss=0.06733028: 621it [15:32,  1.48s/it]2017-06-02 17:06:42,102 root  INFO     step 621.000000 - time: 1.238690, loss: 0.092192, perplexity: 1.096575, precision: 0.781250, batch_len: 96.000000
Train, loss=0.09219194: 622it [15:33,  1.41s/it]2017-06-02 17:06:43,573 root  INFO     step 622.000000 - time: 1.459690, loss: 0.026113, perplexity: 1.026457, precision: 0.937500, batch_len: 136.000000
Train, loss=0.02611304: 623it [15:34,  1.43s/it]2017-06-02 17:06:45,506 root  INFO     step 623.000000 - time: 1.766274, loss: 0.018582, perplexity: 1.018756, precision: 0.937500, batch_len: 150.000000
Train, loss=0.01858203: 624it [15:36,  1.58s/it]2017-06-02 17:06:46,909 root  INFO     step 624.000000 - time: 1.346326, loss: 0.034627, perplexity: 1.035234, precision: 0.906250, batch_len: 122.000000
Train, loss=0.03462745: 625it [15:38,  1.53s/it]2017-06-02 17:06:48,118 root  INFO     step 625.000000 - time: 1.183898, loss: 0.045903, perplexity: 1.046973, precision: 0.875000, batch_len: 78.000000
Train, loss=0.04590289: 626it [15:39,  1.43s/it]2017-06-02 17:06:49,674 root  INFO     step 626.000000 - time: 1.437399, loss: 0.038717, perplexity: 1.039476, precision: 0.937500, batch_len: 139.000000
Train, loss=0.03871715: 627it [15:40,  1.47s/it]2017-06-02 17:06:51,252 root  INFO     step 627.000000 - time: 1.428067, loss: 0.032018, perplexity: 1.032536, precision: 0.906250, batch_len: 138.000000
Train, loss=0.03201823: 628it [15:42,  1.50s/it]2017-06-02 17:06:53,162 root  INFO     step 628.000000 - time: 1.873943, loss: 0.013128, perplexity: 1.013215, precision: 0.968750, batch_len: 152.000000
Train, loss=0.01312821: 629it [15:44,  1.62s/it]2017-06-02 17:06:54,573 root  INFO     step 629.000000 - time: 1.226905, loss: 0.018835, perplexity: 1.019014, precision: 0.984375, batch_len: 74.000000
Train, loss=0.01883539: 630it [15:45,  1.56s/it]2017-06-02 17:06:55,871 root  INFO     step 630.000000 - time: 1.230663, loss: 0.038290, perplexity: 1.039032, precision: 0.968750, batch_len: 72.000000
Train, loss=0.03828956: 631it [15:47,  1.48s/it]2017-06-02 17:06:57,321 root  INFO     step 631.000000 - time: 1.331328, loss: 0.019077, perplexity: 1.019260, precision: 0.984375, batch_len: 132.000000
Train, loss=0.01907717: 632it [15:48,  1.47s/it]2017-06-02 17:06:58,304 root  INFO     step 632.000000 - time: 0.916939, loss: 0.041798, perplexity: 1.042684, precision: 0.906250, batch_len: 76.000000
Train, loss=0.04179819: 633it [15:49,  1.33s/it]2017-06-02 17:06:59,219 root  INFO     step 633.000000 - time: 0.829572, loss: 0.036479, perplexity: 1.037153, precision: 0.937500, batch_len: 77.000000
Train, loss=0.03647919: 634it [15:50,  1.20s/it]2017-06-02 17:07:00,786 root  INFO     step 634.000000 - time: 1.561915, loss: 0.027994, perplexity: 1.028390, precision: 0.953125, batch_len: 141.000000
Train, loss=0.02799417: 635it [15:52,  1.31s/it]2017-06-02 17:07:02,337 root  INFO     step 635.000000 - time: 1.420582, loss: 0.020993, perplexity: 1.021215, precision: 0.953125, batch_len: 134.000000
Train, loss=0.02099276: 636it [15:53,  1.38s/it]2017-06-02 17:07:03,853 root  INFO     step 636.000000 - time: 1.392611, loss: 0.035568, perplexity: 1.036208, precision: 0.953125, batch_len: 131.000000
Train, loss=0.03556835: 637it [15:55,  1.42s/it]2017-06-02 17:07:05,274 root  INFO     step 637.000000 - time: 1.363438, loss: 0.026760, perplexity: 1.027121, precision: 0.937500, batch_len: 142.000000
Train, loss=0.02675985: 638it [15:56,  1.42s/it]2017-06-02 17:07:06,527 root  INFO     step 638.000000 - time: 1.101287, loss: 0.039557, perplexity: 1.040350, precision: 0.968750, batch_len: 71.000000
Train, loss=0.03955717: 639it [15:57,  1.37s/it]2017-06-02 17:07:06,657 root  INFO     Generating first batch)
2017-06-02 17:07:10,322 root  INFO     step 639.000000 - time: 0.949013, loss: 0.073708, perplexity: 1.076493, precision: 0.937500, batch_len: 96.000000
Train, loss=0.07370825: 640it [16:01,  2.10s/it]2017-06-02 17:07:11,624 root  INFO     step 640.000000 - time: 1.229685, loss: 0.036195, perplexity: 1.036858, precision: 0.921875, batch_len: 110.000000
Train, loss=0.03619513: 641it [16:02,  1.86s/it]2017-06-02 17:07:13,000 root  INFO     step 641.000000 - time: 1.182754, loss: 0.032080, perplexity: 1.032601, precision: 0.968750, batch_len: 92.000000
Train, loss=0.03208048: 642it [16:04,  1.71s/it]2017-06-02 17:07:14,077 root  INFO     step 642.000000 - time: 1.044241, loss: 0.027470, perplexity: 1.027851, precision: 0.953125, batch_len: 105.000000
Train, loss=0.02747043: 643it [16:05,  1.52s/it]2017-06-02 17:07:15,430 root  INFO     step 643.000000 - time: 1.065900, loss: 0.013893, perplexity: 1.013990, precision: 0.953125, batch_len: 101.000000
Train, loss=0.01389308: 644it [16:06,  1.47s/it]2017-06-02 17:07:16,485 root  INFO     step 644.000000 - time: 1.029588, loss: 0.056068, perplexity: 1.057670, precision: 0.906250, batch_len: 88.000000
Train, loss=0.05606791: 645it [16:07,  1.35s/it]2017-06-02 17:07:17,386 root  INFO     step 645.000000 - time: 0.876602, loss: 0.022161, perplexity: 1.022408, precision: 0.968750, batch_len: 81.000000
Train, loss=0.02216091: 646it [16:08,  1.21s/it]2017-06-02 17:07:19,253 root  INFO     step 646.000000 - time: 1.721117, loss: 0.022631, perplexity: 1.022889, precision: 0.921875, batch_len: 120.000000
Train, loss=0.02263055: 647it [16:10,  1.41s/it]2017-06-02 17:07:20,413 root  INFO     step 647.000000 - time: 1.105104, loss: 0.020536, perplexity: 1.020749, precision: 0.921875, batch_len: 108.000000
Train, loss=0.02053634: 648it [16:11,  1.33s/it]2017-06-02 17:07:21,435 root  INFO     step 648.000000 - time: 0.989929, loss: 0.040076, perplexity: 1.040890, precision: 0.921875, batch_len: 93.000000
Train, loss=0.04007627: 649it [16:12,  1.24s/it]2017-06-02 17:07:23,157 root  INFO     step 649.000000 - time: 1.545182, loss: 0.050654, perplexity: 1.051959, precision: 0.890625, batch_len: 90.000000
Train, loss=0.05065377: 650it [16:14,  1.39s/it]2017-06-02 17:07:24,831 root  INFO     step 650.000000 - time: 1.511227, loss: 0.005643, perplexity: 1.005659, precision: 1.000000, batch_len: 124.000000
Train, loss=0.00564269: 651it [16:16,  1.47s/it]2017-06-02 17:07:26,199 root  INFO     step 651.000000 - time: 1.317681, loss: 0.032384, perplexity: 1.032914, precision: 0.937500, batch_len: 111.000000
Train, loss=0.03238380: 652it [16:17,  1.44s/it]2017-06-02 17:07:27,327 root  INFO     step 652.000000 - time: 1.108239, loss: 0.025067, perplexity: 1.025384, precision: 0.953125, batch_len: 89.000000
Train, loss=0.02506712: 653it [16:18,  1.35s/it]2017-06-02 17:07:28,722 root  INFO     step 653.000000 - time: 1.381283, loss: 0.013140, perplexity: 1.013227, precision: 0.953125, batch_len: 128.000000
Train, loss=0.01314039: 654it [16:20,  1.36s/it]2017-06-02 17:07:29,848 root  INFO     step 654.000000 - time: 1.087117, loss: 0.058623, perplexity: 1.060375, precision: 0.921875, batch_len: 100.000000
Train, loss=0.05862308: 655it [16:21,  1.29s/it]2017-06-02 17:07:30,973 root  INFO     step 655.000000 - time: 1.117320, loss: 0.022931, perplexity: 1.023196, precision: 0.984375, batch_len: 106.000000
Train, loss=0.02293063: 656it [16:22,  1.24s/it]2017-06-02 17:07:32,225 root  INFO     step 656.000000 - time: 1.100434, loss: 0.029004, perplexity: 1.029428, precision: 0.937500, batch_len: 115.000000
Train, loss=0.02900378: 657it [16:23,  1.24s/it]2017-06-02 17:07:33,399 root  INFO     step 657.000000 - time: 1.159626, loss: 0.022039, perplexity: 1.022283, precision: 0.953125, batch_len: 97.000000
Train, loss=0.02203877: 658it [16:24,  1.22s/it]2017-06-02 17:07:34,705 root  INFO     step 658.000000 - time: 1.287353, loss: 0.023142, perplexity: 1.023411, precision: 0.953125, batch_len: 112.000000
Train, loss=0.02314165: 659it [16:26,  1.25s/it]2017-06-02 17:07:36,136 root  INFO     step 659.000000 - time: 1.371195, loss: 0.015855, perplexity: 1.015982, precision: 0.937500, batch_len: 116.000000
Train, loss=0.01585526: 660it [16:27,  1.30s/it]2017-06-02 17:07:37,290 root  INFO     step 660.000000 - time: 1.118172, loss: 0.031488, perplexity: 1.031989, precision: 0.953125, batch_len: 113.000000
Train, loss=0.03148780: 661it [16:28,  1.26s/it]2017-06-02 17:07:38,382 root  INFO     step 661.000000 - time: 1.075733, loss: 0.004399, perplexity: 1.004409, precision: 1.000000, batch_len: 104.000000
Train, loss=0.00439917: 662it [16:29,  1.21s/it]2017-06-02 17:07:39,615 root  INFO     step 662.000000 - time: 1.058491, loss: 0.029338, perplexity: 1.029772, precision: 0.953125, batch_len: 85.000000
Train, loss=0.02933779: 663it [16:30,  1.22s/it]2017-06-02 17:07:41,248 root  INFO     step 663.000000 - time: 1.618981, loss: 0.013884, perplexity: 1.013981, precision: 0.984375, batch_len: 121.000000
Train, loss=0.01388426: 664it [16:32,  1.34s/it]2017-06-02 17:07:42,337 root  INFO     step 664.000000 - time: 1.061246, loss: 0.030878, perplexity: 1.031360, precision: 0.953125, batch_len: 107.000000
Train, loss=0.03087806: 665it [16:33,  1.27s/it]2017-06-02 17:07:43,536 root  INFO     step 665.000000 - time: 1.142467, loss: 0.049253, perplexity: 1.050486, precision: 0.921875, batch_len: 114.000000
Train, loss=0.04925255: 666it [16:34,  1.25s/it]2017-06-02 17:07:44,931 root  INFO     step 666.000000 - time: 1.342779, loss: 0.029024, perplexity: 1.029449, precision: 0.906250, batch_len: 117.000000
Train, loss=0.02902378: 667it [16:36,  1.29s/it]2017-06-02 17:07:45,952 root  INFO     step 667.000000 - time: 0.966225, loss: 0.064596, perplexity: 1.066728, precision: 0.921875, batch_len: 103.000000
Train, loss=0.06459582: 668it [16:37,  1.21s/it]2017-06-02 17:07:47,171 root  INFO     step 668.000000 - time: 1.169530, loss: 0.028059, perplexity: 1.028456, precision: 0.921875, batch_len: 102.000000
Train, loss=0.02805903: 669it [16:38,  1.21s/it]2017-06-02 17:07:48,676 root  INFO     step 669.000000 - time: 1.438771, loss: 0.020414, perplexity: 1.020624, precision: 0.968750, batch_len: 123.000000
Train, loss=0.02041419: 670it [16:39,  1.30s/it]2017-06-02 17:07:50,110 root  INFO     step 670.000000 - time: 1.392143, loss: 0.023677, perplexity: 1.023959, precision: 0.937500, batch_len: 125.000000
Train, loss=0.02367683: 671it [16:41,  1.34s/it]2017-06-02 17:07:51,199 root  INFO     step 671.000000 - time: 0.993970, loss: 0.010197, perplexity: 1.010250, precision: 0.953125, batch_len: 99.000000
Train, loss=0.01019739: 672it [16:42,  1.26s/it]2017-06-02 17:07:52,128 root  INFO     step 672.000000 - time: 0.922612, loss: 0.025977, perplexity: 1.026317, precision: 0.953125, batch_len: 86.000000
Train, loss=0.02597701: 673it [16:43,  1.16s/it]2017-06-02 17:07:53,263 root  INFO     step 673.000000 - time: 1.040666, loss: 0.023231, perplexity: 1.023503, precision: 0.953125, batch_len: 79.000000
Train, loss=0.02323059: 674it [16:44,  1.16s/it]2017-06-02 17:07:54,537 root  INFO     step 674.000000 - time: 1.253009, loss: 0.009368, perplexity: 1.009412, precision: 0.968750, batch_len: 87.000000
Train, loss=0.00936809: 675it [16:45,  1.19s/it]2017-06-02 17:07:55,500 root  INFO     step 675.000000 - time: 0.940212, loss: 0.015510, perplexity: 1.015631, precision: 0.953125, batch_len: 80.000000
Train, loss=0.01551013: 676it [16:46,  1.12s/it]2017-06-02 17:07:56,790 root  INFO     step 676.000000 - time: 1.147053, loss: 0.022893, perplexity: 1.023158, precision: 0.906250, batch_len: 109.000000
Train, loss=0.02289350: 677it [16:48,  1.17s/it]2017-06-02 17:07:57,867 root  INFO     step 677.000000 - time: 0.999179, loss: 0.006498, perplexity: 1.006519, precision: 0.984375, batch_len: 98.000000
Train, loss=0.00649777: 678it [16:49,  1.14s/it]2017-06-02 17:07:58,937 root  INFO     step 678.000000 - time: 0.966956, loss: 0.061689, perplexity: 1.063632, precision: 0.875000, batch_len: 94.000000
Train, loss=0.06168915: 679it [16:50,  1.12s/it]2017-06-02 17:08:00,026 root  INFO     step 679.000000 - time: 1.078110, loss: 0.029751, perplexity: 1.030198, precision: 0.953125, batch_len: 91.000000
Train, loss=0.02975096: 680it [16:51,  1.11s/it]2017-06-02 17:08:01,562 root  INFO     step 680.000000 - time: 1.523055, loss: 0.008130, perplexity: 1.008164, precision: 1.000000, batch_len: 118.000000
Train, loss=0.00813042: 681it [16:52,  1.24s/it]2017-06-02 17:08:02,609 root  INFO     step 681.000000 - time: 1.026269, loss: 0.017921, perplexity: 1.018082, precision: 0.984375, batch_len: 83.000000
Train, loss=0.01792071: 682it [16:53,  1.18s/it]2017-06-02 17:08:04,122 root  INFO     step 682.000000 - time: 1.370164, loss: 0.048245, perplexity: 1.049428, precision: 0.953125, batch_len: 126.000000
Train, loss=0.04824489: 683it [16:55,  1.28s/it]2017-06-02 17:08:05,438 root  INFO     step 683.000000 - time: 1.195552, loss: 0.039010, perplexity: 1.039781, precision: 0.906250, batch_len: 119.000000
Train, loss=0.03900977: 684it [16:56,  1.29s/it]2017-06-02 17:08:06,571 root  INFO     step 684.000000 - time: 1.100820, loss: 0.012656, perplexity: 1.012737, precision: 0.984375, batch_len: 84.000000
Train, loss=0.01265620: 685it [16:57,  1.24s/it]2017-06-02 17:08:08,242 root  INFO     step 685.000000 - time: 1.665128, loss: 0.047639, perplexity: 1.048792, precision: 0.953125, batch_len: 137.000000
Train, loss=0.04763877: 686it [16:59,  1.37s/it]2017-06-02 17:08:09,595 root  INFO     step 686.000000 - time: 1.316392, loss: 0.009544, perplexity: 1.009590, precision: 0.984375, batch_len: 133.000000
Train, loss=0.00954440: 687it [17:00,  1.37s/it]2017-06-02 17:08:11,071 root  INFO     step 687.000000 - time: 1.419154, loss: 0.025901, perplexity: 1.026239, precision: 0.921875, batch_len: 144.000000
Train, loss=0.02590058: 688it [17:02,  1.40s/it]2017-06-02 17:08:12,352 root  INFO     step 688.000000 - time: 1.191005, loss: 0.013009, perplexity: 1.013094, precision: 0.968750, batch_len: 129.000000
Train, loss=0.01300900: 689it [17:03,  1.36s/it]2017-06-02 17:08:13,904 root  INFO     step 689.000000 - time: 1.454794, loss: 0.021029, perplexity: 1.021252, precision: 0.937500, batch_len: 135.000000
Train, loss=0.02102934: 690it [17:05,  1.42s/it]2017-06-02 17:08:15,585 root  INFO     step 690.000000 - time: 1.591804, loss: 0.007069, perplexity: 1.007094, precision: 0.984375, batch_len: 136.000000
Train, loss=0.00706906: 691it [17:06,  1.50s/it]2017-06-02 17:08:17,057 root  INFO     step 691.000000 - time: 1.294460, loss: 0.004119, perplexity: 1.004127, precision: 1.000000, batch_len: 134.000000
Train, loss=0.00411872: 692it [17:08,  1.49s/it]2017-06-02 17:08:18,188 root  INFO     step 692.000000 - time: 0.901376, loss: 0.020954, perplexity: 1.021175, precision: 0.968750, batch_len: 78.000000
Train, loss=0.02095398: 693it [17:09,  1.38s/it]2017-06-02 17:08:19,212 root  INFO     step 693.000000 - time: 0.988856, loss: 0.019601, perplexity: 1.019795, precision: 0.953125, batch_len: 122.000000
Train, loss=0.01960134: 694it [17:10,  1.27s/it]2017-06-02 17:08:20,505 root  INFO     step 694.000000 - time: 1.190260, loss: 0.006700, perplexity: 1.006723, precision: 1.000000, batch_len: 96.000000
Train, loss=0.00670025: 695it [17:11,  1.28s/it]2017-06-02 17:08:22,248 root  INFO     step 695.000000 - time: 1.587083, loss: 0.003552, perplexity: 1.003558, precision: 1.000000, batch_len: 138.000000
Train, loss=0.00355153: 696it [17:13,  1.42s/it]2017-06-02 17:08:23,355 root  INFO     step 696.000000 - time: 1.008746, loss: 0.021685, perplexity: 1.021922, precision: 0.984375, batch_len: 82.000000
Train, loss=0.02168495: 697it [17:14,  1.33s/it]2017-06-02 17:08:24,951 root  INFO     step 697.000000 - time: 1.448273, loss: 0.021057, perplexity: 1.021280, precision: 0.953125, batch_len: 141.000000
Train, loss=0.02105660: 698it [17:16,  1.41s/it]2017-06-02 17:08:25,750 root  INFO     step 698.000000 - time: 0.758752, loss: 0.002681, perplexity: 1.002685, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00268139: 699it [17:17,  1.22s/it]2017-06-02 17:08:27,363 root  INFO     step 699.000000 - time: 1.494630, loss: 0.011931, perplexity: 1.012003, precision: 0.968750, batch_len: 130.000000
Train, loss=0.01193115: 700it [17:18,  1.34s/it]2017-06-02 17:08:29,548 root  INFO     step 700.000000 - time: 2.162517, loss: 0.006125, perplexity: 1.006143, precision: 1.000000, batch_len: 152.000000
Train, loss=0.00612457: 701it [17:20,  1.59s/it]2017-06-02 17:08:31,001 root  INFO     step 701.000000 - time: 1.437342, loss: 0.009572, perplexity: 1.009618, precision: 0.968750, batch_len: 132.000000
Train, loss=0.00957211: 702it [17:22,  1.55s/it]2017-06-02 17:08:32,746 root  INFO     step 702.000000 - time: 1.476741, loss: 0.005931, perplexity: 1.005949, precision: 1.000000, batch_len: 139.000000
Train, loss=0.00593111: 703it [17:24,  1.61s/it]2017-06-02 17:08:33,720 root  INFO     step 703.000000 - time: 0.930312, loss: 0.024108, perplexity: 1.024401, precision: 0.953125, batch_len: 76.000000
Train, loss=0.02410794: 704it [17:25,  1.42s/it]2017-06-02 17:08:35,226 root  INFO     step 704.000000 - time: 1.502151, loss: 0.085684, perplexity: 1.089462, precision: 0.984375, batch_len: 131.000000
Train, loss=0.08568367: 705it [17:26,  1.45s/it]2017-06-02 17:08:36,432 root  INFO     step 705.000000 - time: 1.110165, loss: 0.029956, perplexity: 1.030409, precision: 0.953125, batch_len: 77.000000
Train, loss=0.02995568: 706it [17:27,  1.37s/it]2017-06-02 17:08:38,312 root  INFO     step 706.000000 - time: 1.836425, loss: 0.076699, perplexity: 1.079717, precision: 0.906250, batch_len: 150.000000
Train, loss=0.07669889: 707it [17:29,  1.53s/it]2017-06-02 17:08:39,543 root  INFO     step 707.000000 - time: 1.196436, loss: 0.038124, perplexity: 1.038860, precision: 0.953125, batch_len: 72.000000
Train, loss=0.03812365: 708it [17:30,  1.44s/it]2017-06-02 17:08:40,952 root  INFO     step 708.000000 - time: 1.357620, loss: 0.042782, perplexity: 1.043711, precision: 0.906250, batch_len: 142.000000
Train, loss=0.04278236: 709it [17:32,  1.43s/it]2017-06-02 17:08:42,179 root  INFO     step 709.000000 - time: 1.126874, loss: 0.035998, perplexity: 1.036654, precision: 0.953125, batch_len: 71.000000
Train, loss=0.03599823: 710it [17:33,  1.37s/it]2017-06-02 17:08:42,385 root  INFO     Generating first batch)
2017-06-02 17:08:46,439 root  INFO     step 710.000000 - time: 1.185209, loss: 0.009685, perplexity: 1.009732, precision: 0.984375, batch_len: 96.000000
Train, loss=0.00968530: 711it [17:37,  2.24s/it]2017-06-02 17:08:48,007 root  INFO     step 711.000000 - time: 1.292812, loss: 0.029848, perplexity: 1.030298, precision: 0.921875, batch_len: 110.000000
Train, loss=0.02984830: 712it [17:39,  2.04s/it]2017-06-02 17:08:49,097 root  INFO     step 712.000000 - time: 1.014972, loss: 0.045909, perplexity: 1.046979, precision: 0.890625, batch_len: 93.000000
Train, loss=0.04590926: 713it [17:40,  1.75s/it]2017-06-02 17:08:50,467 root  INFO     step 713.000000 - time: 1.361203, loss: 0.029550, perplexity: 1.029991, precision: 0.937500, batch_len: 120.000000
Train, loss=0.02955021: 714it [17:41,  1.64s/it]2017-06-02 17:08:51,492 root  INFO     step 714.000000 - time: 1.020840, loss: 0.020816, perplexity: 1.021034, precision: 0.953125, batch_len: 105.000000
Train, loss=0.02081598: 715it [17:42,  1.45s/it]2017-06-02 17:08:53,105 root  INFO     step 715.000000 - time: 1.441008, loss: 0.012172, perplexity: 1.012246, precision: 0.953125, batch_len: 128.000000
Train, loss=0.01217161: 716it [17:44,  1.50s/it]2017-06-02 17:08:54,467 root  INFO     step 716.000000 - time: 1.188126, loss: 0.086782, perplexity: 1.090659, precision: 0.906250, batch_len: 88.000000
Train, loss=0.08678217: 717it [17:45,  1.46s/it]2017-06-02 17:08:55,732 root  INFO     step 717.000000 - time: 1.148965, loss: 0.067782, perplexity: 1.070132, precision: 0.890625, batch_len: 108.000000
Train, loss=0.06778155: 718it [17:47,  1.40s/it]2017-06-02 17:08:57,422 root  INFO     step 718.000000 - time: 1.599003, loss: 0.101831, perplexity: 1.107196, precision: 0.828125, batch_len: 90.000000
Train, loss=0.10183108: 719it [17:48,  1.49s/it]2017-06-02 17:08:58,342 root  INFO     step 719.000000 - time: 0.898142, loss: 0.072742, perplexity: 1.075453, precision: 0.875000, batch_len: 91.000000
Train, loss=0.07274154: 720it [17:49,  1.32s/it]2017-06-02 17:08:59,722 root  INFO     step 720.000000 - time: 1.336518, loss: 0.048316, perplexity: 1.049502, precision: 0.875000, batch_len: 113.000000
Train, loss=0.04831561: 721it [17:51,  1.34s/it]2017-06-02 17:09:01,018 root  INFO     step 721.000000 - time: 1.254160, loss: 0.139699, perplexity: 1.149928, precision: 0.828125, batch_len: 101.000000
Train, loss=0.13969943: 722it [17:52,  1.32s/it]2017-06-02 17:09:02,068 root  INFO     step 722.000000 - time: 1.008556, loss: 0.124490, perplexity: 1.132571, precision: 0.843750, batch_len: 92.000000
Train, loss=0.12449005: 723it [17:53,  1.24s/it]2017-06-02 17:09:03,423 root  INFO     step 723.000000 - time: 1.345524, loss: 0.297091, perplexity: 1.345937, precision: 0.765625, batch_len: 117.000000
Train, loss=0.29709074: 724it [17:54,  1.28s/it]2017-06-02 17:09:04,508 root  INFO     step 724.000000 - time: 1.050425, loss: 0.087428, perplexity: 1.091363, precision: 0.843750, batch_len: 97.000000
Train, loss=0.08742754: 725it [17:55,  1.22s/it]2017-06-02 17:09:05,533 root  INFO     step 725.000000 - time: 1.015623, loss: 0.180374, perplexity: 1.197666, precision: 0.781250, batch_len: 100.000000
Train, loss=0.18037429: 726it [17:56,  1.16s/it]2017-06-02 17:09:06,799 root  INFO     step 726.000000 - time: 1.099687, loss: 0.241608, perplexity: 1.273295, precision: 0.750000, batch_len: 79.000000
Train, loss=0.24160799: 727it [17:58,  1.19s/it]2017-06-02 17:09:08,009 root  INFO     step 727.000000 - time: 1.202805, loss: 0.157156, perplexity: 1.170178, precision: 0.796875, batch_len: 103.000000
Train, loss=0.15715620: 728it [17:59,  1.20s/it]2017-06-02 17:09:09,062 root  INFO     step 728.000000 - time: 1.021720, loss: 0.111472, perplexity: 1.117922, precision: 0.750000, batch_len: 86.000000
Train, loss=0.11147162: 729it [18:00,  1.15s/it]2017-06-02 17:09:10,520 root  INFO     step 729.000000 - time: 1.304864, loss: 0.087448, perplexity: 1.091385, precision: 0.843750, batch_len: 112.000000
Train, loss=0.08744799: 730it [18:01,  1.25s/it]2017-06-02 17:09:11,448 root  INFO     step 730.000000 - time: 0.915968, loss: 0.046152, perplexity: 1.047233, precision: 0.875000, batch_len: 89.000000
Train, loss=0.04615167: 731it [18:02,  1.15s/it]2017-06-02 17:09:12,822 root  INFO     step 731.000000 - time: 1.348708, loss: 0.035220, perplexity: 1.035848, precision: 0.906250, batch_len: 115.000000
Train, loss=0.03522000: 732it [18:04,  1.22s/it]2017-06-02 17:09:14,178 root  INFO     step 732.000000 - time: 1.350068, loss: 0.052809, perplexity: 1.054228, precision: 0.906250, batch_len: 114.000000
Train, loss=0.05280898: 733it [18:05,  1.26s/it]2017-06-02 17:09:15,331 root  INFO     step 733.000000 - time: 1.103445, loss: 0.018471, perplexity: 1.018642, precision: 0.968750, batch_len: 106.000000
Train, loss=0.01847078: 734it [18:06,  1.23s/it]2017-06-02 17:09:16,360 root  INFO     step 734.000000 - time: 0.994340, loss: 0.037091, perplexity: 1.037788, precision: 0.906250, batch_len: 102.000000
Train, loss=0.03709126: 735it [18:07,  1.17s/it]2017-06-02 17:09:17,431 root  INFO     step 735.000000 - time: 0.961182, loss: 0.044361, perplexity: 1.045359, precision: 0.906250, batch_len: 85.000000
Train, loss=0.04436059: 736it [18:08,  1.14s/it]2017-06-02 17:09:18,313 root  INFO     step 736.000000 - time: 0.866182, loss: 0.033529, perplexity: 1.034098, precision: 0.921875, batch_len: 81.000000
Train, loss=0.03352933: 737it [18:09,  1.06s/it]2017-06-02 17:09:19,652 root  INFO     step 737.000000 - time: 1.282331, loss: 0.055083, perplexity: 1.056628, precision: 0.890625, batch_len: 109.000000
Train, loss=0.05508286: 738it [18:10,  1.14s/it]2017-06-02 17:09:20,933 root  INFO     step 738.000000 - time: 1.256291, loss: 0.030896, perplexity: 1.031378, precision: 0.906250, batch_len: 104.000000
Train, loss=0.03089579: 739it [18:12,  1.19s/it]2017-06-02 17:09:22,403 root  INFO     step 739.000000 - time: 1.352419, loss: 0.020704, perplexity: 1.020920, precision: 0.953125, batch_len: 124.000000
Train, loss=0.02070396: 740it [18:13,  1.27s/it]2017-06-02 17:09:23,917 root  INFO     step 740.000000 - time: 1.486906, loss: 0.030301, perplexity: 1.030765, precision: 0.937500, batch_len: 125.000000
Train, loss=0.03030094: 741it [18:15,  1.34s/it]2017-06-02 17:09:25,265 root  INFO     step 741.000000 - time: 1.227800, loss: 0.012783, perplexity: 1.012865, precision: 0.953125, batch_len: 123.000000
Train, loss=0.01278285: 742it [18:16,  1.34s/it]2017-06-02 17:09:26,388 root  INFO     step 742.000000 - time: 1.083314, loss: 0.019403, perplexity: 1.019593, precision: 0.937500, batch_len: 80.000000
Train, loss=0.01940325: 743it [18:17,  1.28s/it]2017-06-02 17:09:27,464 root  INFO     step 743.000000 - time: 1.029482, loss: 0.123837, perplexity: 1.131831, precision: 0.859375, batch_len: 111.000000
Train, loss=0.12383705: 744it [18:18,  1.22s/it]2017-06-02 17:09:29,204 root  INFO     step 744.000000 - time: 1.735194, loss: 0.063694, perplexity: 1.065767, precision: 0.875000, batch_len: 121.000000
Train, loss=0.06369428: 745it [18:20,  1.37s/it]2017-06-02 17:09:30,414 root  INFO     step 745.000000 - time: 1.135802, loss: 0.071981, perplexity: 1.074634, precision: 0.890625, batch_len: 107.000000
Train, loss=0.07198052: 746it [18:21,  1.33s/it]2017-06-02 17:09:31,874 root  INFO     step 746.000000 - time: 1.445425, loss: 0.095457, perplexity: 1.100161, precision: 0.953125, batch_len: 116.000000
Train, loss=0.09545660: 747it [18:23,  1.37s/it]2017-06-02 17:09:33,118 root  INFO     step 747.000000 - time: 0.971409, loss: 0.100091, perplexity: 1.105271, precision: 0.781250, batch_len: 84.000000
Train, loss=0.10009075: 748it [18:24,  1.33s/it]2017-06-02 17:09:34,237 root  INFO     step 748.000000 - time: 1.052837, loss: 0.010341, perplexity: 1.010394, precision: 1.000000, batch_len: 98.000000
Train, loss=0.01034055: 749it [18:25,  1.27s/it]2017-06-02 17:09:35,466 root  INFO     step 749.000000 - time: 1.221958, loss: 0.065027, perplexity: 1.067188, precision: 0.890625, batch_len: 94.000000
Train, loss=0.06502686: 750it [18:26,  1.26s/it]2017-06-02 17:09:36,664 root  INFO     step 750.000000 - time: 1.163125, loss: 0.042374, perplexity: 1.043284, precision: 0.906250, batch_len: 87.000000
Train, loss=0.04237356: 751it [18:27,  1.24s/it]2017-06-02 17:09:37,741 root  INFO     step 751.000000 - time: 1.000630, loss: 0.049044, perplexity: 1.050266, precision: 0.875000, batch_len: 83.000000
Train, loss=0.04904370: 752it [18:29,  1.19s/it]2017-06-02 17:09:39,277 root  INFO     step 752.000000 - time: 1.297435, loss: 0.018827, perplexity: 1.019005, precision: 0.953125, batch_len: 129.000000
Train, loss=0.01882705: 753it [18:30,  1.29s/it]2017-06-02 17:09:40,678 root  INFO     step 753.000000 - time: 1.252761, loss: 0.012762, perplexity: 1.012844, precision: 0.984375, batch_len: 118.000000
Train, loss=0.01276204: 754it [18:31,  1.33s/it]2017-06-02 17:09:41,860 root  INFO     step 754.000000 - time: 1.152605, loss: 0.046495, perplexity: 1.047593, precision: 0.953125, batch_len: 82.000000
Train, loss=0.04649507: 755it [18:33,  1.28s/it]2017-06-02 17:09:43,415 root  INFO     step 755.000000 - time: 1.515917, loss: 0.025118, perplexity: 1.025437, precision: 0.937500, batch_len: 126.000000
Train, loss=0.02511845: 756it [18:34,  1.36s/it]2017-06-02 17:09:44,851 root  INFO     step 756.000000 - time: 1.327226, loss: 0.011360, perplexity: 1.011425, precision: 0.984375, batch_len: 133.000000
Train, loss=0.01136048: 757it [18:36,  1.39s/it]2017-06-02 17:09:45,977 root  INFO     step 757.000000 - time: 1.044141, loss: 0.026137, perplexity: 1.026482, precision: 0.921875, batch_len: 99.000000
Train, loss=0.02613749: 758it [18:37,  1.31s/it]2017-06-02 17:09:47,463 root  INFO     step 758.000000 - time: 1.376836, loss: 0.019421, perplexity: 1.019611, precision: 0.937500, batch_len: 136.000000
Train, loss=0.01942074: 759it [18:38,  1.36s/it]2017-06-02 17:09:49,179 root  INFO     step 759.000000 - time: 1.674337, loss: 0.011207, perplexity: 1.011270, precision: 0.968750, batch_len: 144.000000
Train, loss=0.01120655: 760it [18:40,  1.47s/it]2017-06-02 17:09:50,697 root  INFO     step 760.000000 - time: 1.366238, loss: 0.030465, perplexity: 1.030934, precision: 0.953125, batch_len: 119.000000
Train, loss=0.03046547: 761it [18:42,  1.48s/it]2017-06-02 17:09:51,841 root  INFO     step 761.000000 - time: 1.069835, loss: 0.010853, perplexity: 1.010912, precision: 1.000000, batch_len: 96.000000
Train, loss=0.01085265: 762it [18:43,  1.38s/it]2017-06-02 17:09:53,186 root  INFO     step 762.000000 - time: 1.306482, loss: 0.022356, perplexity: 1.022608, precision: 0.921875, batch_len: 137.000000
Train, loss=0.02235597: 763it [18:44,  1.37s/it]2017-06-02 17:09:54,859 root  INFO     step 763.000000 - time: 1.633898, loss: 0.008493, perplexity: 1.008529, precision: 0.968750, batch_len: 138.000000
Train, loss=0.00849294: 764it [18:46,  1.46s/it]2017-06-02 17:09:56,433 root  INFO     step 764.000000 - time: 1.555859, loss: 0.006597, perplexity: 1.006619, precision: 1.000000, batch_len: 130.000000
Train, loss=0.00659702: 765it [18:47,  1.49s/it]2017-06-02 17:09:57,863 root  INFO     step 765.000000 - time: 1.337839, loss: 0.031549, perplexity: 1.032052, precision: 0.921875, batch_len: 135.000000
Train, loss=0.03154875: 766it [18:49,  1.48s/it]2017-06-02 17:09:59,236 root  INFO     step 766.000000 - time: 1.284627, loss: 0.027979, perplexity: 1.028374, precision: 0.968750, batch_len: 122.000000
Train, loss=0.02797918: 767it [18:50,  1.44s/it]2017-06-02 17:10:00,863 root  INFO     step 767.000000 - time: 1.378747, loss: 0.012349, perplexity: 1.012426, precision: 0.984375, batch_len: 132.000000
Train, loss=0.01234917: 768it [18:52,  1.50s/it]2017-06-02 17:10:03,049 root  INFO     step 768.000000 - time: 2.170171, loss: 0.006461, perplexity: 1.006482, precision: 0.984375, batch_len: 152.000000
Train, loss=0.00646099: 769it [18:54,  1.71s/it]2017-06-02 17:10:04,014 root  INFO     step 769.000000 - time: 0.932411, loss: 0.022655, perplexity: 1.022913, precision: 0.937500, batch_len: 78.000000
Train, loss=0.02265472: 770it [18:55,  1.48s/it]2017-06-02 17:10:05,433 root  INFO     step 770.000000 - time: 1.408461, loss: 0.020530, perplexity: 1.020742, precision: 0.937500, batch_len: 141.000000
Train, loss=0.02052972: 771it [18:56,  1.46s/it]2017-06-02 17:10:06,482 root  INFO     step 771.000000 - time: 0.882225, loss: 0.003217, perplexity: 1.003222, precision: 1.000000, batch_len: 74.000000
Train, loss=0.00321726: 772it [18:57,  1.34s/it]2017-06-02 17:10:08,551 root  INFO     step 772.000000 - time: 2.059863, loss: 0.032686, perplexity: 1.033226, precision: 0.937500, batch_len: 150.000000
Train, loss=0.03268591: 773it [18:59,  1.56s/it]2017-06-02 17:10:10,074 root  INFO     step 773.000000 - time: 1.432860, loss: 0.004341, perplexity: 1.004351, precision: 1.000000, batch_len: 134.000000
Train, loss=0.00434115: 774it [19:01,  1.55s/it]2017-06-02 17:10:11,708 root  INFO     step 774.000000 - time: 1.440240, loss: 0.007770, perplexity: 1.007800, precision: 0.984375, batch_len: 139.000000
Train, loss=0.00776972: 775it [19:03,  1.57s/it]2017-06-02 17:10:12,736 root  INFO     step 775.000000 - time: 0.878006, loss: 0.034163, perplexity: 1.034753, precision: 0.968750, batch_len: 77.000000
Train, loss=0.03416292: 776it [19:04,  1.41s/it]2017-06-02 17:10:13,910 root  INFO     step 776.000000 - time: 1.114178, loss: 0.069692, perplexity: 1.072178, precision: 0.890625, batch_len: 72.000000
Train, loss=0.06969223: 777it [19:05,  1.34s/it]2017-06-02 17:10:15,113 root  INFO     step 777.000000 - time: 1.146082, loss: 0.042064, perplexity: 1.042961, precision: 0.906250, batch_len: 76.000000
Train, loss=0.04206366: 778it [19:06,  1.30s/it]2017-06-02 17:10:16,779 root  INFO     step 778.000000 - time: 1.462823, loss: 0.029515, perplexity: 1.029955, precision: 0.937500, batch_len: 142.000000
Train, loss=0.02951499: 779it [19:08,  1.41s/it]2017-06-02 17:10:18,316 root  INFO     step 779.000000 - time: 1.410035, loss: 0.017141, perplexity: 1.017288, precision: 0.953125, batch_len: 131.000000
Train, loss=0.01714054: 780it [19:09,  1.45s/it]2017-06-02 17:10:19,288 root  INFO     step 780.000000 - time: 0.940252, loss: 0.017817, perplexity: 1.017976, precision: 0.968750, batch_len: 71.000000
Train, loss=0.01781678: 781it [19:10,  1.30s/it]2017-06-02 17:10:19,423 root  INFO     Generating first batch)
2017-06-02 17:10:23,346 root  INFO     step 781.000000 - time: 1.033320, loss: 0.015331, perplexity: 1.015449, precision: 0.953125, batch_len: 96.000000
Train, loss=0.01533109: 782it [19:14,  2.13s/it]2017-06-02 17:10:24,510 root  INFO     step 782.000000 - time: 0.895813, loss: 0.023805, perplexity: 1.024091, precision: 0.906250, batch_len: 90.000000
Train, loss=0.02380548: 783it [19:15,  1.84s/it]2017-06-02 17:10:25,679 root  INFO     step 783.000000 - time: 1.153142, loss: 0.055670, perplexity: 1.057249, precision: 0.890625, batch_len: 93.000000
Train, loss=0.05567050: 784it [19:16,  1.64s/it]2017-06-02 17:10:27,080 root  INFO     step 784.000000 - time: 1.378999, loss: 0.031341, perplexity: 1.031837, precision: 0.953125, batch_len: 113.000000
Train, loss=0.03134067: 785it [19:18,  1.57s/it]2017-06-02 17:10:28,214 root  INFO     step 785.000000 - time: 1.105688, loss: 0.021838, perplexity: 1.022078, precision: 0.968750, batch_len: 100.000000
Train, loss=0.02183790: 786it [19:19,  1.44s/it]2017-06-02 17:10:29,258 root  INFO     step 786.000000 - time: 1.017169, loss: 0.036636, perplexity: 1.037315, precision: 0.968750, batch_len: 92.000000
Train, loss=0.03663569: 787it [19:20,  1.32s/it]2017-06-02 17:10:30,624 root  INFO     step 787.000000 - time: 1.312285, loss: 0.028346, perplexity: 1.028752, precision: 0.953125, batch_len: 128.000000
Train, loss=0.02834628: 788it [19:21,  1.33s/it]2017-06-02 17:10:32,260 root  INFO     step 788.000000 - time: 1.288522, loss: 0.031983, perplexity: 1.032500, precision: 0.906250, batch_len: 101.000000
Train, loss=0.03198332: 789it [19:23,  1.42s/it]2017-06-02 17:10:33,314 root  INFO     step 789.000000 - time: 0.976383, loss: 0.015140, perplexity: 1.015255, precision: 0.968750, batch_len: 102.000000
Train, loss=0.01513979: 790it [19:24,  1.31s/it]2017-06-02 17:10:34,686 root  INFO     step 790.000000 - time: 1.285701, loss: 0.016751, perplexity: 1.016892, precision: 0.953125, batch_len: 104.000000
Train, loss=0.01675085: 791it [19:25,  1.33s/it]2017-06-02 17:10:35,988 root  INFO     step 791.000000 - time: 1.251265, loss: 0.031430, perplexity: 1.031929, precision: 0.921875, batch_len: 108.000000
Train, loss=0.03142953: 792it [19:27,  1.32s/it]2017-06-02 17:10:37,203 root  INFO     step 792.000000 - time: 1.163813, loss: 0.039644, perplexity: 1.040440, precision: 0.906250, batch_len: 114.000000
Train, loss=0.03964399: 793it [19:28,  1.29s/it]2017-06-02 17:10:38,587 root  INFO     step 793.000000 - time: 1.359108, loss: 0.010136, perplexity: 1.010187, precision: 0.984375, batch_len: 120.000000
Train, loss=0.01013594: 794it [19:29,  1.32s/it]2017-06-02 17:10:39,577 root  INFO     step 794.000000 - time: 0.938816, loss: 0.018724, perplexity: 1.018901, precision: 0.937500, batch_len: 105.000000
Train, loss=0.01872437: 795it [19:30,  1.22s/it]2017-06-02 17:10:41,072 root  INFO     step 795.000000 - time: 1.489022, loss: 0.006313, perplexity: 1.006333, precision: 0.968750, batch_len: 117.000000
Train, loss=0.00631299: 796it [19:32,  1.30s/it]2017-06-02 17:10:42,797 root  INFO     step 796.000000 - time: 1.545174, loss: 0.006086, perplexity: 1.006104, precision: 1.000000, batch_len: 124.000000
Train, loss=0.00608560: 797it [19:34,  1.43s/it]2017-06-02 17:10:43,877 root  INFO     step 797.000000 - time: 1.021183, loss: 0.024410, perplexity: 1.024711, precision: 0.968750, batch_len: 88.000000
Train, loss=0.02441033: 798it [19:35,  1.32s/it]2017-06-02 17:10:45,026 root  INFO     step 798.000000 - time: 1.137137, loss: 0.013685, perplexity: 1.013779, precision: 0.953125, batch_len: 110.000000
Train, loss=0.01368496: 799it [19:36,  1.27s/it]2017-06-02 17:10:46,037 root  INFO     step 799.000000 - time: 1.000768, loss: 0.011421, perplexity: 1.011487, precision: 0.953125, batch_len: 106.000000
Train, loss=0.01142148: 800it [19:37,  1.19s/it]2017-06-02 17:10:47,315 root  INFO     step 800.000000 - time: 1.204726, loss: 0.016739, perplexity: 1.016880, precision: 0.968750, batch_len: 111.000000
Train, loss=0.01673943: 801it [19:38,  1.22s/it]2017-06-02 17:10:48,999 root  INFO     step 801.000000 - time: 1.503720, loss: 0.012075, perplexity: 1.012148, precision: 0.984375, batch_len: 121.000000
Train, loss=0.01207485: 802it [19:40,  1.36s/it]2017-06-02 17:10:50,137 root  INFO     step 802.000000 - time: 1.024299, loss: 0.004616, perplexity: 1.004627, precision: 1.000000, batch_len: 103.000000
Train, loss=0.00461587: 803it [19:41,  1.29s/it]2017-06-02 17:10:51,195 root  INFO     step 803.000000 - time: 1.050002, loss: 0.014602, perplexity: 1.014709, precision: 0.953125, batch_len: 89.000000
Train, loss=0.01460202: 804it [19:42,  1.22s/it]2017-06-02 17:10:52,592 root  INFO     step 804.000000 - time: 1.312073, loss: 0.015316, perplexity: 1.015434, precision: 0.968750, batch_len: 112.000000
Train, loss=0.01531569: 805it [19:43,  1.27s/it]2017-06-02 17:10:53,654 root  INFO     step 805.000000 - time: 0.993087, loss: 0.023338, perplexity: 1.023612, precision: 0.953125, batch_len: 86.000000
Train, loss=0.02333769: 806it [19:44,  1.21s/it]2017-06-02 17:10:55,125 root  INFO     step 806.000000 - time: 1.370919, loss: 0.010886, perplexity: 1.010946, precision: 0.968750, batch_len: 115.000000
Train, loss=0.01088611: 807it [19:46,  1.29s/it]2017-06-02 17:10:56,209 root  INFO     step 807.000000 - time: 1.055709, loss: 0.050405, perplexity: 1.051697, precision: 0.937500, batch_len: 97.000000
Train, loss=0.05040549: 808it [19:47,  1.23s/it]2017-06-02 17:10:57,258 root  INFO     step 808.000000 - time: 1.000748, loss: 0.180625, perplexity: 1.197966, precision: 0.828125, batch_len: 85.000000
Train, loss=0.18062523: 809it [19:48,  1.17s/it]2017-06-02 17:10:58,245 root  INFO     step 809.000000 - time: 0.945188, loss: 0.077165, perplexity: 1.080220, precision: 0.875000, batch_len: 81.000000
Train, loss=0.07716502: 810it [19:49,  1.12s/it]2017-06-02 17:10:59,256 root  INFO     step 810.000000 - time: 0.972218, loss: 0.021692, perplexity: 1.021929, precision: 0.937500, batch_len: 87.000000
Train, loss=0.02169198: 811it [19:50,  1.09s/it]2017-06-02 17:11:00,425 root  INFO     step 811.000000 - time: 1.026826, loss: 0.029817, perplexity: 1.030266, precision: 0.921875, batch_len: 91.000000
Train, loss=0.02981748: 812it [19:51,  1.11s/it]2017-06-02 17:11:01,897 root  INFO     step 812.000000 - time: 1.467352, loss: 0.025386, perplexity: 1.025711, precision: 0.937500, batch_len: 109.000000
Train, loss=0.02538641: 813it [19:53,  1.22s/it]2017-06-02 17:11:03,287 root  INFO     step 813.000000 - time: 1.345902, loss: 0.013290, perplexity: 1.013379, precision: 0.968750, batch_len: 116.000000
Train, loss=0.01329030: 814it [19:54,  1.27s/it]2017-06-02 17:11:04,770 root  INFO     step 814.000000 - time: 1.392905, loss: 0.017437, perplexity: 1.017589, precision: 0.968750, batch_len: 125.000000
Train, loss=0.01743657: 815it [19:56,  1.33s/it]2017-06-02 17:11:06,101 root  INFO     step 815.000000 - time: 1.268297, loss: 0.012374, perplexity: 1.012451, precision: 0.984375, batch_len: 130.000000
Train, loss=0.01237366: 816it [19:57,  1.33s/it]2017-06-02 17:11:07,173 root  INFO     step 816.000000 - time: 1.045781, loss: 0.010510, perplexity: 1.010566, precision: 0.984375, batch_len: 80.000000
Train, loss=0.01051025: 817it [19:58,  1.25s/it]2017-06-02 17:11:08,849 root  INFO     step 817.000000 - time: 1.644876, loss: 0.007275, perplexity: 1.007302, precision: 1.000000, batch_len: 129.000000
Train, loss=0.00727539: 818it [20:00,  1.38s/it]2017-06-02 17:11:10,247 root  INFO     step 818.000000 - time: 1.374592, loss: 0.009916, perplexity: 1.009965, precision: 0.984375, batch_len: 126.000000
Train, loss=0.00991597: 819it [20:01,  1.39s/it]2017-06-02 17:11:11,361 root  INFO     step 819.000000 - time: 1.085005, loss: 0.034862, perplexity: 1.035477, precision: 0.968750, batch_len: 94.000000
Train, loss=0.03486200: 820it [20:02,  1.30s/it]2017-06-02 17:11:12,376 root  INFO     step 820.000000 - time: 0.891399, loss: 0.022367, perplexity: 1.022619, precision: 0.953125, batch_len: 79.000000
Train, loss=0.02236717: 821it [20:03,  1.22s/it]2017-06-02 17:11:13,581 root  INFO     step 821.000000 - time: 1.194955, loss: 0.004867, perplexity: 1.004879, precision: 0.984375, batch_len: 123.000000
Train, loss=0.00486730: 822it [20:04,  1.21s/it]2017-06-02 17:11:14,941 root  INFO     step 822.000000 - time: 1.264844, loss: 0.037708, perplexity: 1.038428, precision: 0.953125, batch_len: 107.000000
Train, loss=0.03770772: 823it [20:06,  1.26s/it]2017-06-02 17:11:16,177 root  INFO     step 823.000000 - time: 1.036815, loss: 0.029353, perplexity: 1.029788, precision: 0.953125, batch_len: 83.000000
Train, loss=0.02935307: 824it [20:07,  1.25s/it]2017-06-02 17:11:17,478 root  INFO     step 824.000000 - time: 1.282155, loss: 0.009905, perplexity: 1.009955, precision: 0.953125, batch_len: 133.000000
Train, loss=0.00990540: 825it [20:08,  1.27s/it]2017-06-02 17:11:18,648 root  INFO     step 825.000000 - time: 1.054484, loss: 0.010033, perplexity: 1.010084, precision: 0.984375, batch_len: 99.000000
Train, loss=0.01003308: 826it [20:09,  1.24s/it]2017-06-02 17:11:19,615 root  INFO     step 826.000000 - time: 0.879387, loss: 0.014048, perplexity: 1.014147, precision: 0.968750, batch_len: 82.000000
Train, loss=0.01404777: 827it [20:10,  1.16s/it]train_demo.sh: 行 19: 80602 已终止               $py src/launcher.py --phase=train --data-path=sample/sample.txt --data-base-dir=sample --log-path=log_01_16.log --attn-num-hidden 256 --batch-size 64 --model-dir=model_01_16 --initial-learning-rate=1.0 --load-model --num-epoch=30 --gpu-id=0 --use-gru --steps-per-checkpoint=2000 --target-embedding-size=10
